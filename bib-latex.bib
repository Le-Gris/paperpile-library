@ARTICLE{Knudsen2004-ny,
  title    = "Sensitive periods in the development of the brain and behavior",
  author   = "Knudsen, Eric I",
  journal  = "Journal of cognitive neuroscience",
  volume   =  16,
  number   =  8,
  pages    = "1412--1425",
  abstract = "Experience exerts a profound influence on the brain and,
              therefore, on behavior. When the effect of experience on the brain
              is particularly strong during a limited period in development,
              this period is referred to as a sensitive period. Such periods
              allow experience to instruct neural circuits to process or
              represent information in a way that is adaptive for the
              individual. When experience provides information that is essential
              for normal development and alters performance permanently, such
              sensitive periods are referred to as critical periods. Although
              sensitive periods are reflected in behavior, they are actually a
              property of neural circuits. Mechanisms of plasticity at the
              circuit level are discussed that have been shown to operate during
              sensitive periods. A hypothesis is proposed that experience during
              a sensitive period modifies the architecture of a circuit in
              fundamental ways, causing certain patterns of connectivity to
              become highly stable and, therefore, energetically preferred.
              Plasticity that occurs beyond the end of a sensitive period, which
              is substantial in many circuits, alters connectivity patterns
              within the architectural constraints established during the
              sensitive period. Preferences in a circuit that result from
              experience during sensitive periods are illustrated graphically as
              changes in a ''stability landscape,'' a metaphor that represents
              the relative contributions of genetic and experiential influences
              in shaping the information processing capabilities of a neural
              circuit. By understanding sensitive periods at the circuit level,
              as well as understanding the relationship between circuit
              properties and behavior, we gain a deeper insight into the
              critical role that experience plays in shaping the development of
              the brain and behavior.",
  month    =  oct,
  year     =  2004,
  doi      = "10.1162/0898929042304796",
  pmid     =  15509387,
  issn     = "0898-929X",
  language = "en"
}

@ARTICLE{Tang2022-vx,
  title         = "From Perception to Programs: Regularize, Overparameterize,
                   and Amortize",
  author        = "Tang, Hao and Ellis, Kevin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Toward combining inductive reasoning with perception
                   abilities, we develop techniques for neurosymbolic program
                   synthesis where perceptual input is first parsed by neural
                   nets into a low-dimensional interpretable representation,
                   which is then processed by a synthesized program. We explore
                   several techniques for relaxing the problem and jointly
                   learning all modules end-to-end with gradient descent:
                   multitask learning; amortized inference;
                   overparameterization; and a differentiable strategy for
                   penalizing lengthy programs. Collectedly this toolbox
                   improves the stability of gradient-guided program search, and
                   suggests ways of learning both how to perceive input as
                   discrete abstractions, and how to symbolically process those
                   abstractions as programs.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2206.05922"
}

@ARTICLE{Braams2015-vm,
  title     = "Longitudinal Changes in Adolescent Risk-Taking: A Comprehensive
               Study of Neural Responses to Rewards, Pubertal Development, and
               Risk-Taking Behavior",
  author    = "Braams, Barbara R and van Duijvenvoorde, Anna C K and Peper,
               Jiska S and Crone, Eveline A",
  journal   = "The Journal of neuroscience: the official journal of the Society
               for Neuroscience",
  publisher = "Society for Neuroscience",
  volume    =  35,
  number    =  18,
  pages     = "7226--7238",
  abstract  = "Prior studies have highlighted adolescence as a period of
               increased risk-taking, which is postulated to result from an
               overactive reward system in the brain. Longitudinal studies are
               pivotal for testing these brain-behavior relations because
               individual slopes are more sensitive for detecting change. The
               aim of the current study was twofold: (1) to test patterns of
               age-related change (i.e., linear, quadratic, and cubic) in
               activity in the nucleus accumbens, a key reward region in the
               brain, in relation to change in puberty (self-report and
               testosterone levels), laboratory risk-taking and self-reported
               risk-taking tendency; and (2) to test whether individual
               differences in pubertal development and risk-taking behavior were
               contributors to longitudinal change in nucleus accumbens
               activity. We included 299 human participants at the first time
               point and 254 participants at the second time point, ranging
               between ages 8–27 years, time points were separated by a 2 year
               interval. Neural responses to rewards, pubertal development
               (self-report and testosterone levels), laboratory risk-taking
               (balloon analog risk task; BART), and self-reported risk-taking
               tendency (Behavior Inhibition System/Behavior Activation System
               questionnaire) were collected at both time points. The
               longitudinal analyses confirmed the quadratic age pattern for
               nucleus accumbens activity to rewards (peaking in adolescence),
               and the same quadratic pattern was found for laboratory
               risk-taking (BART). Nucleus accumbens activity change was further
               related to change in testosterone and self-reported
               reward-sensitivity (BAS Drive). Thus, this longitudinal analysis
               provides new insight in risk-taking and reward sensitivity in
               adolescence: (1) confirming an adolescent peak in nucleus
               accumbens activity, and (2) underlining a critical role for
               pubertal hormones and individual differences in risk-taking
               tendency.",
  month     =  may,
  year      =  2015,
  doi       = "10.1523/JNEUROSCI.4764-14.2015",
  pmid      =  25948271,
  issn      = "0270-6474,1529-2401",
  language  = "en"
}

@ARTICLE{Reynolds2009-de,
  title    = "The normalization model of attention",
  author   = "Reynolds, John H and Heeger, David J",
  journal  = "Neuron",
  volume   =  61,
  number   =  2,
  pages    = "168--185",
  abstract = "Attention has been found to have a wide variety of effects on the
              responses of neurons in visual cortex. We describe a model of
              attention that exhibits each of these different forms of
              attentional modulation, depending on the stimulus conditions and
              the spread (or selectivity) of the attention field in the model.
              The model helps reconcile proposals that have been taken to
              represent alternative theories of attention. We argue that the
              variety and complexity of the results reported in the literature
              emerge from the variety of empirical protocols that were used,
              such that the results observed in any one experiment depended on
              the stimulus conditions and the subject's attentional strategy, a
              notion that we define precisely in terms of the attention field in
              the model, but that has not typically been completely under
              experimental control.",
  month    =  jan,
  year     =  2009,
  doi      = "10.1016/j.neuron.2009.01.002",
  pmc      = "PMC2752446",
  pmid     =  19186161,
  issn     = "0896-6273,1097-4199",
  language = "en"
}

@ARTICLE{Herrmann2012-tk,
  title    = "Feature-based attention enhances performance by increasing
              response gain",
  author   = "Herrmann, Katrin and Heeger, David J and Carrasco, Marisa",
  journal  = "Vision research",
  volume   =  74,
  pages    = "10--20",
  abstract = "Covert spatial attention can increase contrast sensitivity either
              by changes in contrast gain or by changes in response gain,
              depending on the size of the attention field and the size of the
              stimulus (Herrmann et al., 2010), as predicted by the
              normalization model of attention (Reynolds \& Heeger, 2009). For
              feature-based attention, unlike spatial attention, the model
              predicts only changes in response gain, regardless of whether the
              featural extent of the attention field is small or large. To test
              this prediction, we measured the contrast dependence of
              feature-based attention. Observers performed an
              orientation-discrimination task on a spatial array of grating
              patches. The spatial locations of the gratings were varied
              randomly so that observers could not attend to specific locations.
              Feature-based attention was manipulated with a 75\% valid and 25\%
              invalid pre-cue, and the featural extent of the attention field
              was manipulated by introducing uncertainty about the upcoming
              grating orientation. Performance accuracy was better for valid
              than for invalid pre-cues, consistent with a change in response
              gain, when the featural extent of the attention field was small
              (low uncertainty) or when it was large (high uncertainty) relative
              to the featural extent of the stimulus. These results for
              feature-based attention clearly differ from results of analogous
              experiments with spatial attention, yet both support key
              predictions of the normalization model of attention.",
  month    =  dec,
  year     =  2012,
  doi      = "10.1016/j.visres.2012.04.016",
  pmc      = "PMC3427403",
  pmid     =  22580017,
  issn     = "0042-6989,1878-5646",
  language = "en"
}

@ARTICLE{Pirrone2023-qy,
  title    = "Toward an Atlas of Canonical Cognitive Mechanisms",
  author   = "Pirrone, Angelo and Tsetsos, Konstantinos",
  journal  = "Cognitive science",
  volume   =  47,
  number   =  2,
  pages    = "e13243",
  abstract = "A central goal in Cognitive Science is understanding the
              mechanisms that underlie cognition. Here, we contend that
              Cognitive Science, despite intense multidisciplinary efforts, has
              furnished surprisingly few mechanistic insights. We attribute this
              slow mechanistic progress to the fact that cognitive scientists
              insist on performing underdetermined exercises, deriving
              overparametrized mechanistic theories of complex behaviors and
              seeking validation of these theories to the elusive notions of
              optimality and biological plausibility. We propose that
              mechanistic progress in Cognitive Science will accelerate once
              cognitive scientists start focusing on simpler explananda that
              will enable them to chart an atlas of elementary cognitive
              operations. Looking forward, the next challenge for Cognitive
              Science will be to understand how these elementary cognitive
              processes are pieced together to explain complex behavior.",
  month    =  feb,
  year     =  2023,
  keywords = "Biological plausibility; Cognitive science; Inference; Mechanisms;
              Optimality",
  doi      = "10.1111/cogs.13243",
  pmid     =  36744746,
  issn     = "0364-0213,1551-6709",
  language = "en"
}

@ARTICLE{Dubova2022-ci,
  title    = "Against theory-motivated experimentation in science",
  author   = "Dubova, Marina and Moskvichev, Arseny and Zollman, Kevin",
  abstract = "Scientists must choose which among many experiments to perform. We
              study the epistemic success of experimental choice strategies
              proposed by philosophers of science or executed by scientists
              themselves. We develop a multi-agent model of the scientific
              process that jointly formalizes its core aspects: active
              experimentation, theorizing, and social learning. We find that
              agents who choose new experiments at random develop the most
              accurate theories of the world. The agents aiming to confirm,
              falsify theories, or resolve theoretical disagreements end up with
              an illusion of epistemic success: they develop promising accounts
              for the data they collected, while completely misrepresenting the
              ground truth that they intended to learn about. Agents
              experimenting in theory-motivated ways acquire less diverse or
              less representative samples from the ground truth that also turn
              out to be easier to account for. Random data collection, on the
              other hand, combines virtues of diverse and representative
              sampling from a target scientific domain which enables cumulative
              development of the successful theoretical accounts of it. We
              suggest that randomization, already a gold standard within
              experiments, is also beneficial at the level of experiments
              themselves.",
  month    =  jun,
  year     =  2022,
  keywords = "data collection; experimentation; formal epistemology;
              methodology; social learning",
  doi      = "10.31222/osf.io/ysv2u"
}

@ARTICLE{Reddy2018-lj,
  title         = "Where Do You Think You're Going?: Inferring Beliefs about
                   Dynamics from Behavior",
  author        = "Reddy, Siddharth and Dragan, Anca D and Levine, Sergey",
  journal       = "arXiv [cs.LG]",
  abstract      = "Inferring intent from observed behavior has been studied
                   extensively within the frameworks of Bayesian inverse
                   planning and inverse reinforcement learning. These methods
                   infer a goal or reward function that best explains the
                   actions of the observed agent, typically a human
                   demonstrator. Another agent can use this inferred intent to
                   predict, imitate, or assist the human user. However, a
                   central assumption in inverse reinforcement learning is that
                   the demonstrator is close to optimal. While models of
                   suboptimal behavior exist, they typically assume that
                   suboptimal actions are the result of some type of random
                   noise or a known cognitive bias, like temporal inconsistency.
                   In this paper, we take an alternative approach, and model
                   suboptimal behavior as the result of internal model
                   misspecification: the reason that user actions might deviate
                   from near-optimal actions is that the user has an incorrect
                   set of beliefs about the rules -- the dynamics -- governing
                   how actions affect the environment. Our insight is that while
                   demonstrated actions may be suboptimal in the real world,
                   they may actually be near-optimal with respect to the user's
                   internal model of the dynamics. By estimating these internal
                   beliefs from observed behavior, we arrive at a new method for
                   inferring intent. We demonstrate in simulation and in a user
                   study with 12 participants that this approach enables us to
                   more accurately model human intent, and can be used in a
                   variety of applications, including offering assistance in a
                   shared autonomy framework and inferring human preferences.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1805.08010"
}

@ARTICLE{Johnson-Frey2004-qv,
  title     = "The neural bases of complex tool use in humans",
  author    = "Johnson-Frey, Scott H",
  journal   = "Trends in cognitive sciences",
  publisher = "Elsevier BV",
  volume    =  8,
  number    =  2,
  pages     = "71--78",
  abstract  = "The behaviors involved in complex human tool use cut across
               boundaries traditionally drawn between social, cognitive,
               perceptual and motor processes. Longstanding neuropsychological
               evidence suggests a distinction between brain systems responsible
               for representing: (1) semantic knowledge about familiar tools and
               their uses, and (2) the acquired skills necessary for performing
               these actions. Contemporary findings in functional neuroimaging
               support and refine this distinction by revealing the distributed
               neural systems that support these processes and the conditions
               under which they interact. Together, these findings indicate that
               behaviors associated with complex tool use arise from
               functionally specialized networks involving temporal, parietal
               and frontal areas within the left cerebral hemisphere.",
  month     =  feb,
  year      =  2004,
  doi       = "10.1016/j.tics.2003.12.002",
  pmid      =  15588811,
  issn      = "1364-6613,1879-307X",
  language  = "en"
}

@INPROCEEDINGS{Johnson2021-cn,
  title     = "Fast and Flexible: Human program induction in abstract reasoning
               tasks",
  author    = "Johnson, Aysja and Vong, Wai Keen and Lake, Brenden and Gureckis,
               Todd M",
  booktitle = "Proceedings of the Annual Meeting of the Cognitive Science
               Society",
  volume    =  43,
  year      =  2021
}

@ARTICLE{Johnson2021-jk,
  title    = "Fast and flexible: Human program induction in abstract reasoning
              tasks",
  author   = "Johnson, Aysja and Vong, Wai Keen and Lake, B and Gureckis, T",
  journal  = "Annual Meeting of the Cognitive Science Society",
  volume   = "abs/2103.05823",
  number   =  43,
  abstract = "The Abstraction and Reasoning Corpus (ARC) is a challenging
              program induction dataset that was recently proposed by Chollet
              (2019). Here, we report the first set of results collected from a
              behavioral study of humans solving a subset of tasks from ARC (40
              out of 1000). Although this subset of tasks contains considerable
              variation, our results showed that humans were able to infer the
              underlying program and generate the correct test output for a
              novel test input example, with an average of 80\% of tasks solved
              per participant, and with 65\% of tasks being solved by more than
              80\% of participants. Additionally, we find interesting patterns
              of behavioral consistency and variability within the action
              sequences during the generation process, the natural language
              descriptions to describe the transformations for each task, and
              the errors people made. Our findings suggest that people can
              quickly and reliably determine the relevant features and
              properties of a task to compose a correct solution. Future
              modeling work could incorporate these findings, potentially by
              connecting the natural language descriptions we collected here to
              the underlying semantics of ARC.",
  month    =  mar,
  year     =  2021,
  eprint   = "2103.05823"
}

@ARTICLE{Boesch1990-rx,
  title     = "Tool use and tool making in wild chimpanzees",
  author    = "Boesch, C and Boesch, H",
  journal   = "Folia primatologica; international journal of primatology",
  publisher = "Brill",
  volume    =  54,
  number    = "1-2",
  pages     = "86--99",
  abstract  = "Reported incidences of tool use and tool making for three wild
               chimpanzee populations increase from Mahale (12 and 3 types of
               use and making, respectively), Gombe (16 and 3) to Taï (19 and
               6). Sticks are commonly used and prepared at all three sites.
               However, Taï chimpanzees seem to perform more modifications on
               the material before using it. They are also the only chimpanzees
               seen to pound objects with tools and to combine two different
               tool uses to get access to one food item. Tool making is the rule
               for abundant material (grass, twigs), but appears to be rarer for
               scarce, hard material (clubs, stones). Factors involved in the
               acquisition and the benefit of tool use are discussed along with
               factors affecting the frequency and complexity of tool making in
               chimpanzees.",
  month     =  feb,
  year      =  1990,
  keywords  = "Zoology; Biology; Journal",
  doi       = "10.1159/000156428",
  pmid      =  2157651,
  issn      = "1421-9980,0015-5713",
  language  = "en"
}

@ARTICLE{Sutton1999-uw,
  title     = "Between {MDPs} and semi-{MDPs}: A framework for temporal
               abstraction in reinforcement learning",
  author    = "Sutton, Richard S and Precup, Doina and Singh, Satinder",
  journal   = "Artificial intelligence",
  publisher = "Elsevier BV",
  volume    =  112,
  number    = "1-2",
  pages     = "181--211",
  abstract  = "Learning, planning, and representing knowledge at multiple levels
               of temporal abstraction are key, longstanding challenges for AI.
               In this paper we consider how these challenges can be addressed
               within the mathematical framework of reinforcement learning and
               Markov decision processes (MDPs). We extend the usual notion of
               action in this framework to include options—closed-loop policies
               for taking action over a period of time. Examples of options
               include picking up an object, going to lunch, and traveling to a
               distant city, as well as primitive actions such as muscle
               twitches and joint torques. Overall, we show that options enable
               temporally abstract knowledge and action to be included in the
               reinforcement learning framework in a natural and general way. In
               particular, we show that options may be used interchangeably with
               primitive actions in planning methods such as dynamic programming
               and in learning methods such as Q-learning. Formally, a set of
               options defined over an MDP constitutes a semi-Markov decision
               process (SMDP), and the theory of SMDPs provides the foundation
               for the theory of options. However, the most interesting issues
               concern the interplay between the underlying MDP and the SMDP and
               are thus beyond SMDP theory. We present results for three such
               cases: (1) we show that the results of planning with options can
               be used during execution to interrupt options and thereby perform
               even better than planned, (2) we introduce new intra-option
               methods that are able to learn about an option from fragments of
               its execution, and (3) we propose a notion of subgoal that can be
               used to improve the options themselves. All of these results have
               precursors in the existing literature; the contribution of this
               paper is to establish them in a simpler and more general setting
               with fewer changes to the existing reinforcement learning
               framework. In particular, we show that these results can be
               obtained without committing to (or ruling out) any particular
               approach to state abstraction, hierarchy, function approximation,
               or the macro-utility problem.",
  month     =  aug,
  year      =  1999,
  doi       = "10.1016/s0004-3702(99)00052-1",
  issn      = "0004-3702,1872-7921",
  language  = "en"
}

@ARTICLE{Oakley1944-fp,
  title     = "Man the tool-maker",
  author    = "Oakley, K P",
  journal   = "Proceedings of the Geologists' Association. Geologists'
               Association",
  publisher = "Elsevier BV",
  volume    =  55,
  number    =  2,
  pages     = "115--118",
  month     =  jan,
  year      =  1944,
  doi       = "10.1016/s0016-7878(44)80012-8",
  issn      = "0016-7878",
  language  = "en"
}

@ARTICLE{Hunt1996-sw,
  title     = "Manufacture and use of hook-tools by New Caledonian crows",
  author    = "Hunt, Gavin R",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  379,
  number    =  6562,
  pages     = "249--251",
  abstract  = "TOOL behaviour in wild birds has been described as mostly
               stereotyped1,2, and tool manufacture involves little modification
               of material3–5. Here I report in New Caledonian crows Corvus
               moneduloides the manufacture and use of two different types of
               hook tool to aid prey capture: hooked-twig and stepped-cut barbed
               pandanus leaf. Crow tool manufacture had three features new to
               tool use in free-living nonhumans: a high degree of
               standardization, distinctly discrete tool types with definite
               imposition of form in tool shaping, and the use of hooks. These
               features only first appeared in the stone6 and bone7 tool-using
               cultures of early humans after the Lower Palaeolithic6,7, which
               indicates that crows have achieved a considerable technical
               capability in their tool manufacture and use.",
  month     =  jan,
  year      =  1996,
  doi       = "10.1038/379249a0",
  issn      = "0028-0836,1476-4687",
  language  = "en"
}

@ARTICLE{Grant2003-tf,
  title    = "Eye movements and problem solving: Guiding attention guides
              thought",
  author   = "Grant, Elizabeth R and Spivey, Michael J",
  journal  = "Psychological science",
  volume   =  14,
  number   =  5,
  pages    = "462--466",
  abstract = "Overt visual attention during diagram-based problem solving, as
              measured by eye movements, has been used in numerous studies to
              reveal critical aspects of the problem-solving process that
              traditional measures like solution time and accuracy cannot
              address. In Experiment 1, we used this methodology to show that
              particular fixation patterns correlate with success in solving the
              tumor-and-lasers radiation problem. Given this correlation between
              attention to a particular diagram feature and problem-solving
              insight, we investigated participants' cognitive sensitivity to
              perceptual changes in that diagram feature. In Experiment 2, we
              found that perceptually highlighting the critical diagram
              component, identified in Experiment 1, significantly increased the
              frequency of correct solutions. Taking a situated perspective on
              cognition, we suggest that environmentally controlled perceptual
              properties can guide attention and eye movements in ways that
              assist in developing problem-solving insights that dramatically
              improve reasoning. (PsycINFO Database Record (c) 2016 APA, all
              rights reserved)",
  month    =  sep,
  year     =  2003,
  doi      = "10.1111/1467-9280.02454",
  issn     = "0956-7976,1467-9280"
}

@ARTICLE{Van_Opheusden2023-ol,
  title    = "Expertise increases planning depth in human gameplay",
  author   = "van Opheusden, Bas and Kuperwajs, Ionatan and Galbiati, Gianni and
              Bnaya, Zahy and Li, Yunqi and Ma, Wei Ji",
  journal  = "Nature",
  volume   =  618,
  number   =  7967,
  pages    = "1000--1005",
  abstract = "A hallmark of human intelligence is the ability to plan multiple
              steps into the future1,2. Despite decades of research3-5, it is
              still debated whether skilled decision-makers plan more steps
              ahead than novices6-8. Traditionally, the study of expertise in
              planning has used board games such as chess, but the complexity of
              these games poses a barrier to quantitative estimates of planning
              depth. Conversely, common planning tasks in cognitive science
              often have a lower complexity9,10 and impose a ceiling for the
              depth to which any player can plan. Here we investigate expertise
              in a complex board game that offers ample opportunity for skilled
              players to plan deeply. We use model fitting methods to show that
              human behaviour can be captured using a computational cognitive
              model based on heuristic search. To validate this model, we
              predict human choices, response times and eye movements. We also
              perform a Turing test and a reconstruction experiment. Using the
              model, we find robust evidence for increased planning depth with
              expertise in both laboratory and large-scale mobile data. Experts
              memorize and reconstruct board features more accurately. Using
              complex tasks combined with precise behavioural modelling might
              expand our understanding of human planning and help to bridge the
              gap with progress in artificial intelligence.",
  month    =  jun,
  year     =  2023,
  doi      = "10.1038/s41586-023-06124-2",
  pmc      =  3077926,
  pmid     =  37258667,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@INCOLLECTION{Fragaszy2018-ux,
  title     = "Tooling",
  author    = "Fragaszy, Dorothy M and Mangalam, Madhur",
  booktitle = "Advances in the Study of Behavior",
  publisher = "Elsevier",
  volume    =  50,
  pages     = "177--241",
  abstract  = "Animals (including humans) use objects to solve problems or
               accomplish goals in diverse ways. Sometimes while doing so they
               achieve a mechanical effec…",
  series    = "Advances in the study of behavior",
  month     =  jan,
  year      =  2018,
  doi       = "10.1016/bs.asb.2018.01.001",
  isbn      =  9780128150849,
  issn      = "0065-3454,2162-8823",
  language  = "en"
}

@ARTICLE{Hasani2020-ve,
  title         = "Liquid Time-constant Networks",
  author        = "Hasani, Ramin and Lechner, Mathias and Amini, Alexander and
                   Rus, Daniela and Grosu, Radu",
  journal       = "arXiv [cs.LG]",
  abstract      = "We introduce a new class of time-continuous recurrent neural
                   network models. Instead of declaring a learning system's
                   dynamics by implicit nonlinearities, we construct networks of
                   linear first-order dynamical systems modulated via nonlinear
                   interlinked gates. The resulting models represent dynamical
                   systems with varying (i.e., liquid) time-constants coupled to
                   their hidden state, with outputs being computed by numerical
                   differential equation solvers. These neural networks exhibit
                   stable and bounded behavior, yield superior expressivity
                   within the family of neural ordinary differential equations,
                   and give rise to improved performance on time-series
                   prediction tasks. To demonstrate these properties, we first
                   take a theoretical approach to find bounds over their
                   dynamics and compute their expressive power by the trajectory
                   length measure in latent trajectory space. We then conduct a
                   series of time-series prediction experiments to manifest the
                   approximation capability of Liquid Time-Constant Networks
                   (LTCs) compared to classical and modern RNNs. Code and data
                   are available at
                   https://github.com/raminmh/liquid\_time\_constant\_networks",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.04439"
}

@ARTICLE{Lechner2020-fw,
  title     = "Neural circuit policies enabling auditable autonomy",
  author    = "Lechner, Mathias and Hasani, Ramin and Amini, Alexander and
               Henzinger, Thomas A and Rus, Daniela and Grosu, Radu",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  2,
  number    =  10,
  pages     = "642--652",
  abstract  = "A central goal of artificial intelligence in high-stakes
               decision-making applications is to design a single algorithm that
               simultaneously expresses generalizability by learning coherent
               representations of their world and interpretable explanations of
               its dynamics. Here, we combine brain-inspired neural computation
               principles and scalable deep learning architectures to design
               compact neural controllers for task-specific compartments of a
               full-stack autonomous vehicle control system. We discover that a
               single algorithm with 19 control neurons, connecting 32
               encapsulated input features to outputs by 253 synapses, learns to
               map high-dimensional inputs into steering commands. This system
               shows superior generalizability, interpretability and robustness
               compared with orders-of-magnitude larger black-box learning
               systems. The obtained neural agents enable high-fidelity autonomy
               for task-specific parts of a complex autonomous system. Inspired
               by the brain of the roundworm Caenorhabditis elegans, the authors
               design a highly compact neural network controller directly from
               raw input pixels. Compared with larger networks, this compact
               controller demonstrates improved generalization, robustness and
               interpretability on a lane-keeping task.",
  month     =  oct,
  year      =  2020,
  doi       = "10.1038/s42256-020-00237-3",
  issn      = "2522-5839,2522-5839",
  language  = "en"
}

@ARTICLE{Ho_undated-eh,
  title       = "Rational simplification in human planning",
  author      = "Ho, Mark K and Cohen, Jonathan D and Griffiths, Thomas L",
  institution = "Princeton University"
}

@ARTICLE{Chevalier2022-ot,
  title     = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part {II}: Editorial note",
  author    = "Chevalier, Nicolas and Lipina, Sebastián and Scerif, Gaia and
               Segretin, M Soledad",
  journal   = "Developmental science",
  publisher = "research.ed.ac.uk",
  volume    =  25,
  number    =  6,
  pages     = "e13326",
  abstract  = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part II: Editorial note —
               University of Edinburgh Research Explorer Skip to main navigation
               Skip to …",
  month     =  nov,
  year      =  2022,
  doi       = "10.1111/desc.13326",
  pmid      =  36112772,
  issn      = "1363-755X,1467-7687",
  language  = "en"
}

@ARTICLE{Douglas2023-nd,
  title    = "Data quality in online human-subjects research: Comparisons
              between {MTurk}, Prolific, {CloudResearch}, Qualtrics, and {SONA}",
  author   = "Douglas, Benjamin D and Ewell, Patrick J and Brauer, Markus",
  journal  = "PloS one",
  volume   =  18,
  number   =  3,
  pages    = "e0279720",
  abstract = "With the proliferation of online data collection in human-subjects
              research, concerns have been raised over the presence of
              inattentive survey participants and non-human respondents (bots).
              We compared the quality of the data collected through five
              commonly used platforms. Data quality was indicated by the
              percentage of participants who meaningfully respond to the
              researcher's question (high quality) versus those who only
              contribute noise (low quality). We found that compared to MTurk,
              Qualtrics, or an undergraduate student sample (i.e., SONA),
              participants on Prolific and CloudResearch were more likely to
              pass various attention checks, provide meaningful answers, follow
              instructions, remember previously presented information, have a
              unique IP address and geolocation, and work slowly enough to be
              able to read all the items. We divided the samples into high- and
              low-quality respondents and computed the cost we paid per
              high-quality respondent. Prolific ($1.90) and CloudResearch
              ($2.00) were cheaper than MTurk ($4.36) and Qualtrics ($8.17).
              SONA cost \$0.00, yet took the longest to collect the data.",
  month    =  mar,
  year     =  2023,
  doi      = "10.1371/journal.pone.0279720",
  pmc      = "PMC10013894",
  pmid     =  36917576,
  issn     = "1932-6203",
  language = "en"
}

@ARTICLE{Tolman1946-aq,
  title     = "Studies in spatial learning. {I}. Orientation and the short-cut",
  author    = "Tolman, E C and Ritchie, B F and Kalish, D",
  journal   = "Journal of experimental psychology",
  publisher = "US: American Psychological Association",
  volume    =  36,
  number    =  1,
  pages     =  13,
  abstract  = "The original rough formulation of the expectancy theory is
               difficult to distinguish from the alternative stimulus-response
               doctrines, partly because of the implicit definition of the
               matrix`` x …",
  year      =  1946,
  doi       = "10.1037/h0053944",
  issn      = "0022-1015"
}

@INCOLLECTION{Kelemen2007-gz,
  title     = "The essence of artifacts: Developing the design stance",
  author    = "Kelemen, Deborah and Carey, Susan",
  booktitle = "Creations of the Mind",
  publisher = "Oxford University PressOxford",
  pages     = "212--230",
  abstract  = "Abstract First some terminology: by ‘concept’, we simply mean a
               mental representation. We endorse a ‘two factor’ theory of
               conceptual content: concepts are individuated both by causal
               processes that relate mental symbols to their referents and also
               by internal inferential role. Philosophers sometimes treat these
               two factors as determining distinct kinds of content (wide and
               narrow), but we will not take a stance on this issue here. We
               assume that the meanings of terms in natural language are fixed
               in the same way, and thus we will sometimes speak of the meaning
               of a word such as ‘accordion’ and sometimes speak of the concept
               accordion.",
  month     =  jun,
  year      =  2007,
  doi       = "10.1093/oso/9780199250981.003.0012",
  isbn      = "9780199250981,9781383039061"
}

@ARTICLE{Griffiths2019-bw,
  title     = "Doing more with less: meta-reasoning and meta-learning in humans
               and machines",
  author    = "Griffiths, Thomas L and Callaway, Frederick and Chang, Michael B
               and Grant, Erin and Krueger, Paul M and Lieder, Falk",
  journal   = "Current opinion in behavioral sciences",
  publisher = "Elsevier BV",
  volume    =  29,
  pages     = "24--30",
  month     =  oct,
  year      =  2019,
  doi       = "10.1016/j.cobeha.2019.01.005",
  issn      = "2352-1546,2352-1554",
  language  = "en"
}

@ARTICLE{Barkovich2019-on,
  title     = "Challenges in pediatric neuroimaging",
  author    = "Barkovich, Matthew J and Li, Yi and Desikan, Rahul S and
               Barkovich, A James and Xu, Duan",
  journal   = "NeuroImage",
  publisher = "Academic Press",
  volume    =  185,
  pages     = "793--801",
  abstract  = "Pediatric neuroimaging is challenging due the rapid structural,
               metabolic, and functional changes that occur in the developing
               brain. A specially trained team is needed to produce high quality
               diagnostic images in children, due to their small physical size
               and immaturity. Patient motion, cooperation and medical condition
               dictate the methods and equipment used. A customized approach
               tailored to each child's age and functional status with the
               appropriate combination of dedicated staff, imaging hardware, and
               software is key; these range from low-tech techniques, such as
               feed and swaddle, to specialized small bore MRI scanners, MRI
               compatible incubators and neonatal head coils. New pre-and
               post-processing techniques can also compensate for the motion
               artifacts and low signal that often degrade neonatal scans.",
  month     =  jan,
  year      =  2019,
  keywords  = "Fetal imaging; MRI; Neonatal imaging; Pediatric neuroimaging",
  doi       = "10.1016/j.neuroimage.2018.04.044",
  pmc       = "PMC6197938",
  pmid      =  29684645,
  issn      = "1053-8119,1095-9572",
  language  = "en"
}

@ARTICLE{Fum2007-dq,
  title   = "The cognitive modeling of human behavior: Why a model is
             (sometimes) better than 10,000 words",
  author  = "Fum, Danilo and Missier, Fabio Del and Stocco, Andrea",
  journal = "Cognitive systems research",
  volume  =  8,
  number  =  3,
  pages   = "135--142",
  month   =  sep,
  year    =  2007,
  doi     = "10.1016/j.cogsys.2007.07.001",
  issn    = "1389-0417"
}

@ARTICLE{Ho2023-dg,
  title     = "Rational simplification and rigidity in human planning",
  author    = "Ho, Mark K and Cohen, Jonathan D and Griffiths, Thomas L",
  journal   = "Psychological science",
  publisher = "SAGE Publications",
  volume    =  34,
  number    =  11,
  pages     = "1281--1292",
  abstract  = "Planning underpins the impressive flexibility of goal-directed
               behavior. However, even when planning, people can display
               surprising rigidity in how they think about problems (e.g.,
               ``functional fixedness'') that lead them astray. How can our
               capacity for behavioral flexibility be reconciled with our
               susceptibility to conceptual inflexibility? We propose that these
               tendencies reflect avoidance of two cognitive costs: the cost of
               representing task details and the cost of switching between
               representations. To test this hypothesis, we developed a novel
               paradigm that affords participants opportunities to choose
               different families of simplified representations to plan. In two
               preregistered, online studies (Ns = 377 and 294 adults), we found
               that participants' optimal behavior, suboptimal behavior, and
               reaction time were explained by a computational model that
               formalized people's avoidance of representational complexity and
               switching. These results demonstrate how the selection of
               simplified, rigid representations leads to the otherwise puzzling
               combination of flexibility and inflexibility observed in problem
               solving.",
  month     =  nov,
  year      =  2023,
  keywords  = "causal reasoning; functional fixedness; open data; open
               materials; planning; preregistered; problem solving; task
               switching",
  doi       = "10.1177/09567976231200547",
  pmid      =  37878525,
  issn      = "0956-7976,1467-9280",
  language  = "en"
}

@ARTICLE{Vorbach2021-nz,
  title    = "Causal Navigation by Continuous-time Neural Networks",
  author   = "Vorbach, Charles J and Hasani, Ramin M and Amini, Alexander and
              Lechner, Mathias and Rus, D",
  journal  = "Advances in neural information processing systems",
  abstract = "The results demonstrate that causal continuous-time deep models
              can perform robust navigation tasks, where advanced recurrent
              models fail, and learn complex causal control representations
              directly from raw visual inputs and scale to solve a variety of
              tasks using imitation learning. Imitation learning enables
              high-fidelity, vision-based learning of policies within rich,
              photorealistic environments. However, such techniques often rely
              on traditional discrete-time neural models and face difficulties
              in generalizing to domain shifts by failing to account for the
              causal relationships between the agent and the environment. In
              this paper, we propose a theoretical and experimental framework
              for learning causal representations using continuous-time neural
              networks, specifically over their discrete-time counterparts. We
              evaluate our method in the context of visual-control learning of
              drones over a series of complex tasks, ranging from short- and
              long-term navigation, to chasing static and dynamic objects
              through photorealistic environments. Our results demonstrate that
              causal continuous-time deep models can perform robust navigation
              tasks, where advanced recurrent models fail. These models learn
              complex causal control representations directly from raw visual
              inputs and scale to solve a variety of tasks using imitation
              learning.",
  year     =  2021,
  eprint   = "2106.08314",
  issn     = "1049-5258",
  language = "en"
}

@INPROCEEDINGS{Madhushani2020-ah,
  title     = "A Dynamic Observation Strategy for Multi-agent Multi-armed Bandit
               Problem",
  author    = "Madhushani, Udari and Leonard, Naomi Ehrich",
  booktitle = "2020 European Control Conference (ECC)",
  pages     = "1677--1682",
  abstract  = "We define and analyze a multi-agent multi-armed bandit problem in
               which decision-making agents can observe the choices and rewards
               of their neighbors under a linear observation cost. Neighbors are
               defined by a network graph that encodes the inherent observation
               constraints of the system. We define a cost associated with
               observations such that at every instance an agent makes an
               observation it receives a constant observation regret. We design
               a sampling algorithm and an observation protocol for each agent
               to maximize its own expected cumulative reward through minimizing
               expected cumulative sampling regret and expected cumulative
               observation regret. For our proposed protocol, we prove that
               total cumulative regret is logarithmically bounded. We verify the
               accuracy of analytical bounds using numerical simulations.",
  month     =  may,
  year      =  2020,
  keywords  = "Random variables;Uncertainty;Protocols;Upper bound;Decision
               making;Numerical simulation;Estimation",
  doi       = "10.23919/ECC51009.2020.9143736"
}

@ARTICLE{Moradipari2022-uu,
  title         = "Collaborative Multi-agent Stochastic Linear Bandits",
  author        = "Moradipari, Ahmadreza and Ghavamzadeh, Mohammad and Alizadeh,
                   Mahnoosh",
  journal       = "arXiv [cs.LG]",
  abstract      = "We study a collaborative multi-agent stochastic linear bandit
                   setting, where $N$ agents that form a network communicate
                   locally to minimize their overall regret. In this setting,
                   each agent has its own linear bandit problem (its own reward
                   parameter) and the goal is to select the best global action
                   w.r.t. the average of their reward parameters. At each round,
                   each agent proposes an action, and one action is randomly
                   selected and played as the network action. All the agents
                   observe the corresponding rewards of the played actions and
                   use an accelerated consensus procedure to compute an estimate
                   of the average of the rewards obtained by all the agents. We
                   propose a distributed upper confidence bound (UCB) algorithm
                   and prove a high probability bound on its $T$-round regret in
                   which we include a linear growth of regret associated with
                   each communication round. Our regret bound is of order
                   $\mathcal{O}\Big(\sqrt{\frac{T}{N \log(1/|\lambda_2|)}}\cdot
                   (\log T)^2\Big)$, where $\lambda_2$ is the second largest (in
                   absolute value) eigenvalue of the communication matrix.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2205.06331"
}

@ARTICLE{Landgren2020-ri,
  title         = "Distributed Cooperative Decision Making in Multi-agent
                   Multi-armed Bandits",
  author        = "Landgren, Peter and Srivastava, Vaibhav and Leonard, Naomi
                   Ehrich",
  journal       = "arXiv [math.OC]",
  abstract      = "We study a distributed decision-making problem in which
                   multiple agents face the same multi-armed bandit (MAB), and
                   each agent makes sequential choices among arms to maximize
                   its own individual reward. The agents cooperate by sharing
                   their estimates over a fixed communication graph. We consider
                   an unconstrained reward model in which two or more agents can
                   choose the same arm and collect independent rewards. And we
                   consider a constrained reward model in which agents that
                   choose the same arm at the same time receive no reward. We
                   design a dynamic, consensus-based, distributed estimation
                   algorithm for cooperative estimation of mean rewards at each
                   arm. We leverage the estimates from this algorithm to develop
                   two distributed algorithms: coop-UCB2 and
                   coop-UCB2-selective-learning, for the unconstrained and
                   constrained reward models, respectively. We show that both
                   algorithms achieve group performance close to the performance
                   of a centralized fusion center. Further, we investigate the
                   influence of the communication graph structure on
                   performance. We propose a novel graph explore-exploit index
                   that predicts the relative performance of groups in terms of
                   the communication graph, and we propose a novel nodal
                   explore-exploit centrality index that predicts the relative
                   performance of agents in terms of the agent locations in the
                   communication graph.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "math.OC",
  eprint        = "2003.01312"
}

@INCOLLECTION{Whitley1991-ut,
  title     = "Fundamental principles of deception in genetic search",
  author    = "Whitley, L Darrell",
  booktitle = "Foundations of Genetic Algorithms",
  publisher = "Elsevier",
  volume    =  1,
  pages     = "221--241",
  abstract  = "This paper presents several theorems concerning the nature of
               deception and the central role that deception plays in function
               optimization using genet…",
  month     =  jan,
  year      =  1991,
  doi       = "10.1016/b978-0-08-050684-5.50017-3",
  isbn      =  9780080506845,
  issn      = "1081-6593"
}

@ARTICLE{Bontrager2019-hz,
  title         = "Superstition in the network: Deep reinforcement learning
                   plays deceptive games",
  author        = "Bontrager, Philip and Khalifa, Ahmed and Anderson, Damien and
                   Stephenson, Matthew and Salge, Christoph and Togelius, Julian",
  journal       = "arXiv [cs.LG]",
  abstract      = "Deep reinforcement learning has learned to play many games
                   well, but failed on others. To better characterize the modes
                   and reasons of failure of deep reinforcement learners, we
                   test the widely used Asynchronous Actor-Critic (A2C)
                   algorithm on four deceptive games, which are specially
                   designed to provide challenges to game-playing agents. These
                   games are implemented in the General Video Game AI framework,
                   which allows us to compare the behavior of reinforcement
                   learning-based agents with planning agents based on tree
                   search. We find that several of these games reliably deceive
                   deep reinforcement learners, and that the resulting behavior
                   highlights the shortcomings of the learning algorithm. The
                   particular ways in which agents fail differ from how
                   planning-based agents fail, further illuminating the
                   character of these algorithms. We propose an initial typology
                   of deceptions which could help us better understand pitfalls
                   and failure modes of (deep) reinforcement learning.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1908.04436"
}

@MISC{Anthropic2024-zl,
  title        = "Claude 3.5 Sonnet Model Card Addendum",
  author       = "{Anthropic}",
  year         =  2024,
  howpublished = "Preprint"
}

@ARTICLE{Sarathy2024-gj,
  title    = "Using Puzzle Video Games to Study Cognitive Processes in Human
              Insight and Creative Problem-Solving",
  author   = "Sarathy, Vasanth and Rabb, Nicholas and Kasenberg, Daniel M and
              Scheutz, Matthias",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  volume   =  46,
  number   =  0,
  abstract = "Author(s): Sarathy, Vasanth; Rabb, Nicholas; Kasenberg, Daniel M;
              Scheutz, Matthias | Abstract: Classical approaches to studying
              insight problem-solving typically use specialized problems (e.g.,
              nine-dot problem, compound-remote associates task) as stimuli
              together with verbal reports from subjects during problem-solving
              to reveal their thought processes, possibly adding other
              task-related metrics such as completion rate and physiological
              measures like eye fixation and neural activity. This approach has
              led to the claims that insight and creative thought require
              impasse and mental restructuring. What is missing from this
              literature is a cognitive process model of insight, and one reason
              for the lack of such a model is the lack of a unified, scalable,
              and tunable experimental framework with which to study human
              creative problem-solving with higher fidelity. In this paper, we
              introduce ESCAPE, an experimental paradigm using puzzle video
              games as stimuli which allow for the collection of process data
              that can serve as a basis for computational models. We have
              specifically developed a set of puzzle games based on this
              paradigm and conducted experiments that demonstrate the utility of
              the approach by revealing a set of computational principles that
              need to be accounted for by a theory of creative problems and the
              computational models based on it.",
  year     =  2024
}

@ARTICLE{Iriki2005-qk,
  title    = "A prototype of Homo faber: a silent precursor of human
              intelligence in the tool-using monkey brain",
  author   = "Iriki, Atsushi",
  journal  = "MIT Press",
  abstract = "silent precursors of the monkey brain could be recruited to be
              expressed during the course of evolution to finally represent
              human intelligence with which we define ourselves today.",
  year     =  2005
}

@ARTICLE{Rule2024-hu,
  title     = "Symbolic metaprogram search improves learning efficiency and
               explains rule learning in humans",
  author    = "Rule, Joshua S and Piantadosi, Steven T and Cropper, Andrew and
               Ellis, Kevin and Nye, Maxwell and Tenenbaum, Joshua B",
  journal   = "Nature communications",
  publisher = "Springer Science and Business Media LLC",
  volume    =  15,
  number    =  1,
  pages     =  6847,
  abstract  = "Throughout their lives, humans seem to learn a variety of rules
               for things like applying category labels, following procedures,
               and explaining causal relationships. These rules are often
               algorithmically rich but are nonetheless acquired with minimal
               data and computation. Symbolic models based on program learning
               successfully explain rule-learning in many domains, but
               performance degrades quickly as program complexity increases. It
               remains unclear how to scale symbolic rule-learning methods to
               model human performance in challenging domains. Here we show that
               symbolic search over the space of metaprograms-programs that
               revise programs-dramatically improves learning efficiency. On a
               behavioral benchmark of 100 algorithmically rich rules, this
               approach fits human learning more accurately than alternative
               models while also using orders of magnitude less search. The
               computation required to match median human performance is
               consistent with conservative estimates of human thinking time.
               Our results suggest that metaprogram-like representations may
               help human learners to efficiently acquire rules.",
  month     =  aug,
  year      =  2024,
  doi       = "10.1038/s41467-024-50966-x",
  pmc       = "PMC11316799",
  pmid      =  39127796,
  issn      = "2041-1723,2041-1723",
  language  = "en"
}

@ARTICLE{Iacoboni2006-kf,
  title     = "Visuo-motor integration and control in the human posterior
               parietal cortex: evidence from {TMS} and {fMRI}",
  author    = "Iacoboni, Marco",
  journal   = "Neuropsychologia",
  publisher = "Elsevier BV",
  volume    =  44,
  number    =  13,
  pages     = "2691--2699",
  abstract  = "The posterior parietal cortex is a fundamental structure for
               visuo-motor integration and control. Here I discuss recent
               transcranial magnetic stimulation (TMS) and functional magnetic
               resonance imaging (fMRI) studies that I interpret as suggesting
               four concepts. The evolutionary process has enlarged the human
               posterior parietal cortex while still preserving the internal
               structure of the posterior parietal cortex of other primates.
               Visuo-motor control in the posterior parietal cortex may be
               implemented by coding primarily action goals. The lateralization
               of visuo-motor functions in the posterior parietal cortex
               suggests that the left posterior parietal cortex is more
               concerned with tool use and the right posterior parietal cortex
               is more concerned with imitation of the actions of others.
               Finally, visuo-motor inter-hemispheric transfer through parietal
               callosal fibers occurs at the level of 'motor intention'.",
  month     =  jun,
  year      =  2006,
  doi       = "10.1016/j.neuropsychologia.2006.04.029",
  pmid      =  16759673,
  issn      = "0028-3932,1873-3514",
  language  = "en"
}

@ARTICLE{Chesebrough2024-da,
  title     = "Waves of Insight",
  author    = "Chesebrough, Christine and Salvi, Carola and Beeman, Mark and Oh,
               Yongtaek and Kounios, John",
  journal   = "The Emergence of Insight",
  publisher = "Cambridge University Press",
  pages     =  223,
  abstract  = "Insight problem-solving has been a subject of fascination in the
               psychological sciences for approximately 100 years. As with most
               complex cognitive phenomena, perspectives on how to define,
               operationalize, and measure insight have evolved over time based
               on developments in theory, methodology, and technology. Research
               on insight can be broken into several phases, or waves,
               characterized by different theoretical and methodological
               paradigms. In the first wave, Gestalt psychologists introduced
               the concept of insight as a discontinuous form of learning and
               problemsolving that arises from changes in one’s global
               representation of a problem, in opposition to contemporary
               associationist views. In the second wave, psychologists examined
               insight in deliberate contrast with analytical problem-solving
               and found that insight involves nonreportable mental operations
               leading to a discrete, all-or-none availability of
               representational change. In the third wave, thanks to advances in
               behavioral methods and neuroimaging technology, cognitive
               neuroscientists began to examine how insight occurs in the brain
               with the goal of studying the neural states that co-occur with
               and precede the insight experience to better understand its
               cognitive mechanisms. The methodological advances made during
               these initial waves enabled the proliferation of research on
               insight over the last several decades and continue to inspire new
               discoveries and perspectives.In the first part of this chapter,
               we provide a brief retrospective on the first two waves of
               insight research with a particular focus on the theories and
               methods that informed subsequent neuroscience approaches …",
  month     =  may,
  year      =  2024
}

@ARTICLE{Cabrera-Alvarez2020-zf,
  title     = "Neural processes underlying tool use in humans, macaques, and
               corvids",
  author    = "Cabrera-Álvarez, María J and Clayton, Nicola S",
  journal   = "Frontiers in psychology",
  publisher = "Frontiers Media SA",
  volume    =  11,
  pages     =  560669,
  abstract  = "It was thought that tool use in animals is an adaptive
               specialization. Recent studies, however, have shown that some
               non-tool-users, such as rooks and jays, can use and manufacture
               tools in laboratory settings. Despite the abundant evidence of
               tool use in corvids, little is known about the neural mechanisms
               underlying tool use in this family of birds. This review
               summarizes the current knowledge on the neural processes
               underlying tool use in humans, macaques and corvids. We suggest a
               possible neural network for tool use in macaques and hope this
               might inspire research to discover a similar brain network in
               corvids. We hope to establish a framework to elucidate the neural
               mechanisms that supported the convergent evolution of tool use in
               birds and mammals.",
  month     =  sep,
  year      =  2020,
  keywords  = "causal reasoning; corvids; macaques; neural mechanisms; neural
               network; tool use",
  doi       = "10.3389/fpsyg.2020.560669",
  pmc       = "PMC7561402",
  pmid      =  33117228,
  issn      = "1664-1078",
  language  = "en"
}

@ARTICLE{Orban2014-fm,
  title     = "The neural basis of human tool use",
  author    = "Orban, Guy A and Caruana, Fausto",
  journal   = "Frontiers in psychology",
  publisher = "Frontiers Media SA",
  volume    =  5,
  pages     =  310,
  abstract  = "In this review, we propose that the neural basis for the
               spontaneous, diversified human tool use is an area devoted to the
               execution and observation of tool actions, located in the left
               anterior supramarginal gyrus (aSMG). The aSMG activation elicited
               by observing tool use is typical of human subjects, as macaques
               show no similar activation, even after an extensive training to
               use tools. The execution of tool actions, as well as their
               observation, requires the convergence upon aSMG of inputs from
               different parts of the dorsal and ventral visual streams.
               Non-semantic features of the target object may be provided by the
               posterior parietal cortex (PPC) for tool-object interaction,
               paralleling the well-known PPC input to anterior intraparietal
               (AIP) for hand-object interaction. Semantic information regarding
               tool identity, and knowledge of the typical manner of handling
               the tool, could be provided by inferior and middle regions of the
               temporal lobe. Somatosensory feedback and technical reasoning, as
               well as motor and intentional constraints also play roles during
               the planning of tool actions and consequently their signals
               likewise converge upon aSMG. We further propose that aSMG may
               have arisen though duplication of monkey AIP and invasion of the
               duplicate area by afferents from PPC providing distinct signals
               depending on the kinematics of the manipulative action. This
               duplication may have occurred when Homo Habilis or Homo Erectus
               emerged, generating the Oldowan or Acheulean Industrial complexes
               respectively. Hence tool use may have emerged during hominid
               evolution between bipedalism and language. We conclude that
               humans have two parietal systems involved in tool behavior: a
               biological circuit for grasping objects, including tools, and an
               artifactual system devoted specifically to tool use. Only the
               latter allows humans to understand the causal relationship
               between tool use and obtaining the goal, and is likely to be the
               basis of all technological developments.",
  month     =  apr,
  year      =  2014,
  keywords  = "affordances; anterior intraparietal sulcus; anterior
               supramarginal gyrus; mechanical problem solving; tool use",
  doi       = "10.3389/fpsyg.2014.00310",
  pmc       = "PMC3988392",
  pmid      =  24782809,
  issn      = "1664-1078",
  language  = "en"
}

@ARTICLE{Newcombe2013-vt,
  title    = "Cognitive development: changing views of cognitive change",
  author   = "Newcombe, Nora S",
  journal  = "Wiley interdisciplinary reviews. Cognitive science",
  volume   =  4,
  number   =  5,
  pages    = "479--491",
  abstract = "UNLABELLED: The aim of research in cognitive development is to
              understand the origins of human knowledge and to provide an
              account of cognitive change. Theorizing regarding these issues is
              rooted in the nativist-empiricist debate. This article traces
              changing views in that debate, from the beginnings of psychology,
              through the cognitive revolution, Piaget, and alternatives to
              Piaget, including nativism, Vygotskyan theory, and
              information-processing work. The last section presents current
              theorizing and outlines various modern versions of nativism,
              constructivism, and empiricism. WIREs Cogn Sci 2013, 4:479-491.
              doi: 10.1002/wcs.1245 For further resources related to this
              article, please visit the WIREs website. CONFLICT OF INTEREST: The
              author has declared no conflicts of interest for this article.",
  month    =  sep,
  year     =  2013,
  doi      = "10.1002/wcs.1245",
  pmid     =  26304241,
  issn     = "1939-5078,1939-5086",
  language = "en"
}

@ARTICLE{Knudsen1998-ry,
  title    = "Capacity for plasticity in the adult owl auditory system expanded
              by juvenile experience",
  author   = "Knudsen, E I",
  journal  = "Science",
  volume   =  279,
  number   =  5356,
  pages    = "1531--1533",
  abstract = "In the process of creating a multimodal map of space,
              auditory-visual neurons in the optic tectum establish associations
              between particular values of auditory spatial cues and locations
              in the visual field. In the barn owl, tectal neurons reveal these
              associations in the match between their tuning for interaural time
              differences (ITDs) and the locations of their visual receptive
              fields (VRFs). In young owls ITD-VRF associations can be adjusted
              by experience over a wide range, but the range of adjustment
              normally becomes quite restricted in adults. This normal range of
              adjustment in adults was greatly expanded in owls that had
              previously learned abnormal ITD-VRF associations as juveniles.
              Thus, the act of learning abnormal associations early in life
              leaves an enduring trace in this pathway that enables unusual
              functional connections to be reestablished, as needed, in
              adulthood, even when the associations represented by these
              connections have not been used for an extended period of time.",
  month    =  mar,
  year     =  1998,
  doi      = "10.1126/science.279.5356.1531",
  pmid     =  9488651,
  issn     = "0036-8075",
  language = "en"
}

@ARTICLE{Kounios2014-wq,
  title    = "The cognitive neuroscience of insight",
  author   = "Kounios, John and Beeman, Mark",
  journal  = "Annual review of psychology",
  volume   =  65,
  pages    = "71--93",
  abstract = "Insight occurs when a person suddenly reinterprets a stimulus,
              situation, or event to produce a nonobvious, nondominant
              interpretation. This can take the form of a solution to a problem
              (an ``aha moment''), comprehension of a joke or metaphor, or
              recognition of an ambiguous percept. Insight research began a
              century ago, but neuroimaging and electrophysiological techniques
              have been applied to its study only during the past decade. Recent
              work has revealed insight-related coarse semantic coding in the
              right hemisphere and internally focused attention preceding and
              during problem solving. Individual differences in the tendency to
              solve problems insightfully rather than in a deliberate, analytic
              fashion are associated with different patterns of resting-state
              brain activity. Recent studies have begun to apply direct brain
              stimulation to facilitate insight. In sum, the cognitive
              neuroscience of insight is an exciting new area of research with
              connections to fundamental neurocognitive processes.",
  year     =  2014,
  doi      = "10.1146/annurev-psych-010213-115154",
  pmid     =  24405359,
  issn     = "0066-4308,1545-2085",
  language = "en"
}

@ARTICLE{Fleck2008-zf,
  title     = "Working memory demands in insight versus analytic problem solving",
  author    = "Fleck, Jessica I",
  journal   = "The European journal of cognitive psychology",
  publisher = "Routledge",
  volume    =  20,
  number    =  1,
  pages     = "139--176",
  abstract  = "Working memory is one of the cognitive processes thought to
               differentiate insight and analytic forms of problem solving. The
               present research examined memory involvement in the solution of
               insight versus analytic problems. Participants completed verbal
               and spatial working memory and short-term memory measures and a
               series of analytic and insight problems. Results demonstrated a
               relationship between working-memory capacity and the solution of
               analytic problems and between verbal short-term memory capacity
               and the solution of insight problems. This distinction was
               generally though not universally supported when memory was
               examined in relation to individual problems. Memory involvement
               in insight problem solving was further examined to clarify
               whether restructuring in insight is the end result of active
               memory search or spontaneous processes. The present research
               supports the theory that differences exist in the cognitive
               processes underlying insight versus analytic problem solving, and
               provides support for the spontaneous theory of restructuring in
               insight.",
  month     =  jan,
  year      =  2008,
  doi       = "10.1080/09541440601016954",
  issn      = "0954-1446"
}

@ARTICLE{Ord2024-kn,
  title         = "Interpolation, Extrapolation, Hyperpolation: Generalising
                   into new dimensions",
  author        = "Ord, Toby",
  journal       = "arXiv [cs.LG]",
  abstract      = "This paper introduces the concept of hyperpolation: a way of
                   generalising from a limited set of data points that is a peer
                   to the more familiar concepts of interpolation and
                   extrapolation. Hyperpolation is the task of estimating the
                   value of a function at new locations that lie outside the
                   subspace (or manifold) of the existing data. We shall see
                   that hyperpolation is possible and explore its links to
                   creativity in the arts and sciences. We will also examine the
                   role of hyperpolation in machine learning and suggest that
                   the lack of fundamental creativity in current AI systems is
                   deeply connected to their limited ability to hyperpolate.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2409.05513"
}

@ARTICLE{Feldman2023-si,
  title    = "Probabilistic origins of compositional mental representations",
  author   = "Feldman, Jacob",
  journal  = "Psychological review",
  abstract = "The representation of complex phenomena via combinations of simple
              discrete features is a hallmark of human cognition. But it is not
              clear exactly how (or whether) discrete features can effectively
              represent the complex probabilistic fabric of the environment.
              This article introduces information-theoretic tools for
              quantifying the fidelity and efficiency of a featural
              representation with respect to a probability model. In this
              framework, a feature or combination of features is ``faithful'' to
              the extent that knowing the value of the features reduces
              uncertainty about the true state of the world. In a single
              dimension, a discrete feature is faithful if the values of the
              feature correspond isomorphically to distinct classes in the
              probability model. But in multiple dimensions, the situation is
              more complicated: The fidelity of each feature depends on the
              direction in multidimensional feature space in which the feature
              is projected from the underlying distribution. More interestingly,
              distributions may be more effectively represented by combinations
              of projected features-that is, compositionality. For any given
              distribution, a variety of compositional forms (features and
              combination rules) are possible, which can be quite different from
              one another, entailing different degrees of fidelity, different
              numbers of features, and even different induced regularities. This
              article proposes three specific criteria for a compositional
              representation: fidelity, simplicity, and robustness. The
              information-theoretic framework introduces a new and potentially
              useful way to look at the problem of compositionality in human
              mental representation. (PsycInfo Database Record (c) 2023 APA, all
              rights reserved).",
  month    =  nov,
  year     =  2023,
  doi      = "10.1037/rev0000452",
  pmid     =  37917443,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@ARTICLE{Dubey2020-ae,
  title    = "Reconciling novelty and complexity through a rational analysis of
              curiosity",
  author   = "Dubey, Rachit and Griffiths, Thomas L",
  journal  = "Psychological review",
  volume   =  127,
  number   =  3,
  pages    = "455--476",
  abstract = "Curiosity is considered to be the essence of science and an
              integral component of cognition. What prompts curiosity in a
              learner? Previous theoretical accounts of curiosity remain
              divided-novelty-based theories propose that new and highly
              uncertain stimuli pique curiosity, whereas complexity-based
              theories propose that stimuli with an intermediate degree of
              uncertainty stimulate curiosity. In this article, we present a
              rational analysis of curiosity by considering the computational
              problem underlying curiosity, which allows us to model these
              distinct accounts of curiosity in a common framework. Our approach
              posits that a rational agent should explore stimuli that maximally
              increase the usefulness of its knowledge and that curiosity is the
              mechanism by which humans approximate this rational behavior.
              Critically, our analysis show that the causal structure of the
              environment can determine whether curiosity is driven by either
              highly uncertain or moderately uncertain stimuli. This suggests
              that previous theories need not be in contention but are special
              cases of a more general account of curiosity. Experimental results
              confirm our predictions and demonstrate that our theory explains a
              wide range of findings about human curiosity, including its
              subjectivity and malleability. (PsycInfo Database Record (c) 2020
              APA, all rights reserved).",
  month    =  apr,
  year     =  2020,
  doi      = "10.1037/rev0000175",
  pmid     =  31868394,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@ARTICLE{Johnston2023-gv,
  title    = "Abstract representations emerge naturally in neural networks
              trained to perform multiple tasks",
  author   = "Johnston, W Jeffrey and Fusi, Stefano",
  journal  = "Nature communications",
  volume   =  14,
  number   =  1,
  pages    =  1040,
  abstract = "Humans and other animals demonstrate a remarkable ability to
              generalize knowledge across distinct contexts and objects during
              natural behavior. We posit that this ability to generalize arises
              from a specific representational geometry, that we call abstract
              and that is referred to as disentangled in machine learning. These
              abstract representations have been observed in recent
              neurophysiological studies. However, it is unknown how they
              emerge. Here, using feedforward neural networks, we demonstrate
              that the learning of multiple tasks causes abstract
              representations to emerge, using both supervised and reinforcement
              learning. We show that these abstract representations enable
              few-sample learning and reliable generalization on novel tasks. We
              conclude that abstract representations of sensory and cognitive
              variables may emerge from the multiple behaviors that animals
              exhibit in the natural world, and, as a consequence, could be
              pervasive in high-level brain regions. We also make several
              specific predictions about which variables will be represented
              abstractly.",
  month    =  feb,
  year     =  2023,
  doi      = "10.1038/s41467-023-36583-0",
  pmc      = "PMC9950464",
  pmid     =  36823136,
  issn     = "2041-1723",
  language = "en"
}

@ARTICLE{Keen2011-np,
  title    = "The development of problem solving in young children: a critical
              cognitive skill",
  author   = "Keen, Rachel",
  journal  = "Annual review of psychology",
  volume   =  62,
  pages    = "1--21",
  abstract = "Problem solving is a signature attribute of adult humans, but we
              need to understand how this develops in children. Tool use is
              proposed as an ideal way to study problem solving in children less
              than 3 years of age because overt manual action can reveal how the
              child plans to achieve a goal. Motor errors are as informative as
              successful actions. Research is reviewed on intentional actions,
              beginning with block play and progressing to picking up a spoon in
              different orientations, and finally retrieving objects with rakes
              and from inside tubes. Behavioral and kinematic measures of motor
              action are combined to show different facets of skill acquisition
              and mastery. We need to design environments that encourage and
              enhance problem solving from a young age. One goal of this review
              is to excite interest and spur new research on the beginnings of
              problem solving and its elaboration during development.",
  year     =  2011,
  doi      = "10.1146/annurev.psych.031809.130730",
  pmid     =  20822435,
  issn     = "0066-4308,1545-2085",
  language = "en"
}

@ARTICLE{Sharma2021-xf,
  title         = "Skill Induction and Planning with Latent Language",
  author        = "Sharma, Pratyusha and Torralba, Antonio and Andreas, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present a framework for learning hierarchical policies
                   from demonstrations, using sparse natural language
                   annotations to guide the discovery of reusable skills for
                   autonomous decision-making. We formulate a generative model
                   of action sequences in which goals generate sequences of
                   high-level subtask descriptions, and these descriptions
                   generate sequences of low-level actions. We describe how to
                   train this model using primarily unannotated demonstrations
                   by parsing demonstrations into sequences of named high-level
                   subtasks, using only a small number of seed annotations to
                   ground language in action. In trained models, natural
                   language commands index a combinatorial library of skills;
                   agents can use these skills to plan by generating high-level
                   instruction sequences tailored to novel goals. We evaluate
                   this approach in the ALFRED household simulation environment,
                   providing natural language annotations for only 10\% of
                   demonstrations. It achieves task completion rates comparable
                   to state-of-the-art models (outperforming several recent
                   methods with access to ground-truth plans during training and
                   evaluation) while providing structured and human-readable
                   high-level plans.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.01517"
}

@ARTICLE{Maier1930-ck,
  title     = "Reasoning in humans. {I}. On direction",
  author    = "Maier, N R F",
  journal   = "Journal of comparative psychology",
  publisher = "psycnet.apa.org",
  volume    =  10,
  number    =  2,
  pages     = "115--143",
  abstract  = "A problematical situation was arranged so that it could be broken
               into three parts and presented to the subject as three separate
               experiences. Under these conditions the subjects could not find a
               solution. ``Thus a selected presentation of the experience is not
               enough. The parts of the experience must be combined in a certain
               manner and a 'direction' or way the problem is attacked, seems to
               be a factor which determines the nature of the combination.
               'Trial and error' may be present in the attempts at the solution,
               but is inadequate to explain the sudden appearance of the correct
               solution, when such solution requires productive rather than
               reproductive thinking.'' These results are oriented with respect
               to the theories of Ach, Selz, Wertheimer, etc. The author favors
               an explanation in terms of Gestalt. (PsycINFO Database Record (c)
               2016 APA, all rights reserved)",
  month     =  apr,
  year      =  1930,
  doi       = "10.1037/h0073232",
  issn      = "0093-4127"
}

@ARTICLE{Gilhooly2010-fb,
  title     = "Verbalization and problem solving: insight and spatial factors",
  author    = "Gilhooly, K J and Fioratou, E and Henretty, N",
  journal   = "British journal of psychology",
  publisher = "Wiley Online Library",
  volume    =  101,
  number    = "Pt 1",
  pages     = "81--93",
  abstract  = "Two groups of participants attempted eight examples of each of
               four different problem types formed by combining insight versus
               non-insight and verbal versus spatial factors. The groups were
               given different verbalization instructions viz., Silent (N=40) or
               Direct Concurrent (N=40). There were significant differences
               between insight and non-insight tasks and between spatial and
               verbal tasks in terms of solution rates and latencies.
               Significant interactions between the verbal versus spatial factor
               and verbalization condition on solution rates and latencies
               reflected a greater (negative) effect of verbalizing on spatial
               as against verbal problems. However, no significant interactions
               of the insight versus non-insight factor with verbalization
               condition on solution rates or latencies were found. These
               results favoured the 'business as usual' view of insight problem
               solving as against the 'special process' view which predicted
               larger effects of verbalization for insight problems as against
               non-insight problems.",
  month     =  feb,
  year      =  2010,
  doi       = "10.1348/000712609X422656",
  pmid      =  19309537,
  issn      = "0007-1269,2044-8295",
  language  = "en"
}

@ARTICLE{Duncker1945-ed,
  title     = "On problem-solving",
  author    = "Duncker, Karl and Lees, Lynne S",
  journal   = "Psychological monographs",
  publisher = "American Psychological Association",
  volume    =  58,
  number    =  5,
  pages     = "i",
  abstract  = "… with the specific problems and with the technical possibilities
               of this field. In 1935 Karl Duncker … It is therefore most
               fortunate that after Duncker's death one of his students decided
               to pre…",
  year      =  1945,
  issn      = "0096-9753"
}

@ARTICLE{Wise2023-jo,
  title    = "Interactive cognitive maps support flexible behavior under threat",
  author   = "Wise, Toby and Charpentier, Caroline J and Dayan, Peter and Mobbs,
              Dean",
  journal  = "Cell reports",
  volume   =  42,
  number   =  8,
  pages    =  113008,
  abstract = "In social environments, survival can depend upon inferring and
              adapting to other agents' goal-directed behavior. However, it
              remains unclear how humans achieve this, despite the fact that
              many decisions must account for complex, dynamic agents acting
              according to their own goals. Here, we use a predator-prey task
              (total n = 510) to demonstrate that humans exploit an interactive
              cognitive map of the social environment to infer other agents'
              preferences and simulate their future behavior, providing for
              flexible, generalizable responses. A model-based inverse
              reinforcement learning model explained participants' inferences
              about threatening agents' preferences, with participants using
              this inferred knowledge to enact generalizable, model-based
              behavioral responses. Using tree-search planning models, we then
              found that behavior was best explained by a planning algorithm
              that incorporated simulations of the threat's goal-directed
              behavior. Our results indicate that humans use a cognitive map to
              determine other agents' preferences, facilitating generalized
              predictions of their behavior and effective responses.",
  month    =  aug,
  year     =  2023,
  keywords = "CP: Neuroscience; avoidance; cognitive maps; decision-making;
              learning; planning; social inference",
  doi      = "10.1016/j.celrep.2023.113008",
  pmid     =  37610871,
  issn     = "2211-1247",
  language = "en"
}

@ARTICLE{Holyoak_undated-vo,
  title     = "Analogy and the Generation of Ideas",
  author    = "Holyoak, Keith J and Ichien, Nicholas and Lu, Hongjing",
  journal   = "Creativity research journal",
  publisher = "Routledge",
  pages     = "1--12",
  abstract  = "Creativity is typically defined as the generation of novel and
               useful ideas or artifacts. This generative capacity is crucial to
               everyday problem solving, technological innovation, scientific
               discovery, and the arts. A central concern of cognitive
               scientists is to understand the processes that underlie human
               creative thinking. We review evidence that one process
               contributing to human creativity is the ability to generate novel
               representations of unfamiliar situations by completing a
               partially specified relation or an analogy. In particular,
               cognitive tasks that trigger generation of relational
               similarities between dissimilar situations ? distant analogies ?
               foster a kind of creative mind-set. We discuss possible
               computational mechanisms that might enable relation-driven
               generation, and hence may contribute to human creativity, and
               conclude with suggested directions for future research.",
  doi       = "10.1080/10400419.2023.2232673",
  issn      = "1040-0419"
}

@ARTICLE{German2000-px,
  title    = "Immunity to functional fixedness in young children",
  author   = "German, T P and Defeyter, M A",
  journal  = "Psychonomic bulletin \& review",
  volume   =  7,
  number   =  4,
  pages    = "707--712",
  abstract = "In the candle problem (Duncker, 1945), subjects must attach a
              candle to a vertical surface, using only a box of tacks and a book
              of matches. Subjects exhibit functional fixedness by failing, or
              being slow, to make use of one object (the tack box) as a support,
              rather than as a container, in their solutions. This failure to
              produce alternate functions is measured against improved
              performance when the tack box is presented empty rather than full
              of tacks (i.e., not preutilized as a container). Using an
              analogous task, we show that functional fixedness can be
              demonstrated in older children (6- and 7-year-olds); they are
              significantly slower to use a box as a support when its
              containment function has been demonstrated than when it has not.
              However, younger children (5-year-olds) are immune to this effect,
              showing no advantage when the standard function is not
              demonstrated. Moreover, their performance under conditions of
              preutilization is better than that of both older groups. These
              results are interpreted in terms of children's developing
              intuitions about function and the effects of past experience on
              problem solving.",
  month    =  dec,
  year     =  2000,
  doi      = "10.3758/bf03213010",
  pmid     =  11206213,
  issn     = "1069-9384",
  language = "en"
}

@INCOLLECTION{Dominowski1994-tb,
  title     = "Insight and problem solving",
  author    = "Dominowski, Roger L and Dallob, Pamela",
  booktitle = "The Nature of Insight",
  publisher = "The MIT Press",
  year      =  1994,
  doi       = "10.7551/mitpress/4879.003.0005",
  isbn      =  9780262284370
}

@INCOLLECTION{Ollinger2009-tk,
  title     = "Psychological Research on Insight Problem Solving",
  author    = "Öllinger, Michael and Knoblich, Günther",
  editor    = "Atmanspacher, Harald and Primas, Hans",
  booktitle = "Recasting Reality: Wolfgang Pauli’s Philosophical Ideas and
               Contemporary Science",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "275--300",
  year      =  2009,
  doi       = "10.1007/978-3-540-85198-1\_14",
  isbn      =  9783540851981
}

@ARTICLE{Hills2015-ki,
  title    = "Exploration versus exploitation in space, mind, and society",
  author   = "Hills, Thomas T and Todd, Peter M and Lazer, David and Redish, A
              David and Couzin, Iain D and {Cognitive Search Research Group}",
  journal  = "Trends in cognitive sciences",
  volume   =  19,
  number   =  1,
  pages    = "46--54",
  abstract = "Search is a ubiquitous property of life. Although diverse domains
              have worked on search problems largely in isolation, recent trends
              across disciplines indicate that the formal properties of these
              problems share similar structures and, often, similar solutions.
              Moreover, internal search (e.g., memory search) shows similar
              characteristics to external search (e.g., spatial foraging),
              including shared neural mechanisms consistent with a common
              evolutionary origin across species. Search problems and their
              solutions also scale from individuals to societies, underlying and
              constraining problem solving, memory, information search, and
              scientific and cultural innovation. In summary, search represents
              a core feature of cognition, with a vast influence on its
              evolution and processes across contexts and requiring input from
              multiple domains to understand its implications and scope.",
  month    =  jan,
  year     =  2015,
  doi      = "10.1016/j.tics.2014.10.004",
  pmc      = "PMC4410143",
  pmid     =  25487706,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Keane1989-el,
  title     = "Modelling Problem Solving in Gestalt ‘Insight’ Problems",
  author    = "Keane, Mark",
  journal   = "The Irish Journal of Psychology",
  publisher = "Routledge",
  volume    =  10,
  number    =  2,
  pages     = "201--215",
  abstract  = "In this paper a cognitive science perspective is applied to the
               practical construction problems examined by researchers in the
               Gestalt school of psychology. An information processing theory of
               these problems is developed and implemented in a computer
               program. The theory is then considered in the light of previous
               research and used to account for the phenomenon of ?functional
               fixedness?",
  month     =  jan,
  year      =  1989,
  doi       = "10.1080/03033910.1989.10557742",
  issn      = "0303-3910"
}

@ARTICLE{Defeyter2003-sh,
  title    = "Acquiring an understanding of design: evidence from children's
              insight problem solving",
  author   = "Defeyter, Margaret Anne and German, Tim P",
  journal  = "Cognition",
  volume   =  89,
  number   =  2,
  pages    = "133--155",
  abstract = "The human ability to make tools and use them to solve problems may
              not be zoologically unique, but it is certainly extraordinary. Yet
              little is known about the conceptual machinery that makes humans
              so competent at making and using tools. Do adults and children
              have concepts specialized for understanding human-made artifacts?
              If so, are these concepts deployed in attempts to solve novel
              problems? Here we present new data, derived from problem-solving
              experiments, which support the following. (i) The structure of the
              child's concept of artifact function changes profoundly between
              ages 5 and 7. At age 5, the child's conceptual machinery defines
              the function of an artifact as any goal a user might have; by age
              7, its function is defined by the artifact's typical or intended
              use. (ii) This conceptual shift has a striking effect on
              problem-solving performance, i.e. the child's concept of artifact
              function appears to be deployed in problem solving. (iii) This
              effect on problem solving is not caused by differences in the
              amount of knowledge that children have about the typical use of a
              particular tool; it is mediated by the structure of the child's
              artifact concept (which organizes and deploys the child's
              knowledge). In two studies, children between 5 and 7 years of age
              were matched for their knowledge of what a particular artifact
              ``is for'', and then given a problem that can only be solved if
              that tool is used for an atypical purpose. All children performed
              well in a baseline condition. But when they were primed by a
              demonstration of the artifact's typical function, 5-year-old
              children solved the problem much faster than 6-7-year-old
              children. Because all children knew what the tools were for,
              differences in knowledge alone cannot explain the results. We
              argue that the older children were slower to solve the problem
              when the typical function was primed because (i) their artifact
              concept plays a role in problem solving, and (ii) intended purpose
              is central to their concept of artifact function, but not to that
              of the younger children.",
  month    =  sep,
  year     =  2003,
  doi      = "10.1016/s0010-0277(03)00098-2",
  pmid     =  12915298,
  issn     = "0010-0277",
  language = "en"
}

@ARTICLE{Rawlings2021-ui,
  title    = "Toddlers, Tools, and Tech: The Cognitive Ontogenesis of Innovation",
  author   = "Rawlings, Bruce and Legare, Cristine H",
  journal  = "Trends in cognitive sciences",
  volume   =  25,
  number   =  1,
  pages    = "81--92",
  abstract = "The development of tool innovation presents a paradox. How do
              humans have such diverse and complex technology, ranging from
              smartphones to aircraft, and yet young children find even simple
              tool innovation challenges, such as fashioning a hook to retrieve
              a basket from a tube, remarkably difficult? We propose that the
              solution to this paradox is the cognitive ontogenesis of tool
              innovation. Using a common measure of children's tool innovation,
              we describe how multiple cognitive mechanisms work in concert at
              each step of its process: recognizing the problem, generating
              appropriate solutions, and the social transmission of innovations.
              We discuss what the ontogeny of this skill tells us about
              cognitive and cultural evolution and provide recommendations for
              future research.",
  month    =  jan,
  year     =  2021,
  keywords = "causal reasoning; cognition; cumulative culture; executive
              functioning; problem solving; social learning",
  doi      = "10.1016/j.tics.2020.10.006",
  pmid     =  33223481,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Wu2022-mv,
  title    = "Time pressure changes how people explore and respond to
              uncertainty",
  author   = "Wu, Charley M and Schulz, Eric and Pleskac, Timothy J and
              Speekenbrink, Maarten",
  journal  = "Scientific reports",
  volume   =  12,
  number   =  1,
  pages    =  4122,
  abstract = "How does time pressure influence exploration and decision-making?
              We investigated this question with several four-armed bandit tasks
              manipulating (within subjects) expected reward, uncertainty, and
              time pressure (limited vs. unlimited). With limited time, people
              have less opportunity to perform costly computations, thus
              shifting the cost-benefit balance of different exploration
              strategies. Through behavioral, reinforcement learning (RL),
              reaction time (RT), and evidence accumulation analyses, we show
              that time pressure changes how people explore and respond to
              uncertainty. Specifically, participants reduced their
              uncertainty-directed exploration under time pressure, were less
              value-directed, and repeated choices more often. Since our
              analyses relate uncertainty to slower responses and dampened
              evidence accumulation (i.e., drift rates), this demonstrates a
              resource-rational shift towards simpler, lower-cost strategies
              under time pressure. These results shed light on how people adapt
              their exploration and decision-making strategies to externally
              imposed cognitive constraints.",
  month    =  mar,
  year     =  2022,
  doi      = "10.1038/s41598-022-07901-1",
  pmc      = "PMC8904509",
  pmid     =  35260717,
  issn     = "2045-2322",
  language = "en"
}

@ARTICLE{Binz2023-bk,
  title    = "Reconstructing the Einstellung Effect",
  author   = "Binz, Marcel and Schulz, Eric",
  journal  = "Computational Brain \& Behavior",
  volume   =  6,
  number   =  3,
  pages    = "526--542",
  abstract = "The Einstellung effect was first described by Abraham Luchins in
              his doctoral thesis published in 1942. The effect occurs when a
              repeated solution to old problems is applied to a new problem even
              though a more appropriate response is available. In Luchins’
              so-called water jar task, participants had to measure a specific
              amount of water using three jars of different capacities. Luchins
              found that subjects kept using methods they had applied in
              previous trials, even if a more efficient solution for the current
              trial was available: an Einstellung effect. Moreover, Luchins
              studied the different conditions that could possibly mediate this
              effect, including telling participants to pay more attention,
              changing the number of tasks, alternating between different types
              of tasks, as well as putting participants under time pressure. In
              the current work, we reconstruct and reanalyze the data of the
              various experimental conditions published in Luchins’ thesis. We
              furthermore show that a model of resource-rational decision-making
              can explain all of the observed effects. This model assumes that
              people transform prior preferences into a posterior policy to
              maximize rewards under time constraints. Taken together, our
              reconstructive and modeling results put the Einstellung effect
              under the lens of modern-day psychology and show how
              resource-rational models can explain effects that have
              historically been seen as deficiencies of human problem-solving.",
  month    =  sep,
  year     =  2023,
  doi      = "10.1007/s42113-022-00161-2",
  issn     = "2522-087X"
}

@INCOLLECTION{Weisberg1994-iw,
  title     = "Prolegomena to theories of insight in problem solving: A taxonomy
               of problems",
  author    = "Weisberg, Robert W",
  booktitle = "The Nature of Insight",
  publisher = "The MIT Press",
  year      =  1994,
  doi       = "10.7551/mitpress/4879.003.0009",
  isbn      =  9780262284370
}

@ARTICLE{Bowden2005-bk,
  title    = "New approaches to demystifying insight",
  author   = "Bowden, Edward M and Jung-Beeman, Mark and Fleck, Jessica and
              Kounios, John",
  journal  = "Trends in cognitive sciences",
  volume   =  9,
  number   =  7,
  pages    = "322--328",
  abstract = "After a person has become stuck on a problem, they sometimes
              achieve a clear and sudden solution through insight--the so-called
              Aha! experience. Because of its distinctive experience, the
              origins and characteristics of insight have received considerable
              attention historically in psychological research. However, despite
              considerable progress in characterizing insight, the underlying
              mechanisms remain mysterious. We argue that research on insight
              could be greatly advanced by supplementing traditional insight
              research, which depends on a few complex problems, with paradigms
              common in other domains of cognitive science. We describe a large
              set of mini-insight problems to which multiple methods can be
              applied, together with subjective reports to identify insight
              problem-solving. Behavioral priming and neuroimaging methods are
              providing evidence about what, where, and how neural activity
              occurs during insight. Such evidence constrains theories of
              component processes, and will help to demystify insight.",
  month    =  jul,
  year     =  2005,
  doi      = "10.1016/j.tics.2005.05.012",
  pmid     =  15953756,
  issn     = "1364-6613",
  language = "en"
}

@ARTICLE{Ongchoco2024-rc,
  title    = "People's thinking plans adapt to the problem they're trying to
              solve",
  author   = "Ongchoco, Joan Danielle K and Knobe, Joshua and Jara-Ettinger,
              Julian",
  journal  = "Cognition",
  volume   =  243,
  pages    =  105669,
  abstract = "Much of our thinking focuses on deciding what to do in situations
              where the space of possible options is too large to evaluate
              exhaustively. Previous work has found that people do this by
              learning the general value of different behaviors, and
              prioritizing thinking about high-value options in new situations.
              Is this good-action bias always the best strategy, or can thinking
              about low-value options sometimes become more beneficial? Can
              people adapt their thinking accordingly based on the situation?
              And how do we know what to think about in novel events? Here, we
              developed a block-puzzle paradigm that enabled us to measure
              people's thinking plans and compare them to a computational model
              of rational thought. We used two distinct response methods to
              explore what people think about—a self-report method, in which we
              asked people explicitly to report what they thought about, and an
              implicit response time method, in which we used people's
              decision-making times to reveal what they thought about. Our
              results suggest that people can quickly estimate the apparent
              value of different options and use this to decide what to think
              about. Critically, we find that people can flexibly prioritize
              whether to think about high-value options (Experiments 1 and 2) or
              low-value options (Experiments 3, 4, and 5), depending on the
              problem. Through computational modeling, we show that these
              thinking strategies are broadly rational, enabling people to
              maximize the value of long-term decisions. Our results suggest
              that thinking plans are flexible: What we think about depends on
              the structure of the problems we are trying to solve.",
  month    =  feb,
  year     =  2024,
  keywords = "Computational modeling; Thinking; Decision-making",
  doi      = "10.1016/j.cognition.2023.105669",
  issn     = "0010-0277"
}

@ARTICLE{Baumard2024-ch,
  title     = "Cognitive fossils: using cultural artifacts to reconstruct
               psychological changes throughout history",
  author    = "Baumard, Nicolas and Safra, Lou and Martins, Mauricio and
               Chevallier, Coralie",
  journal   = "Trends in cognitive sciences",
  publisher = "Elsevier",
  volume    =  28,
  number    =  2,
  pages     = "172--186",
  abstract  = "AbstractPsychology is crucial for understanding human history.
               When aggregated, changes in the psychology of individuals – in
               the intensity of social trust, parental care, or intellectual
               curiosity – can lead to important changes in institutions, social
               norms, and cultures. However, studying the role of psychology in
               shaping human history has been hindered by the difficulty of
               documenting the psychological traits of people who are no longer
               alive. Recent developments in psychology suggest that cultural
               artifacts reflect in part the psychological traits of the
               individuals who produced or consumed them. Cultural artifacts can
               thus serve as 'cognitive fossils' – physical imprints of the
               psychological traits of long-dead people. We review the range of
               materials available to cognitive and behavioral scientists, and
               discuss the methods that can be used to recover and quantify
               changes in psychological traits throughout history.",
  month     =  feb,
  year      =  2024,
  keywords  = "culture; art; literature; history; personality traits",
  doi       = "10.1016/j.tics.2023.10.001",
  pmid      =  37949792,
  issn      = "1364-6613,1879-307X",
  language  = "en"
}

@ARTICLE{Defeyter2007-hu,
  title    = "Developmental changes in information central to artifact
              representation: evidence from 'functional fluency' tasks",
  author   = "Defeyter, Margaret Anne and Avons, S E and German, Tamsin C",
  journal  = "Developmental science",
  volume   =  10,
  number   =  5,
  pages    = "538--546",
  abstract = "Research suggests that while information about design is a central
              feature of older children's artifact representations it may be
              less important in the artifact representations of younger
              children. Three experiments explore the pattern of responses that
              5- and 7-year-old children generate when asked to produce multiple
              uses for familiar (Experiments 1, 2) and novel (Experiment 3)
              named objects. Results showed that while older children tended to
              produce responses based on the known design function of the
              object, younger children's responses were more flexible, though
              still constrained by the mechanical structure of the object. Only
              when ignorant of a novel object's design function did older
              children produce more varied functions than did younger children.
              These results suggest that representations supporting object
              function undergo change across this period of development, with
              information about design assuming more importance later than it
              does earlier.",
  month    =  sep,
  year     =  2007,
  doi      = "10.1111/j.1467-7687.2007.00617.x",
  pmid     =  17683340,
  issn     = "1363-755X",
  language = "en"
}

@ARTICLE{Sharp2023-ox,
  title    = "Humans Adaptively Deploy Forward and Backward Prediction",
  author   = "Sharp, Paul B and Eldar, Eran",
  journal  = "https://osf.io › preprints › psyarxiv › wdbg4https://osf.io ›
              preprints › psyarxiv › wdbg4",
  abstract = "Leading models of human decision-making assume humans learn
              forward predictions, from a given state to the outcomes that
              typically follow it. Here, we show that in many situations
              decision-making can be made more efficient by relying on backward
              predictions, from a given outcome to the states that typically
              precede it. This holds specifically in environments where the
              number of possible outcomes exceeds the number of possible
              starting states (i.e., in divergent environments), because in such
              environments, backward predictions can be more compactly
              represented than forward predictions. Correspondingly, in five
              preregistered experiments, we find that humans engage in backward
              prediction in a divergent environment, and in forward prediction
              in a convergent environment. Importantly, we show that such
              predictions are flexibly employed for singles-step decisions, for
              multistep planning, and for real-world predictions. Thus, we
              establish that humans adaptively deploy forward and backward
              prediction in the service of efficient decision-making.",
  month    =  apr,
  year     =  2023,
  doi      = "10.31234/osf.io/wdbg4"
}

@ARTICLE{Carr2015-gv,
  title    = "Imitate or innovate? Children's innovation is influenced by the
              efficacy of observed behaviour",
  author   = "Carr, Kayleigh and Kendal, Rachel L and Flynn, Emma G",
  journal  = "Cognition",
  volume   =  142,
  pages    = "322--332",
  abstract = "This study investigated the age at which children judge it futile
              to imitate unreliable information, in the form of a visibly
              ineffective demonstrated solution, and deviate to produce novel
              solutions ('innovations'). Children aged 4-9 years were presented
              with a novel puzzle box, the Multiple-Methods Box (MMB), which
              offered multiple innovation opportunities to extract a reward
              using different tools, access points and exits. 209 children were
              assigned to conditions in which eight social demonstrations of a
              reward retrieval method were provided; each condition differed
              incrementally in terms of the method's efficacy (0\%, 25\%, 75\%,
              and 100\% success at extracting the reward). An additional 47
              children were assigned to a no-demonstration control condition.
              Innovative reward extractions from the MMB increased with
              decreasing efficacy of the demonstrated method. However, imitation
              remained a widely used strategy irrespective of the efficacy of
              the method being reproduced (90\% of children produced at least
              one imitative attempt, and imitated on an average of 4.9 out of 8
              attempt trials). Children were more likely to innovate in relation
              to the tool than exit, even though the latter would have been more
              effective. Overall, innovation was rare: only 12.4\% of children
              innovated by discovering at least one novel reward exit.
              Children's prioritisation of social information is consistent with
              theories of cultural evolution indicating imitation is a prepotent
              response following observation of behaviour, and that innovation
              is a rarity; so much so, that even maladaptive behaviour is
              copied.",
  month    =  sep,
  year     =  2015,
  keywords = "Asocial learning; Behaviour efficacy; Imitation; Innovation;
              Selective social learning; Trade-offs",
  doi      = "10.1016/j.cognition.2015.05.005",
  pmid     =  26072278,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Platt2009-jm,
  title     = "What can developmental and comparative cognitive neuroscience
               tell us about the adult human brain?",
  author    = "Platt, Michael L and Spelke, Elizabeth S",
  journal   = "Current opinion in neurobiology",
  publisher = "Elsevier BV",
  volume    =  19,
  number    =  1,
  pages     = "1--5",
  month     =  feb,
  year      =  2009,
  doi       = "10.1016/j.conb.2009.06.002",
  pmc       = "PMC3376397",
  pmid      =  19545995,
  issn      = "0959-4388,1873-6882",
  language  = "en"
}

@ARTICLE{Jiang2021-vx,
  title         = "Replay-Guided Adversarial Environment Design",
  author        = "Jiang, Minqi and Dennis, Michael and Parker-Holder, Jack and
                   Foerster, Jakob and Grefenstette, Edward and Rocktäschel, Tim",
  journal       = "arXiv [cs.LG]",
  abstract      = "Deep reinforcement learning (RL) agents may successfully
                   generalize to new settings if trained on an appropriately
                   diverse set of environment and task configurations.
                   Unsupervised Environment Design (UED) is a promising
                   self-supervised RL paradigm, wherein the free parameters of
                   an underspecified environment are automatically adapted
                   during training to the agent's capabilities, leading to the
                   emergence of diverse training environments. Here, we cast
                   Prioritized Level Replay (PLR), an empirically successful but
                   theoretically unmotivated method that selectively samples
                   randomly-generated training levels, as UED. We argue that by
                   curating completely random levels, PLR, too, can generate
                   novel and complex levels for effective training. This insight
                   reveals a natural class of UED methods we call Dual
                   Curriculum Design (DCD). Crucially, DCD includes both PLR and
                   a popular UED algorithm, PAIRED, as special cases and
                   inherits similar theoretical guarantees. This connection
                   allows us to develop novel theory for PLR, providing a
                   version with a robustness guarantee at Nash equilibria.
                   Furthermore, our theory suggests a highly counterintuitive
                   improvement to PLR: by stopping the agent from updating its
                   policy on uncurated levels (training on less data), we can
                   improve the convergence to Nash equilibria. Indeed, our
                   experiments confirm that our new method, PLR$^{\perp}$,
                   obtains better results on a suite of out-of-distribution,
                   zero-shot transfer tasks, in addition to demonstrating that
                   PLR$^{\perp}$ improves the performance of PAIRED, from which
                   it inherited its theoretical framework.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.02439"
}

@ARTICLE{Qiu2023-jd,
  title         = "Phenomenal Yet Puzzling: Testing Inductive Reasoning
                   Capabilities of Language Models with Hypothesis Refinement",
  author        = "Qiu, Linlu and Jiang, Liwei and Lu, Ximing and Sclar, Melanie
                   and Pyatkin, Valentina and Bhagavatula, Chandra and Wang,
                   Bailin and Kim, Yoon and Choi, Yejin and Dziri, Nouha and
                   Ren, Xiang",
  journal       = "arXiv [cs.CL]",
  abstract      = "The ability to derive underlying principles from a handful of
                   observations and then generalize to novel situations -- known
                   as inductive reasoning -- is central to human intelligence.
                   Prior work suggests that language models (LMs) often fall
                   short on inductive reasoning, despite achieving impressive
                   success on research benchmarks. In this work, we conduct a
                   systematic study of the inductive reasoning capabilities of
                   LMs through iterative hypothesis refinement, a technique that
                   more closely mirrors the human inductive process than
                   standard input-output prompting. Iterative hypothesis
                   refinement employs a three-step process: proposing,
                   selecting, and refining hypotheses in the form of textual
                   rules. By examining the intermediate rules, we observe that
                   LMs are phenomenal hypothesis proposers (i.e., generating
                   candidate rules), and when coupled with a (task-specific)
                   symbolic interpreter that is able to systematically filter
                   the proposed set of rules, this hybrid approach achieves
                   strong results across inductive reasoning benchmarks that
                   require inducing causal relations, language-like
                   instructions, and symbolic concepts. However, they also
                   behave as puzzling inductive reasoners, showing notable
                   performance gaps in rule induction (i.e., identifying
                   plausible rules) and rule application (i.e., applying
                   proposed rules to instances), suggesting that LMs are
                   proposing hypotheses without being able to actually apply the
                   rules. Through empirical and human analyses, we further
                   reveal several discrepancies between the inductive reasoning
                   processes of LMs and humans, shedding light on both the
                   potentials and limitations of using LMs in inductive
                   reasoning tasks.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.08559"
}

@ARTICLE{Heit2000-ya,
  title    = "Properties of inductive reasoning",
  author   = "Heit, E",
  journal  = "Psychonomic bulletin \& review",
  volume   =  7,
  number   =  4,
  pages    = "569--592",
  abstract = "This paper reviews the main psychological phenomena of inductive
              reasoning, covering 25 years of experimental and model-based
              research, in particular addressing four questions. First, what
              makes a case or event generalizable to other cases? Second, what
              makes a set of cases generalizable? Third, what makes a property
              or predicate projectable? Fourth, how do psychological models of
              induction address these results? The key results in inductive
              reasoning are outlined, and several recent models, including a new
              Bayesian account, are evaluated with respect to these results. In
              addition, future directions for experimental and model-based work
              are proposed.",
  month    =  dec,
  year     =  2000,
  doi      = "10.3758/bf03212996",
  pmid     =  11206199,
  issn     = "1069-9384",
  language = "en"
}

@ARTICLE{Franken2022-lt,
  title    = "Algorithms of adaptation in inductive inference",
  author   = "Fränken, Jan-Philipp and Theodoropoulos, Nikos C and Bramley, Neil
              R",
  journal  = "Cognitive psychology",
  volume   =  137,
  pages    =  101506,
  abstract = "We investigate the idea that human concept inference utilizes
              local adaptive search within a compositional mental theory space.
              To explore this, we study human judgments in a challenging task
              that involves actively gathering evidence about a symbolic rule
              governing the behavior of a simulated environment. Participants
              learn by performing mini-experiments before making generalizations
              and explicit guesses about a hidden rule. They then collect
              additional evidence themselves (Experiment 1) or observe evidence
              gathered by someone else (Experiment 2) before revising their own
              generalizations and guesses. In each case, we focus on the
              relationship between participants' initial and revised guesses
              about the hidden rule concept. We find an order effect whereby
              revised guesses are anchored to idiosyncratic elements of the
              earlier guess. To explain this pattern, we develop a family of
              process accounts that combine program induction ideas with local
              (MCMC-like) adaptation mechanisms. A particularly local variant of
              this adaptive account captures participants' hypothesis revisions
              better than a range of alternative explanations. We take this as
              suggestive that people deal with the inherent complexity of
              concept inference partly through use of local adaptive search in a
              latent compositional theory space.",
  month    =  sep,
  year     =  2022,
  keywords = "Adaptive search; Concept learning; Language of thought; Markov
              chain Monte Carlo; Program induction",
  doi      = "10.1016/j.cogpsych.2022.101506",
  pmid     =  35872374,
  issn     = "0010-0285,1095-5623",
  language = "en"
}

@INPROCEEDINGS{Mueller2022-pc,
  title     = "Coloring the Blank Slate: Pre-training Imparts a Hierarchical
               Inductive Bias to Sequence-to-sequence Models",
  author    = "Mueller, Aaron and Frank, Robert and Linzen, Tal and Wang, Luheng
               and Schuster, Sebastian",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland",
  pages     = "1352--1368",
  abstract  = "Relations between words are governed by hierarchical structure
               rather than linear ordering. Sequence-to-sequence (seq2seq)
               models, despite their success in downstream NLP applications,
               often fail to generalize in a hierarchy-sensitive manner when
               performing syntactic transformations---for example, transforming
               declarative sentences into questions. However, syntactic
               evaluations of seq2seq models have only observed models that were
               not pre-trained on natural language data before being trained to
               perform syntactic transformations, in spite of the fact that
               pre-training has been found to induce hierarchical linguistic
               generalizations in language models; in other words, the syntactic
               capabilities of seq2seq models may have been greatly understated.
               We address this gap using the pre-trained seq2seq models T5 and
               BART, as well as their multilingual variants mT5 and mBART. We
               evaluate whether they generalize hierarchically on two
               transformations in two languages: question formation and
               passivization in English and German. We find that pre-trained
               seq2seq models generalize hierarchically when performing
               syntactic transformations, whereas models trained from scratch on
               syntactic transformations do not. This result presents evidence
               for the learnability of hierarchical syntactic information from
               non-annotated natural language text while also demonstrating that
               seq2seq models are capable of syntactic generalization, though
               only after exposure to much more language data than human
               learners receive.",
  month     =  may,
  year      =  2022,
  doi       = "10.18653/v1/2022.findings-acl.106"
}

@ARTICLE{Ambridge2008-jt,
  title    = "Is Structure Dependence an Innate Constraint? New Experimental
              Evidence From Children's Complex-Question Production",
  author   = "Ambridge, Ben and Rowland, Caroline F and Pine, Julian M",
  journal  = "Cognitive science",
  volume   =  32,
  number   =  1,
  pages    = "222--255",
  abstract = "According to Crain and Nakayama (1987), when forming complex
              yes/no questions, children do not make errors such as Is the boy
              who smoking is crazy? because they have innate knowledge of
              structure dependence and so will not move the auxiliary from the
              relative clause. However, simple recurrent networks are also able
              to avoid such errors, on the basis of surface distributional
              properties of the input (Lewis \& Elman, 2001; Reali \&
              Christiansen, 2005). Two new elicited production studies revealed
              that (a) children occasionally produce structure-dependence errors
              and (b) the pattern of children's auxiliary-doubling errors (Is
              the boy who is smoking is crazy?) suggests a sensitivity to
              surface co-occurrence patterns in the input. This article
              concludes that current data do not provide any support for the
              claim that structure dependence is an innate constraint, and that
              it is possible that children form a structure-dependent grammar on
              the basis of exposure to input that exhibits this property.",
  month    =  jan,
  year     =  2008,
  doi      = "10.1080/03640210701703766",
  pmid     =  21635337,
  issn     = "0364-0213",
  language = "en"
}

@ARTICLE{Kim1997-ni,
  title     = "Distinct cortical areas associated with native and second
               languages",
  author    = "Kim, Karl H S and Relkin, Norman R and Lee, Kyoung-Min and
               Hirsch, Joy",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  388,
  number    =  6638,
  pages     = "171--174",
  abstract  = "The ability to acquire and use several languages selectively is a
               unique and essential human capacity. Here we investigate the
               fundamental question of how multiple languages are represented in
               a human brain. We applied functional magnetic resonance imaging
               (fMRI) to determine the spatial relationship between native and
               second languages in the human cortex, and show that within the
               frontal-lobe language-sensitive regions (Broca's area)1,2,3,
               second languages acquired in adulthood (‘late’ bilingual
               subjects) are spatially separated from native languages. However,
               when acquired during the early language acquisition stage of
               development (‘early’ bilingual subjects), native and second
               languages tend to be represented in common frontal cortical
               areas. In both late and early bilingual subjects, the
               temporal-lobe language-sensitive regions (Wernicke's area)1,2,3
               also show effectively little or no separation of activity based
               on the age of language acquisition. This discovery of
               language-specific regions in Broca's area advances our
               understanding of the cortical representation that underlies
               multiple language functions.",
  month     =  jul,
  year      =  1997,
  doi       = "10.1038/40623",
  issn      = "0028-0836",
  language  = "en"
}

@ARTICLE{Skeide2016-pf,
  title    = "The ontogeny of the cortical language network",
  author   = "Skeide, Michael A and Friederici, Angela D",
  journal  = "Nature reviews. Neuroscience",
  volume   =  17,
  number   =  5,
  pages    = "323--332",
  abstract = "Language-processing functions follow heterogeneous developmental
              trajectories. The human embryo can already distinguish vowels in
              utero, but grammatical complexity is usually not fully mastered
              until at least 7 years of age. Examining the current literature,
              we propose that the ontogeny of the cortical language network can
              be roughly subdivided into two main developmental stages. In the
              first stage extending over the first 3 years of life, the infant
              rapidly acquires bottom-up processing capacities, which are
              primarily implemented bilaterally in the temporal cortices. In the
              second stage continuing into adolescence, top-down processes
              emerge gradually with the increasing functional selectivity and
              structural connectivity of the left inferior frontal cortex.",
  month    =  may,
  year     =  2016,
  doi      = "10.1038/nrn.2016.23",
  pmid     =  27040907,
  issn     = "1471-003X,1471-0048",
  language = "en"
}

@ARTICLE{Kuhl2010-es,
  title    = "Brain mechanisms in early language acquisition",
  author   = "Kuhl, Patricia K",
  journal  = "Neuron",
  volume   =  67,
  number   =  5,
  pages    = "713--727",
  abstract = "The last decade has produced an explosion in neuroscience research
              examining young children's early processing of language.
              Noninvasive, safe functional brain measurements have now been
              proven feasible for use with children starting at birth. The
              phonetic level of language is especially accessible to
              experimental studies that document the innate state and the effect
              of learning on the brain. The neural signatures of learning at the
              phonetic level can be documented at a remarkably early point in
              development. Continuity in linguistic development from infants'
              earliest brain responses to phonetic stimuli is reflected in their
              language and prereading abilities in the second, third, and fifth
              year of life, a finding with theoretical and clinical impact.
              There is evidence that early mastery of the phonetic units of
              language requires learning in a social context. Neuroscience on
              early language learning is beginning to reveal the multiple brain
              systems that underlie the human language faculty.",
  month    =  sep,
  year     =  2010,
  doi      = "10.1016/j.neuron.2010.08.038",
  pmc      = "PMC2947444",
  pmid     =  20826304,
  issn     = "0896-6273,1097-4199",
  language = "en"
}

@ARTICLE{Allen2021-ji,
  title    = "Meta-strategy learning in physical problem-solving: the effect of
              embodied experience",
  author   = "Allen, Kelsey R and Smith, Kevin A and Bird, Laura-Ashleigh and
              Tenenbaum, Josh and Makin, Tamar and Cowie, Dorothy",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  volume   =  43,
  number   =  43,
  abstract = "Author(s): Allen, Kelsey R; Smith, Kevin A; Bird, Laura-Ashleigh;
              Tenenbaum, Josh; Makin, Tamar; Cowie, Dorothy | Abstract:
              `Embodied cognition' suggests that our motor experiences shape our
              cognitive and perceptual capabilities broadly, but often considers
              tasks that directly relate to or manipulate the body. Here we
              study how a history of natural embodied experience affects
              abstract physical problem-solving in a virtual, disembodied
              physical reasoning task. We compare how groups with different
              embodied experience -- congenitally limb-different versus
              two-handed children and adults -- perform on this task, and find
              that while there is no difference in overall performance,
              limb-different participants solved problems using fewer actions,
              and spent a longer time thinking before acting. This suggests that
              differences in embodied experience drive the acquisition of
              different meta-strategies for balancing acting with thinking, even
              on tasks that are designed to equalize differences in embodiment.",
  year     =  2021,
  keywords = "Social and Behavioral Sciences"
}

@ARTICLE{Camposampiero2023-sg,
  title         = "Visual Abstraction and Reasoning through Language",
  author        = "Camposampiero, Giacomo and Houmard, Loic and Estermann,
                   Benjamin and Mathys, Joël and Wattenhofer, Roger",
  journal       = "arXiv [cs.AI]",
  abstract      = "While Artificial Intelligence (AI) models have achieved human
                   or even superhuman performance in narrowly defined
                   applications, they still struggle to show signs of broader
                   and more flexible intelligence. The Abstraction and Reasoning
                   Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to
                   assess how close AI systems are to human-like cognitive
                   abilities. Most current approaches rely on carefully
                   handcrafted domain-specific languages (DSLs), which are used
                   to brute-force solutions to the tasks present in ARC. In this
                   work, we propose a general framework for solving ARC based on
                   natural language descriptions of the tasks. While not yet
                   beating state-of-the-art DSL models on ARC, we demonstrate
                   the immense potential of our approach hinted at by the
                   ability to solve previously unsolved tasks.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2303.04091"
}

@ARTICLE{Bonnasse-Gahot2022-it,
  title     = "Categorical perception: A groundwork for deep learning",
  author    = "Bonnasse-Gahot, Laurent and Nadal, Jean-Pierre",
  journal   = "Neural computation",
  publisher = "MIT Press",
  volume    =  34,
  number    =  2,
  pages     = "437--475",
  abstract  = "Classification is one of the major tasks that deep learning is
               successfully tackling. Categorization is also a fundamental
               cognitive ability. A well-known perceptual consequence of
               categorization in humans and other animals, categorical
               perception, is notably characterized by a within-category
               compression and a between-category separation: two items, close
               in input space, are perceived closer if they belong to the same
               category than if they belong to different categories. Elaborating
               on experimental and theoretical results in cognitive science,
               here we study categorical effects in artificial neural networks.
               We combine a theoretical analysis that makes use of mutual and
               Fisher information quantities and a series of numerical
               simulations on networks of increasing complexity. These formal
               and numerical analyses provide insights into the geometry of the
               neural representation in deep layers, with expansion of space
               near category boundaries and contraction far from category
               boundaries. We investigate categorical representation by using
               two complementary approaches: one mimics experiments in
               psychophysics and cognitive neuroscience by means of morphed
               continua between stimuli of different categories, while the other
               introduces a categoricality index that, for each layer in the
               network, quantifies the separability of the categories at the
               neural population level. We show on both shallow and deep neural
               networks that category learning automatically induces categorical
               perception. We further show that the deeper a layer, the stronger
               the categorical effects. As an outcome of our study, we propose a
               coherent view of the efficacy of different heuristic practices of
               the dropout regularization technique. More generally, our view,
               which finds echoes in the neuroscience literature, insists on the
               differential impact of noise in any given layer depending on the
               geometry of the neural representation that is being learned, that
               is, on how this geometry reflects the structure of the
               categories.",
  month     =  jan,
  year      =  2022,
  doi       = "10.1162/neco\_a\_01454",
  pmid      =  34758487,
  issn      = "0899-7667,1530-888X",
  language  = "en"
}

@ARTICLE{Ollinger2006-sz,
  title    = "Heuristics and representational change in two-move matchstick
              arithmetic tasks",
  author   = "Öllinger, Michael and Jones, Gary and Knoblich, Günther",
  journal  = "Advances in cognitive psychology / University of Finance and
              Management in Warsaw",
  volume   =  2,
  number   =  4,
  pages    = "239--253",
  abstract = "Insight problems are problems where the problem solver struggles
              to find a solution until * aha! * the solution suddenly appears.
              Two contemporary theories suggest that insight problems are
              difficult either because problem solvers begin with an incorrect
              representation of the problem, or that problem solvers apply
              inappropriate heuristics to the problem. The relative
              contributions of representational change and inappropriate
              heuristics on the process of insight problem solving was studied
              with a task that required the problem solver to move two
              matchsticks in order to transform an incorrect arithmetic
              statement into a correct one. Problem solvers (N = 120) worked on
              two different types of two-move matchstick arithmetic problems
              that both varied with respect to the effectiveness of heuristics
              and to the degree of a necessary representational change of the
              problem representation. A strong influence of representational
              change on solution rates was found whereas the influence of
              heuristics had minimal effects on solution rates. That is, the
              difficulty of insight problems within the two-move matchstick
              arithmetic domain is governed by the degree of representational
              change required. A model is presented that details
              representational change as the necessary condition for ensuring
              that appropriate heuristics can be applied on the proper problem
              representation. (PsycInfo Database Record (c) 2022 APA, all rights
              reserved)",
  year     =  2006,
  doi      = "10.2478/v10053-008-0059-3",
  issn     = "1895-1171"
}

@ARTICLE{Shaker2010-by,
  title    = "Towards Automatic Personalized Content Generation for Platform
              Games",
  author   = "Shaker, Noor and Yannakakis, Georgios and Togelius, Julian",
  journal  = "Proceedings of the AAAI Conference on Artificial Intelligence and
              Interactive Digital Entertainment",
  volume   =  6,
  number   =  1,
  pages    = "63--68",
  abstract = "In this paper, we show that personalized levels can be auto-
              matically generated for platform games. We build on previ- ous
              work, where models were derived that predicted player experience
              based on features of level design and on playing styles. These
              models are constructed using preference learn- ing, based on
              questionnaires administered to players after playing different
              levels. The contributions of the current pa- per are (1) more
              accurate models based on a much larger data set; (2) a mechanism
              for adapting level design parameters to given players and playing
              style; (3) evaluation of this adap- tation mechanism using both
              algorithmic and human players. The results indicate that the
              adaptation mechanism effectively optimizes level design parameters
              for particular players.",
  month    =  oct,
  year     =  2010,
  keywords = "Game Adaptation; Player Modeling",
  doi      = "10.1609/aiide.v6i1.12399",
  issn     = "2334-0924,2334-0924",
  language = "en"
}

@ARTICLE{Hospedales2022-on,
  title    = "Meta-Learning in Neural Networks: A Survey",
  author   = "{Hospedales} and {Antoniou} and {Micaelli} and {Storkey}",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  44,
  pages    = "5149--5169",
  abstract = "The field of meta-learning, or learning-to-learn, has seen a
              dramatic rise in interest in recent years. Contrary to
              conventional approaches to AI where tasks are solved from scratch
              using a fixed learning algorithm, meta-learning aims to improve
              the learning algorithm itself, given the experience of multiple
              learning episodes. This paradigm provides an opportunity to tackle
              many conventional challenges of deep learning, including data and
              computation bottlenecks, as well as generalization. This survey
              describes the contemporary meta-learning landscape. We first
              discuss definitions of meta-learning and position it with respect
              to related fields, such as transfer learning and hyperparameter
              optimization. We then propose a new taxonomy that provides a more
              comprehensive breakdown of the space of meta-learning methods
              today. We survey promising applications and successes of
              meta-learning such as few-shot learning and reinforcement
              learning. Finally, we discuss outstanding challenges and promising
              areas for future research.",
  month    =  sep,
  year     =  2022,
  doi      = "10.1109/TPAMI.2021.3079209",
  issn     = "0162-8828"
}

@ARTICLE{Skinner1948-ks,
  title     = "'Superstition' in the pigeon",
  author    = "Skinner, B F",
  journal   = "Journal of experimental psychology",
  publisher = "American Psychological Association (APA)",
  volume    =  38,
  number    =  2,
  pages     = "168--172",
  year      =  1948,
  doi       = "10.1037/h0055873",
  issn      = "0022-1015,1946-1941"
}

@ARTICLE{Casler2005-sa,
  title    = "Young children's rapid learning about artifacts",
  author   = "Casler, Krista and Kelemen, Deborah",
  journal  = "Developmental science",
  volume   =  8,
  number   =  6,
  pages    = "472--480",
  abstract = "Tool use is central to interdisciplinary debates about the
              evolution and distinctiveness of human intelligence, yet little is
              actually known about how human conceptions of artifacts develop.
              Results across these two studies show that even 2-year-olds
              approach artifacts in ways distinct from captive tool-using
              monkeys. Contrary to adult intuition, children do not treat all
              objects with appropriate properties as equally good means to an
              end. Instead, they use social information to rapidly form enduring
              artifact categories. After only one exposure to an artifact's
              functional use, children will construe the tool as 'for' that
              particular purpose and, furthermore, avoid using it for another
              feasible purpose. This teleo-functional tendency to categorize
              tools by intentional use represents a precursor to the design
              stance - the adult-like tendency to understand objects in terms of
              intended function - and provides an early foundation for
              apparently distinctive human abilities in efficient long-term tool
              use and design.",
  month    =  nov,
  year     =  2005,
  doi      = "10.1111/j.1467-7687.2005.00438.x",
  pmid     =  16246238,
  issn     = "1363-755X",
  language = "en"
}

@ARTICLE{Li2023-oq,
  title         = "{I}-{PHYRE}: Interactive Physical Reasoning",
  author        = "Li, Shiqian and Wu, Kewen and Zhang, Chi and Zhu, Yixin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Current evaluation protocols predominantly assess physical
                   reasoning in stationary scenes, creating a gap in evaluating
                   agents' abilities to interact with dynamic events. While
                   contemporary methods allow agents to modify initial scene
                   configurations and observe consequences, they lack the
                   capability to interact with events in real time. To address
                   this, we introduce I-PHYRE, a framework that challenges
                   agents to simultaneously exhibit intuitive physical
                   reasoning, multi-step planning, and in-situ intervention.
                   Here, intuitive physical reasoning refers to a quick,
                   approximate understanding of physics to address complex
                   problems; multi-step denotes the need for extensive sequence
                   planning in I-PHYRE, considering each intervention can
                   significantly alter subsequent choices; and in-situ implies
                   the necessity for timely object manipulation within a scene,
                   where minor timing deviations can result in task failure. We
                   formulate four game splits to scrutinize agents' learning and
                   generalization of essential principles of interactive
                   physical reasoning, fostering learning through interaction
                   with representative scenarios. Our exploration involves three
                   planning strategies and examines several supervised and
                   reinforcement agents' zero-shot generalization proficiency on
                   I-PHYRE. The outcomes highlight a notable gap between
                   existing learning algorithms and human performance,
                   emphasizing the imperative for more research in enhancing
                   agents with interactive physical reasoning capabilities. The
                   environment and baselines will be made publicly available.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2312.03009"
}

@ARTICLE{Kaur2020-hs,
  title    = "Unconventional tool use in infants: Using a familiar tool in a
              novel way in the second year of life",
  author   = "Kaur, Maninderjit and Detherage, Ashley and Needham, Amy Work",
  journal  = "Cognitive development",
  volume   =  54,
  pages    =  100881,
  abstract = "The current study explored the unconventional use of a familiar
              tool (a spoon) in infants during the second year of life. Infants
              between 14 and 18 months of age were given a task requiring an
              unconventional grasp of a spoon (at its bowl) and inserting its
              handle into a box’s hole. Success in this task requires infants to
              inhibit their conventional action plan (grasping the spoon’s
              handle) and instead perform the task as demonstrated by the
              experimenter at the beginning of the trial (grasping the spoon’s
              bowl). Prior research had revealed that infants in this age range
              had a low success rate in this task. To investigate the
              possibility that infants’ difficulty in the task was related to
              their inability to inhibit the well-practiced conventional
              grasping behavior (e.g., grasping the spoon’s handle), the tools
              (the spoon and a novel tool on different trials) were presented in
              a holder that restricted grasping to the bowl of the tools, and on
              the table that did not restrict the grasping of the tools. The
              study measures included overall success, time taken to succeed, as
              well as the grasping strategies used by infants during the task.
              Parents responded to questions about their infant’s
              effortful/inhibitory control and prior experience with spoons;
              associations between these measures and infants’ performance on
              the tool task were obtained. Our data revealed that type of tool
              (spoon or novel tool) and method of presentation (in the holder or
              on the table) were reliable predictors of infants’ success in the
              task. In addition, infants who had greater active use of spoons at
              home, as well as better inhibitory control showed more appropriate
              grasping of tools and required less time to successfully complete
              the task.",
  month    =  apr,
  year     =  2020,
  keywords = "Tool use; Spoon use; Infants; Inhibitory control; Prior
              experience; Grasping",
  doi      = "10.1016/j.cogdev.2020.100881",
  issn     = "0885-2014"
}

@ARTICLE{Hodges1999-ez,
  title     = "“What” and “how”: Evidence for the dissociation of object
               knowledge and mechanical problem-solving skills in the human
               brain",
  author    = "Hodges, John R and Spatt, Josef and Patterson, Karalyn",
  journal   = "Proceedings of the National Academy of Sciences of the United
               States of America",
  publisher = "National Academy of Sciences",
  volume    =  96,
  number    =  16,
  pages     =  9444,
  abstract  = "Patients with profound semantic deterioration resulting from
               temporal lobe atrophy have been reported to use many real objects
               appropriately. Does this preserved ability reflect (i) a separate
               component of the conceptual knowledge system (“action semantics”)
               or (ii) the operation of a system that is independent of
               conceptual knowledge of specific objects, and rather is
               responsible for general mechanical problem-solving skills,
               triggered by object affordances? We contrast the performance of
               three patients—two with semantic dementia and focal temporal lobe
               atrophy and the third with corticobasal degeneration and
               biparietal atrophy—on tests of real object identification and
               usage, picture-based tests of functional semantic knowledge, and
               a task requiring selection and use of novel tools. The patient
               with corticobasal degeneration showed poor novel tool selection
               and impaired use of real objects, despite near normal semantic
               knowledge of the same objects’ functions. The patients with
               semantic dementia had the expected deficit in object
               identification and functional semantics, but achieved flawless
               and effortless performance on the novel tool task. Their attempts
               to use this same mechanical problem-solving ability to deduce
               (sometimes successfully but often incorrectly) the use of the
               real objects provide no support for the hypothesis of a separate
               action-semantic system. Although the temporal lobe system clearly
               is necessary to identify “what” an object is, we suggest that
               sensory inputs to a parietal “how” system can trigger the use of
               objects without reference to object-specific conceptual
               knowledge.",
  month     =  aug,
  year      =  1999,
  doi       = "10.1073/pnas.96.16.9444",
  pmc       = "PMC17802",
  pmid      =  10430962,
  issn      = "0027-8424",
  language  = "en"
}

@ARTICLE{Hassabis2017-cm,
  title    = "Neuroscience-Inspired Artificial Intelligence",
  author   = "Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher
              and Botvinick, Matthew",
  journal  = "Neuron",
  volume   =  95,
  number   =  2,
  pages    = "245--258",
  abstract = "The fields of neuroscience and artificial intelligence (AI) have a
              long and intertwined history. In more recent times, however,
              communication and collaboration between the two fields has become
              less commonplace. In this article, we argue that better
              understanding biological brains could play a vital role in
              building intelligent machines. We survey historical interactions
              between the AI and neuroscience fields and emphasize current
              advances in AI that have been inspired by the study of neural
              computation in humans and other animals. We conclude by
              highlighting shared themes that may be key for advancing future
              research in both fields.",
  month    =  jul,
  year     =  2017,
  keywords = "artificial intelligence; brain; cognition; learning; neural
              network",
  doi      = "10.1016/j.neuron.2017.06.011",
  pmid     =  28728020,
  issn     = "0896-6273,1097-4199",
  language = "en"
}

@INPROCEEDINGS{Linderman2017-sg,
  title     = "{Bayesian Learning and Inference in Recurrent Switching Linear
               Dynamical Systems}",
  author    = "Linderman, Scott and Johnson, Matthew and Miller, Andrew and
               Adams, Ryan and Blei, David and Paninski, Liam",
  editor    = "Singh, Aarti and Zhu, Jerry",
  booktitle = "Proceedings of the 20th International Conference on Artificial
               Intelligence and Statistics",
  publisher = "PMLR",
  volume    =  54,
  pages     = "914--922",
  abstract  = "Many natural systems, such as neurons firing in the brain or
               basketball teams traversing a court, give rise to time series
               data with complex, nonlinear dynamics. We can gain insight into
               these systems by decomposing the data into segments that are each
               explained by simpler dynamic units. Building on switching linear
               dynamical systems (SLDS), we develop a model class and Bayesian
               inference algorithms that not only discover these dynamical units
               but also, by learning how transition probabilities depend on
               observations or continuous latent states, explain their switching
               behavior. Our key innovation is to design these recurrent SLDS
               models to enable recent Pólya-gamma auxiliary variable techniques
               and thus make approximate Bayesian learning and inference in
               these models easy, fast, and scalable.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2017
}

@ARTICLE{Chen2021-hh,
  title         = "Decision Transformer: Reinforcement Learning via Sequence
                   Modeling",
  author        = "Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee,
                   Kimin and Grover, Aditya and Laskin, Michael and Abbeel,
                   Pieter and Srinivas, Aravind and Mordatch, Igor",
  journal       = "arXiv [cs.LG]",
  abstract      = "We introduce a framework that abstracts Reinforcement
                   Learning (RL) as a sequence modeling problem. This allows us
                   to draw upon the simplicity and scalability of the
                   Transformer architecture, and associated advances in language
                   modeling such as GPT-x and BERT. In particular, we present
                   Decision Transformer, an architecture that casts the problem
                   of RL as conditional sequence modeling. Unlike prior
                   approaches to RL that fit value functions or compute policy
                   gradients, Decision Transformer simply outputs the optimal
                   actions by leveraging a causally masked Transformer. By
                   conditioning an autoregressive model on the desired return
                   (reward), past states, and actions, our Decision Transformer
                   model can generate future actions that achieve the desired
                   return. Despite its simplicity, Decision Transformer matches
                   or exceeds the performance of state-of-the-art model-free
                   offline RL baselines on Atari, OpenAI Gym, and Key-to-Door
                   tasks.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.01345"
}

@ARTICLE{Knoblich2001-mr,
  title     = "An eye movement study of insight problem solving",
  author    = "Knoblich, G and Ohlsson, S and Raney, G E",
  journal   = "Memory \& cognition",
  publisher = "Springer",
  volume    =  29,
  number    =  7,
  pages     = "1000--1009",
  abstract  = "The representational change theory of insight claims that insight
               problems cause impasses because they mislead problem solvers into
               constructing inappropriate initial representations. Insight is
               attained when the initial representation is changed. In the
               present study (N = 24), we tested three specific implications of
               these hypotheses against eye movements recorded while
               participants solved matchstick arithmetic problems. The results
               were consistent with the predictions, providing converging
               evidence with prior findings using solution rates and solution
               times. Alternative theories of insight can explain individual
               findings, but only the representational change theory accounts
               for both the performance data and the eye movement data. The
               present study also suggests that eye movement recordings provide
               an important new window into processes of insight problem
               solving.",
  month     =  oct,
  year      =  2001,
  doi       = "10.3758/bf03195762",
  pmid      =  11820744,
  issn      = "0090-502X",
  language  = "en"
}

@ARTICLE{Greenough1987-cn,
  title    = "Experience and brain development",
  author   = "Greenough, W T and Black, J E and Wallace, C S",
  journal  = "Child development",
  volume   =  58,
  number   =  3,
  pages    = "539--559",
  abstract = "This article considers how experience can influence the developing
              and mature brain and proposes a new categorization scheme based
              upon the type of information stored and the brain mechanisms that
              appear to be involved in storing it. In this scheme,
              experience-expectant information storage refers to incorporation
              of environmental information that is ubiquitous in the environment
              and common to all species members, such as the basic elements of
              pattern perception. Experience-expectant processes appear to have
              evolved as a neural preparation for incorporating specific
              information: in many sensory systems, synaptic connections between
              nerve cells are overproduced, and a subsequent selection process
              occurs in which aspects of sensory experience determine the
              pattern of connections that remains. Experience-dependent
              information storage refers to incorporation of environmental
              information that is idiosyncratic, or unique to the individual,
              such as learning about one's specific physical environment or
              vocabulary. The neural basis of experience-dependent processes
              appears to involve active formation of new synaptic connections in
              response to the events providing the information to be stored.
              Although these processes probably do not occur entirely
              independently of one another in development, the categories offer
              a new view more in accord with neural mechanisms than were terms
              like ``critical'' or ``sensitive period.''",
  month    =  jun,
  year     =  1987,
  doi      = "10.2307/1130197",
  pmid     =  3038480,
  issn     = "0009-3920",
  language = "en"
}

@ARTICLE{Brandle2021-bo,
  title    = "Exploration beyond bandits",
  author   = "Brändle, Franziska and Binz, Marcel and Schulz, Eric",
  journal  = "https://psyarxiv.com › ...https://psyarxiv.com › ...",
  abstract = "We describe novel approaches to study exploration in humans beyond
              simple multi-armed bandit tasks.",
  month    =  feb,
  year     =  2021,
  keywords = "Bandits; Exploration; Information search; MDPs",
  doi      = "10.31234/osf.io/9fnmj"
}

@ARTICLE{Hubel1970-co,
  title    = "The period of susceptibility to the physiological effects of
              unilateral eye closure in kittens",
  author   = "Hubel, D H and Wiesel, T N",
  journal  = "The Journal of physiology",
  volume   =  206,
  number   =  2,
  pages    = "419--436",
  abstract = "1. Kittens were visually deprived by suturing the lids of the
              right eye for various periods of time at different ages.
              Recordings were subsequently made from the striate cortex, and
              responses from the two eyes compared. As previously reported,
              monocular eye closure during the first few months of life causes a
              sharp decline in the number of cells that can be influenced by the
              previously closed eye.2. Susceptibility to the effects of eye
              closure begins suddenly near the start of the fourth week, remains
              high until some time between the sixth and eighth weeks, and then
              declines, disappearing finally around the end of the third month.
              Monocular closure for over a year in an adult cat produces no
              detectable effects.3. During the period of high susceptibility in
              the fourth and fifth weeks eye closure for as little as 3-4 days
              leads to a sharp decline in the number of cells that can be driven
              from both eyes, as well as an over-all decline in the relative
              influence of the previously closed eye. A 6-day closure is enough
              to give a reduction in the number of cells that can be driven by
              the closed eye to a fraction of the normal. The physiological
              picture is similar to that following a 3-month monocular
              deprivation from birth, in which the proportion of cells the eye
              can influence drops from 85 to about 7\%.4. Cells of the lateral
              geniculate receiving input from a deprived eye are noticeably
              smaller and paler to Nissl stain following 3 or 6 days'
              deprivation during the fourth week.5. Following 3 months of
              monocular deprivation, opening the eye for up to 5 yr produces
              only a very limited recovery in the cortical physiology, and no
              obvious recovery of the geniculate atrophy, even though
              behaviourally there is some return of vision in the deprived eye.
              Closing the normal eye, though necessary for behavioural recovery,
              has no detectable effect on the cortical physiology. The amount of
              possible recovery in the striate cortex is probably no greater if
              the period of eye closure is limited to weeks, but after a 5-week
              closure there is a definite enhancement of the recovery, even
              though it is far from complete.",
  month    =  feb,
  year     =  1970,
  doi      = "10.1113/jphysiol.1970.sp009022",
  pmc      = "PMC1348655",
  pmid     =  5498493,
  issn     = "0022-3751",
  language = "en"
}

@ARTICLE{Hoemann2019-tt,
  title    = "Emotion words, emotion concepts, and emotional development in
              children: A constructionist hypothesis",
  author   = "Hoemann, Katie and Xu, Fei and Barrett, Lisa Feldman",
  journal  = "Developmental psychology",
  volume   =  55,
  number   =  9,
  pages    = "1830--1849",
  abstract = "In this article, we integrate two constructionist approaches-the
              theory of constructed emotion and rational constructivism-to
              introduce several novel hypotheses for understanding emotional
              development. We first discuss the hypothesis that emotion
              categories are abstract and conceptual, whose instances share a
              goal-based function in a particular context but are highly
              variable in their affective, physical, and perceptual features.
              Next, we discuss the possibility that emotional development is the
              process of developing emotion concepts, and that emotion words may
              be a critical part of this process. We hypothesize that infants
              and children learn emotion categories the way they learn other
              abstract conceptual categories-by observing others use the same
              emotion word to label highly variable events. Finally, we
              hypothesize that emotional development can be understood as a
              concept construction problem: a child becomes capable of
              experiencing and perceiving emotion only when her brain develops
              the capacity to assemble ad hoc, situated emotion concepts for the
              purposes of guiding behavior and giving meaning to sensory inputs.
              Specifically, we offer a predictive processing account of
              emotional development. (PsycINFO Database Record (c) 2019 APA, all
              rights reserved).",
  month    =  sep,
  year     =  2019,
  doi      = "10.1037/dev0000686",
  pmc      = "PMC6716622",
  pmid     =  31464489,
  issn     = "0012-1649,1939-0599",
  language = "en"
}

@INCOLLECTION{Morales2019-pe,
  title     = "A neuroscience perspective on emotional development",
  author    = "Morales, Santiago and Fox, Nathan A",
  editor    = "LoBue, Vanessa",
  booktitle = "Handbook of emotional development , (pp",
  volume    =  836,
  pages     = "57--81",
  abstract  = "In the current chapter, we suggest that a neuroscientific
               approach provides a valuable perspective to the study of
               emotional development. We discuss how a neuroscientific approach
               offers unique contributions to notable practical and theoretical
               challenges in the study of the development of emotion and emotion
               regulation. We exemplify these contributions by reviewing the
               current knowledge on the development of the expression and
               regulation of fear and anxiety and their associated neural bases.
               The literature reviewed highlights the fact that a
               neuroscientific approach situates the study of emotional
               development in a larger biological and evolutionary framework
               facilitating the translation of research across species and
               providing an account for species-typical development as well as
               individual variation. A neuroscientific approach also provides
               methods that permit studying emotional development across several
               levels of analyses, providing information on the similarity
               and/or differentiation between processes and mechanisms. We also
               cover literature that exemplifies how a neuroscientific approach
               can expand our understanding of how constitutional factors and
               experiences create the brain networks that support the expression
               and regulation of emotion across development. Finally, we discuss
               outstanding issues and future directions with the neuroscientific
               approach to the study of emotional development. (PsycInfo
               Database Record (c) 2022 APA, all rights reserved)",
  year      =  2019,
  doi       = "10.1007/978-3-030-17332-6\_4"
}

@ARTICLE{Kim2013-ms,
  title    = "Effects of childhood poverty and chronic stress on emotion
              regulatory brain function in adulthood",
  author   = "Kim, Pilyoung and Evans, Gary W and Angstadt, Michael and Ho, S
              Shaun and Sripada, Chandra S and Swain, James E and Liberzon,
              Israel and Phan, K Luan",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  110,
  number   =  46,
  pages    = "18442--18447",
  abstract = "Childhood poverty has pervasive negative physical and
              psychological health sequelae in adulthood. Exposure to chronic
              stressors may be one underlying mechanism for childhood
              poverty−health relations by influencing emotion regulatory
              systems. Animal work and human cross-sectional studies both
              suggest that chronic stressor exposure is associated with amygdala
              and prefrontal cortex regions important for emotion regulation. In
              this longitudinal functional magnetic resonance imaging study of
              49 participants, we examined associations between childhood
              poverty at age 9 and adult neural circuitry activation during
              emotion regulation at age 24. To test developmental timing,
              concurrent, adult income was included as a covariate. Adults with
              lower family income at age 9 exhibited reduced ventrolateral and
              dorsolateral prefrontal cortex activity and failure to suppress
              amygdala activation during effortful regulation of negative
              emotion at age 24. In contrast to childhood income, concurrent
              adult income was not associated with neural activity during
              emotion regulation. Furthermore, chronic stressor exposure across
              childhood (at age 9, 13, and 17) mediated the relations between
              family income at age 9 and ventrolateral and dorsolateral
              prefrontal cortex activity at age 24. The findings demonstrate the
              significance of childhood chronic stress exposures in predicting
              neural outcomes during emotion regulation in adults who grew up in
              poverty.",
  year     =  2013,
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.1308240110",
  doi      = "10.1073/pnas.1308240110"
}

@ARTICLE{Zaslavsky2018-vn,
  title    = "Efficient compression in color naming and its evolution",
  author   = "Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby,
              Naftali",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  115,
  number   =  31,
  pages    = "7937--7942",
  abstract = "We derive a principled information-theoretic account of
              cross-language semantic variation. Specifically, we argue that
              languages efficiently compress ideas into words by optimizing the
              information bottleneck (IB) trade-off between the complexity and
              accuracy of the lexicon. We test this proposal in the domain of
              color naming and show that (i) color-naming systems across
              languages achieve near-optimal compression; (ii) small changes in
              a single trade-off parameter account to a large extent for
              observed cross-language variation; (iii) efficient IB color-naming
              systems exhibit soft rather than hard category boundaries and
              often leave large regions of color space inconsistently named,
              both of which phenomena are found empirically; and (iv) these IB
              systems evolve through a sequence of structural phase transitions,
              in a single process that captures key ideas associated with
              different accounts of color category evolution. These results
              suggest that a drive for information-theoretic efficiency may
              shape color-naming systems across languages. This principle is not
              specific to color, and so it may also apply to cross-language
              variation in other semantic domains.",
  month    =  jul,
  year     =  2018,
  keywords = "categories; color naming; information theory; language evolution;
              semantic typology",
  doi      = "10.1073/pnas.1800521115",
  pmc      = "PMC6077716",
  pmid     =  30021851,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Acquaviva2021-ru,
  title         = "Communicating Natural Programs to Humans and Machines",
  author        = "Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and
                   Sechopoulos, Theodoros and Wong, Catherine and Ecanow,
                   Gabrielle E and Nye, Maxwell and Tessler, Michael Henry and
                   Tenenbaum, Joshua B",
  journal       = "arXiv [cs.AI]",
  abstract      = "The Abstraction and Reasoning Corpus (ARC) is a set of
                   procedural tasks that tests an agent's ability to flexibly
                   solve novel problems. While most ARC tasks are easy for
                   humans, they are challenging for state-of-the-art AI. What
                   makes building intelligent systems that can generalize to
                   novel situations such as ARC difficult? We posit that the
                   answer might be found by studying the difference of
                   \emph{language}: While humans readily generate and interpret
                   instructions in a general language, computer systems are
                   shackled to a narrow domain-specific language that they can
                   precisely execute. We present LARC, the
                   \textit{Language-complete ARC}: a collection of natural
                   language descriptions by a group of human participants who
                   instruct each other on how to solve ARC tasks using language
                   alone, which contains successful instructions for 88\% of the
                   ARC tasks. We analyze the collected instructions as `natural
                   programs', finding that while they resemble computer
                   programs, they are distinct in two ways: First, they contain
                   a wide range of primitives; Second, they frequently leverage
                   communicative strategies beyond directly executable codes. We
                   demonstrate that these two distinctions prevent current
                   program synthesis techniques from leveraging LARC to its full
                   potential, and give concrete suggestions on how to build the
                   next-generation program synthesizers.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2106.07824"
}

@ARTICLE{Lake2023-eu,
  title     = "Human-like systematic generalization through a meta-learning
               neural network",
  author    = "Lake, Brenden M and Baroni, Marco",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  pages     = "1--7",
  abstract  = "The power of human language and thought arises from systematic
               compositionality—the algebraic ability to understand and produce
               novel combinations from known components. Fodor and Pylyshyn1
               famously argued that artificial neural networks lack this
               capacity and are therefore not viable models of the mind. Neural
               networks have advanced considerably in the years since, yet the
               systematicity challenge persists. Here we successfully address
               Fodor and Pylyshyn’s challenge by providing evidence that neural
               networks can achieve human-like systematicity when optimized for
               their compositional skills. To do so, we introduce the
               meta-learning for compositionality (MLC) approach for guiding
               training through a dynamic stream of compositional tasks. To
               compare humans and machines, we conducted human behavioural
               experiments using an instruction learning paradigm. After
               considering seven different models, we found that, in contrast to
               perfectly systematic but rigid probabilistic symbolic models, and
               perfectly flexible but unsystematic neural networks, only MLC
               achieves both the systematicity and flexibility needed for
               human-like generalization. MLC also advances the compositional
               skills of machine learning systems in several systematic
               generalization benchmarks. Our results show how a standard neural
               network architecture, optimized for its compositional skills, can
               mimic human systematic generalization in a head-to-head
               comparison. The meta-learning for compositionality approach
               achieves the systematicity and flexibility needed for human-like
               generalization.",
  month     =  oct,
  year      =  2023,
  doi       = "10.1038/s41586-023-06668-3",
  issn      = "0028-0836",
  language  = "en"
}

@ARTICLE{Ichien2023-is,
  title    = "Relational and lexical similarity in analogical reasoning and
              recognition memory: Behavioral evidence and computational
              evaluation",
  author   = "Ichien, Nicholas and Alfred, Katherine L and Baia, Sophia and
              Kraemer, David J M and Holyoak, Keith J and Bunge, Silvia A and
              Lu, Hongjing",
  journal  = "Cognitive psychology",
  volume   =  141,
  pages    =  101550,
  abstract = "We examined the role of different types of similarity in both
              analogical reasoning and recognition memory. On recognition tasks,
              people more often falsely report having seen a recombined word
              pair (e.g., flower: garden) if it instantiates the same semantic
              relation (e.g., is a part of) as a studied word pair (e.g., house:
              town). This phenomenon, termed relational luring, has been
              interpreted as evidence that explicit relation
              representations-known to play a central role in analogical
              reasoning-also impact episodic memory. We replicate and extend
              previous studies, showing that relation-based false alarms in
              recognition memory occur after participants encode word pairs
              either by making relatedness judgments about individual words
              presented sequentially, or by evaluating analogies between pairs
              of word pairs. To test alternative explanations of relational
              luring, we implemented an established model of recognition memory,
              the Generalized Context Model (GCM). Within this basic framework,
              we compared representations of word pairs based on similarities
              derived either from explicit relations or from lexical semantics
              (i.e., individual word meanings). In two experiments on
              recognition memory, best-fitting values of GCM parameters enabled
              both similarity models (even the model based solely on lexical
              semantics) to predict relational luring with comparable accuracy.
              However, the model based on explicit relations proved more robust
              to parameter variations than that based on lexical similarity. We
              found this same pattern of modeling results when applying GCM to
              an independent set of data reported by Popov, Hristova, and Anders
              (2017). In accord with previous work, we also found that explicit
              relation representations are necessary for modeling analogical
              reasoning. Our findings support the possibility that explicit
              relations, which are central to analogical reasoning, also play an
              important role in episodic memory.",
  month    =  mar,
  year     =  2023,
  keywords = "Analogy; False memory; Recognition memory; Similarity",
  doi      = "10.1016/j.cogpsych.2023.101550",
  pmid     =  36724645,
  issn     = "0010-0285,1095-5623",
  language = "en"
}

@ARTICLE{Chandra2023-kc,
  title         = "Inferring the future by imagining the past",
  author        = "Chandra, Kartik and Chen, Tony and Li, Tzu-Mao and
                   Ragan-Kelley, Jonathan and Tenenbaum, Josh",
  journal       = "arXiv [cs.AI]",
  abstract      = "A single panel of a comic book can say a lot: it can depict
                   not only where the characters currently are, but also their
                   motions, their motivations, their emotions, and what they
                   might do next. More generally, humans routinely infer complex
                   sequences of past and future events from a *static snapshot*
                   of a *dynamic scene*, even in situations they have never seen
                   before. In this paper, we model how humans make such rapid
                   and flexible inferences. Building on a long line of work in
                   cognitive science, we offer a Monte Carlo algorithm whose
                   inferences correlate well with human intuitions in a wide
                   variety of domains, while only using a small,
                   cognitively-plausible number of samples. Our key technical
                   insight is a surprising connection between our inference
                   problem and Monte Carlo path tracing, which allows us to
                   apply decades of ideas from the computer graphics community
                   to this seemingly-unrelated theory of mind task.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2305.17195"
}

@MISC{Parisi_undated-dc,
  title        = "The (Un)surprising effectiveness of pre-trained vision models
                  for control",
  author       = "Parisi, Simone and Rajeswaran, Aravind and Purushwalkam,
                  Senthil and Gupta, Abhinav",
  howpublished = "\url{https://proceedings.mlr.press/v162/parisi22a/parisi22a.pdf}",
  note         = "Accessed: 2024-5-14"
}

@ARTICLE{Stephen2008-ce,
  title    = "The Self-Organization of Insight: Entropy and Power Laws in
              Problem Solving",
  author   = "Stephen, Damian G and Dixon, James A",
  journal  = "The Journal of Problem Solving",
  volume   =  2,
  number   =  1,
  pages    =  6,
  abstract = "Explaining emergent structure remains a challenge for all areas of
              cognitive science, and problem solving is no exception. The modern
              study of insight has drawn attention to the issue of emergent
              cognitive structure in problem solving research. We propose that
              the explanation of insight is beyond the scope of conventional
              approaches to cognitive science in terms of symbolic
              representation. Cognition may be better described in terms of an
              open, nonlinear dynamical system. By this reasoning, insight would
              be the self-organization of novel structure. Self-organization is
              a well-studied phenomenon of dynamical systems theory, associated
              with specific trends in entropy and power-law behavior. We present
              work using nonlinear dynamics to capture these trends in entropy
              and power-law behavior and thus to predict the self-organization
              of novel cognitive structure in a problem-solving task. Future
              explorations of problem solving will benefit from considerations
              of the continuous nonlinear interactions among action, cognition,
              and the environment.",
  year     =  2008,
  doi      = "10.7771/1932-6246.1043",
  issn     = "1932-6246"
}

@ARTICLE{Klingberg2014-ya,
  title    = "Childhood cognitive development as a skill",
  author   = "Klingberg, Torkel",
  journal  = "Trends in cognitive sciences",
  volume   =  18,
  number   =  11,
  pages    = "573--579",
  abstract = "Theories view childhood development as being either driven by
              structural maturation of the brain or being driven by
              skill-learning. It is hypothesized here that working memory (WM)
              development during childhood is partly driven by training effects
              in the environment, and that similar neural mechanisms underlie
              training-induced plasticity and childhood development. In
              particular, the functional connectivity of a fronto-parietal
              network is suggested to be associated with WM capacity. The
              striatum, dopamine receptor D2 (DRD2) activity, and
              corticostriatal white-matter tracts, on the other hand, seem to be
              more important for plasticity and change of WM capacity during
              both training and development. In this view, the development of WM
              capacity during childhood partly involves the same mechanisms as
              skill-learning.",
  month    =  nov,
  year     =  2014,
  doi      = "10.1016/j.tics.2014.06.007",
  pmid     =  25042686,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Fisher2019-ut,
  title    = "Selective sustained attention: a developmental foundation for
              cognition",
  author   = "Fisher, Anna V",
  journal  = "Current Opinion in Psychology",
  volume   =  29,
  pages    = "248--253",
  abstract = "Higher-order cognition, particularly in real-life settings, often
              requires that parts of the sensory input be processed at the
              exclusion of others over a period of time. Consequently, this
              review focuses on the development of attention that is both
              selective (which entails processing parts of the sensory input at
              the exclusion of others) and sustained (which entails maintaining
              sensitivity to incoming stimuli for a period of time). Recent
              findings from four distinct areas of research reviewed here
              suggest that: (1) the underlying neural circuitry of selective
              sustained attention involves multiple cortical and subcortical
              brain regions; (2) selective sustained attention in infancy
              provides a developmental foundation for the emergence of executive
              function later in life; (3) suppression-based mechanisms of
              attentional selection that begin to emerge during the first year
              of life are important for memory and learning; and (4) selective
              sustained attention appears to be malleable through pre-natal and
              post-natal nutritional supplementation and interactions with
              mature social partners.",
  month    =  oct,
  year     =  2019,
  doi      = "10.1016/j.copsyc.2019.06.002",
  issn     = "2352-250X"
}

@ARTICLE{Averbeck2022-bj,
  title    = "Pruning recurrent neural networks replicates adolescent changes in
              working memory and reinforcement learning",
  author   = "Averbeck, Bruno B",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  119,
  number   =  22,
  pages    = "e2121331119",
  abstract = "Adolescent development is characterized by an improvement in
              multiple cognitive processes. While performance on cognitive
              operations improves during this period, the ability to learn new
              skills quickly, for example, a new language, decreases. During
              this time, there is substantial pruning of excitatory synapses in
              cortex and specifically in prefrontal cortex. We have trained a
              series of recurrent neural networks to solve a working memory task
              and a reinforcement learning (RL) task. Performance on both of
              these tasks is known to improve during adolescence. After
              training, we pruned the networks by removing weak synapses.
              Pruning was done incrementally, and the networks were retrained
              during pruning. We found that pruned networks trained on the
              working memory task were more resistant to distraction. The pruned
              RL networks were able to produce more accurate value estimates and
              also make optimal choices more consistently. Both results are
              consistent with developmental improvements on these tasks. Pruned
              networks, however, learned some, but not all, new problems more
              slowly. Thus, improvements in task performance can come at the
              cost of flexibility. Our results show that overproduction and
              subsequent pruning of synapses is a computationally advantageous
              approach to building a competent brain.",
  year     =  2022,
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.2121331119",
  doi      = "10.1073/pnas.2121331119"
}

@ARTICLE{Mitchell2023-af,
  title         = "Comparing Humans, {GPT}-4, and {GPT}-{4V} On Abstraction and
                   Reasoning Tasks",
  author        = "Mitchell, Melanie and Palmarini, Alessandro B and Moskvichev,
                   Arseny",
  journal       = "arXiv [cs.AI]",
  abstract      = "We explore the abstract reasoning abilities of text-only and
                   multimodal versions of GPT-4, using the ConceptARC benchmark
                   [10], which is designed to evaluate robust understanding and
                   reasoning with core-knowledge concepts. We extend the work of
                   Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
                   one-shot prompting (rather than simple, zero-shot prompts)
                   with text versions of ConceptARC tasks, and by evaluating
                   GPT-4V, the multimodal version of GPT-4, on zero- and
                   one-shot prompts using image versions of the simplest tasks.
                   Our experimental results support the conclusion that neither
                   version of GPT-4 has developed robust abstraction abilities
                   at humanlike levels.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2311.09247"
}

@ARTICLE{Lu2012-ao,
  title    = "Bayesian analogy with relational transformations",
  author   = "Lu, Hongjing and Chen, Dawn and Holyoak, Keith J",
  journal  = "Psychological review",
  volume   =  119,
  number   =  3,
  pages    = "617--648",
  abstract = "How can humans acquire relational representations that enable
              analogical inference and other forms of high-level reasoning?
              Using comparative relations as a model domain, we explore the
              possibility that bottom-up learning mechanisms applied to objects
              coded as feature vectors can yield representations of relations
              sufficient to solve analogy problems. We introduce Bayesian
              analogy with relational transformations (BART) and apply the model
              to the task of learning first-order comparative relations (e.g.,
              larger, smaller, fiercer, meeker) from a set of animal pairs.
              Inputs are coded by vectors of continuous-valued features, based
              either on human magnitude ratings, normed feature ratings (De
              Deyne et al., 2008), or outputs of the topics model (Griffiths,
              Steyvers, \& Tenenbaum, 2007). Bootstrapping from empirical
              priors, the model is able to induce first-order relations
              represented as probabilistic weight distributions, even when given
              positive examples only. These learned representations allow
              classification of novel instantiations of the relations and yield
              a symbolic distance effect of the sort obtained with both humans
              and other primates. BART then transforms its learned weight
              distributions by importance-guided mapping, thereby placing
              distinct dimensions into correspondence. These transformed
              representations allow BART to reliably solve 4-term analogies
              (e.g., larger:smaller::fiercer:meeker), a type of reasoning that
              is arguably specific to humans. Our results provide a
              proof-of-concept that structured analogies can be solved with
              representations induced from unstructured feature vectors by
              mechanisms that operate in a largely bottom-up fashion. We discuss
              potential implications for algorithmic and neural models of
              relational thinking, as well as for the evolution of abstract
              thought.",
  month    =  jul,
  year     =  2012,
  doi      = "10.1037/a0028719",
  pmid     =  22775500,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@ARTICLE{Jumper2021-ig,
  title    = "Highly accurate protein structure prediction with {AlphaFold}",
  author   = "Jumper, John and Evans, Richard and Pritzel, Alexander and Green,
              Tim and Figurnov, Michael and Ronneberger, Olaf and
              Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and
              Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl,
              Simon A A and Ballard, Andrew J and Cowie, Andrew and
              Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub
              and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman,
              David and Clancy, Ellen and Zielinski, Michal and Steinegger,
              Martin and Pacholska, Michalina and Berghammer, Tamas and
              Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and
              Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and
              Hassabis, Demis",
  journal  = "Nature",
  volume   =  596,
  number   =  7873,
  pages    = "583--589",
  abstract = "Proteins are essential to life, and understanding their structure
              can facilitate a mechanistic understanding of their function.
              Through an enormous experimental effort1-4, the structures of
              around 100,000 unique proteins have been determined5, but this
              represents a small fraction of the billions of known protein
              sequences6,7. Structural coverage is bottlenecked by the months to
              years of painstaking effort required to determine a single protein
              structure. Accurate computational approaches are needed to address
              this gap and to enable large-scale structural bioinformatics.
              Predicting the three-dimensional structure that a protein will
              adopt based solely on its amino acid sequence-the structure
              prediction component of the 'protein folding problem'8-has been an
              important open research problem for more than 50 years9. Despite
              recent progress10-14, existing methods fall far short of atomic
              accuracy, especially when no homologous structure is available.
              Here we provide the first computational method that can regularly
              predict protein structures with atomic accuracy even in cases in
              which no similar structure is known. We validated an entirely
              redesigned version of our neural network-based model, AlphaFold,
              in the challenging 14th Critical Assessment of protein Structure
              Prediction (CASP14)15, demonstrating accuracy competitive with
              experimental structures in a majority of cases and greatly
              outperforming other methods. Underpinning the latest version of
              AlphaFold is a novel machine learning approach that incorporates
              physical and biological knowledge about protein structure,
              leveraging multi-sequence alignments, into the design of the deep
              learning algorithm.",
  month    =  aug,
  year     =  2021,
  doi      = "10.1038/s41586-021-03819-2",
  pmc      = "PMC8371605",
  pmid     =  34265844,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Zador2023-gi,
  title    = "Catalyzing next-generation Artificial Intelligence through
              {NeuroAI}",
  author   = "Zador, Anthony and Escola, Sean and Richards, Blake and Ölveczky,
              Bence and Bengio, Yoshua and Boahen, Kwabena and Botvinick,
              Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath,
              Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff
              and Körding, Konrad and Koulakov, Alexei and LeCun, Yann and
              Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and
              Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and
              Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias,
              Andreas S and Tsao, Doris",
  journal  = "Nature communications",
  volume   =  14,
  number   =  1,
  pages    =  1597,
  abstract = "Neuroscience has long been an essential driver of progress in
              artificial intelligence (AI). We propose that to accelerate
              progress in AI, we must invest in fundamental research in NeuroAI.
              A core component of this is the embodied Turing test, which
              challenges AI animal models to interact with the sensorimotor
              world at skill levels akin to their living counterparts. The
              embodied Turing test shifts the focus from those capabilities like
              game playing and language that are especially well-developed or
              uniquely human to those capabilities - inherited from over 500
              million years of evolution - that are shared with all animals.
              Building models that can pass the embodied Turing test will
              provide a roadmap for the next generation of AI.",
  month    =  mar,
  year     =  2023,
  doi      = "10.1038/s41467-023-37180-x",
  pmc      = "PMC10033876",
  pmid     =  36949048,
  issn     = "2041-1723",
  language = "en"
}

@ARTICLE{Carroll2015-xp,
  title    = "Evaluating the inverse reasoning account of object discovery",
  author   = "Carroll, Christopher D and Kemp, Charles",
  journal  = "Cognition",
  volume   =  139,
  pages    = "130--153",
  abstract = "People routinely make inferences about unobserved objects. A hotel
              guest with welts on his arms, for example, will often worry about
              bed bugs. The discovery of unobserved objects almost always
              involves a backward inference from some observed effects (e.g.,
              welts) to unobserved causes (e.g., bed bugs). The inverse
              reasoning account, which is typically formalized as Bayesian
              inference, posits that the strength of a backward inference is
              closely connected to the strength of the corresponding forward
              inference from the unobserved causes to the observed effects. We
              evaluated the inverse reasoning account of object discovery in
              three experiments where participants were asked to discover the
              unobserved ``attractors'' and ``repellers'' that controlled a
              ``particle'' moving within an arena. Experiments 1 and 2 showed
              that participants often failed to provide the best explanations
              for various particle motions, even when the best explanations were
              simple and when participants enthusiastically endorsed these
              explanations when presented with them. This failure demonstrates
              that object discovery is critically dependent on the processes
              that support hypothesis generation-processes that the inverse
              reasoning account does not explain. Experiment 3 demonstrated that
              people sometimes generate explanations that are invalid even
              according to their own forward inferences, suggesting that the
              psychological processes that support forward and backward
              inference are less intertwined than the inverse reasoning account
              suggests. The experimental findings support an alternative account
              of object discovery in which people rely on heuristics to generate
              possible explanations.",
  month    =  jun,
  year     =  2015,
  keywords = "Bayesian inference; Inverse reasoning; Object discovery; Physical
              reasoning",
  doi      = "10.1016/j.cognition.2015.03.003",
  pmid     =  25824861,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Dasgupta2017-jq,
  title    = "Where do hypotheses come from?",
  author   = "Dasgupta, Ishita and Schulz, Eric and Gershman, Samuel J",
  journal  = "Cognitive psychology",
  volume   =  96,
  pages    = "1--25",
  abstract = "Why are human inferences sometimes remarkably close to the
              Bayesian ideal and other times systematically biased? In
              particular, why do humans make near-rational inferences in some
              natural domains where the candidate hypotheses are explicitly
              available, whereas tasks in similar domains requiring the
              self-generation of hypotheses produce systematic deviations from
              rational inference. We propose that these deviations arise from
              algorithmic processes approximating Bayes' rule. Specifically in
              our account, hypotheses are generated stochastically from a
              sampling process, such that the sampled hypotheses form a Monte
              Carlo approximation of the posterior. While this approximation
              will converge to the true posterior in the limit of infinite
              samples, we take a small number of samples as we expect that the
              number of samples humans take is limited. We show that this model
              recreates several well-documented experimental findings such as
              anchoring and adjustment, subadditivity, superadditivity, the
              crowd within as well as the self-generation effect, the weak
              evidence, and the dud alternative effects. We confirm the model's
              prediction that superadditivity and subadditivity can be induced
              within the same paradigm by manipulating the unpacking and
              typicality of hypotheses. We also partially confirm our model's
              prediction about the effect of time pressure and cognitive load on
              these effects.",
  month    =  aug,
  year     =  2017,
  keywords = "Bayesian inference; Hypothesis generation; Monte Carlo methods",
  doi      = "10.1016/j.cogpsych.2017.05.001",
  pmid     =  28586634,
  issn     = "0010-0285,1095-5623",
  language = "en"
}

@ARTICLE{Zhou2016-gy,
  title    = "Behavioral response inhibition and maturation of goal
              representation in prefrontal cortex after puberty",
  author   = "Zhou, Xin and Zhu, Dantong and King, Samson G and Lees, Cynthia J
              and Bennett, Allyson J and Salinas, Emilio and Stanford, Terrence
              R and Constantinidis, Christos",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  113,
  number   =  12,
  pages    = "3353--3358",
  abstract = "Executive functions including behavioral response inhibition
              mature after puberty, in tandem with structural changes in the
              prefrontal cortex. Little is known about how activity of
              prefrontal neurons relates to this profound cognitive development.
              To examine this, we tracked neuronal responses of the prefrontal
              cortex in monkeys as they transitioned from puberty into adulthood
              and compared activity at different developmental stages.
              Performance of the antisaccade task greatly improved in this
              period. Among neural mechanisms that could facilitate it,
              reduction of stimulus-driven activity, increased saccadic
              activity, or enhanced representation of the opposing goal
              location, only the latter was evident in adulthood. Greatly
              accentuated in adults, this neural correlate of vector inversion
              may be a prerequisite to the formation of a motor plan to look
              away from the stimulus. Our results suggest that the prefrontal
              mechanisms that underlie mature performance on the antisaccade
              task are more strongly associated with forming an alternative plan
              of action than with suppressing the neural impact of the prepotent
              stimulus.",
  year     =  2016,
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.1518147113",
  doi      = "10.1073/pnas.1518147113"
}

@ARTICLE{Carrasco2011-zo,
  title    = "Visual attention: the past 25 years",
  author   = "Carrasco, Marisa",
  journal  = "Vision research",
  volume   =  51,
  number   =  13,
  pages    = "1484--1525",
  abstract = "This review focuses on covert attention and how it alters early
              vision. I explain why attention is considered a selective process,
              the constructs of covert attention, spatial endogenous and
              exogenous attention, and feature-based attention. I explain how in
              the last 25 years research on attention has characterized the
              effects of covert attention on spatial filters and how attention
              influences the selection of stimuli of interest. This review
              includes the effects of spatial attention on discriminability and
              appearance in tasks mediated by contrast sensitivity and spatial
              resolution; the effects of feature-based attention on basic visual
              processes, and a comparison of the effects of spatial and
              feature-based attention. The emphasis of this review is on
              psychophysical studies, but relevant electrophysiological and
              neuroimaging studies and models regarding how and where neuronal
              responses are modulated are also discussed.",
  month    =  jul,
  year     =  2011,
  doi      = "10.1016/j.visres.2011.04.012",
  pmc      = "PMC3390154",
  pmid     =  21549742,
  issn     = "0042-6989,1878-5646",
  language = "en"
}

@ARTICLE{Knoblich2005-zx,
  title    = "Tracking the eyes to obtain insight into insight problem solving",
  author   = "Knoblich, Günther and Öllinger, Michael and Spivey, Michael J",
  abstract = "Abstract. This chapter addresses the use of the eye-movement
              methodology in research on insight problem solving. First, it
              provides a short review of classical",
  month    =  jul,
  year     =  2005,
  doi      = "10.1093/acprof:oso/9780198566816.003.0015"
}

@ARTICLE{Russell1991-pd,
  title     = "Principles of metareasoning",
  author    = "Russell, Stuart and Wefald, Eric",
  journal   = "Artificial intelligence",
  publisher = "Elsevier BV",
  volume    =  49,
  number    = "1-3",
  pages     = "361--395",
  abstract  = "In this paper we outline a general approach to the study of
               metareasoning, not in the sense of explicating the semantics of
               explicitly specified meta-level control policies, but in the
               sense of providing a basis for selecting and justifying
               computational actions. This research contributes to a developing
               attack on the problem of resource-bounded rationality, by
               providing a means for analyzing and generating optimal
               computational strategies. Because reasoning about a computation
               without doing it necessarily involves uncertainty as to its
               outcome, probability and decision theory will be our main tools.
               We develop a general formula for the utility of computations,
               this utility being derived directly from the ability of
               computations to affect an agent's external actions. We address
               some philosophical difficulties that arise in specifying this
               formula, given our assumption of limited rationality. We also
               describe a methodology for applying the theory to particular
               problem-solving systems, and provide a brief sketch of the
               resulting algorithms and their performance.",
  month     =  may,
  year      =  1991,
  doi       = "10.1016/0004-3702(91)90015-c",
  issn      = "0004-3702,1872-7921",
  language  = "en"
}

@ARTICLE{Hayes1974-he,
  title     = "Understanding written problem instructions",
  author    = "Hayes, J R and Simon, H A",
  journal   = "Knowledge and cognition.",
  publisher = "Lawrence Erlbaum, ix",
  address   = "Knowledge and cognition , Oxford, England",
  volume    =  321,
  abstract  = "Briefly summarizes the general ideas of behavior and cognitive
               theories and examines, from the point of view of the behavior
               theorist, the 11 designs for dissociation experiments reviewed by
               W. F. Brewer (see PA, Vol 54:Issue 1) and used to investigate the
               conditioning process. It is argued that experiments within these
               designs do not crucially support cognitive theory in opposition
               to behavior theory. (PsycInfo Database Record (c) 2022 APA, all
               rights reserved)",
  year      =  1974
}

@ARTICLE{Bowden2003-xu,
  title    = "Aha! Insight experience correlates with solution activation in the
              right hemisphere",
  author   = "Bowden, Edward M and Jung-Beeman, Mark",
  journal  = "Psychonomic bulletin \& review",
  volume   =  10,
  number   =  3,
  pages    = "730--737",
  abstract = "In one experiment, we tested for an association between semantic
              activation in the right hemisphere (RH) and left hemisphere (LH)
              and the Aha! experience when people recognize solutions to
              insight-like problems. The compound remote associate problems used
              in this experiment sometimes evoke an Aha! experience and
              sometimes do not On each trial, participants (N=44) attempted to
              solve these problems and, after 7 sec, named a target word, made a
              solution decision, and rated their insight experience of
              recognizing the solution. As in prior studies, the participants
              demonstrated more solution priming for solutions presented to the
              left visual field-RH (lvf-RH) than for solutions presented to the
              right visual field-LH (rvf-LH). As was predicted, following
              unsolved problems the participants showed greater priming for
              solutions that they rated as evoking an insight experience on the
              subsequent solution decision than for solutions that did not evoke
              an insight experience. This association was stronger for solutions
              presented to the lvf-RH than for those presented to the rvf-LH.
              These results tie the subjective experience of insight to an
              objective measure-semantic priming-and suggest that people have an
              Aha! experience in part because they already had semantic
              activation that could... (PsycINFO Database Record (c) 2018 APA,
              all rights reserved)",
  month    =  sep,
  year     =  2003,
  doi      = "10.3758/BF03196539",
  issn     = "1069-9384,1531-5320"
}

@ARTICLE{Simon1976-aq,
  title   = "The understanding process: Problem isomorphs",
  author  = "Simon, Herbert A and Hayes, John R",
  journal = "Cognitive psychology",
  volume  =  8,
  number  =  2,
  pages   = "165--190",
  month   =  apr,
  year    =  1976,
  doi     = "10.1016/0010-0285(76)90022-0",
  issn    = "0010-0285"
}

@ARTICLE{Silver2016-rv,
  title    = "Mastering the game of Go with deep neural networks and tree search",
  author   = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
              Arthur and Sifre, Laurent and van den Driessche, George and
              Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
              Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and
              Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
              Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and
              Graepel, Thore and Hassabis, Demis",
  journal  = "Nature",
  volume   =  529,
  number   =  7587,
  pages    = "484--489",
  abstract = "The game of Go has long been viewed as the most challenging of
              classic games for artificial intelligence owing to its enormous
              search space and the difficulty of evaluating board positions and
              moves. Here we introduce a new approach to computer Go that uses
              'value networks' to evaluate board positions and 'policy networks'
              to select moves. These deep neural networks are trained by a novel
              combination of supervised learning from human expert games, and
              reinforcement learning from games of self-play. Without any
              lookahead search, the neural networks play Go at the level of
              state-of-the-art Monte Carlo tree search programs that simulate
              thousands of random games of self-play. We also introduce a new
              search algorithm that combines Monte Carlo simulation with value
              and policy networks. Using this search algorithm, our program
              AlphaGo achieved a 99.8\% winning rate against other Go programs,
              and defeated the human European Go champion by 5 games to 0. This
              is the first time that a computer program has defeated a human
              professional player in the full-sized game of Go, a feat
              previously thought to be at least a decade away.",
  month    =  jan,
  year     =  2016,
  doi      = "10.1038/nature16961",
  pmid     =  26819042,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Carandini2011-ie,
  title    = "Normalization as a canonical neural computation",
  author   = "Carandini, Matteo and Heeger, David J",
  journal  = "Nature reviews. Neuroscience",
  volume   =  13,
  number   =  1,
  pages    = "51--62",
  abstract = "There is increasing evidence that the brain relies on a set of
              canonical neural computations, repeating them across brain regions
              and modalities to apply similar operations to different problems.
              A promising candidate for such a computation is normalization, in
              which the responses of neurons are divided by a common factor that
              typically includes the summed activity of a pool of neurons.
              Normalization was developed to explain responses in the primary
              visual cortex and is now thought to operate throughout the visual
              system, and in many other sensory modalities and brain regions.
              Normalization may underlie operations such as the representation
              of odours, the modulatory effects of visual attention, the
              encoding of value and the integration of multisensory information.
              Its presence in such a diversity of neural systems in multiple
              species, from invertebrates to mammals, suggests that it serves as
              a canonical neural computation.",
  month    =  nov,
  year     =  2011,
  doi      = "10.1038/nrn3136",
  pmc      = "PMC3273486",
  pmid     =  22108672,
  issn     = "1471-003X,1471-0048",
  language = "en"
}

@ARTICLE{Heeger1992-bj,
  title     = "Normalization of cell responses in cat striate cortex",
  author    = "Heeger, D J",
  journal   = "Visual neuroscience",
  publisher = "cambridge.org",
  volume    =  9,
  number    =  2,
  pages     = "181--197",
  abstract  = "Simple cells in the striate cortex have been depicted as
               half-wave-rectified linear operators. Complex cells have been
               depicted as energy mechanisms, constructed from the squared sum
               of the outputs of quadrature pairs of linear operators. However,
               the linear/energy model falls short of a complete explanation of
               striate cell responses. In this paper, a modified version of the
               linear/energy model is presented in which striate cells mutually
               inhibit one another, effectively normalizing their responses with
               respect to stimulus contrast. This paper reviews experimental
               measurements of striate cell responses, and shows that the new
               model explains a significantly larger body of physiological data.",
  month     =  aug,
  year      =  1992,
  doi       = "10.1017/s0952523800009640",
  pmid      =  1504027,
  issn      = "0952-5238",
  language  = "en"
}

@ARTICLE{Snowden1992-xb,
  title    = "Orientation bandwidth: the effect of spatial and temporal
              frequency",
  author   = "Snowden, R J",
  journal  = "Vision research",
  volume   =  32,
  number   =  10,
  pages    = "1965--1974",
  abstract = "The orientation bandwidths of psychophysically defined channels of
              human vision were estimated by two techniques for a wide range of
              spatial and temporal frequencies. The first technique was an
              adaptation paradigm, where the subjects' ability to see patterns
              of various orientations was measured before and after adapting to
              a high contrast pattern. The second technique evaluated subjects'
              ability to discriminate between two gratings of different
              orientations in relation to their ability to detect the patterns.
              Bandwidths were unaffected by temporal frequency at high spatial
              frequencies but increased with temporal frequency at low spatial
              frequencies. Bandwidths increased modestly with decreasing spatial
              frequency at low temporal frequencies but more drastically at high
              temporal frequencies. Both techniques gave similar results except
              for patterns with very low spatial and high temporal frequencies.
              In this region the stimulus appears ``spatial-frequency doubled''
              which may be used as a cue for the orientation discrimination
              task.",
  month    =  oct,
  year     =  1992,
  doi      = "10.1016/0042-6989(92)90056-o",
  pmid     =  1287993,
  issn     = "0042-6989",
  language = "en"
}

@ARTICLE{Mirchandani2023-ns,
  title         = "Large Language Models as General Pattern Machines",
  author        = "Mirchandani, Suvir and Xia, Fei and Florence, Pete and
                   Ichter, Brian and Driess, Danny and Arenas, Montserrat
                   Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy",
  journal       = "arXiv [cs.AI]",
  abstract      = "We observe that pre-trained large language models (LLMs) are
                   capable of autoregressively completing complex token
                   sequences -- from arbitrary ones procedurally generated by
                   probabilistic context-free grammars (PCFG), to more rich
                   spatial patterns found in the Abstraction and Reasoning
                   Corpus (ARC), a general AI benchmark, prompted in the style
                   of ASCII art. Surprisingly, pattern completion proficiency
                   can be partially retained even when the sequences are
                   expressed using tokens randomly sampled from the vocabulary.
                   These results suggest that without any additional training,
                   LLMs can serve as general sequence modelers, driven by
                   in-context learning. In this work, we investigate how these
                   zero-shot capabilities may be applied to problems in robotics
                   -- from extrapolating sequences of numbers that represent
                   states over time to complete simple motions, to least-to-most
                   prompting of reward-conditioned trajectories that can
                   discover and represent closed-loop policies (e.g., a
                   stabilizing controller for CartPole). While difficult to
                   deploy today for real systems due to latency, context size
                   limitations, and compute costs, the approach of using LLMs to
                   drive low-level control may provide an exciting glimpse into
                   how the patterns among words could be transferred to actions.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2307.04721"
}

@ARTICLE{Simsek2008-zw,
  title    = "Skill characterization based on betweenness",
  author   = "Simsek, Özgür and Barto, A",
  journal  = "Advances in neural information processing systems",
  pages    = "1497--1504",
  abstract = "We present a characterization of a useful class of skills based on
              a graphical representation of an agent's interaction with its
              environment. Our characterization uses betweenness, a measure of
              centrality on graphs. It captures and generalizes (at least
              intuitively) the bottleneck concept, which has inspired many of
              the existing skill-discovery algorithms. Our characterization may
              be used directly to form a set of skills suitable for a given
              task. More importantly, it serves as a useful guide for developing
              incremental skill-discovery algorithms that do not rely on knowing
              or representing the interaction graph in its entirety.",
  month    =  dec,
  year     =  2008,
  issn     = "1049-5258"
}

@ARTICLE{Kim2021-mm,
  title    = "Unsupervised skill discovery with bottleneck option learning",
  author   = "Kim, Jaekyeom and Park, Seohong and Kim, Gunhee",
  journal  = "International Conference on Machine Learning",
  pages    = "5572--5582",
  abstract = "Having the ability to acquire inherent skills from environments
              without any external rewards or supervision like humans is an
              important problem. We propose a novel unsupervised skill discovery
              method named Information Bottleneck Option Learning (IBOL). On top
              of the linearization of environments that promotes more various
              and distant state transitions, IBOL enables the discovery of
              diverse skills. It provides the abstraction of the skills learned
              with the information bottleneck framework for the options with
              improved stability and encouraged disentanglement. We empirically
              demonstrate that IBOL outperforms multiple state-of-the-art
              unsupervised skill discovery methods on the information-theoretic
              evaluations and downstream tasks in MuJoCo environments, including
              Ant, HalfCheetah, Hopper and D'Kitty.",
  month    =  jun,
  year     =  2021,
  eprint   = "2106.14305"
}

@ARTICLE{Stolle2002-vp,
  title   = "Learning options in reinforcement learning",
  author  = "Stolle, M and Precup, Doina",
  journal = "Sarajevo Journal of Mathematics",
  pages   = "212--223",
  month   =  aug,
  year    =  2002,
  doi     = "10.1007/3-540-45622-8\_16",
  issn    = "1840-0655"
}

@ARTICLE{Bhatt2017-ao,
  title    = "Macro actions in reinforcement learning",
  author   = "Bhatt, Varun and Krishna, K and Piratla, Vihari",
  journal  = "https://scholarworks.umass.edu › cgi ›
              viewcontenthttps://scholarworks.umass.edu › cgi › viewcontent",
  abstract = "Repeating the same action across multiple contiguous time-steps
              (“macro-actions”) in a reinforcement learning setting speeds up
              the computation and performs better (on certain tasks) than the
              case when action is chosen at every time step. In this work, we
              compare multiple algorithms that exercise the macro-actions
              heuristic on a task to learn a defense agent in Half Field Offense
              problem (HFO). A version of agent which repeats an action for a
              static number of time steps (DI-SARSA) is found to perform much
              worse than an algorithm that predicts how many time steps to
              repeat a certain action (FiGAR) on HFO. Further, we propose and
              compare a few other simple techniques that align with the
              heuristic and improve over DI-SARSA.",
  year     =  2017
}

@ARTICLE{Liu2024-mw,
  title         = "An Incomplete Loop: Deductive, Inductive, and Abductive
                   Learning in Large Language Models",
  author        = "Liu, Emmy and Neubig, Graham and Andreas, Jacob",
  journal       = "arXiv [cs.CL]",
  abstract      = "Modern language models (LMs) can learn to perform new tasks
                   in different ways: in instruction following, the target task
                   is described explicitly in natural language; in few-shot
                   prompting, the task is specified implicitly with a small
                   number of examples; in instruction inference, LMs are
                   presented with in-context examples and are then prompted to
                   generate a natural language task description before making
                   predictions. Each of these procedures may be thought of as
                   invoking a different form of reasoning: instruction following
                   involves deductive reasoning, few-shot prompting involves
                   inductive reasoning, and instruction inference involves
                   abductive reasoning. How do these different capabilities
                   relate? Across four LMs (from the gpt and llama families) and
                   two learning problems (involving arithmetic functions and
                   machine translation) we find a strong dissociation between
                   the different types of reasoning: LMs can sometimes learn
                   effectively from few-shot prompts even when they are unable
                   to explain their own prediction rules; conversely, they
                   sometimes infer useful task descriptions while completely
                   failing to learn from human-generated descriptions of the
                   same task. Our results highlight the non-systematic nature of
                   reasoning even in some of today's largest LMs, and underscore
                   the fact that very different learning mechanisms may be
                   invoked by seemingly similar prompting procedures.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2404.03028"
}

@ARTICLE{Lupyan2016-te,
  title    = "How Language Programs the Mind",
  author   = "Lupyan, Gary and Bergen, Benjamin",
  journal  = "Topics in cognitive science",
  volume   =  8,
  number   =  2,
  pages    = "408--424",
  abstract = "Many animals can be trained to perform novel tasks. People, too,
              can be trained, but sometime in early childhood people transition
              from being trainable to something qualitatively more
              powerful-being programmable. We argue that such programmability
              constitutes a leap in the way that organisms learn, interact, and
              transmit knowledge, and that what facilitates or enables this
              programmability is the learning and use of language. We then
              examine how language programs the mind and argue that it does so
              through the manipulation of embodied, sensorimotor
              representations. The role language plays in controlling mental
              representations offers important insights for understanding its
              origin and evolution.",
  month    =  apr,
  year     =  2016,
  keywords = "Categorization; Embodiment; Language; Language and thought;
              Language evolution",
  doi      = "10.1111/tops.12155",
  pmid     =  26184465,
  issn     = "1756-8757,1756-8765",
  language = "en"
}

@ARTICLE{Barot2024-md,
  title     = "“now {I} get it!”: Eureka experiences during the acquisition of
               mathematical concepts",
  author    = "Barot, Charlotte and Chevalier, Louise and Martin, Lucie and
               Izard, Véronique",
  journal   = "Open mind",
  publisher = "MIT Press",
  volume    =  8,
  pages     = "17--41",
  abstract  = "Abstract Many famous scientists have reported anecdotes where a
               new understanding occurred to them suddenly, in an unexpected
               flash. Do people generally experience such “Eureka” moments when
               learning science concepts? And if so, do these episodes truly
               vehicle sudden insights, or is this impression illusory? To
               address these questions, we developed a paradigm where
               participants were taught the mathematical concept of geodesic,
               which generalizes the common notion of straight line to straight
               trajectories drawn on curved surfaces. After studying lessons
               introducing this concept on the sphere, participants (N = 56)
               were tested on their understanding of geodesics on the sphere and
               on other surfaces. Our findings indicate that Eureka experiences
               are common when learning mathematics, with reports by 34 (61\%)
               participants. Moreover, Eureka experiences proved an accurate
               description of participants’ learning, in two respects. First,
               Eureka experiences were associated with learning and
               generalization: the participants who reported experiencing
               Eurekas performed better at identifying counterintuitive
               geodesics on new surfaces. Second, and in line with the
               firstperson experience of a sudden insight, our findings suggest
               that the learning mechanisms responsible for Eureka experiences
               are inaccessible to reflective introspection. Specifically,
               reports of Eureka experiences and of participants’ confidence in
               their own understanding were associated with different profiles
               of performance, indicating that the mechanisms bringing about
               Eureka experiences and those informing reflective confidence were
               at least partially dissociated. Learning mathematical concepts
               thus appears to involve mechanisms that operate unconsciously,
               except when a key computational step is reached and a sudden
               insight breaks into consciousness.",
  month     =  feb,
  year      =  2024,
  doi       = "10.1162/opmi\_a\_00116",
  issn      = "2470-2986",
  language  = "en"
}

@ARTICLE{Wang2023-wb,
  title         = "Hypothesis Search: Inductive Reasoning with Language Models",
  author        = "Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu,
                   Yewen and Haber, Nick and Goodman, Noah D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Inductive reasoning is a core problem-solving capacity:
                   humans can identify underlying principles from a few
                   examples, which can then be robustly generalized to novel
                   scenarios. Recent work has evaluated large language models
                   (LLMs) on inductive reasoning tasks by directly prompting
                   them yielding ``in context learning.'' This can work well for
                   straightforward inductive tasks, but performs very poorly on
                   more complex tasks such as the Abstraction and Reasoning
                   Corpus (ARC). In this work, we propose to improve the
                   inductive reasoning ability of LLMs by generating explicit
                   hypotheses at multiple levels of abstraction: we prompt the
                   LLM to propose multiple abstract hypotheses about the
                   problem, in natural language, then implement the natural
                   language hypotheses as concrete Python programs. These
                   programs can be directly verified by running on the observed
                   examples and generalized to novel inputs. Because of the
                   prohibitive cost of generation with state-of-the-art LLMs, we
                   consider a middle step to filter the set of hypotheses that
                   will be implemented into programs: we either ask the LLM to
                   summarize into a smaller set of hypotheses, or ask human
                   annotators to select a subset of the hypotheses. We verify
                   our pipeline's effectiveness on the ARC visual inductive
                   reasoning benchmark, its variant 1D-ARC, and string
                   transformation dataset SyGuS. On a random 40-problem subset
                   of ARC, our automated pipeline using LLM summaries achieves
                   27.5\% accuracy, significantly outperforming the direct
                   prompting baseline (accuracy of 12.5\%). With the minimal
                   human input of selecting from LLM-generated candidates, the
                   performance is boosted to 37.5\%. (And we argue this is a
                   lower bound on the performance of our approach without
                   filtering.) Our ablation studies show that abstract
                   hypothesis generation and concrete program representations
                   are both beneficial for LLMs to perform inductive reasoning
                   tasks.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2309.05660"
}

@ARTICLE{Vinyals2019-uw,
  title    = "Grandmaster level in {StarCraft} {II} using multi-agent
              reinforcement learning",
  author   = "Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and
              Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi,
              David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko
              and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka,
              Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou,
              John P and Jaderberg, Max and Vezhnevets, Alexander S and Leblond,
              Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David
              and Sulsky, Yury and Molloy, James and Paine, Tom L and Gulcehre,
              Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring,
              Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina
              and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and
              Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver,
              David",
  journal  = "Nature",
  volume   =  575,
  number   =  7782,
  pages    = "350--354",
  abstract = "Many real-world applications require artificial agents to compete
              and coordinate with other agents in complex environments. As a
              stepping stone to this goal, the domain of StarCraft has emerged
              as an important challenge for artificial intelligence research,
              owing to its iconic and enduring status among the most difficult
              professional esports and its relevance to the real world in terms
              of its raw complexity and multi-agent challenges. Over the course
              of a decade and numerous competitions1-3, the strongest agents
              have simplified important aspects of the game, utilized superhuman
              capabilities, or employed hand-crafted sub-systems4. Despite these
              advantages, no previous agent has come close to matching the
              overall skill of top StarCraft players. We chose to address the
              challenge of StarCraft using general-purpose learning methods that
              are in principle applicable to other complex domains: a
              multi-agent reinforcement learning algorithm that uses data from
              both human and agent games within a diverse league of continually
              adapting strategies and counter-strategies, each represented by
              deep neural networks5,6. We evaluated our agent, AlphaStar, in the
              full game of StarCraft II, through a series of online games
              against human players. AlphaStar was rated at Grandmaster level
              for all three StarCraft races and above 99.8\% of officially
              ranked human players.",
  month    =  nov,
  year     =  2019,
  doi      = "10.1038/s41586-019-1724-z",
  pmid     =  31666705,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Silver2017-mz,
  title    = "Mastering the game of Go without human knowledge",
  author   = "Silver, David and Schrittwieser, Julian and Simonyan, Karen and
              Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert,
              Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and
              Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre,
              Laurent and van den Driessche, George and Graepel, Thore and
              Hassabis, Demis",
  journal  = "Nature",
  volume   =  550,
  number   =  7676,
  pages    = "354--359",
  abstract = "A long-standing goal of artificial intelligence is an algorithm
              that learns, tabula rasa, superhuman proficiency in challenging
              domains. Recently, AlphaGo became the first program to defeat a
              world champion in the game of Go. The tree search in AlphaGo
              evaluated positions and selected moves using deep neural networks.
              These neural networks were trained by supervised learning from
              human expert moves, and by reinforcement learning from self-play.
              Here we introduce an algorithm based solely on reinforcement
              learning, without human data, guidance or domain knowledge beyond
              game rules. AlphaGo becomes its own teacher: a neural network is
              trained to predict AlphaGo's own move selections and also the
              winner of AlphaGo's games. This neural network improves the
              strength of the tree search, resulting in higher quality move
              selection and stronger self-play in the next iteration. Starting
              tabula rasa, our new program AlphaGo Zero achieved superhuman
              performance, winning 100-0 against the previously published,
              champion-defeating AlphaGo.",
  month    =  oct,
  year     =  2017,
  doi      = "10.1038/nature24270",
  pmid     =  29052630,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Davis2023-vw,
  title         = "Benchmarks for Automated Commonsense Reasoning: A Survey",
  author        = "Davis, Ernest",
  journal       = "arXiv [cs.AI]",
  abstract      = "More than one hundred benchmarks have been developed to test
                   the commonsense knowledge and commonsense reasoning abilities
                   of artificial intelligence (AI) systems. However, these
                   benchmarks are often flawed and many aspects of common sense
                   remain untested. Consequently, we do not currently have any
                   reliable way of measuring to what extent existing AI systems
                   have achieved these abilities. This paper surveys the
                   development and uses of AI commonsense benchmarks. We discuss
                   the nature of common sense; the role of common sense in AI;
                   the goals served by constructing commonsense benchmarks; and
                   desirable features of commonsense benchmarks. We analyze the
                   common flaws in benchmarks, and we argue that it is
                   worthwhile to invest the work needed ensure that benchmark
                   examples are consistently high quality. We survey the various
                   methods of constructing commonsense benchmarks. We enumerate
                   139 commonsense benchmarks that have been developed: 102
                   text-based, 18 image-based, 12 video based, and 7 simulated
                   physical environments. We discuss the gaps in the existing
                   benchmarks and aspects of commonsense reasoning that are not
                   addressed in any existing benchmark. We conclude with a
                   number of recommendations for future development of
                   commonsense AI benchmarks.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2302.04752"
}

@ARTICLE{Newell1980-rk,
  title     = "Physical symbol systems",
  author    = "Newell, Allen",
  journal   = "Cognitive science",
  publisher = "Wiley",
  volume    =  4,
  number    =  2,
  pages     = "135--183",
  abstract  = "On the occasion of a first conference on Cognitive Science, it
               seems appropriate to review the basis of common understanding
               between the various disciplines. In my estimate, the most
               fundamental contribution so far of artificial intelligence and
               computer science to the joint enterprise of cognitive science has
               been the notion of a physical symbol system, i.e., the concept of
               a broad class of systems capable of having and manipulating
               symbols, yet realizable in the physical universe. The notion of
               symbol so defined is internal to this concept, so it becomes a
               hypothesis that this notion of symbols includes the symbols that
               we humans use every day of our lives. In this paper we attempt
               systematically, but plainly, to lay out the nature of physical
               symbol systems. Such a review is in ways familiar, but not
               thereby useless. Restatement of fundamentals is an important
               exercise.",
  month     =  apr,
  year      =  1980,
  doi       = "10.1207/s15516709cog0402\_2",
  issn      = "0364-0213,1551-6709",
  language  = "en"
}

@ARTICLE{Lynch2022-su,
  title         = "Interactive Language: Talking to Robots in Real Time",
  author        = "Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and
                   Ding, Tianli and Betker, James and Baruch, Robert and
                   Armstrong, Travis and Florence, Pete",
  journal       = "arXiv [cs.RO]",
  abstract      = "We present a framework for building interactive, real-time,
                   natural language-instructable robots in the real world, and
                   we open source related assets (dataset, environment,
                   benchmark, and policies). Trained with behavioral cloning on
                   a dataset of hundreds of thousands of language-annotated
                   trajectories, a produced policy can proficiently execute an
                   order of magnitude more commands than previous works:
                   specifically we estimate a 93.5\% success rate on a set of
                   87,000 unique natural language strings specifying raw
                   end-to-end visuo-linguo-motor skills in the real world. We
                   find that the same policy is capable of being guided by a
                   human via real-time language to address a wide range of
                   precise long-horizon rearrangement goals, e.g. ``make a
                   smiley face out of blocks''. The dataset we release comprises
                   nearly 600,000 language-labeled trajectories, an order of
                   magnitude larger than prior available datasets. We hope the
                   demonstrated results and associated assets enable further
                   advancement of helpful, capable,
                   natural-language-interactable robots. See videos at
                   https://interactive-language.github.io.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2210.06407"
}

@ARTICLE{Matan2001-mg,
  title    = "Developmental changes within the core of artifact concepts",
  author   = "Matan, A and Carey, S",
  journal  = "Cognition",
  volume   =  78,
  number   =  1,
  pages    = "1--26",
  abstract = "Three experiments addressed the relative importance of original
              function and current function in artifact categorization. Subjects
              were asked to judge whether an artifact that was made for one
              purpose (e.g. making tea) and was currently being used for another
              purpose (e.g. watering flowers) was a teapot or a watering can.
              Experiment 1 replicated the finding by Hall (1995) (unpublished
              manuscript) that adults rely on the original function of an
              artifact over a current function in their kind judgments.
              Experiments 2 and 3 revealed that whereas the kind judgments of
              6-year-olds, like those of adults, patterned with the original
              function, those of 4-year-olds did not. Four-year-olds were
              influenced by the order in which the functions were mentioned in
              the story. Further, in their justifications 6-year-olds and adults
              referred to the origin of the objects, whereas 4-year-olds
              virtually never did. We conclude that 6-year-olds have begun to
              organize their understanding of artifacts around the notion of
              original function, and that 4-year-olds have not. The data are
              discussed as they bear on children's understanding of the design
              stance (Dennett, D. C. (1987).",
  month    =  jan,
  year     =  2001,
  doi      = "10.1016/s0010-0277(00)00094-9",
  pmid     =  11062320,
  issn     = "0010-0277",
  language = "en"
}

@ARTICLE{German2002-bg,
  title    = "Function and the origins of the design stance",
  author   = "German, Tim P and Johnson, Susan C",
  journal  = "Journal of cognition and development: official journal of the
              Cognitive Development Society",
  volume   =  3,
  number   =  3,
  pages    = "279--300",
  abstract = "Reports 2 experiments addressing children's developing
              understanding of design. Experiment 1 (subjects included 25 female
              and 15 male undergraduate students, aged 18-46 yrs; and 14 female
              and 18 male 5-yr-olds) showed that although 5-year-old children
              judged an object's function according to its original design
              rather than a subsequent accidental activity, design was not
              preferred over a subsequent intentional use. Adults select the
              design function in both cases, suggesting that children's initial
              assignment of function is based on any intended goals for which
              the object is used. Experiment 2 (subjects included 27 women and
              13 men, aged 18-46; and 17 girls and 15 boys, aged 5 yrs) compared
              assignment of function with object categorization, demonstrating
              that although 5-year-old children's assignment of object function
              is based on any goals associated with the object, their
              categorization is adult-like and based on the category intended by
              the object's creator (over a category assigned by another agent).
              The authors conclude that preschoolers appreciate the link between
              creators and categories before constructing a design stance
              supporting reasoning about artifact functions. (PsycINFO Database
              Record (c) 2016 APA, all rights reserved)",
  month    =  aug,
  year     =  2002,
  doi      = "10.1207/S15327647JCD0303\_2",
  issn     = "1524-8372,1532-7647"
}

@ARTICLE{Tsividis2021-yc,
  title         = "Human-Level Reinforcement Learning through Theory-Based
                   Modeling, Exploration, and Planning",
  author        = "Tsividis, Pedro A and Loula, Joao and Burga, Jake and Foss,
                   Nathan and Campero, Andres and Pouncy, Thomas and Gershman,
                   Samuel J and Tenenbaum, Joshua B",
  journal       = "arXiv [cs.AI]",
  abstract      = "Reinforcement learning (RL) studies how an agent comes to
                   achieve reward in an environment through interactions over
                   time. Recent advances in machine RL have surpassed human
                   expertise at the world's oldest board games and many classic
                   video games, but they require vast quantities of experience
                   to learn successfully -- none of today's algorithms account
                   for the human ability to learn so many different tasks, so
                   quickly. Here we propose a new approach to this challenge
                   based on a particularly strong form of model-based RL which
                   we call Theory-Based Reinforcement Learning, because it uses
                   human-like intuitive theories -- rich, abstract, causal
                   models of physical objects, intentional agents, and their
                   interactions -- to explore and model an environment, and plan
                   effectively to achieve task goals. We instantiate the
                   approach in a video game playing agent called EMPA (the
                   Exploring, Modeling, and Planning Agent), which performs
                   Bayesian inference to learn probabilistic generative models
                   expressed as programs for a game-engine simulator, and runs
                   internal simulations over these models to support efficient
                   object-based, relational exploration and heuristic planning.
                   EMPA closely matches human learning efficiency on a suite of
                   90 challenging Atari-style video games, learning new games in
                   just minutes of game play and generalizing robustly to new
                   game situations and new levels. The model also captures
                   fine-grained structure in people's exploration trajectories
                   and learning dynamics. Its design and behavior suggest a way
                   forward for building more general human-like AI systems.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2107.12544"
}

@ARTICLE{Browne2012-hm,
  title     = "A Survey of Monte Carlo Tree Search Methods",
  author    = "Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and
               Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and
               Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and
               Colton, Simon",
  journal   = "IEEE Transactions on Computational Intelligence in AI and Games",
  publisher = "IEEE",
  volume    =  4,
  number    =  1,
  pages     = "1--43",
  abstract  = "Monte Carlo tree search (MCTS) is a recently proposed search
               method that combines the precision of tree search with the
               generality of random sampling. It has received considerable
               interest due to its spectacular success in the difficult problem
               of computer Go, but has also proved beneficial in a range of
               other domains. This paper is a survey of the literature to date,
               intended to provide a snapshot of the state of the art after the
               first five years of MCTS research. We outline the core
               algorithm's derivation, impart some structure on the many
               variations and enhancements that have been proposed, and
               summarize the results from the key game and nongame domains to
               which MCTS methods have been applied. A number of open research
               questions indicate that the field is ripe for future work.",
  month     =  mar,
  year      =  2012,
  doi       = "10.1109/TCIAIG.2012.2186810",
  issn      = "1943-068X,1943-0698"
}

@INCOLLECTION{Lombrozo2019-my,
  title     = "“Learning by Thinking” in Science and in Everyday Life",
  author    = "Lombrozo, Tania",
  publisher = "Oxford University Press",
  abstract  = "AbstractThis chapter introduces “learning by thinking” (LbT) as a
               form of learning distinct from familiar forms of learning through
               observation. When learning b",
  month     =  dec,
  year      =  2019,
  doi       = "10.1093/oso/9780190212308.003.0010",
  language  = "en"
}

@ARTICLE{Patel2022-fg,
  title    = "Mapping Language Models to Grounded Conceptual Spaces",
  author   = "Patel, Roma and Pavlick, Ellie",
  abstract = "A fundamental criticism of text-only language models (LMs) is
              their lack of grounding---that is, the ability to tie a word for
              which they have learned a representation, to its actual use in the
              world. However, despite this limitation, large pre-trained LMs
              have been shown to have a remarkable grasp of the conceptual
              structure of language, as demonstrated by their ability to answer
              questions, generate fluent text, or make inferences about
              entities, objects, and properties that they have never physically
              observed. In this work we investigate the extent to which the rich
              conceptual structure that LMs learn indeed reflects the conceptual
              structure of the non-linguistic world---which is something that
              LMs have never observed. We do this by testing whether the LMs can
              learn to map an entire conceptual domain (e.g., direction or
              colour) onto a grounded world representation given only a small
              number of examples. For example, we show a model what the word
              ``left`` means using a textual depiction of a grid world, and
              assess how well it can generalise to related concepts, for
              example, the word ``right'', in a similar grid world. We
              investigate a range of generative language models of varying sizes
              (including GPT-2 and GPT-3), and see that although the smaller
              models struggle to perform this mapping, the largest model can not
              only learn to ground the concepts that it is explicitly taught,
              but appears to generalise to several instances of unseen concepts
              as well. Our results suggest an alternative means of building
              grounded language models: rather than learning grounded
              representations ``from scratch'', it is possible that large
              text-only models learn a sufficiently rich conceptual structure
              that could allow them to be grounded in a data-efficient way.",
  month    =  may,
  year     =  2022
}

@ARTICLE{Das2023-ho,
  title     = "Combining Functional and Automata Synthesis to Discover Causal
               Reactive Programs",
  author    = "Das, Ria and Tenenbaum, Joshua B and Solar-Lezama, Armando and
               Tavares, Zenna",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "POPL",
  pages     = "1628--1658",
  abstract  = "We present a new algorithm that synthesizes functional reactive
               programs from observation data. The key novelty is to iterate
               between a functional synthesis step, which attempts to generate a
               transition function over observed states, and an automata
               synthesis step, which adds any additional latent state necessary
               to fully account for the observations. We develop a functional
               reactive DSL called Autumn that can express a rich variety of
               causal dynamics in time-varying, Atari-style grid worlds, and
               apply our method to synthesize Autumn programs from data. We
               evaluate our algorithm on a benchmark suite of 30 Autumn programs
               as well as a third-party corpus of grid-world-style video games.
               We find that our algorithm synthesizes 27 out of 30 programs in
               our benchmark suite and 21 out of 27 programs from the
               third-party corpus, including several programs describing complex
               latent state transformations, and from input traces containing
               hundreds of observations. We expect that our approach will
               provide a template for how to integrate functional and automata
               synthesis in other induction domains.",
  month     =  jan,
  year      =  2023,
  keywords  = "causal, synthesis, automata, reactive",
  doi       = "10.1145/3571249"
}

@ARTICLE{Bigelow2022-sr,
  title    = "Non-Commitment in Mental Imagery",
  author   = "Bigelow, Eric J and McCoy, John and Ullman, Tomer D",
  abstract = "We examine non-commitment in the imagination. Across 5 studies (N
              > 1, 800), we find that most people are non-committal about basic
              aspects of their mental images, including features that would be
              readily apparent in real images. While previous work on the
              imagination has discussed the possibility of non-commitment, this
              paper is the first, to our knowledge, to examine this
              systematically and empirically. We find that people do not commit
              to basic properties of specified mental scenes (Studies 1 and 2),
              and that people report non-commitment rather than uncertainty or
              forgetfulness (Study 3). Such non-commitment is present even for
              people with generally vivid imaginations, and those who report
              imagining the specified scene very vividly (Studies 4a, 4b).
              People readily confabulate properties of their mental images when
              non-commitment is not offered as an explicit option (Study 5).
              Taken together, these results establish non-commitment as a
              pervasive component of mental imagery.",
  month    =  oct,
  year     =  2022,
  keywords = "Imagination; mental imagery; non-commitment; vividness",
  doi      = "10.31234/osf.io/pn4zd"
}

@INCOLLECTION{Danek2018-by,
  title     = "Magic tricks, sudden restructuring, and the Aha! experience",
  author    = "Danek, Amory H",
  booktitle = "Insight",
  publisher = "Routledge",
  pages     = "51--78",
  month     =  mar,
  year      =  2018,
  doi       = "10.4324/9781315268118-4",
  isbn      =  9781315268118
}

@ARTICLE{Ohlsson1992-mf,
  title     = "Information-processing explanations of insight and related
               phenomena",
  author    = "Ohlsson, Stellan",
  journal   = "Advances in the psychology of thinking",
  publisher = "academia.edu",
  volume    =  1,
  pages     = "1--44",
  abstract  = "The psychology of thinking encounters several perennial problems;
               formal discipline and transfer, general intelligence versus
               specific abilities, nature versus nurture, the relation between
               language and thought, the relation between logic and reasoning,
               visual imagery– these and a handful of other deep problems have
               exercised psycholog-ists throughout the history of systematic
               inquiry into human cognition. Perennial problems are rarely
               solved. Instead, they serve as loci for the accumulation of
               knowledge. Each generation of …",
  year      =  1992
}

@ARTICLE{Gordon2020-qy,
  title         = "Compressing {BERT}: Studying the Effects of Weight Pruning on
                   Transfer Learning",
  author        = "Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained universal feature extractors, such as BERT for
                   natural language processing and VGG for computer vision, have
                   become effective methods for improving deep learning models
                   without requiring more labeled data. While effective, feature
                   extractors like BERT may be prohibitively large for some
                   deployment scenarios. We explore weight pruning for BERT and
                   ask: how does compression during pre-training affect transfer
                   learning? We find that pruning affects transfer learning in
                   three broad regimes. Low levels of pruning (30-40\%) do not
                   affect pre-training loss or transfer to downstream tasks at
                   all. Medium levels of pruning increase the pre-training loss
                   and prevent useful pre-training information from being
                   transferred to downstream tasks. High levels of pruning
                   additionally prevent models from fitting downstream datasets,
                   leading to further degradation. Finally, we observe that
                   fine-tuning BERT on a specific task does not improve its
                   prunability. We conclude that BERT can be pruned once during
                   pre-training rather than separately for each task without
                   affecting performance.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2002.08307"
}

@ARTICLE{Yang2022-ho,
  title         = "{TextPruner}: A Model Pruning Toolkit for Pre-Trained
                   Language Models",
  author        = "Yang, Ziqing and Cui, Yiming and Chen, Zhigang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained language models have been prevailed in natural
                   language processing and become the backbones of many NLP
                   tasks, but the demands for computational resources have
                   limited their applications. In this paper, we introduce
                   TextPruner, an open-source model pruning toolkit designed for
                   pre-trained language models, targeting fast and easy model
                   compression. TextPruner offers structured post-training
                   pruning methods, including vocabulary pruning and transformer
                   pruning, and can be applied to various models and tasks. We
                   also propose a self-supervised pruning method that can be
                   applied without the labeled data. Our experiments with
                   several NLP tasks demonstrate the ability of TextPruner to
                   reduce the model size without re-training the model.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.15996"
}

@ARTICLE{Zafrir2021-pm,
  title         = "Prune Once for All: Sparse Pre-Trained Language Models",
  author        = "Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen,
                   Haihao and Wasserblat, Moshe",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based language models are applied to a wide range
                   of applications in natural language processing. However, they
                   are inefficient and difficult to deploy. In recent years,
                   many compression algorithms have been proposed to increase
                   the implementation efficiency of large Transformer-based
                   models on target hardware. In this work we present a new
                   method for training sparse pre-trained Transformer language
                   models by integrating weight pruning and model distillation.
                   These sparse pre-trained models can be used to transfer
                   learning for a wide range of tasks while maintaining their
                   sparsity pattern. We demonstrate our method with three known
                   architectures to create sparse pre-trained BERT-Base,
                   BERT-Large and DistilBERT. We show how the compressed sparse
                   pre-trained models we trained transfer their knowledge to
                   five different downstream natural language tasks with minimal
                   accuracy loss. Moreover, we show how to further compress the
                   sparse models' weights to 8bit precision using
                   quantization-aware training. For example, with our sparse
                   pre-trained BERT-Large fine-tuned on SQuADv1.1 and quantized
                   to 8bit we achieve a compression ratio of $40$X for the
                   encoder with less than $1\%$ accuracy loss. To the best of
                   our knowledge, our results show the best
                   compression-to-accuracy ratio for BERT-Base, BERT-Large, and
                   DistilBERT.",
  month         =  nov,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2111.05754"
}

@ARTICLE{Lucas2012-hg,
  title    = "Superspace extrapolation reveals inductive biases in function
              learning",
  author   = "Lucas, C and Sterling, D and Kemp, C",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science
              Society, 34(34)",
  abstract = "We introduce a new approach for exploring how humans learn and
              represent functional relationships based on limited obser-
              vations. We focus on a problem called superspace extrapo- lation,
              where learners observe training examples drawn from an
              n-dimensional space and must extrapolate to an n + 1- dimensional
              superspace of the training examples. Many exist- ing psychological
              models predict that superspace extrapolation should be
              fundamentally underdetermined, but we show that humans are able to
              extrapolate both linear and non-linear func- tions under these
              conditions. We also show that a Bayesian model can account for our
              results given a hypothesis space that includes families of simple
              functional relationships.",
  year     =  2012
}

@ARTICLE{Hu2019-wh,
  title   = "Hierarchical decision making by generating and following natural
             language instructions",
  author  = "Hu, Hengyuan and Yarats, Denis and Gong, Qucheng and Tian, Yuandong
             and Lewis, Mike",
  journal = "Advances in neural information processing systems",
  volume  =  32,
  year    =  2019,
  issn    = "1049-5258"
}

@MISC{Lampinen_undated-zq,
  title  = "Tell me why! Explanations support learning relational and causal
            structure",
  author = "Lampinen, Andrew K and Roy, Nicholas A and Dasgupta, Ishita and
            Chan, Stephanie C Y and Tam, Allison C and Mc Clelland, James L and
            Yan, Chen and Santoro, Adam and Rabinowitz, Neil C and Wang, Jane X
            and Hill, Felix"
}

@ARTICLE{Yao2022-vk,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  journal       = "arXiv [cs.CL]",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting) and
                   acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the use
                   of LLMs to generate both reasoning traces and task-specific
                   actions in an interleaved manner, allowing for greater
                   synergy between the two: reasoning traces help the model
                   induce, track, and update action plans as well as handle
                   exceptions, while actions allow it to interface with external
                   sources, such as knowledge bases or environments, to gather
                   additional information. We apply our approach, named ReAct,
                   to a diverse set of language and decision making tasks and
                   demonstrate its effectiveness over state-of-the-art
                   baselines, as well as improved human interpretability and
                   trustworthiness over methods without reasoning or acting
                   components. Concretely, on question answering (HotpotQA) and
                   fact verification (Fever), ReAct overcomes issues of
                   hallucination and error propagation prevalent in
                   chain-of-thought reasoning by interacting with a simple
                   Wikipedia API, and generates human-like task-solving
                   trajectories that are more interpretable than baselines
                   without reasoning traces. On two interactive decision making
                   benchmarks (ALFWorld and WebShop), ReAct outperforms
                   imitation and reinforcement learning methods by an absolute
                   success rate of 34\% and 10\% respectively, while being
                   prompted with only one or two in-context examples. Project
                   site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03629"
}

@ARTICLE{Battaglia2013-eg,
  title    = "Simulation as an engine of physical scene understanding",
  author   = "Battaglia, Peter W and Hamrick, Jessica B and Tenenbaum, Joshua B",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  110,
  number   =  45,
  pages    = "18327--18332",
  abstract = "In a glance, we can perceive whether a stack of dishes will
              topple, a branch will support a child's weight, a grocery bag is
              poorly packed and liable to tear or crush its contents, or a tool
              is firmly attached to a table or free to be lifted. Such rapid
              physical inferences are central to how people interact with the
              world and with each other, yet their computational underpinnings
              are poorly understood. We propose a model based on an ``intuitive
              physics engine,'' a cognitive mechanism similar to computer
              engines that simulate rich physics in video games and graphics,
              but that uses approximate, probabilistic simulations to make
              robust and fast inferences in complex natural scenes where crucial
              information is unobserved. This single model fits data from five
              distinct psychophysical tasks, captures several illusions and
              biases, and explains core aspects of human mental models and
              common-sense reasoning that are instrumental to how humans
              understand their everyday world.",
  month    =  nov,
  year     =  2013,
  doi      = "10.1073/pnas.1306572110",
  pmc      = "PMC3831455",
  pmid     =  24145417,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Jones2015-ss,
  title    = "Models of Semantic Memory",
  author   = "Jones, Michael N and Willits, Jon and Dennis, Simon",
  abstract = "Abstract. Meaning is a fundamental component of nearly all aspects
              of human cognition, but formal models of semantic memory have
              classically lagged behind many",
  month    =  apr,
  year     =  2015,
  doi      = "10.1093/oxfordhb/9780199957996.013.11"
}

@ARTICLE{Squire1992-cp,
  title    = "Declarative and nondeclarative memory: multiple brain systems
              supporting learning and memory",
  author   = "Squire, L R",
  journal  = "Journal of cognitive neuroscience",
  volume   =  4,
  number   =  3,
  pages    = "232--243",
  abstract = "Abstract The topic of multiple forms of memory is considered from
              a biological point of view. Fact-and-event (declarative, explicit)
              memory is contrasted with a collection of non conscious
              (non-declarative, implicit) memory abilities including skills and
              habits, priming, and simple conditioning. Recent evidence is
              reviewed indicating that declarative and non declarative forms of
              memory have different operating characteristics and depend on
              separate brain systems. A brain-systems framework for
              understanding memory phenomena is developed in light of lesion
              studies involving rats, monkeys, and humans, as well as recent
              studies with normal humans using the divided visual field
              technique, event-related potentials, and positron emission
              tomography (PET).",
  year     =  1992,
  doi      = "10.1162/jocn.1992.4.3.232",
  pmid     =  23964880,
  issn     = "0898-929X",
  language = "en"
}

@ARTICLE{Weir2022-cv,
  title         = "One-Shot Learning from a Demonstration with Hierarchical
                   Latent Language",
  author        = "Weir, Nathaniel and Yuan, Xingdi and Côté, Marc-Alexandre and
                   Hausknecht, Matthew and Laroche, Romain and Momennejad, Ida
                   and Van Seijen, Harm and Van Durme, Benjamin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Humans have the capability, aided by the expressive
                   compositionality of their language, to learn quickly by
                   demonstration. They are able to describe unseen
                   task-performing procedures and generalize their execution to
                   other contexts. In this work, we introduce DescribeWorld, an
                   environment designed to test this sort of generalization
                   skill in grounded agents, where tasks are linguistically and
                   procedurally composed of elementary concepts. The agent
                   observes a single task demonstration in a Minecraft-like grid
                   world, and is then asked to carry out the same task in a new
                   map. To enable such a level of generalization, we propose a
                   neural agent infused with hierarchical latent language--both
                   at the level of task inference and subtask planning. Our
                   agent first generates a textual description of the
                   demonstrated unseen task, then leverages this description to
                   replicate it. Through multiple evaluation scenarios and a
                   suite of generalization tests, we find that agents that
                   perform text-based inference are better equipped for the
                   challenge under a random split of tasks.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.04806"
}

@ARTICLE{Kumar2022-lq,
  title         = "Using Natural Language and Program Abstractions to Instill
                   Human Inductive Biases in Machines",
  author        = "Kumar, Sreejan and Correa, Carlos G and Dasgupta, Ishita and
                   Marjieh, Raja and Hu, Michael Y and Hawkins, Robert D and
                   Daw, Nathaniel D and Cohen, Jonathan D and Narasimhan,
                   Karthik and Griffiths, Thomas L",
  journal       = "arXiv [cs.AI]",
  abstract      = "Strong inductive biases give humans the ability to quickly
                   learn to perform a variety of tasks. Although meta-learning
                   is a method to endow neural networks with useful inductive
                   biases, agents trained by meta-learning may sometimes acquire
                   very different strategies from humans. We show that
                   co-training these agents on predicting representations from
                   natural language task descriptions and programs induced to
                   generate such tasks guides them toward more human-like
                   inductive biases. Human-generated language descriptions and
                   program induction models that add new learned primitives both
                   contain abstract concepts that can compress description
                   length. Co-training on these representations result in more
                   human-like behavior in downstream meta-reinforcement learning
                   agents than less abstract controls (synthetic language
                   descriptions, program induction without learned primitives),
                   suggesting that the abstraction supported by these
                   representations is key.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2205.11558"
}

@ARTICLE{Glenberg2000-dv,
  title    = "Symbol Grounding and Meaning: A Comparison of High-Dimensional and
              Embodied Theories of Meaning",
  author   = "Glenberg, Arthur M and Robertson, David A",
  journal  = "Journal of memory and language",
  volume   =  43,
  number   =  3,
  pages    = "379--401",
  abstract = "Latent Semantic Analysis (Landauer \& Dumais, 1997) and Hyperspace
              Analogue to Language (Burgess \& Lund, 1997) model meaning as the
              relations among abstract symbols that are arbitrarily related to
              what they signify. These symbols are ungrounded in that they are
              not tied to perceptual experience or action. Because the symbols
              are ungrounded, they cannot, in principle, capture the meaning of
              novel situations. In contrast, participants in three experiments
              found it trivially easy to discriminate between descriptions of
              sensible novel situations (e.g., using a newspaper to protect
              one's face from the wind) and nonsense novel situations (e.g.,
              using a matchbook to protect one's face from the wind). These
              results support the Indexical Hypothesis that the meaning of a
              sentence is constructed by (a) indexing words and phrases to real
              objects or perceptual, analog symbols; (b) deriving affordances
              from the objects and symbols; and (c) meshing the affordances
              under the guidance of syntax.",
  month    =  oct,
  year     =  2000,
  keywords = "meaning; language; embodiment; computational models; Latent
              Semantic Analysis; Hyperspace Analogue to Language",
  doi      = "10.1006/jmla.2000.2714",
  issn     = "0749-596X"
}

@ARTICLE{Peterson2021-nw,
  title    = "Using large-scale experiments and machine learning to discover
              theories of human decision-making",
  author   = "Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and
              Reichman, Daniel and Griffiths, Thomas L",
  journal  = "Science",
  volume   =  372,
  number   =  6547,
  pages    = "1209--1214",
  abstract = "Predicting and understanding how people make decisions has been a
              long-standing goal in many fields, with quantitative models of
              human decision-making informing research in both the social
              sciences and engineering. We show how progress toward this goal
              can be accelerated by using large datasets to power
              machine-learning algorithms that are constrained to produce
              interpretable psychological theories. Conducting the largest
              experiment on risky choice to date and analyzing the results using
              gradient-based optimization of differentiable decision theories
              implemented through artificial neural networks, we were able to
              recapitulate historical discoveries, establish that there is room
              to improve on existing theories, and discover a new, more accurate
              model of human decision-making in a form that preserves the
              insights from centuries of research.",
  month    =  jun,
  year     =  2021,
  doi      = "10.1126/science.abe2629",
  pmid     =  34112693,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@ARTICLE{Merrill2022-sv,
  title         = "Entailment Semantics Can Be Extracted from an Ideal Language
                   Model",
  author        = "Merrill, William and Warstadt, Alex and Linzen, Tal",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models are often trained on text alone, without
                   additional grounding. There is debate as to how much of
                   natural language semantics can be inferred from such a
                   procedure. We prove that entailment judgments between
                   sentences can be extracted from an ideal language model that
                   has perfectly learned its target distribution, assuming the
                   training sentences are generated by Gricean agents, i.e.,
                   agents who follow fundamental principles of communication
                   from the linguistic theory of pragmatics. We also show
                   entailment judgments can be decoded from the predictions of a
                   language model trained on such Gricean data. Our results
                   reveal a pathway for understanding the semantic information
                   encoded in unlabeled linguistic data and a potential
                   framework for extracting semantics from language models.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2209.12407"
}

@ARTICLE{Odouard2022-dv,
  title         = "Evaluating Understanding on Conceptual Abstraction Benchmarks",
  author        = "Odouard, Victor Vikram and Mitchell, Melanie",
  journal       = "arXiv [cs.AI]",
  abstract      = "A long-held objective in AI is to build systems that
                   understand concepts in a humanlike way. Setting aside the
                   difficulty of building such a system, even trying to evaluate
                   one is a challenge, due to present-day AI's relative opacity
                   and its proclivity for finding shortcut solutions. This is
                   exacerbated by humans' tendency to anthropomorphize, assuming
                   that a system that can recognize one instance of a concept
                   must also understand other instances, as a human would. In
                   this paper, we argue that understanding a concept requires
                   the ability to use it in varied contexts. Accordingly, we
                   propose systematic evaluations centered around concepts, by
                   probing a system's ability to use a given concept in many
                   different instantiations. We present case studies of such an
                   evaluations on two domains -- RAVEN (inspired by Raven's
                   Progressive Matrices) and the Abstraction and Reasoning
                   Corpus (ARC) -- that have been used to develop and assess
                   abstraction abilities in AI systems. Our concept-based
                   approach to evaluation reveals information about AI systems
                   that conventional test sets would have left hidden.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2206.14187"
}

@ARTICLE{Assouel2022-er,
  title    = "Object-centric Compositional Imagination for Visual Abstract
              Reasoning",
  author   = "Assouel, Rim and Rodriguez, Pau and Taslakian, Perouz and Vazquez,
              David and Bengio, Yoshua",
  abstract = "Like humans devoid of imagination, current machine learning
              systems lack the ability to adapt to new, unexpected situations by
              foreseeing them, which makes them unable to solve new tasks by
              analogical reasoning. In this work, we introduce a new
              compositional imagination framework that improves a model's
              ability to generalize. One of the key components of our framework
              is object-centric inductive biases that enables models to perceive
              the environment as a series of objects, properties, and
              transformations. By composing these key ingredients, it is
              possible to generate new unseen tasks that, when used to train the
              model, improve generalization. Experiments on a simplified version
              of the Abstraction and Reasoning Corpus (ARC) demonstrate the
              effectiveness of our framework.",
  month    =  jun,
  year     =  2022
}

@INCOLLECTION{Weisberg2020-zy,
  title     = "Insight in Problem-Solving and Creative Thinking",
  author    = "Weisberg, Robert W",
  booktitle = "Rethinking Creativity: Inside-the-Box Thinking as the Basis for
               Innovation",
  publisher = "Cambridge University Press",
  pages     = "215--248",
  abstract  = "Rethinking Creativity - September 2020",
  month     =  sep,
  year      =  2020,
  keywords  = "Insight in problem-solving; insight in creativity; Aha!
               experiences; analytic thinking in insight",
  doi       = "10.1017/9781108785259.007"
}

@ARTICLE{Vysogorets2021-wi,
  title         = "Connectivity Matters: Neural Network Pruning Through the Lens
                   of Effective Sparsity",
  author        = "Vysogorets, Artem and Kempe, Julia",
  journal       = "arXiv [cs.LG]",
  abstract      = "Neural network pruning is a fruitful area of research with
                   surging interest in high sparsity regimes. Benchmarking in
                   this domain heavily relies on faithful representation of the
                   sparsity of subnetworks, which has been traditionally
                   computed as the fraction of removed connections (direct
                   sparsity). This definition, however, fails to recognize
                   unpruned parameters that detached from input or output layers
                   of underlying subnetworks, potentially underestimating actual
                   effective sparsity: the fraction of inactivated connections.
                   While this effect might be negligible for moderately pruned
                   networks (up to 10-100 compression rates), we find that it
                   plays an increasing role for thinner subnetworks, greatly
                   distorting comparison between different pruning algorithms.
                   For example, we show that effective compression of a randomly
                   pruned LeNet-300-100 can be orders of magnitude larger than
                   its direct counterpart, while no discrepancy is ever observed
                   when using SynFlow for pruning [Tanaka et al., 2020]. In this
                   work, we adopt the lens of effective sparsity to reevaluate
                   several recent pruning algorithms on common benchmark
                   architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and
                   discover that their absolute and relative performance changes
                   dramatically in this new and more appropriate framework. To
                   aim for effective, rather than direct, sparsity, we develop a
                   low-cost extension to most pruning algorithms. Further,
                   equipped with effective sparsity as a reference frame, we
                   partially reconfirm that random pruning with appropriate
                   sparsity allocation across layers performs as well or better
                   than more sophisticated algorithms for pruning at
                   initialization [Su et al., 2020]. In response to this
                   observation, using a simple analogy of pressure distribution
                   in coupled cylinders from physics, we design novel layerwise
                   sparsity quotas that outperform all existing baselines in the
                   context of random pruning.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.02306"
}

@ARTICLE{Brown2020-gd,
  title         = "Language Models are Few-Shot Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has demonstrated substantial gains on many NLP
                   tasks and benchmarks by pre-training on a large corpus of
                   text followed by fine-tuning on a specific task. While
                   typically task-agnostic in architecture, this method still
                   requires task-specific fine-tuning datasets of thousands or
                   tens of thousands of examples. By contrast, humans can
                   generally perform a new language task from only a few
                   examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we
                   show that scaling up language models greatly improves
                   task-agnostic, few-shot performance, sometimes even reaching
                   competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive
                   language model with 175 billion parameters, 10x more than any
                   previous non-sparse language model, and test its performance
                   in the few-shot setting. For all tasks, GPT-3 is applied
                   without any gradient updates or fine-tuning, with tasks and
                   few-shot demonstrations specified purely via text interaction
                   with the model. GPT-3 achieves strong performance on many NLP
                   datasets, including translation, question-answering, and
                   cloze tasks, as well as several tasks that require on-the-fly
                   reasoning or domain adaptation, such as unscrambling words,
                   using a novel word in a sentence, or performing 3-digit
                   arithmetic. At the same time, we also identify some datasets
                   where GPT-3's few-shot learning still struggles, as well as
                   some datasets where GPT-3 faces methodological issues related
                   to training on large web corpora. Finally, we find that GPT-3
                   can generate samples of news articles which human evaluators
                   have difficulty distinguishing from articles written by
                   humans. We discuss broader societal impacts of this finding
                   and of GPT-3 in general.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.14165"
}

@MISC{Wang_undated-gn,
  title        = "Glue: A multi-task benchmark and analysis platform for natural
                  language understand-{ING}",
  author       = "Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill,
                  Felix and Levy, Omer and Bowman, Samuel R",
  abstract     = "For natural language understanding (NLU) technology to be
                  maximally useful, it must be able to process language in a way
                  that is not exclusive to a single task, genre, or dataset. In
                  pursuit of this objective, we introduce the General Language
                  Understanding Evaluation (GLUE) benchmark, a collection of
                  tools for evaluating the performance of models across a
                  diverse set of existing NLU tasks. By including tasks with
                  limited training data, GLUE is designed to favor and encourage
                  models that share general linguistic knowledge across tasks.
                  GLUE also includes a hand-crafted diagnostic test suite that
                  enables detailed linguistic analysis of models. We evaluate
                  baselines based on current methods for transfer and
                  representation learning and find that multi-task training on
                  all tasks performs better than training a separate model per
                  task. However, the low absolute performance of our best model
                  indicates the need for improved general NLU systems.",
  howpublished = "\url{https://openreview.net/pdf?id=rJ4km2R5t7}",
  note         = "Accessed: 2022-10-21"
}

@BOOK{noauthor_2015-uh,
  title    = "The Conceptual Mind : New Directions in the Study of Concepts",
  abstract = "``The study of concepts has advanced dramatically in recent years,
              with exciting new findings and theoretical developments. Core
              concepts have been investigated in greater depth and new lines of
              inquiry have blossomed, with researchers from an ever broader
              range of disciplines making important contributions. In this
              volume, leading philosophers and cognitive scientists offer
              original essays that present the state-of-the-art in the study of
              concepts. These essays, all commissioned for this book, do not
              merely present the usual surveys and overviews; rather, they offer
              the latest work on concepts by a diverse group of theorists as
              well as discussions of the ideas that should guide research over
              the next decade. The book is an essential companion volume to the
              earlier Concepts: Core Readings, the definitive source for classic
              texts on the nature of concepts. The essays cover concepts as they
              relate to animal cognition, the brain, evolution, perception, and
              language, concepts across cultures, concept acquisition and
              conceptual change, concepts and normativity, concepts in context,
              and conceptual individuation''–MIT CogNet.",
  year     =  2015,
  keywords = "Electronic books",
  lccn     =  2014034214,
  isbn     =  9780262326872
}

@INPROCEEDINGS{Krizhevsky2012-bm,
  title     = "{{ImageNet}} Classification with Deep Convolutional Neural
               Networks",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J and Bottou, L and Weinberger, K Q",
  booktitle = "Advances in Neural Information Processing Systems",
  publisher = "Curran Associates, Inc.",
  volume    =  25,
  year      =  2012
}

@ARTICLE{Thomas_McCoy2020-zy,
  title         = "Universal linguistic inductive biases via meta-learning",
  author        = "Thomas McCoy, R and Grant, Erin and Smolensky, Paul and
                   Griffiths, Thomas L and Linzen, Tal",
  journal       = "arXiv [cs.CL]",
  abstract      = "How do learners acquire languages from the limited data
                   available to them? This process must involve some inductive
                   biases - factors that affect how a learner generalizes - but
                   it is unclear which inductive biases can explain observed
                   patterns in language acquisition. To facilitate computational
                   modeling aimed at addressing this question, we introduce a
                   framework for giving particular linguistic inductive biases
                   to a neural network model; such a model can then be used to
                   empirically explore the effects of those inductive biases.
                   This framework disentangles universal inductive biases, which
                   are encoded in the initial values of a neural network's
                   parameters, from non-universal factors, which the neural
                   network must learn from data in a given language. The initial
                   state that encodes the inductive biases is found with
                   meta-learning, a technique through which a model discovers
                   how to acquire new languages more easily via exposure to many
                   possible languages. By controlling the properties of the
                   languages that are used during meta-learning, we can control
                   the inductive biases that meta-learning imparts. We
                   demonstrate this framework with a case study based on
                   syllable structure. First, we specify the inductive biases
                   that we intend to give our model, and then we translate those
                   inductive biases into a space of languages from which a model
                   can meta-learn. Finally, using existing analysis techniques,
                   we verify that our approach has imparted the linguistic
                   inductive biases that it was intended to impart.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.16324"
}

@INCOLLECTION{Bassok2012-sg,
  title     = "Problem Solving",
  author    = "Bassok, Miriam and Novick, Laura R",
  publisher = "Oxford University Press",
  abstract  = "Abstract. This chapter follows the historical development of
               research on problem solving. It begins with a description of two
               research traditions that addressed",
  month     =  mar,
  year      =  2012,
  doi       = "10.1093/oxfordhb/9780199734689.013.0021",
  language  = "en"
}

@ARTICLE{Ellis2022-je,
  title    = "Synthesizing theories of human language with Bayesian program
              induction",
  author   = "Ellis, Kevin and Albright, Adam and Solar-Lezama, Armando and
              Tenenbaum, Joshua B and O'Donnell, Timothy J",
  journal  = "Nature communications",
  volume   =  13,
  number   =  1,
  pages    =  5024,
  abstract = "Automated, data-driven construction and evaluation of scientific
              models and theories is a long-standing challenge in artificial
              intelligence. We present a framework for algorithmically
              synthesizing models of a basic part of human language:
              morpho-phonology, the system that builds word forms from sounds.
              We integrate Bayesian inference with program synthesis and
              representations inspired by linguistic theory and cognitive models
              of learning and discovery. Across 70 datasets from 58 diverse
              languages, our system synthesizes human-interpretable models for
              core aspects of each language's morpho-phonology, sometimes
              approaching models posited by human linguists. Joint inference
              across all 70 data sets automatically synthesizes a meta-model
              encoding interpretable cross-language typological tendencies.
              Finally, the same algorithm captures few-shot learning dynamics,
              acquiring new morphophonological rules from just one or a few
              examples. These results suggest routes to more powerful
              machine-enabled discovery of interpretable models in linguistics
              and other scientific domains.",
  month    =  aug,
  year     =  2022,
  doi      = "10.1038/s41467-022-32012-w",
  pmc      = "PMC9427767",
  pmid     =  36042196,
  issn     = "2041-1723",
  language = "en"
}

@ARTICLE{Dunsmoor2014-np,
  title    = "Stimulus typicality determines how broadly fear is generalized",
  author   = "Dunsmoor, Joseph E and Murphy, Gregory L",
  journal  = "Psychological science",
  volume   =  25,
  number   =  9,
  pages    = "1816--1821",
  abstract = "The ability to represent knowledge at the category level promotes
              the transfer of learning. How this ability integrates with basic
              forms of conditioned learning is unknown but could explain why
              conditioned fear is overgeneralized after aversive experiences. We
              examined the impact of stimulus typicality--an important
              determinant of category-based induction--on fear learning and
              generalization. Typicality is known to affect the strength of
              categorical arguments; a premise involving typical exemplars
              (e.g., sparrow) is believed to apply to other members, whereas a
              premise about atypical exemplars (e.g., penguin) generalizes more
              narrowly to similar items. We adopted this framework to human fear
              conditioning and found that fear conditioned to typical exemplars
              generalized more readily to atypical members than vice versa,
              despite equal feature overlap across conditions. These findings
              have implications for understanding why some fearful events lead
              to broad overgeneralization of fear whereas others are regarded as
              isolated episodes.",
  month    =  sep,
  year     =  2014,
  keywords = "category-based induction; fear conditioning; generalization;
              reasoning; skin conductance responses",
  doi      = "10.1177/0956797614535401",
  pmid     =  25015685,
  issn     = "0956-7976,1467-9280",
  language = "en"
}

@ARTICLE{Valkov2018-sq,
  title         = "{HOUDINI}: Lifelong Learning as Program Synthesis",
  author        = "Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and
                   Sutton, Charles and Chaudhuri, Swarat",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present a neurosymbolic framework for the lifelong
                   learning of algorithmic tasks that mix perception and
                   procedural reasoning. Reusing high-level concepts across
                   domains and learning complex procedures are key challenges in
                   lifelong learning. We show that a program synthesis approach
                   that combines gradient descent with combinatorial search over
                   programs can be a more effective response to these challenges
                   than purely neural methods. Our framework, called HOUDINI,
                   represents neural networks as strongly typed, differentiable
                   functional programs that use symbolic higher-order
                   combinators to compose a library of neural functions. Our
                   learning algorithm consists of: (1) a symbolic program
                   synthesizer that performs a type-directed search over
                   parameterized programs, and decides on the library functions
                   to reuse, and the architectures to combine them, while
                   learning a sequence of tasks; and (2) a neural module that
                   trains these programs using stochastic gradient descent. We
                   evaluate HOUDINI on three benchmarks that combine perception
                   with the algorithmic tasks of counting, summing, and
                   shortest-path computation. Our experiments show that HOUDINI
                   transfers high-level concepts more effectively than
                   traditional transfer learning and progressive neural
                   networks, and that the typed representation of networks
                   significantly accelerates the search.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1804.00218"
}

@ARTICLE{Raffel2019-jw,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   Text-to-Text Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of transfer
                   learning techniques for NLP by introducing a unified
                   framework that converts all text-based language problems into
                   a text-to-text format. Our systematic study compares
                   pre-training objectives, architectures, unlabeled data sets,
                   transfer approaches, and other factors on dozens of language
                   understanding tasks. By combining the insights from our
                   exploration with scale and our new ``Colossal Clean Crawled
                   Corpus'', we achieve state-of-the-art results on many
                   benchmarks covering summarization, question answering, text
                   classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683"
}

@INPROCEEDINGS{Bramley2018-dl,
  title     = "Grounding compositional hypothesis generation in specific
               instances",
  author    = "Bramley, Neil R and Rothe, Anslem and Tenenbaum, Joshua B and Xu,
               Fei and Gureckis, Todd M",
  booktitle = "Proceedings of the Annual Meeting of the Cognitive Science
               Society",
  volume    =  40,
  abstract  = "number of recent computational models treat concept learn-ing as
               a form of probabilistic rule induction in a space
               oflanguage-like, compositional concepts. Inference in such
               mod-els frequently requires repeatedly sampling from a
               (infinite)distribution over possible concept rules and comparing
               theirrelative likelihood in light of current data or evidence.
               How-ever, we argue that most existing algorithms for top-down
               sam-pling are inefficient and cognitively implausible accounts
               ofhuman hypothesis generation. As a result, we propose
               analternative, Instance Driven Generator (IDG), that
               constructsbottom-up hypotheses directly out of encountered
               positive in-stances of a concept. Using a novel rule induction
               task basedon the children’s game Zendo, we compare these
               “bottom-up” and “top-down” approaches to inference. We find
               thatthe bottom-up IDG model accounts better for human infer-ences
               and results in a computationally more tractable
               inferencemechanism for concept learning models based on a
               probabilis-tic language of thought.",
  year      =  2018
}

@ARTICLE{Nisbett1977-oz,
  title    = "Telling more than we can know: Verbal reports on mental processes",
  author   = "Nisbett, Richard E and Wilson, Timothy D",
  journal  = "Psychological review",
  volume   =  84,
  number   =  3,
  pages    = "231--259",
  abstract = "Reviews evidence which suggests that there may be little or no
              direct introspective access to higher order cognitive processes.
              Ss are sometimes (a) unaware of the existence of a stimulus that
              importantly influenced a response, (b) unaware of the existence of
              the response, and (c) unaware that the stimulus has affected the
              response. It is proposed that when people attempt to report on
              their cognitive processes, that is, on the processes mediating the
              effects of a stimulus on a response, they do not do so on the
              basis of any true introspection. Instead, their reports are based
              on a priori, implicit causal theories, or judgments about the
              extent to which a particular stimulus is a plausible cause of a
              given response. This suggests that though people may not be able
              to observe directly their cognitive processes, they will sometimes
              be able to report accurately about them. Accurate reports will
              occur when influential stimuli are salient and are plausible
              causes of the responses they produce, and will not occur when
              stimuli are not salient or are not plausible causes. (86 ref)
              (PsycINFO Database Record (c) 2018 APA, all rights reserved)",
  month    =  may,
  year     =  1977,
  doi      = "10.1037/0033-295X.84.3.231",
  issn     = "0033-295X,1939-1471"
}

@ARTICLE{Griffiths2019-hj,
  title    = "Doing more with less: Meta-reasoning and meta-learning in humans
              and machines",
  author   = "Griffiths, Thomas L and Callaway, Frederick and Chang, Michael B
              and Grant, Erin and Krueger, Paul M and Lieder, Falk",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "24--30",
  abstract = "Artificial intelligence systems use an increasing amount of
              computation and data to solve very specific problems. By contrast,
              human minds solve a wide range of problems using a fixed amount of
              computation and limited experience. We identify two abilities that
              we see as crucial to this kind of general intelligence:
              meta-reasoning (deciding how to allocate computational resources)
              and meta-learning (modeling the learning environment to make
              better use of limited data). We summarize the relevant AI
              literature and relate the resulting ideas to recent work in
              psychology. (PsycINFO Database Record (c) 2019 APA, all rights
              reserved)",
  month    =  oct,
  year     =  2019,
  doi      = "10.1016/j.cobeha.2019.01.005",
  issn     = "2352-1546,2352-1554"
}

@ARTICLE{De_Jong1998-ci,
  title     = "Scientific Discovery Learning with Computer Simulations of
               Conceptual Domains",
  author    = "De Jong, Ton and Van Joolingen, Wouter R",
  journal   = "Review of educational research",
  publisher = "American Educational Research Association",
  volume    =  68,
  number    =  2,
  pages     = "179--201",
  abstract  = "Scientific discovery learning is a highly self-directed and
               constructivistic form of learning. A computer simulation is a
               type of computer-based environment that is well suited for
               discovery learning, the main task of the learner being to infer,
               through experimentation, characteristics of the model underlying
               the simulation. In this article we give a review of the observed
               effectiveness and efficiency of discovery learning in simulation
               environments together with problems that learners may encounter
               in discovery learning, and we discuss how simulations may be
               combined with instructional support in order to overcome these
               problems.",
  month     =  jun,
  year      =  1998,
  doi       = "10.3102/00346543068002179",
  issn      = "0034-6543"
}

@ARTICLE{Gopnik2012-iu,
  title    = "Scientific thinking in young children: theoretical advances,
              empirical research, and policy implications",
  author   = "Gopnik, Alison",
  journal  = "Science",
  volume   =  337,
  number   =  6102,
  pages    = "1623--1627",
  abstract = "New theoretical ideas and empirical research show that very young
              children's learning and thinking are strikingly similar to much
              learning and thinking in science. Preschoolers test hypotheses
              against data and make causal inferences; they learn from
              statistics and informal experimentation, and from watching and
              listening to others. The mathematical framework of probabilistic
              models and Bayesian inference can describe this learning in
              precise ways. These discoveries have implications for early
              childhood education and policy. In particular, they suggest both
              that early childhood experience is extremely important and that
              the trend toward more structured and academic early childhood
              programs is misguided.",
  month    =  sep,
  year     =  2012,
  doi      = "10.1126/science.1223416",
  pmid     =  23019643,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@ARTICLE{Xu2022-eq,
  title         = "{EST}: Evaluating Scientific Thinking in Artificial Agents",
  author        = "Xu, Manjie and Jiang, Guangyuan and Zhang, Chi and Zhu,
                   Song-Chun and Zhu, Yixin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Theoretical ideas and empirical research have shown us a
                   seemingly surprising result: children, even very young
                   toddlers, demonstrate learning and thinking in a strikingly
                   similar manner to scientific reasoning in formal research.
                   Encountering a novel phenomenon, children make hypotheses
                   against data, conduct causal inference from observation, test
                   their theory via experimentation, and correct the proposition
                   if inconsistency arises. Rounds of such processes continue
                   until the underlying mechanism is found. Towards building
                   machines that can learn and think like people, one natural
                   question for us to ask is: whether the intelligence we
                   achieve today manages to perform such a scientific thinking
                   process, and if any, at what level. In this work, we devise
                   the EST environment for evaluating the scientific thinking
                   ability in artificial agents. Motivated by the stream of
                   research on causal discovery, we build our interactive EST
                   environment based on Blicket detection. Specifically, in each
                   episode of EST, an agent is presented with novel observations
                   and asked to figure out all objects' Blicketness. At each
                   time step, the agent proposes new experiments to validate its
                   hypothesis and updates its current belief. By evaluating
                   Reinforcement Learning (RL) agents on both a symbolic and
                   visual version of this task, we notice clear failure of
                   today's learning methods in reaching a level of intelligence
                   comparable to humans. Such inefficacy of learning in
                   scientific thinking calls for future research in building
                   humanlike intelligence.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2206.09203"
}

@ARTICLE{Lohse2022-nm,
  title    = "Hypotheses in adult-child interactions stimulate children's
              reasoning and verbalizations",
  author   = "Lohse, Karoline and Hildebrandt, Andrea and Hildebrandt, Frauke",
  journal  = "Early childhood research quarterly",
  volume   =  58,
  pages    = "254--263",
  abstract = "Adult-child interactions can support children's development and
              are established as predictors of program quality in early
              childhood settings. However, the linguistic components that
              constitute positive interactions have not yet been studied in
              detail. This study investigates the effects of hypotheses proposed
              by adults on children's responses in a dyadic picture-book viewing
              situation. In 2 experiments, adults’ use of hypotheses (e.g.,
              “Maybe this is a dwarf's door”) was tested against the use of
              instructive statements (“This is a dwarf's door”) and in
              combination with open questions (“What do you think, why is the
              door so small?”). In Experiment 1, hypotheses differed from
              instructions only by the modal marker “maybe”. Children's
              responses to hypotheses were longer and contained more
              self-generated explanations as compared to responses to
              instructions. The use of hypotheses also seemed to encourage
              children to attach more importance to their own explanations. In
              Experiment 2, combining hypotheses with open-ended why questions
              elicited longer responses but no more self-generated explanations
              in children than open-ended questions alone. Results indicate that
              subtle differences in adults’ utterances can directly influence
              children's reasoning and children's contributions to dialogues.",
  month    =  jan,
  year     =  2022,
  keywords = "Adult-child interactions; sustained shared thinking; hypotheses;
              open questions",
  doi      = "10.1016/j.ecresq.2021.09.014",
  issn     = "0885-2006"
}

@ARTICLE{Bramley2018-zb,
  title    = "Intuitive experimentation in the physical world",
  author   = "Bramley, Neil R and Gerstenberg, Tobias and Tenenbaum, Joshua B
              and Gureckis, Todd M",
  journal  = "Cognitive psychology",
  volume   =  105,
  pages    = "9--38",
  abstract = "Many aspects of our physical environment are hidden. For example,
              it is hard to estimate how heavy an object is from visual
              observation alone. In this paper we examine how people actively
              ``experiment'' within the physical world to discover such latent
              properties. In the first part of the paper, we develop a novel
              framework for the quantitative analysis of the information
              produced by physical interactions. We then describe two
              experiments that present participants with moving objects in
              ``microworlds'' that operate according to continuous
              spatiotemporal dynamics similar to everyday physics (i.e., forces
              of gravity, friction, etc.). Participants were asked to interact
              with objects in the microworlds in order to identify their masses,
              or the forces of attraction/repulsion that governed their
              movement. Using our modeling framework, we find that learners who
              freely interacted with the physical system selectively produced
              evidence that revealed the physical property consistent with their
              inquiry goal. As a result, their inferences were more accurate
              than for passive observers and, in some contexts, for yoked
              participants who watched video replays of an active learner's
              interactions. We characterize active learners' actions into a
              range of micro-experiment strategies and discuss how these might
              be learned or generalized from past experience. The technical
              contribution of this work is the development of a novel analytic
              framework and methodology for the study of interactively learning
              about the physical world. Its empirical contribution is the
              demonstration of sophisticated goal directed human active learning
              in a naturalistic context.",
  month    =  sep,
  year     =  2018,
  keywords = "Active learning; Experimental design; Mental simulation; Physical
              understanding",
  doi      = "10.1016/j.cogpsych.2018.05.001",
  pmid     =  29885534,
  issn     = "0010-0285,1095-5623",
  language = "en"
}

@ARTICLE{Coenen2015-wp,
  title    = "Strategies to intervene on causal systems are adaptively selected",
  author   = "Coenen, Anna and Rehder, Bob and Gureckis, Todd M",
  journal  = "Cognitive psychology",
  volume   =  79,
  pages    = "102--133",
  abstract = "How do people choose interventions to learn about causal systems?
              Here, we considered two possibilities. First, we test an
              information sampling model, information gain, which values
              interventions that can discriminate between a learner's hypotheses
              (i.e. possible causal structures). We compare this discriminatory
              model to a positive testing strategy that instead aims to confirm
              individual hypotheses. Experiment 1 shows that individual behavior
              is described best by a mixture of these two alternatives. In
              Experiment 2 we find that people are able to adaptively alter
              their behavior and adopt the discriminatory model more often after
              experiencing that the confirmatory strategy leads to a subjective
              performance decrement. In Experiment 3, time pressure leads to the
              opposite effect of inducing a change towards the simpler positive
              testing strategy. These findings suggest that there is no single
              strategy that describes how intervention decisions are made.
              Instead, people select strategies in an adaptive fashion that
              trades off their expected performance and cognitive effort.",
  month    =  jun,
  year     =  2015,
  keywords = "Causal learning; Hypothesis testing; Information gain;
              Interventions; Self-directed learning",
  doi      = "10.1016/j.cogpsych.2015.02.004",
  pmid     =  25935867,
  issn     = "0010-0285,1095-5623",
  language = "en"
}

@ARTICLE{Bonawitz2011-pi,
  title    = "The double-edged sword of pedagogy: Instruction limits spontaneous
              exploration and discovery",
  author   = "Bonawitz, Elizabeth and Shafto, Patrick and Gweon, Hyowon and
              Goodman, Noah D and Spelke, Elizabeth and Schulz, Laura",
  journal  = "Cognition",
  volume   =  120,
  number   =  3,
  pages    = "322--330",
  abstract = "Motivated by computational analyses, we look at how teaching
              affects exploration and discovery. In Experiment 1, we
              investigated children's exploratory play after an adult
              pedagogically demonstrated a function of a toy, after an
              interrupted pedagogical demonstration, after a naïve adult
              demonstrated the function, and at baseline. Preschoolers in the
              pedagogical condition focused almost exclusively on the target
              function; by contrast, children in the other conditions explored
              broadly. In Experiment 2, we show that children restrict their
              exploration both after direct instruction to themselves and after
              overhearing direct instruction given to another child; they do not
              show this constraint after observing direct instruction given to
              an adult or after observing a non-pedagogical intentional action.
              We discuss these findings as the result of rational inductive
              biases. In pedagogical contexts, a teacher's failure to provide
              evidence for additional functions provides evidence for their
              absence; such contexts generalize from child to child (because
              children are likely to have comparable states of knowledge) but
              not from adult to child. Thus, pedagogy promotes efficient
              learning but at a cost: children are less likely to perform
              potentially irrelevant actions but also less likely to discover
              novel information.",
  month    =  sep,
  year     =  2011,
  doi      = "10.1016/j.cognition.2010.10.001",
  pmc      = "PMC3369499",
  pmid     =  21216395,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Dumont2022-am,
  title    = "Transactional longitudinal relations between accuracy and reaction
              time on a measure of cognitive flexibility at 5, 6, and 7 years of
              age",
  author   = "Dumont, Émilie and Castellanos-Ryan, Natalie and Parent, Sophie
              and Jacques, Sophie and Séguin, Jean R and Zelazo, Philip David",
  journal  = "Developmental science",
  volume   =  25,
  number   =  5,
  pages    = "e13254",
  abstract = "Whereas accuracy is used as an indicator of cognitive flexibility
              in preschool-age children, reaction time (RT), or a combination of
              accuracy and RT, provide better indices of performance as children
              transition to school. Theoretical models and cross-sectional
              studies suggest that a speed-accuracy tradeoff may be operating
              across this transition, but the lack of longitudinal studies makes
              this transition difficult to understand. The current study
              explored the longitudinal and bidirectional associations between
              accuracy and RT on the DCCS (mixed block) at 5, 6, and 7 years of
              age using cross-lagged panel analyses. The study also examined the
              roles of working memory and language, as potential longitudinal
              mediators between RT at Time X and accuracy at Time X + 1, and
              explored the role of inhibitory control. The sample consisted of
              425 children from the Quebec Longitudinal Study of Child
              Development. Results show lagged associations from slower RT to
              greater improvements in accuracy between 5 and 6 years and between
              6 and 7 years. Further, higher accuracy at 6 years predicted
              faster RT at 7 years. Only working memory acted as a partial
              mediator between RT at 5 years and accuracy at 6 years. These
              results provide needed longitudinal evidence to support
              theoretical claims that slower RT precedes improved accuracy in
              the development of cognitive flexibility, that working memory may
              be involved in the early stage of this process, and that accuracy
              and reaction time become more efficient in later stages of this
              process.",
  month    =  sep,
  year     =  2022,
  keywords = "DCCS; cognitive flexibility; cross-lagged panel; longitudinal;
              school transition; working memory",
  doi      = "10.1111/desc.13254",
  pmid     =  35195319,
  issn     = "1363-755X,1467-7687",
  language = "en"
}

@ARTICLE{Chowdhery2022-un,
  title    = "{PaLM}: Scaling Language Modeling with Pathways",
  author   = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and
              Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, P
              and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian
              and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and
              Maynez, Joshua and Rao, Abhishek B and Barnes, Parker and Tay, Yi
              and Shazeer, Noam M and Prabhakaran, Vinodkumar and Reif, Emily
              and Du, Nan and Hutchinson, B and Pope, Reiner and Bradbury, James
              and Austin, Jacob and Isard, M and Gur-Ari, Guy and Yin, Pengcheng
              and Duke, Toju and Levskaya, Anselm and Ghemawat, S and Dev,
              Sunipa and Michalewski, H and García, Xavier and Misra, Vedant and
              Robinson, Kevin and Fedus, L and Zhou, Denny and Ippolito, Daphne
              and Luan, D and Lim, Hyeontaek and Zoph, Barret and Spiridonov, A
              and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and
              Omernick, Mark and Dai, Andrew M and Pillai, T S and Pellat, Marie
              and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and
              Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang,
              Xuezhi and Saeta, Brennan and Díaz, Mark and Firat, Orhan and
              Catasta, Michele and Wei, Jason and Meier-Hellstern, K and Eck, D
              and Dean, J and Petrov, Slav and Fiedel, Noah",
  journal  = "ArXiv",
  abstract = "A 540-billion parameter, densely activated, Transformer language
              model, which is called PaLM achieves breakthrough performance,
              outperforming the state-of-the-art on a suite of multi-step
              reasoning tasks, and outperforming average human performance on
              the recently released BIG-bench benchmark. Large language models
              have been shown to achieve remarkable performance across a variety
              of natural language tasks using few-shot learning , which
              drastically reduces the number of task-speciﬁc training examples
              needed to adapt the model to a particular application. To further
              our understanding of the impact of scale on few-shot learning, we
              trained a 540-billion parameter, densely activated, Transformer
              language model, which we call Pathways Language Model (PaLM). We
              trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system
              which enables highly eﬃcient training across multiple TPU Pods. We
              demonstrate continued beneﬁts of scaling by achieving
              state-of-the-art few-shot learning results on hundreds of language
              understanding and generation benchmarks. On a number of these
              tasks, PaLM 540B achieves breakthrough performance, outperforming
              the ﬁnetuned state-of-the-art on a suite of multi-step reasoning
              tasks, and outperforming average human performance on the recently
              released BIG-bench benchmark. A signiﬁcant number of BIG-bench
              tasks showed discontinuous improvements from model scale, meaning
              that performance steeply increased as we scaled to our largest
              model. PaLM also has strong capabilities in multilingual tasks and
              source code generation, which we demonstrate on a wide array of
              benchmarks. We additionally provide a comprehensive analysis on
              bias and toxicity, and study the extent of training data
              memorization with respect to model scale. Finally, we discuss the
              ethical considerations related to large language models and
              discuss potential mitigation strategies.",
  year     =  2022,
  eprint   = "2204.02311",
  language = "en"
}

@ARTICLE{Xu2013-bw,
  title     = "Infants Are Rational Constructivist Learners",
  author    = "Xu, Fei and Kushnir, Tamar",
  journal   = "Current directions in psychological science",
  publisher = "SAGE Publications Inc",
  volume    =  22,
  number    =  1,
  pages     = "28--32",
  abstract  = "What is the nature of human learning, and what insights can be
               gained from understanding early learning in infants and young
               children? This is an important question for understanding the
               human mind, the origins of knowledge, scientific reasoning, and
               how to best structure our educational environment. In this
               article, we argue for a new approach to cognitive development:
               rational constructivism. This view characterizes the child as a
               rational constructive learner, and it sees early learning as
               rational, statistical, and inferential. Empirical evidence for
               this approach has been accumulating rapidly, and a set of
               domain-general statistical and inferential mechanisms have been
               uncovered to explain why infants and young children learn so fast
               and so well.",
  month     =  feb,
  year      =  2013,
  doi       = "10.1177/0963721412469396",
  issn      = "0963-7214"
}

@INPROCEEDINGS{McCoy2019-zo,
  title     = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in
               Natural Language Inference",
  author    = "McCoy, Tom and Pavlick, Ellie and Linzen, Tal",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "3428--3448",
  abstract  = "A machine learning system can score well on a given test set by
               relying on heuristics that are effective for frequent example
               types but break down in more challenging cases. We study this
               issue within natural language inference (NLI), the task of
               determining whether one sentence entails another. We hypothesize
               that statistical NLI models may adopt three fallible syntactic
               heuristics: the lexical overlap heuristic, the subsequence
               heuristic, and the constituent heuristic. To determine whether
               models have adopted these heuristics, we introduce a controlled
               evaluation set called HANS (Heuristic Analysis for NLI Systems),
               which contains many examples where the heuristics fail. We find
               that models trained on MNLI, including BERT, a state-of-the-art
               model, perform very poorly on HANS, suggesting that they have
               indeed adopted these heuristics. We conclude that there is
               substantial room for improvement in NLI systems, and that the
               HANS dataset can motivate and measure progress in this area.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1334"
}

@ARTICLE{Ribeiro2020-xl,
  title         = "Beyond Accuracy: Behavioral Testing of {NLP} models with
                   {CheckList}",
  author        = "Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos
                   and Singh, Sameer",
  journal       = "arXiv [cs.CL]",
  abstract      = "Although measuring held-out accuracy has been the primary
                   approach to evaluate generalization, it often overestimates
                   the performance of NLP models, while alternative approaches
                   for evaluating models either focus on individual tasks or on
                   specific behaviors. Inspired by principles of behavioral
                   testing in software engineering, we introduce CheckList, a
                   task-agnostic methodology for testing NLP models. CheckList
                   includes a matrix of general linguistic capabilities and test
                   types that facilitate comprehensive test ideation, as well as
                   a software tool to generate a large and diverse number of
                   test cases quickly. We illustrate the utility of CheckList
                   with tests for three tasks, identifying critical failures in
                   both commercial and state-of-art models. In a user study, a
                   team responsible for a commercial sentiment analysis model
                   found new and actionable bugs in an extensively tested model.
                   In another user study, NLP practitioners with CheckList
                   created twice as many tests, and found almost three times as
                   many bugs as users without it.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.04118"
}

@ARTICLE{Tenney2022-km,
  title    = "What do you learn from context? Probing for sentence structure in
              contextualized word representations",
  author   = "Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and
              Poliak, Adam and Thomas McCoy, R and Kim, Najoung and Van Durme,
              Benjamin and Bowman, Samuel R and Das, Dipanjan and Pavlick, Ellie",
  journal  = "https://openreview.net › forum ›
              id=SJzSgnRcKXhttps://openreview.net › forum › id=SJzSgnRcKX",
  abstract = "Contextualized representation models such as ELMo (Peters et al.,
              2018a) and BERT (Devlin et al., 2018) have recently achieved
              state-of-the-art results on a diverse array of downstream NLP
              tasks. Building on recent token-level probing work, we introduce a
              novel edge probing task design and construct a broad suite of
              sub-sentence tasks derived from the traditional structured NLP
              pipeline. We probe word-level contextual representations from four
              recent models and investigate how they encode sentence structure
              across a range of syntactic, semantic, local, and long-range
              phenomena. We find that existing models trained on language
              modeling and translation produce strong representations for
              syntactic phenomena, but only offer comparably small improvements
              on semantic tasks over a non-contextual baseline.",
  month    =  feb,
  year     =  2022
}

@ARTICLE{Marjieh2023-jy,
  title         = "What Language Reveals about Perception: Distilling
                   Psychophysical Knowledge from Large Language Models",
  author        = "Marjieh, Raja and Sucholutsky, Ilia and van Rijn, Pol and
                   Jacoby, Nori and Griffiths, Thomas L",
  journal       = "arXiv [cs.CL]",
  abstract      = "Understanding the extent to which the perceptual world can be
                   recovered from language is a fundamental problem in cognitive
                   science. We reformulate this problem as that of distilling
                   psychophysical information from text and show how this can be
                   done by combining large language models (LLMs) with a classic
                   psychophysical method based on similarity judgments.
                   Specifically, we use the prompt auto-completion functionality
                   of GPT3, a state-of-the-art LLM, to elicit similarity scores
                   between stimuli and then apply multidimensional scaling to
                   uncover their underlying psychological space. We test our
                   approach on six perceptual domains and show that the elicited
                   judgments strongly correlate with human data and successfully
                   recover well-known psychophysical structures such as the
                   color wheel and pitch spiral. We also explore meaningful
                   divergences between LLM and human representations. Our work
                   showcases how combining state-of-the-art machine models with
                   well-known cognitive paradigms can shed new light on
                   fundamental questions in perception and language research.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.01308"
}

@ARTICLE{Yi2019-lo,
  title         = "{CLEVRER}: {CoLlision} Events for Video {REpresentation} and
                   Reasoning",
  author        = "Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet
                   and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B",
  journal       = "arXiv [cs.CV]",
  abstract      = "The ability to reason about temporal and causal events from
                   videos lies at the core of human intelligence. Most video
                   reasoning benchmarks, however, focus on pattern recognition
                   from complex visual and language input, instead of on causal
                   structure. We study the complementary problem, exploring the
                   temporal and causal structures behind videos of objects with
                   simple visual appearance. To this end, we introduce the
                   CoLlision Events for Video REpresentation and Reasoning
                   (CLEVRER), a diagnostic video dataset for systematic
                   evaluation of computational models on a wide range of
                   reasoning tasks. Motivated by the theory of human casual
                   judgment, CLEVRER includes four types of questions:
                   descriptive (e.g., ``what color''), explanatory (``what is
                   responsible for''), predictive (``what will happen next''),
                   and counterfactual (``what if''). We evaluate various
                   state-of-the-art models for visual reasoning on our
                   benchmark. While these models thrive on the perception-based
                   task (descriptive), they perform poorly on the causal tasks
                   (explanatory, predictive and counterfactual), suggesting that
                   a principled approach for causal reasoning should incorporate
                   the capability of both perceiving complex visual and language
                   inputs, and understanding the underlying dynamics and causal
                   relations. We also study an oracle model that explicitly
                   combines these components via symbolic representations.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1910.01442"
}

@ARTICLE{Merullo2022-zx,
  title         = "Linearly Mapping from Image to Text Space",
  author        = "Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and
                   Pavlick, Ellie",
  journal       = "arXiv [cs.CL]",
  abstract      = "The extent to which text-only language models (LMs) learn to
                   represent the physical, non-linguistic world is an open
                   question. Prior work has shown that pretrained LMs can be
                   taught to ``understand'' visual inputs when the models'
                   parameters are updated on image captioning tasks. We test a
                   stronger hypothesis: that the conceptual representations
                   learned by text-only models are functionally equivalent (up
                   to a linear transformation) to those learned by models
                   trained on vision tasks. Specifically, we show that the image
                   representations from vision models can be transferred as
                   continuous prompts to frozen LMs by training only a single
                   linear projection. Using these to prompt the LM achieves
                   competitive performance on captioning and visual question
                   answering tasks compared to models that tune both the image
                   encoder and text decoder (such as the MAGMA model). We
                   compare three image encoders with increasing amounts of
                   linguistic supervision seen during pretraining: BEIT (no
                   linguistic information), NF-ResNET (lexical category
                   information), and CLIP (full natural language descriptions).
                   We find that all three encoders perform equally well at
                   transferring visual property information to the language
                   model (e.g., whether an animal is large or small), but that
                   image encoders pretrained with linguistic supervision more
                   saliently encode category information (e.g., distinguishing
                   hippo vs.\\ elephant) and thus perform significantly better
                   on benchmark language-and-vision tasks. Our results indicate
                   that LMs encode conceptual information structurally similarly
                   to vision-based models, even those that are solely trained on
                   images.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2209.15162"
}

@ARTICLE{Dubey2021-zq,
  title    = "Aha! moments correspond to metacognitive prediction errors",
  author   = "Dubey, Rachit and Ho, Mark K and Mehta, Hermish and Griffiths, Tom",
  abstract = "Psychologists have long been fascinated with understanding the
              nature of Aha! moments, moments when we transition from not
              knowing to suddenly realizing the solution to a problem. In this
              work, we present a theoretical framework that explains why we
              experience Aha! moments. Our theory posits that during
              problem-solving, in addition to solving the problem, people also
              maintain a metacognitive model of their ability to solve the
              problem as well as a prediction about the time it would take them
              to solve that problem. Aha! moments arise when we experience a
              positive error in this metacognitive prediction, i.e. when we
              solve a problem much faster than we expected to solve it. We posit
              that this metacognitive error is analogous to a positive reward
              prediction error thereby explaining why we feel so good after an
              Aha! moment. We provide support to our theory across three
              large-scale pre-registered experiments on problem solving,
              demonstrating a link between metacognitive prediction errors and
              Aha! moments. These results highlight the importance of
              metacognitive prediction errors and deepen our understanding of
              human metareasoning.",
  month    =  jun,
  year     =  2021,
  keywords = "Aha! moment; Insight; metacognition; monitoring and control;
              prediction errors; problem solving; reinforcement learning",
  doi      = "10.31234/osf.io/c5v42"
}

@INPROCEEDINGS{Gerstenberg2015-lf,
  title     = "How, whether, why: Causal judgments as counterfactual contrasts",
  author    = "Gerstenberg, Tobias and Goodman, Noah D and Lagnado, David A and
               Tenenbaum, Joshua B",
  booktitle = "CogSci",
  year      =  2015
}

@ARTICLE{Bakhtin2019-hv,
  title         = "{PHYRE}: A New Benchmark for Physical Reasoning",
  author        = "Bakhtin, Anton and van der Maaten, Laurens and Johnson,
                   Justin and Gustafson, Laura and Girshick, Ross",
  journal       = "arXiv [cs.LG]",
  abstract      = "Understanding and reasoning about physics is an important
                   ability of intelligent agents. We develop the PHYRE benchmark
                   for physical reasoning that contains a set of simple
                   classical mechanics puzzles in a 2D physical environment. The
                   benchmark is designed to encourage the development of
                   learning algorithms that are sample-efficient and generalize
                   well across puzzles. We test several modern learning
                   algorithms on PHYRE and find that these algorithms fall short
                   in solving the puzzles efficiently. We expect that PHYRE will
                   encourage the development of novel sample-efficient agents
                   that learn efficient but useful models of physics. For code
                   and to play PHYRE for yourself, please visit
                   https://player.phyre.ai.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1908.05656"
}

@ARTICLE{Xu2023-sf,
  title         = "{LLMs} and the Abstraction and Reasoning Corpus: Successes,
                   Failures, and the Importance of Object-based Representations",
  author        = "Xu, Yudong and Li, Wenhao and Vaezipoor, Pashootan and
                   Sanner, Scott and Khalil, Elias B",
  journal       = "arXiv [cs.CL]",
  abstract      = "Can a Large Language Model (LLM) solve simple abstract
                   reasoning problems? We explore this broad question through a
                   systematic analysis of GPT on the Abstraction and Reasoning
                   Corpus (ARC), a representative benchmark of abstract
                   reasoning ability from limited examples in which solutions
                   require some ``core knowledge'' of concepts such as objects,
                   goal states, counting, and basic geometry. GPT-4 solves only
                   13/50 of the most straightforward ARC tasks when using
                   textual encodings for their two-dimensional input-output
                   grids. Our failure analysis reveals that GPT-4's capacity to
                   identify objects and reason about them is significantly
                   influenced by the sequential nature of the text that
                   represents an object within a text encoding of a task. To
                   test this hypothesis, we design a new benchmark, the 1D-ARC,
                   which consists of one-dimensional (array-like) tasks that are
                   more conducive to GPT-based reasoning, and where it indeed
                   performs better than on the (2D) ARC. To alleviate this
                   issue, we propose an object-based representation that is
                   obtained through an external tool, resulting in nearly
                   doubling the performance on solved ARC tasks and near-perfect
                   scores on the easier 1D-ARC. Although the state-of-the-art
                   GPT-4 is unable to ``reason'' perfectly within non-language
                   domains such as the 1D-ARC or a simple ARC subset, our study
                   reveals that the use of object-based representations can
                   significantly improve its reasoning ability. Visualizations,
                   GPT logs, and data are available at
                   https://khalil-research.github.io/LLM4ARC.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.18354"
}

@ARTICLE{Ludwin-Peery2021-pr,
  title    = "Limits on Simulation Approaches in Intuitive Physics",
  author   = "Ludwin-Peery, Ethan and Bramley, Neil R and Davis, Ernest and
              Gureckis, Todd M",
  abstract = "A popular explanation of the human ability for physical reasoning
              is that it depends on a sophisticated ability to perform mental
              simulations. According to this perspective, physical reasoning
              problems are approached by repeatedly simulating relevant aspects
              of a scenario, with noise, and making judgments based on
              aggregation over these simulations. In this paper, we describe
              three core tenets of simulation approaches, theoretical
              commitments that must be present in order for a simulation
              approach to be viable. The identification of these tenets
              threatens the plausibility of simulation as a theory of physical
              reasoning, because they appear to be incompatible with what we
              know about cognition more generally. To investigate this apparent
              contradiction, we describe three experiments involving simple
              physical judgments and predictions, and argue their results
              challenge these core predictions of theories of mental simulation.",
  month    =  jan,
  year     =  2021,
  doi      = "10.31234/osf.io/xhzuc"
}

@ARTICLE{Fabricius2021-zg,
  title    = "Perceptual Access Reasoning ({PAR}) in Developing a
              Representational Theory of Mind",
  author   = "Fabricius, William V and Gonzales, Christopher R and Pesch,
              Annelise and Weimer, Amy A and Pugliese, John and Carroll,
              Kathleen and Bolnick, Rebecca R and Kupfer, Anne S and Eisenberg,
              Nancy and Spinrad, Tracy L",
  journal  = "Monographs of the Society for Research in Child Development",
  volume   =  86,
  number   =  3,
  pages    = "7--154",
  abstract = "An important part of children's social and cognitive development
              is their understanding that people are psychological beings with
              internal, mental states including desire, intention, perception,
              and belief. A full understanding of people as psychological beings
              requires a representational theory of mind (ToM), which is an
              understanding that mental states can faithfully represent reality,
              or misrepresent reality. For the last 35 years, researchers have
              relied on false-belief tasks as the gold standard to test
              children's understanding that beliefs can misrepresent reality. In
              false-belief tasks, children are asked to reason about the
              behavior of agents who have false beliefs about situations.
              Although a large body of evidence indicates that most children
              pass false-belief tasks by the end of the preschool years, the
              evidence we present in this monograph suggests that most children
              do not understand false beliefs or, surprisingly, even true
              beliefs until middle childhood. We argue that young children pass
              false-belief tasks without understanding false beliefs by using
              perceptual access reasoning (PAR). With PAR, children understand
              that seeing leads to knowing in the moment, but not that knowing
              also arises from thinking or persists as memory and belief after
              the situation changes. By the same token, PAR leads children to
              fail true-belief tasks. PAR theory can account for performance on
              other traditional tests of representational ToM and related tasks,
              and can account for the factors that have been found to correlate
              with or affect both true- and false-belief performance. The theory
              provides a new laboratory measure which we label the belief
              understanding scale (BUS). This scale can distinguish between a
              child who is operating with PAR versus a child who is
              understanding beliefs. This scale provides a method needed to
              allow the study of the development of representational ToM. In
              this monograph, we report the outcome of the tests that we have
              conducted of predictions generated by PAR theory. The findings
              demonstrated signature PAR limitations in reasoning about the mind
              during the ages when children are hypothesized to be using PAR. In
              Chapter II, secondary analyses of the published true-belief
              literature revealed that children failed several types of
              true-belief tasks. Chapters III through IX describe new empirical
              data collected across multiple studies between 2003 and 2014 from
              580 children aged 4-7 years, as well as from a small sample of 14
              adults. Participants were recruited from the Phoenix, Arizona
              metropolitan area. All participants were native English-speakers.
              Children were recruited from university-sponsored and community
              preschools and daycare centers, and from hospital maternity wards.
              Adults were university students who participated to partially
              fulfill course requirements for research participation.
              Sociometric data were collected only in Chapter IX, and are fully
              reported there. In Chapter III, minor alterations in task
              procedures produced wide variations in children's performance in
              3-option false-belief tasks. In Chapter IV, we report findings
              which show that the developmental lag between children's
              understanding ignorance and understanding false belief is longer
              than the lag reported in previous studies. In Chapter V, children
              did not distinguish between agents who have false beliefs versus
              agents who have no beliefs. In Chapter VI, findings showed that
              children found it no easier to reason about true beliefs than to
              reason about false beliefs. In Chapter VII, when children were
              asked to justify their correct answers in false-belief tasks, they
              did not reference agents' false beliefs. Similarly, in Chapter
              VIII, when children were asked to explain agents' actions in
              false-belief tasks, they did not reference agents' false beliefs.
              In Chapter IX, children who were identified as using PAR differed
              from children who understood beliefs along three dimensions-in
              levels of social development, inhibitory control, and kindergarten
              adjustment. Although the findings need replication and additional
              studies of alternative interpretations, the collection of results
              reported in this monograph challenges the prevailing view that
              representational ToM is in place by the end of the preschool
              years. Furthermore, the pattern of findings is consistent with the
              proposal that PAR is the developmental precursor of
              representational ToM. The current findings also raise questions
              about claims that infants and toddlers demonstrate ToM-related
              abilities, and that representational ToM is innate.",
  month    =  sep,
  year     =  2021,
  doi      = "10.1111/mono.12432",
  pmc      = "PMC9292623",
  pmid     =  34580875,
  issn     = "0037-976X,1540-5834",
  language = "en"
}

@ARTICLE{Allen2022-as,
  title    = "Physical Design using Differentiable Learned Simulators",
  author   = "Allen, Kelsey R and Lopez-Guevara, Tatiana and Stachenfeld, K and
              Sanchez-Gonzalez, Alvaro and Battaglia, P and Hamrick, Jessica B
              and Pfaff, T",
  journal  = "ArXiv",
  abstract = "This work explores a simple, fast, and robust approach to inverse
              design which combines learned forward simulators based on graph
              neural networks with gradient-based design optimization, and
              suggests that despite some remaining challenges, machine
              learning-based simulators are maturing to the point where they can
              support general-purpose design optimization across a variety of
              domains. Designing physical artifacts that serve a purpose— such
              as tools and other functional structures—is central to engineering
              as well as everyday human behavior. Though automating design has
              tremendous promise, general-purpose methods do not yet exist. Here
              we explore a simple, fast, and robust approach to inverse design
              which combines learned forward simulators based on graph neural
              networks with gradient-based design optimization. Our approach
              solves high-dimensional problems with complex physical dynamics,
              including designing surfaces and tools to manipulate fluid flows
              and optimizing the shape of an airfoil to minimize drag. This
              framework produces highquality designs by propagating gradients
              through trajectories of hundreds of steps, even when using models
              that were pre-trained for single-step predictions on data
              substantially different from the design tasks. In our fluid
              manipulation tasks, the resulting designs outperformed those found
              by sampling-based optimization techniques. In airfoil design, they
              matched the quality of those obtained with a specialized solver.
              Our results suggest that despite some remaining challenges,
              machine learning-based simulators are maturing to the point where
              they can support general-purpose design optimization across a
              variety of domains.",
  year     =  2022,
  eprint   = "2202.00728",
  language = "en"
}

@ARTICLE{Kaelbling1998-eb,
  title    = "Planning and acting in partially observable stochastic domains",
  author   = "Kaelbling, Leslie Pack and Littman, Michael L and Cassandra,
              Anthony R",
  journal  = "Artificial intelligence",
  volume   =  101,
  number   =  1,
  pages    = "99--134",
  abstract = "In this paper, we bring techniques from operations research to
              bear on the problem of choosing optimal actions in partially
              observable stochastic domains. We begin by introducing the theory
              of Markov decision processes (mdps) and partially observable MDPs
              (pomdps). We then outline a novel algorithm for solving pomdps off
              line and show how, in some cases, a finite-memory controller can
              be extracted from the solution to a POMDP. We conclude with a
              discussion of how our approach relates to previous work, the
              complexity of finding exact solutions to pomdps, and of some
              possibilities for finding approximate solutions.",
  month    =  may,
  year     =  1998,
  keywords = "Planning; Uncertainty; Partially observable Markov decision
              processes",
  doi      = "10.1016/S0004-3702(98)00023-X",
  issn     = "0004-3702"
}

@ARTICLE{Balestriero2021-hw,
  title         = "Learning in High Dimension Always Amounts to Extrapolation",
  author        = "Balestriero, Randall and Pesenti, Jerome and LeCun, Yann",
  journal       = "arXiv [cs.LG]",
  abstract      = "The notion of interpolation and extrapolation is fundamental
                   in various fields from deep learning to function
                   approximation. Interpolation occurs for a sample $x$ whenever
                   this sample falls inside or on the boundary of the given
                   dataset's convex hull. Extrapolation occurs when $x$ falls
                   outside of that convex hull. One fundamental (mis)conception
                   is that state-of-the-art algorithms work so well because of
                   their ability to correctly interpolate training data. A
                   second (mis)conception is that interpolation happens
                   throughout tasks and datasets, in fact, many intuitions and
                   theories rely on that assumption. We empirically and
                   theoretically argue against those two points and demonstrate
                   that on any high-dimensional ($>$100) dataset, interpolation
                   almost surely never happens. Those results challenge the
                   validity of our current interpolation/extrapolation
                   definition as an indicator of generalization performances.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2110.09485"
}

@ARTICLE{Stachenfeld2017-ra,
  title    = "The hippocampus as a predictive map",
  author   = "Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman,
              Samuel J",
  journal  = "Nature neuroscience",
  volume   =  20,
  number   =  11,
  pages    = "1643--1653",
  abstract = "A cognitive map has long been the dominant metaphor for
              hippocampal function, embracing the idea that place cells encode a
              geometric representation of space. However, evidence for
              predictive coding, reward sensitivity and policy dependence in
              place cells suggests that the representation is not purely
              spatial. We approach this puzzle from a reinforcement learning
              perspective: what kind of spatial representation is most useful
              for maximizing future reward? We show that the answer takes the
              form of a predictive representation. This representation captures
              many aspects of place cell responses that fall outside the
              traditional view of a cognitive map. Furthermore, we argue that
              entorhinal grid cells encode a low-dimensionality basis set for
              the predictive representation, useful for suppressing noise in
              predictions and extracting multiscale structure for hierarchical
              planning.",
  month    =  nov,
  year     =  2017,
  doi      = "10.1038/nn.4650",
  pmid     =  28967910,
  issn     = "1097-6256,1546-1726",
  language = "en"
}

@ARTICLE{Ullman2017-fo,
  title    = "Mind Games: Game Engines as an Architecture for Intuitive Physics",
  author   = "Ullman, Tomer D and Spelke, Elizabeth and Battaglia, Peter and
              Tenenbaum, Joshua B",
  journal  = "Trends in cognitive sciences",
  volume   =  21,
  number   =  9,
  pages    = "649--665",
  abstract = "We explore the hypothesis that many intuitive physical inferences
              are based on a mental physics engine that is analogous in many
              ways to the machine physics engines used in building interactive
              video games. We describe the key features of game physics engines
              and their parallels in human mental representation, focusing
              especially on the intuitive physics of young infants where the
              hypothesis helps to unify many classic and otherwise puzzling
              phenomena, and may provide the basis for a computational account
              of how the physical knowledge of infants develops. This hypothesis
              also explains several 'physics illusions', and helps to inform the
              development of artificial intelligence (AI) systems with more
              human-like common sense.",
  month    =  sep,
  year     =  2017,
  doi      = "10.1016/j.tics.2017.05.012",
  pmid     =  28655498,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Johnson-Laird2010-yq,
  title    = "Mental models and human reasoning",
  author   = "Johnson-Laird, Philip N",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  107,
  number   =  43,
  pages    = "18243--18250",
  abstract = "To be rational is to be able to reason. Thirty years ago
              psychologists believed that human reasoning depended on formal
              rules of inference akin to those of a logical calculus. This
              hypothesis ran into difficulties, which led to an alternative
              view: reasoning depends on envisaging the possibilities consistent
              with the starting point—a perception of the world, a set of
              assertions, a memory, or some mixture of them. We construct mental
              models of each distinct possibility and derive a conclusion from
              them. The theory predicts systematic errors in our reasoning, and
              the evidence corroborates this prediction. Yet, our ability to use
              counterexamples to refute invalid inferences provides a foundation
              for rationality. On this account, reasoning is a simulation of the
              world fleshed out with our knowledge, not a formal rearrangement
              of the logical skeletons of sentences.",
  year     =  2010,
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.1012933107",
  doi      = "10.1073/pnas.1012933107"
}

@INCOLLECTION{Forbus1988-zh,
  title     = "Chapter 7 - Qualitative Physics: Past, Present, and Future",
  author    = "Forbus, Kenneth D",
  editor    = "Shrobe, Howard E and {the American Association for Artificial
               Intelligence}",
  booktitle = "Exploring Artificial Intelligence",
  publisher = "Morgan Kaufmann",
  pages     = "239--296",
  abstract  = "Publisher Summary Qualitative physics is concerned with
               representing and reasoning about the physical world. The goal of
               qualitative physics is to capture both the commonsense knowledge
               of the person on the street and the tacit knowledge underlying
               the quantitative knowledge used by engineers and scientists. The
               key to qualitative physics is to find ways to represent
               continuous properties of the world by discrete systems of
               symbols. One can always quantize something continuous, but not
               all quantizations are equally useful. One way to state the idea
               is the relevance principle: The distinctions made by a
               quantization must be relevant to the kind of reasoning performed.
               This chapter describes what qualitative physics is, why one
               should be doing it, and where it came from. It discusses some
               open problems in qualitative physics.",
  month     =  jan,
  year      =  1988,
  doi       = "10.1016/B978-0-934613-67-5.50011-3",
  isbn      =  9780934613675
}

@ARTICLE{Lupyan2016-mq,
  title     = "The centrality of language in human cognition",
  author    = "Lupyan, Gary",
  journal   = "Language learning",
  publisher = "Wiley",
  volume    =  66,
  number    =  3,
  pages     = "516--553",
  abstract  = "The emergence of language—a productive and combinatorial system
               of communication—has been hailed as one of the major transitions
               in evolution. By enabling symbolic culture, language allows
               humans to draw on and expand on the knowledge of their ancestors
               and peers. A common assumption among linguists and psychologists
               is that although language is critical to our ability to share our
               thoughts, it plays a minor, if any, role in generating,
               controlling, and structuring them. I examine some assumptions
               that led to this view of language and discuss an alternative
               according to which normal human cognition is language-augmented
               cognition. I focus on one of the fundamental design features of
               language—the use of words as symbolic cues—and argue that
               language acts as a high-level control system for the mind,
               allowing individuals to sculpt mental representations of others
               as well as their own.",
  month     =  sep,
  year      =  2016,
  doi       = "10.1111/lang.12155",
  issn      = "0023-8333,1467-9922",
  language  = "en"
}

@ARTICLE{Piantadosi2024-df,
  title     = "Why concepts are (probably) vectors",
  author    = "Piantadosi, Steven T and Muller, Dyana C Y and Rule, Joshua S and
               Kaushik, Karthikeya and Gorenstein, Mark and Leib, Elena R and
               Sanford, Emily",
  journal   = "Trends in cognitive sciences",
  publisher = "Elsevier BV",
  volume    =  0,
  number    =  0,
  abstract  = "For decades, cognitive scientists have debated what kind of
               representation might characterize human concepts. Whatever the
               format of the representation, it must allow for the computation
               of varied properties, including similarities, features,
               categories, definitions, and relations. It must also support the
               development of theories, ad hoc categories, and knowledge of
               procedures. Here, we discuss why vector-based representations
               provide a compelling account that can meet all these needs while
               being plausibly encoded into neural architectures. This view has
               become especially promising with recent advances in both large
               language models and vector symbolic architectures. These
               innovations show how vectors can handle many properties
               traditionally thought to be out of reach for neural models,
               including compositionality, definitions, structures, and symbolic
               computational processes.",
  month     =  aug,
  year      =  2024,
  keywords  = "church encoding; concepts; conceptual role; vector; vector
               symbolic architecture",
  doi       = "10.1016/j.tics.2024.06.011",
  pmid      =  39112125,
  issn      = "1364-6613,1879-307X",
  language  = "en"
}

@ARTICLE{Rosedahl2022-py,
  title    = "Linear separability, irrelevant variability, and categorization
              difficulty",
  author   = "Rosedahl, Luke A and Ashby, F Gregory",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  48,
  number   =  2,
  pages    = "159--172",
  abstract = "In rule-based (RB) category-learning tasks, the optimal strategy
              is a simple explicit rule, whereas in information-integration (II)
              tasks, the optimal strategy is impossible to describe verbally.
              This study investigates the effects of two different category
              properties on learning difficulty in category learning
              tasks-namely, linear separability and variability on stimulus
              dimensions that are irrelevant to the categorization decision.
              Previous research had reported that linearly separable II
              categories are easier to learn than nonlinearly separable
              categories, but Experiment 1, which compared performance on
              linearly and nonlinearly separable categories that were equated as
              closely as possible on all other factors that might affect
              difficulty, found that linear separability had no effect on
              learning. Experiments 1 and 2 together also established a novel
              dissociation between RB and II category learning: increasing
              variability on irrelevant stimulus dimensions impaired II learning
              but not RB learning. These results are all predicted by the best
              available measures of difficulty in RB and II tasks. (PsycInfo
              Database Record (c) 2022 APA, all rights reserved).",
  month    =  feb,
  year     =  2022,
  doi      = "10.1037/xlm0001000",
  pmc      = "PMC8523591",
  pmid     =  33871263,
  issn     = "0278-7393,1939-1285",
  language = "en"
}

@ARTICLE{Chronicle2004-bv,
  title    = "What makes an insight problem? The roles of heuristics, goal
              conception, and solution recoding in knowledge-lean problems",
  author   = "Chronicle, Edward P and MacGregor, James N and Ormerod, Thomas C",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  30,
  number   =  1,
  pages    = "14--27",
  abstract = "Four experiments investigated transformation problems with insight
              characteristics. In Experiment 1, performance on a version of the
              6-coin problem that had a concrete and visualizable solution
              followed a hill-climbing heuristic. Experiment 2 demonstrated that
              the difficulty of a version of the problem that potentially
              required insight for solution stems from the same hill-climbing
              heuristic, which creates an implicit conceptual block. Experiment
              3 confirmed that the difficulty of the potential insight solution
              is conceptual, not procedural. Experiment 4 demonstrated the same
              principles of move selection on the 6-coin problem and the 10-coin
              (triangle) problem. It is argued that hill-climbing heuristics
              provide a common framework for understanding transformation and
              insight problem solving. Postsolution receding may account for
              part of the phenomenology of insight.",
  month    =  jan,
  year     =  2004,
  doi      = "10.1037/0278-7393.30.1.14",
  pmid     =  14736293,
  issn     = "0278-7393",
  language = "en"
}

@MISC{Gershman_undated-tc,
  title        = "Perceptual multistability as Markov chain Monte Carlo
                  inference",
  author       = "Gershman, Samuel J and Vul, Edward and Tenenbaum, Joshua B",
  howpublished = "\url{https://gershmanlab.com/pubs/GershmanVulTenenbaum09.pdf}",
  note         = "Accessed: 2023-1-19"
}

@ARTICLE{Wilson2013-cg,
  title    = "Embodied Cognition is Not What you Think it is",
  author   = "Wilson, Andrew D and Golonka, Sabrina",
  journal  = "Frontiers in psychology",
  volume   =  4,
  pages    =  58,
  abstract = "The most exciting hypothesis in cognitive science right now is the
              theory that cognition is embodied. Like all good ideas in
              cognitive science, however, embodiment immediately came to mean
              six different things. The most common definitions involve the
              straight-forward claim that ``states of the body modify states of
              the mind.'' However, the implications of embodiment are actually
              much more radical than this. If cognition can span the brain,
              body, and the environment, then the ``states of mind'' of
              disembodied cognitive science won't exist to be modified.
              Cognition will instead be an extended system assembled from a
              broad array of resources. Taking embodiment seriously therefore
              requires both new methods and theory. Here we outline four key
              steps that research programs should follow in order to fully
              engage with the implications of embodiment. The first step is to
              conduct a task analysis, which characterizes from a first person
              perspective the specific task that a perceiving-acting cognitive
              agent is faced with. The second step is to identify the
              task-relevant resources the agent has access to in order to solve
              the task. These resources can span brain, body, and environment.
              The third step is to identify how the agent can assemble these
              resources into a system capable of solving the problem at hand.
              The last step is to test the agent's performance to confirm that
              agent is actually using the solution identified in step 3. We
              explore these steps in more detail with reference to two useful
              examples (the outfielder problem and the A-not-B error), and
              introduce how to apply this analysis to the thorny question of
              language use. Embodied cognition is more than we think it is, and
              we have the tools we need to realize its full potential.",
  month    =  feb,
  year     =  2013,
  keywords = "A-not-B error; dynamical systems; embodied cognition; language;
              outfielder problem; replacement hypothesis; robotics",
  doi      = "10.3389/fpsyg.2013.00058",
  pmc      = "PMC3569617",
  pmid     =  23408669,
  issn     = "1664-1078",
  language = "en"
}

@ARTICLE{Schulz2017-ij,
  title    = "Strategic exploration in human adaptive control",
  author   = "Schulz, Eric and Klenske, Edgar D and Bramley, Neil R and
              Speekenbrink, Maarten",
  journal  = "bioRxiv",
  pages    =  110486,
  abstract = "Abstract How do people explore in order to gain rewards in
              uncertain dynamical systems? Within a reinforcement learning
              paradigm, control normally involves trading off between
              exploration (i.e. trying out actions in order to gain more
              knowledge about the system) and exploitation (i.e. using current
              knowledge of the system to maximize reward). We study a novel
              control task in which participants must steer a boat on a grid,
              aiming to follow a path of high reward whilst learning how their
              actions affect the boat’s position. We find that participants
              explore strategically yet conservatively, exploring more when
              mistakes are less costly and practicing actions that will be
              required later on.",
  month    =  may,
  year     =  2017,
  doi      = "10.1101/110486",
  language = "en"
}

@ARTICLE{Ohlsson1984-ni,
  title     = "Restructuring revisited",
  author    = "Ohlsson, S",
  journal   = "Scandinavian journal of psychology",
  publisher = "Wiley",
  volume    =  25,
  number    =  2,
  pages     = "117--129",
  abstract  = "The central concept of the information processing theory of
               problem solving is search. In contrast, the central concept of
               the Gestalt theory of problem solving is restructuring. Both
               concepts express important aspects of human thinking. A theory is
               presented which interprets restructuring and the related concept
               of insight in information processing terms. It is hypothesised
               that restructuring is a change in mental representation which
               affects the applicability of problem solving operators. Insight
               is hypothesized to occur when restructuring of the search space
               brings the goal state within the horizon of mental look-ahead.",
  month     =  jun,
  year      =  1984,
  doi       = "10.1111/j.1467-9450.1984.tb01005.x",
  issn      = "0036-5564,1467-9450",
  language  = "en"
}

@ARTICLE{Thevenot2008-ob,
  title    = "A generalization of the representational change theory from
              insight to non-insight problems: the case of arithmetic word
              problems",
  author   = "Thevenot, Catherine and Oakhill, Jane",
  journal  = "Acta psychologica",
  volume   =  129,
  number   =  3,
  pages    = "315--324",
  abstract = "This paper provides evidence for a possible generalization of
              Knoblich and colleagues' representational change theory [Knoblich,
              G., Ohlsson, S., Haider, H., \& Rhenius, D. (1999). Constraint
              relaxation and chunk decomposition in insight problem solving.
              Journal of Experimental Psychology: Learning, Memory, and
              Cognition, 25, 1534-1555; Knoblich, G., Ohlsson, S., \& Raney, G.
              E. (2001). An eye movement study of insight problem. Memory and
              Cognition, 29, 1000-1009] outside its original scope of
              application. While this theory has been proposed to explain
              insight problem solving, we demonstrate here that its main
              concepts, namely, constraint relaxation and chunk decomposition,
              are applicable to incremental problem solving. In a first
              experiment, we confirm, as already shown by problem solving and
              reasoning researchers, that individuals avoid the construction of
              alternative representations of the problems when possible. In the
              second and third experiments, we show that alternative
              representations of arithmetic problems are easier to construct and
              maintain when they violate constraints of narrow rather than wide
              scope. The specificity of insight problem solving is discussed in
              the light of these new findings.",
  month    =  nov,
  year     =  2008,
  doi      = "10.1016/j.actpsy.2008.08.008",
  pmid     =  18834964,
  issn     = "0001-6918,1873-6297",
  language = "en"
}

@ARTICLE{Ollinger2008-fw,
  title    = "Investigating the effect of mental set on insight problem solving",
  author   = "Ollinger, Michael and Jones, Gary and Knoblich, Günther",
  journal  = "Experimental psychology",
  volume   =  55,
  number   =  4,
  pages    = "269--282",
  abstract = "Mental set is the tendency to solve certain problems in a fixed
              way based on previous solutions to similar problems. The moment of
              insight occurs when a problem cannot be solved using solution
              methods suggested by prior experience and the problem solver
              suddenly realizes that the solution requires different solution
              methods. Mental set and insight have often been linked together
              and yet no attempt thus far has systematically examined the
              interplay between the two. Three experiments are presented that
              examine the extent to which sets of noninsight and insight
              problems affect the subsequent solutions of insight test problems.
              The results indicate a subtle interplay between mental set and
              insight: when the set involves noninsight problems, no mental set
              effects are shown for the insight test problems, yet when the set
              involves insight problems, both facilitation and inhibition can be
              seen depending on the type of insight problem presented in the
              set. A two process model is detailed to explain these findings
              that combines the representational change mechanism with that of
              proceduralization.",
  year     =  2008,
  doi      = "10.1027/1618-3169.55.4.269",
  pmid     =  18683624,
  issn     = "1618-3169",
  language = "en"
}

@INCOLLECTION{McClelland1988-qm,
  title     = "The appeal of parallel distributed processing",
  author    = "McClelland, James L and Rumelhart, David E and Hinton, G E",
  editor    = "Collins, Allan M",
  booktitle = "Readings in cognitive science: A perspective from psychology and
               artificial intelligence , (pp",
  volume    =  661,
  pages     = "52--72",
  abstract  = "multiple simultaneous constraints parallel distributed processing
               [PDP] / examples of PDP models representation and learning in PDP
               models origins of parallel distributed processing (PsycInfo
               Database Record (c) 2022 APA, all rights reserved)",
  year      =  1988
}

@ARTICLE{Elman1990-pd,
  title     = "Finding structure in time",
  author    = "Elman, Jeffrey L",
  journal   = "Cognitive science",
  publisher = "Wiley",
  volume    =  14,
  number    =  2,
  pages     = "179--211",
  abstract  = "Time underlies many interesting human behaviors. Thus, the
               question of how to represent time in connectionist models is very
               important. One approach is to represent time implicitly by its
               effects on processing rather than explicitly (as in a spatial
               representation). The current report develops a proposal along
               these lines first described by Jordan (1986) which involves the
               use of recurrent links in order to provide networks with a
               dynamic memory. In this approach, hidden unit patterns are fed
               back to themselves; the internal representations which develop
               thus reflect task demands in the context of prior internal
               states. A set of simulations is reported which range from
               relatively simple problems (temporal version of XOR) to
               discovering syntactic/semantic features for words. The networks
               are able to learn interesting internal representations which
               incorporate task demands with memory demands; indeed, in this
               approach the notion of memory is inextricably bound up with task
               processing. These representations reveal a rich structure, which
               allows them to be highly context-dependent while also expressing
               generalizations across classes of items. These representations
               suggest a method for representing lexical categories and the
               type/token distinction.",
  month     =  mar,
  year      =  1990,
  doi       = "10.1207/s15516709cog1402\_1",
  issn      = "0364-0213,1551-6709",
  language  = "en"
}

@ARTICLE{Ollinger2014-qq,
  title    = "The dynamics of search, impasse, and representational change
              provide a coherent explanation of difficulty in the nine-dot
              problem",
  author   = "Öllinger, Michael and Jones, Gary and Knoblich, Günther",
  journal  = "Psychological research",
  volume   =  78,
  number   =  2,
  pages    = "266--275",
  abstract = "The nine-dot problem is often used to demonstrate and explain
              mental impasse, creativity, and out of the box thinking. The
              present study investigated the interplay of a restricted initial
              search space, the likelihood of invoking a representational
              change, and the subsequent constraining of an unrestricted search
              space. In three experimental conditions, participants worked on
              different versions of the nine-dot problem that hinted at removing
              particular sources of difficulty from the standard problem. The
              hints were incremental such that the first suggested a possible
              route for a solution attempt; the second additionally indicated
              the dot at which lines meet on the solution path; and the final
              condition also provided non-dot locations that appear in the
              solution path. The results showed that in the experimental
              conditions, representational change is encountered more quickly
              and problems are solved more often than for the control group. We
              propose a cognitive model that focuses on general problem-solving
              heuristics and representational change to explain problem
              difficulty.",
  month    =  mar,
  year     =  2014,
  doi      = "10.1007/s00426-013-0494-8",
  pmid     =  23708954,
  issn     = "0340-0727,1430-2772",
  language = "en"
}

@ARTICLE{Xue2023-uc,
  title     = "Phy-{Q} as a measure for physical reasoning intelligence",
  author    = "Xue, Cheng and Pinto, Vimukthini and Gamage, Chathura and
               Nikonova, Ekaterina and Zhang, Peng and Renz, Jochen",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  5,
  number    =  1,
  pages     = "83--93",
  abstract  = "Humans are well versed in reasoning about the behaviours of
               physical objects and choosing actions accordingly to accomplish
               tasks, while this remains a major challenge for artificial
               intelligence. To facilitate research addressing this problem, we
               propose a new testbed that requires an agent to reason about
               physical scenarios and take an action appropriately. Inspired by
               the physical knowledge acquired in infancy and the capabilities
               required for robots to operate in real-world environments, we
               identify 15 essential physical scenarios. We create a wide
               variety of distinct task templates, and we ensure that all the
               task templates within the same scenario can be solved by using
               one specific strategic physical rule. By having such a design, we
               evaluate two distinct levels of generalization, namely local
               generalization and broad generalization. We conduct an extensive
               evaluation with human players, learning agents with various input
               types and architectures, and heuristic agents with different
               strategies. Inspired by how the human intelligence quotient is
               calculated, we define the physical reasoning quotient (Phy-Q
               score) that reflects the physical reasoning intelligence of an
               agent using the physical scenarios we considered. Our evaluation
               shows that (1) all the agents are far below human performance,
               and (2) learning agents, even with good local generalization
               ability, struggle to learn the underlying physical reasoning
               rules and fail to generalize broadly. We encourage the
               development of intelligent agents that can reach the human-level
               Phy-Q score. When it comes to reasoning about the motion of
               physical objects, humans have natural intuitive physics
               knowledge. To test how good artificial learning agents are in
               similar predictive abilities, Xue and colleagues present a
               benchmark based on a two-dimensional physics environment in which
               15 physical reasoning skills are measured.",
  month     =  jan,
  year      =  2023,
  doi       = "10.1038/s42256-022-00583-4",
  issn      = "2522-5839,2522-5839",
  language  = "en"
}

@ARTICLE{Goyal2021-dr,
  title         = "Neural Production Systems: Learning Rule-Governed Visual
                   Dynamics",
  author        = "Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and
                   Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas
                   and Mozer, Michael and Bengio, Yoshua",
  journal       = "arXiv [cs.AI]",
  abstract      = "Visual environments are structured, consisting of distinct
                   objects or entities. These entities have properties -- both
                   visible and latent -- that determine the manner in which they
                   interact with one another. To partition images into entities,
                   deep-learning researchers have proposed structural inductive
                   biases such as slot-based architectures. To model
                   interactions among entities, equivariant graph neural nets
                   (GNNs) are used, but these are not particularly well suited
                   to the task for two reasons. First, GNNs do not predispose
                   interactions to be sparse, as relationships among independent
                   entities are likely to be. Second, GNNs do not factorize
                   knowledge about interactions in an entity-conditional manner.
                   As an alternative, we take inspiration from cognitive science
                   and resurrect a classic approach, production systems, which
                   consist of a set of rule templates that are applied by
                   binding placeholder variables in the rules to specific
                   entities. Rules are scored on their match to entities, and
                   the best fitting rules are applied to update entity
                   properties. In a series of experiments, we demonstrate that
                   this architecture achieves a flexible, dynamic flow of
                   control and serves to factorize entity-specific and
                   rule-based information. This disentangling of knowledge
                   achieves robust future-state prediction in rich visual
                   environments, outperforming state-of-the-art methods using
                   GNNs, and allows for the extrapolation from simple (few
                   object) environments to more complex environments.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2103.01937"
}

@ARTICLE{Sumers2023-kl,
  title    = "Show or tell? Exploring when (and why) teaching with language
              outperforms demonstration",
  author   = "Sumers, Theodore R and Ho, Mark K and Hawkins, Robert D and
              Griffiths, Thomas L",
  journal  = "Cognition",
  volume   =  232,
  pages    =  105326,
  abstract = "People use a wide range of communicative acts across different
              modalities, from concrete demonstrations to abstract language.
              While these modalities are typically studied independently, we
              take a comparative approach and ask when and why one modality
              might outperform another. We present a series of real-time,
              multi-player experiments asking participants to teach concepts
              using either demonstrations or language. Our first experiment
              (N=416) asks when language might outperform demonstration. We
              manipulate the complexity of the concept being taught and find
              that language communicates complex concepts more effectively than
              demonstration. We then ask why language succeeds in this setting.
              We hypothesized that language allowed teachers to reference
              abstract object features (e.g., shapes and colors), while
              demonstration teachers could only provide concrete examples
              (specific positive or negative objects). To test this hypothesis,
              our second experiment (N=568) ablated object features from the
              teacher's interface. This manipulation severely impaired
              linguistic (but not demonstrative) teaching. Our findings suggest
              that language communicates complex concepts by directly
              transmitting abstract rules. In contrast, demonstrations transmit
              examples, requiring the learner to infer the rules.",
  month    =  mar,
  year     =  2023,
  keywords = "Abstraction; Communication; Demonstration; Language; Pedagogy",
  doi      = "10.1016/j.cognition.2022.105326",
  pmid     =  36473238,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@MISC{Kosoy_undated-ah,
  title  = "Learning Causal Overhypotheses through Exploration in Children and
            Computational Models",
  author = "Kosoy, Eliza and Liu, Adrian and Collins, Jasmine and Chan, David M
            and Hamrick, Jessica B and Rosemary, Nan and Huang, Sandy Han and
            Kaufmann, Bryanna and Gopnik, Alison and Schölkopf, Bernhard and
            Uhler, Caroline and Zhang, Kun and Ke, N R and Canny, J",
  eprint = "2202.10430v1"
}

@ARTICLE{Knoblich1999-jd,
  title    = "Constraint relaxation and chunk decomposition in insight problem
              solving",
  author   = "Knoblich, Günther and Ohlsson, Stellan and Haider, Hilde and
              Rhenius, Detlef",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  25,
  number   =  6,
  pages    = "1534--1555",
  abstract = "Insight problem solving is characterized by impasses, states of
              mind in which the thinker does not know what to do next. The
              authors hypothesized that impasses are broken by changing the
              problem representation, and 2 hypothetical mechanisms for
              representational change are described: the relaxation of
              constraints on the solution and the decomposition of perceptual
              chunks. These 2 mechanisms generate specific predictions about the
              relative difficulty of individual problems and about differential
              transfer effects. The predictions were tested in 4 experiments
              using matchstick arithmetic problems. The results were consistent
              with the predictions. Representational change is a more powerful
              explanation for insight than alternative hypotheses, if the
              hypothesized change processes are specified in detail. Overcoming
              impasses in insight is a special case of the general need to
              override the imperatives of past experience in the face of novel
              conditions. (PsycINFO Database Record (c) 2016 APA, all rights
              reserved)",
  month    =  nov,
  year     =  1999,
  doi      = "10.1037/0278-7393.25.6.1534",
  issn     = "0278-7393,1939-1285"
}

@ARTICLE{Meltzoff2009-eq,
  title    = "Foundations for a new science of learning",
  author   = "Meltzoff, Andrew N and Kuhl, Patricia K and Movellan, Javier and
              Sejnowski, Terrence J",
  journal  = "Science",
  volume   =  325,
  number   =  5938,
  pages    = "284--288",
  abstract = "Human learning is distinguished by the range and complexity of
              skills that can be learned and the degree of abstraction that can
              be achieved compared with those of other species. Homo sapiens is
              also the only species that has developed formal ways to enhance
              learning: teachers, schools, and curricula. Human infants have an
              intense interest in people and their behavior and possess powerful
              implicit learning mechanisms that are affected by social
              interaction. Neuroscientists are beginning to understand the brain
              mechanisms underlying learning and how shared brain systems for
              perception and action support social learning. Machine learning
              algorithms are being developed that allow robots and computers to
              learn autonomously. New insights from many different fields are
              converging to create a new science of learning that may transform
              educational practices.",
  month    =  jul,
  year     =  2009,
  doi      = "10.1126/science.1175626",
  pmc      = "PMC2776823",
  pmid     =  19608908,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@ARTICLE{Zador2019-hq,
  title    = "A critique of pure learning and what artificial neural networks
              can learn from animal brains",
  author   = "Zador, Anthony M",
  journal  = "Nature communications",
  volume   =  10,
  number   =  1,
  pages    =  3770,
  abstract = "Artificial neural networks (ANNs) have undergone a revolution,
              catalyzed by better supervised learning algorithms. However, in
              stark contrast to young animals (including humans), training such
              networks requires enormous numbers of labeled examples, leading to
              the belief that animals must rely instead mainly on unsupervised
              learning. Here we argue that most animal behavior is not the
              result of clever learning algorithms-supervised or
              unsupervised-but is encoded in the genome. Specifically, animals
              are born with highly structured brain connectivity, which enables
              them to learn very rapidly. Because the wiring diagram is far too
              complex to be specified explicitly in the genome, it must be
              compressed through a ``genomic bottleneck''. The genomic
              bottleneck suggests a path toward ANNs capable of rapid learning.",
  month    =  aug,
  year     =  2019,
  doi      = "10.1038/s41467-019-11786-6",
  pmc      = "PMC6704116",
  pmid     =  31434893,
  issn     = "2041-1723",
  language = "en"
}

@ARTICLE{McClelland2003-bz,
  title    = "The parallel distributed processing approach to semantic cognition",
  author   = "McClelland, James L and Rogers, Timothy T",
  journal  = "Nature reviews. Neuroscience",
  volume   =  4,
  number   =  4,
  pages    = "310--322",
  month    =  apr,
  year     =  2003,
  doi      = "10.1038/nrn1076",
  pmid     =  12671647,
  issn     = "1471-003X",
  language = "en"
}

@ARTICLE{Kaplan1990-zd,
  title    = "In search of insight",
  author   = "Kaplan, Craig A and Simon, Herbert A",
  journal  = "Cognitive psychology",
  volume   =  22,
  number   =  3,
  pages    = "374--419",
  abstract = "This paper describes the process of attaining the insight required
              to solve a particular problem—the Mutilated Checkerboard (MC)
              problem. It shows that attaining insight requires discovering an
              effective problem representation, and that performance on insight
              problems can be predicted from the availability of generators and
              constraints in the search for such a representation. To test these
              claims we varied the salience of features leading to the critical
              concept of parity in the MC problem. Using chronometric measures,
              verbal protocols, and computer simulations, we explored first why
              it is difficult to find a representation for the Checkerboard
              problem, and then tested four potential sources of search
              constraint for reducing the difficulty: cue salience
              manipulations, prior knowledge, hints, and heuristics. While
              subjects used each of these four sources of constraint, a
              particular heuristic—noticing properties of the situation that
              remained invariant during solution attempts (the Notice Invariants
              heuristic)—proved to be a particularly powerful means for focusing
              search. In conjunction with hints and independently, it played a
              major part in producing the insight that yielded an effective
              problem representation and solution.",
  month    =  jul,
  year     =  1990,
  doi      = "10.1016/0010-0285(90)90008-R",
  issn     = "0010-0285"
}

@ARTICLE{Nosofsky2022-fh,
  title    = "Generalization in Distant Regions of a Rule-Described Category
              Space: a Mixed Exemplar and Logical-Rule-Based Account",
  author   = "Nosofsky, Robert M and Hu, Mingjia",
  journal  = "Computational Brain \& Behavior",
  volume   =  5,
  number   =  4,
  pages    = "435--466",
  abstract = "An important question in the cognitive-psychology of category
              learning concerns the manner in which observers generalize their
              trained category knowledge at time of transfer. In recent work,
              Conaway and Kurtz (Conaway and Kurtz, Psychonomic Bulletin \&
              Review 24:1312–1323, 2017) reported results from a novel paradigm
              in which participants learned rule-described categories defined
              over two dimensions and then classified test items in distant
              transfer regions of the category space. The paradigm yielded
              results that challenged the predictions from both exemplar-based
              and logical-rule-based models of categorization but that the
              authors suggested were as predicted by a divergent auto-encoder
              (DIVA) model (Kurtz, Psychonomic Bulletin \& Review 14:560–576,
              2007, Kurtz, Psychology of learning and motivation, Academic
              Press, New York, 2015). In this article, we pursue these
              challenges by conducting replications and extensions of the
              original experiment and fitting a variety of computational models
              to the resulting data. We find that even an extended version of
              the exemplar model that makes allowance for learning-during-test
              (LDT) processes fails to account for the results. In addition,
              models that presume a mixture of salient logical rules also fail
              to account for the findings. However, as a proof of concept, we
              illustrate that a model that assumes a mixture of strategies
              across subjects—some relying on exemplar-based memories with LDT,
              and others on salient logical rules—provides an outstanding
              account of the data. By comparison, DIVA performs considerably
              worse than does this LDT-exemplar-rule mixture account. These
              results converge with past ones reported in the literature that
              point to multiple forms of category representation as well as to
              the role of LDT processes in influencing how observers generalize
              their category knowledge.",
  month    =  dec,
  year     =  2022,
  doi      = "10.1007/s42113-022-00151-4",
  issn     = "2522-087X"
}

@ARTICLE{Ash2006-bc,
  title    = "The nature of restructuring in insight: an individual-differences
              approach",
  author   = "Ash, Ivan K and Wiley, Jennifer",
  journal  = "Psychonomic bulletin \& review",
  volume   =  13,
  number   =  1,
  pages    = "66--73",
  abstract = "The insightful problem-solving process has been proposed to
              involve three main phases: an initial representation phase, in
              which the solver inappropriately represents the problem; an
              initial search through the faulty problem space that may lead to
              impasse; and a postimpasse restructuring phase. Some theories
              propose that the restructuring phase involves controlled search
              processes, whereas other theories propose that restructuring is
              achieved through the automatic redistribution of activation in
              long-term memory. In this study, we used correlations between
              working memory (WM) span measures and problem-solving success to
              test the predictions of these different theories. One group of
              participants received a set of insight problems that allowed for a
              large initial faulty search space, whereas another group received
              a matched set that constrained the initial faulty search space in
              order to isolate the restructuring phase of the insightful
              process. The results suggest that increased ability to control
              attention (as measured by WM span tasks) predicts an individual's
              ability to successfully solve problems that involve both the
              initial search phase and the restructuring phase. However,
              individual differences in ability to control attention do not
              predict success on problems that isolate the restructuring phase.
              These results are interpreted as supporting an
              automatic-redistribution-of-activation account of restructuring.",
  month    =  feb,
  year     =  2006,
  doi      = "10.3758/bf03193814",
  pmid     =  16724770,
  issn     = "1069-9384",
  language = "en"
}

@ARTICLE{Jones2003-sh,
  title    = "Testing two cognitive theories of insight",
  author   = "Jones, Gary",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  29,
  number   =  5,
  pages    = "1017--1027",
  abstract = "Insight in problem solving occurs when the problem solver fails to
              see how to solve a problem and then--``aha!''--there is a sudden
              realization how to solve it. Two contemporary theories have been
              proposed to explain insight. The representational change theory
              (e.g., G. Knoblich, S. Ohlsson, \& G. E. Rainey, 2001) proposes
              that insight occurs through relaxing self-imposed constraints on a
              problem and by decomposing chunked items in the problem. The
              progress monitoring theory (e.g., J. N. MacGregor, T. C. Ormerod,
              \& E. P. Chronicle, 2001) proposes that insight is only sought
              once it becomes apparent that the distance to the goal is
              unachievable in the moves remaining. These 2 theories are tested
              in an unlimited move problem, to which neither theory has
              previously been applied. The results lend support to both, but
              experimental manipulations to the problem suggest that the
              representational change theory is the better indicator of
              performance. The findings suggest that testable opposing
              predictions can be made to examine theories of insight and that
              the use of eye movement data is a fruitful method of both
              examining insight and testing theories of insight.",
  month    =  sep,
  year     =  2003,
  doi      = "10.1037/0278-7393.29.5.1017",
  pmid     =  14516232,
  issn     = "0278-7393",
  language = "en"
}

@ARTICLE{Ollinger2013-zu,
  title    = "Cognitive mechanisms of insight: the role of heuristics and
              representational change in solving the eight-coin problem",
  author   = "Öllinger, Michael and Jones, Gary and Faber, Amory H and Knoblich,
              Günther",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  39,
  number   =  3,
  pages    = "931--939",
  abstract = "The 8-coin insight problem requires the problem solver to move 2
              coins so that each coin touches exactly 3 others. Ormerod,
              MacGregor, and Chronicle (2002) explained differences in task
              performance across different versions of the 8-coin problem using
              the availability of particular moves in a 2-dimensional search
              space. We explored 2 further explanations by developing 6 new
              versions of the 8-coin problem in order to investigate the
              influence of grouping and self-imposed constraints on solutions.
              The results identified 2 sources of problem difficulty: first, the
              necessity to overcome the constraint that a solution can be found
              in 2-dimensional space and, second, the necessity to decompose
              perceptual groupings. A detailed move analysis suggested that the
              selection of moves was driven by the established representation
              rather than the application of the appropriate heuristics. Both
              results support the assumptions of representational change theory
              (Ohlsson, 1992).",
  month    =  may,
  year     =  2013,
  doi      = "10.1037/a0029194",
  pmid     =  22799283,
  issn     = "0278-7393,1939-1285",
  language = "en"
}

@ARTICLE{Gallistel2021-ft,
  title    = "The physical basis of memory",
  author   = "Gallistel, C R",
  journal  = "Cognition",
  volume   =  213,
  pages    =  104533,
  abstract = "Neuroscientists are searching for the engram within the conceptual
              framework established by John Locke's theory of mind. This
              framework was elaborated before the development of information
              theory, before the development of information processing machines
              and the science of computation, before the discovery that
              molecules carry hereditary information, before the discovery of
              the codon code and the molecular machinery for editing the
              messages written in this code and translating it into
              transcription factors that mark abstract features of organic
              structure such as anterior and distal. The search for the engram
              needs to abandon Locke's conceptual framework and work within a
              framework informed by these developments. The engram is the medium
              by which information extracted from past experience is transmitted
              to the computations that inform future behavior. The
              information-conveying symbols in the engram are rapidly generated
              in the course of computations, which implies that they are
              molecules.",
  month    =  aug,
  year     =  2021,
  keywords = "Communication channel; Engram; Molecules; Plastic synapse",
  doi      = "10.1016/j.cognition.2020.104533",
  pmid     =  33375954,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Kershaw2013-hb,
  title     = "Multiple paths to transfer and constraint relaxation in insight
               problem solving",
  author    = "Kershaw, Trina C and Flynn, Christopher K and Gordon, Leamarie T",
  journal   = "Thinking \& reasoning",
  publisher = "Routledge",
  volume    =  19,
  number    =  1,
  pages     = "96--136",
  abstract  = "In two experiments participants received various training methods
               designed to relax constraints present in the Four-Tree problem
               (deBono, 1967), a difficult insight problem. Geometry
               misconceptions were corrected via direct instruction.
               Participants? difficulty with developing three-dimensional
               representations was addressed via spontaneous analogical transfer
               (Experiment 1) or via cued analogical transfer (Experiment 2). We
               found that, while both training methods were effective,
               alleviating multiple constraints was more effective than the
               alleviation of single constraints via training programmes (c.f.
               Kershaw Nokes \& Ohlsson, 2005) and multiple constraints are
               discussed.",
  month     =  feb,
  year      =  2013,
  doi       = "10.1080/13546783.2012.742852",
  issn      = "1354-6783"
}

@ARTICLE{Chu2007-xx,
  title    = "Theory Driven Hints in the Cheap Necklace Problem: A Preliminary
              Investigation",
  author   = "Chu, Yun and Dewald, Andrew D and Chronicle, Edward P",
  journal  = "The Journal of Problem Solving",
  volume   =  1,
  number   =  2,
  pages    =  4,
  abstract = "Three experiments investigated the effects of two hints derived
              from the Criterion for Satisfactory Progress theory (CSP) and
              Representational Change Theory (RCT) on the cheap necklace problem
              (insight problem). In Experiment 1, fewer participants given the
              CSP hint used an incorrect (maximizing) first move than
              participants given the RCT hint or control participants given no
              hint on a single attempt at the problem. Experiment 2 found the
              number of trials to solution was fewer in the CSP condition than
              in the control over ten trials, and there were fewer incorrect
              first moves in the CSP. The results appear to support the CSP
              theory. However, in Experiment 3, the CSP and RCT hints were
              combined yielding a 75\% solution rate over 34.88\% in the
              control. Perhaps aspects from both theories are employed during
              the problem solving process.",
  year     =  2007,
  doi      = "10.7771/1932-6246.1010",
  issn     = "1932-6246"
}

@ARTICLE{Chater2013-hv,
  title    = "Programs as causal models: speculations on mental programs and
              mental representation",
  author   = "Chater, Nick and Oaksford, Mike",
  journal  = "Cognitive science",
  volume   =  37,
  number   =  6,
  pages    = "1171--1191",
  abstract = "Judea Pearl has argued that counterfactuals and causality are
              central to intelligence, whether natural or artificial, and has
              helped create a rich mathematical and computational framework for
              formally analyzing causality. Here, we draw out connections
              between these notions and various current issues in cognitive
              science, including the nature of mental ``programs'' and mental
              representation. We argue that programs (consisting of algorithms
              and data structures) have a causal (counterfactual-supporting)
              structure; these counterfactuals can reveal the nature of mental
              representations. Programs can also provide a causal model of the
              external world. Such models are, we suggest, ubiquitous in
              perception, cognition, and language processing.",
  month    =  aug,
  year     =  2013,
  doi      = "10.1111/cogs.12062",
  pmid     =  23855554,
  issn     = "0364-0213,1551-6709",
  language = "en"
}

@ARTICLE{Chang2016-qi,
  title         = "A Compositional Object-Based Approach to Learning Physical
                   Dynamics",
  author        = "Chang, Michael B and Ullman, Tomer and Torralba, Antonio and
                   Tenenbaum, Joshua B",
  journal       = "arXiv [cs.AI]",
  abstract      = "We present the Neural Physics Engine (NPE), a framework for
                   learning simulators of intuitive physics that naturally
                   generalize across variable object count and different scene
                   configurations. We propose a factorization of a physical
                   scene into composable object-based representations and a
                   neural network architecture whose compositional structure
                   factorizes object dynamics into pairwise interactions. Like a
                   symbolic physics engine, the NPE is endowed with generic
                   notions of objects and their interactions; realized as a
                   neural network, it can be trained via stochastic gradient
                   descent to adapt to specific object properties and dynamics
                   of different worlds. We evaluate the efficacy of our approach
                   on simple rigid body dynamics in two-dimensional worlds. By
                   comparing to less structured architectures, we show that the
                   NPE's compositional representation of the structure in
                   physical interactions improves its ability to predict
                   movement, generalize across variable object count and
                   different scene configurations, and infer latent properties
                   of objects such as mass.",
  month         =  dec,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1612.00341"
}

@ARTICLE{Van_Schijndel2021-qq,
  title    = "Single-Stage Prediction Models Do Not Explain the Magnitude of
              Syntactic Disambiguation Difficulty",
  author   = "van Schijndel, Marten and Linzen, Tal",
  journal  = "Cognitive science",
  volume   =  45,
  number   =  6,
  pages    = "e12988",
  abstract = "The disambiguation of a syntactically ambiguous sentence in favor
              of a less preferred parse can lead to slower reading at the
              disambiguation point. This phenomenon, referred to as a
              garden-path effect, has motivated models in which readers
              initially maintain only a subset of the possible parses of the
              sentence, and subsequently require time-consuming reanalysis to
              reconstruct a discarded parse. A more recent proposal argues that
              the garden-path effect can be reduced to surprisal arising in a
              fully parallel parser: words consistent with the initially
              dispreferred but ultimately correct parse are simply less
              predictable than those consistent with the incorrect parse. Since
              predictability has pervasive effects in reading far beyond
              garden-path sentences, this account, which dispenses with
              reanalysis mechanisms, is more parsimonious. Crucially, it
              predicts a linear effect of surprisal: the garden-path effect is
              expected to be proportional to the difference in word surprisal
              between the ultimately correct and ultimately incorrect
              interpretations. To test this prediction, we used recurrent neural
              network language models to estimate word-by-word surprisal for
              three temporarily ambiguous constructions. We then estimated the
              slowdown attributed to each bit of surprisal from human self-paced
              reading times, and used that quantity to predict syntactic
              disambiguation difficulty. Surprisal successfully predicted the
              existence of garden-path effects, but drastically underpredicted
              their magnitude, and failed to predict their relative severity
              across constructions. We conclude that a full explanation of
              syntactic disambiguation difficulty may require recovery
              mechanisms beyond predictability.",
  month    =  jun,
  year     =  2021,
  keywords = "Garden paths; Information theory; Neural networks; Self-paced
              reading; Surprisal",
  doi      = "10.1111/cogs.12988",
  pmid     =  34170031,
  issn     = "0364-0213,1551-6709",
  language = "en"
}

@ARTICLE{Arehalli2022-xn,
  title         = "Syntactic Surprisal From Neural Models Predicts, But
                   Underestimates, Human Processing Difficulty From Syntactic
                   Ambiguities",
  author        = "Arehalli, Suhas and Dillon, Brian and Linzen, Tal",
  journal       = "arXiv [cs.CL]",
  abstract      = "Humans exhibit garden path effects: When reading sentences
                   that are temporarily structurally ambiguous, they slow down
                   when the structure is disambiguated in favor of the less
                   preferred alternative. Surprisal theory (Hale, 2001; Levy,
                   2008), a prominent explanation of this finding, proposes that
                   these slowdowns are due to the unpredictability of each of
                   the words that occur in these sentences. Challenging this
                   hypothesis, van Schijndel \& Linzen (2021) find that
                   estimates of the cost of word predictability derived from
                   language models severely underestimate the magnitude of human
                   garden path effects. In this work, we consider whether this
                   underestimation is due to the fact that humans weight
                   syntactic factors in their predictions more highly than
                   language models do. We propose a method for estimating
                   syntactic predictability from a language model, allowing us
                   to weigh the cost of lexical and syntactic predictability
                   independently. We find that treating syntactic predictability
                   independently from lexical predictability indeed results in
                   larger estimates of garden path. At the same time, even when
                   syntactic predictability is independently weighted, surprisal
                   still greatly underestimate the magnitude of human garden
                   path effects. Our results support the hypothesis that
                   predictability is not the only factor responsible for the
                   processing cost associated with garden path sentences.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.12187"
}

@ARTICLE{Almaatouq2022-xz,
  title    = "Beyond Playing 20 Questions with Nature: Integrative Experiment
              Design in the Social and Behavioral Sciences",
  author   = "Almaatouq, Abdullah and Griffiths, Thomas L and Suchow, Jordan W
              and Whiting, Mark E and Evans, James and Watts, Duncan J",
  journal  = "The Behavioral and brain sciences",
  pages    = "1--55",
  abstract = "The dominant paradigm of experiments in the social and behavioral
              sciences views an experiment as a test of a theory, where the
              theory is assumed to generalize beyond the experiment's specific
              conditions. According to this view, which Alan Newell once
              characterized as ``playing twenty questions with nature,'' theory
              is advanced one experiment at a time, and the integration of
              disparate findings is assumed to happen via the scientific
              publishing process. In this article, we argue that the process of
              integration is at best inefficient, and at worst it does not, in
              fact, occur. We further show that the challenge of integration
              cannot be adequately addressed by recently proposed reforms that
              focus on the reliability and replicability of individual findings,
              nor simply by conducting more or larger experiments. Rather, the
              problem arises from the imprecise nature of social and behavioral
              theories and, consequently, a lack of commensurability across
              experiments conducted under different conditions. Therefore,
              researchers must fundamentally rethink how they design experiments
              and how the experiments relate to theory. We specifically describe
              an alternative framework, integrative experiment design, which
              intrinsically promotes commensurability and continuous integration
              of knowledge. In this paradigm, researchers explicitly map the
              design space of possible experiments associated with a given
              research question, embracing many potentially relevant theories
              rather than focusing on just one. The researchers then iteratively
              generate theories and test them with experiments explicitly
              sampled from the design space, allowing results to be integrated
              across experiments. Given recent methodological and technological
              developments, we conclude that this approach is feasible and would
              generate more-reliable, more-cumulative empirical and theoretical
              knowledge than the current paradigm-and with far greater
              efficiency.",
  month    =  dec,
  year     =  2022,
  keywords = "(in)commensurability; cumulative knowledge; experiments;
              generalizability",
  doi      = "10.1017/S0140525X22002874",
  pmid     =  36539303,
  issn     = "0140-525X,1469-1825",
  language = "en"
}

@MISC{Eichenbaum2008-pi,
  title        = "Learning \& memory",
  author       = "Eichenbaum, H",
  publisher    = "aabmc.org",
  abstract     = "… Next, Eichenbaum incorporates animal and human … Eichenbaum
                  organizes the text around multiple memory systems, moving from
                  simple to more complex forms of learning and memory …",
  year         =  2008,
  howpublished = "\url{https://aabmc.org/sites/default/files/webform/stories-photos/pdf-learning--memory-howard-eichenbaum-pdf-download-free-book-3e8075a.pdf}",
  note         = "Accessed: 2023-1-30"
}

@ARTICLE{Ormerod2002-tb,
  title    = "Dynamics and constraints in insight problem solving",
  author   = "Ormerod, Thomas C and MacGregor, James N and Chronicle, Edward P",
  journal  = "Journal of experimental psychology. Learning, memory, and
              cognition",
  volume   =  28,
  number   =  4,
  pages    = "791--799",
  abstract = "This article reports 2 experiments that investigated performance
              on a novel insight problem, the 8-coin problem. The authors
              hypothesized that participants would make certain initial moves
              (strategic moves) that seemed to make progress according to the
              problem instructions but that nonetheless would guarantee failure
              to solve the problem. Experiment 1 manipulated the starting state
              of the problem and showed that overall solution rates were lower
              when such strategic moves were available. Experiment 2 showed that
              failure to capitalize on visual hints about the correct first move
              was also associated with the availability of strategic moves. The
              results are interpreted in terms of an information-processing
              framework previously applied to the 9-dot problem. The authors
              argue that in addition to the operation of inappropriate
              constraints, a full account of insight problem solving must
              incorporate a dynamic that steers solution-seeking activity toward
              the constraints.",
  month    =  jul,
  year     =  2002,
  doi      = "10.1037//0278-7393.28.4.791",
  pmid     =  12109769,
  issn     = "0278-7393",
  language = "en"
}

@ARTICLE{Gilhooly2005-zf,
  title     = "Differentiating insight from non-insight problems",
  author    = "Gilhooly, K J and Murphy, P",
  journal   = "Thinking \& reasoning",
  publisher = "Routledge",
  volume    =  11,
  number    =  3,
  pages     = "279--302",
  abstract  = "This study aimed to investigate whether a range of tasks that
               have been generally classed as requiring insight form an
               empirically separable group of tasks distinct from tasks
               generally classed as non-insight. In this study, 24 insight
               tasks, 10 non-insight tasks, and tests of individual differences
               in cognitive abilities and working memory were administered to 60
               participants. Cluster analysis of the problem-solving tasks
               indicated that the presumed insight problems did tend to cluster
               with other presumed insight problems, and similarly the presumed
               non-insight problems tended to cluster with other presumed
               non-insight tasks. Performance on presumed insight problems was
               particularly linked to measures of ideational flexibility with a
               different pattern of results for the non-insight tasks. Spatial
               insight problems were linked to spatial flexibility and verbal
               insight tasks were linked to vocabulary scores. The results are
               discussed in relation to recent developments of dual process
               theories of thinking.",
  month     =  aug,
  year      =  2005,
  doi       = "10.1080/13546780442000187",
  issn      = "1354-6783"
}

@ARTICLE{Schilling2005-bz,
  title     = "A ``Small-World'' Network Model of Cognitive Insight",
  author    = "Schilling, Melissa A",
  journal   = "Creativity research journal",
  publisher = "Routledge",
  volume    =  17,
  number    = "2-3",
  pages     = "131--154",
  abstract  = "Despite many decades of study, scientists still puzzle over the
               process of insight. By what mechanism does a person experience
               that ``Aha!'' moment, when sudden clarity emerges from a tangled
               web of thoughts and ideas? This research integrates psychological
               work on insight with graph theoretic work on ``small-world''
               phenomenon, to construct a theory that explains how insight
               occurs, how it is similar to and different from more typical
               learning processes, and why it yields an affective response in
               the individual. I propose that cognitive insight occurs when an
               atypical association, forged through random recombination or
               directed search, results in a ``shortcut'' in an individual's
               network of representations. This causes a rapid decrease in path
               length, reorients the individual's understanding of the
               relationships within and among the affected representations, and
               can prompt a cascade of other connections. This result is
               demonstrated by applying graph theoretical analysis to network
               translations of commonly used insight problems.",
  month     =  jul,
  year      =  2005,
  doi       = "10.1080/10400419.2005.9651475",
  issn      = "1040-0419"
}

@ARTICLE{Lakretz2021-go,
  title    = "Mechanisms for handling nested dependencies in neural-network
              language models and humans",
  author   = "Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and
              Marelli, Marco and Baroni, Marco and Dehaene, Stanislas",
  journal  = "Cognition",
  volume   =  213,
  pages    =  104699,
  abstract = "Recursive processing in sentence comprehension is considered a
              hallmark of human linguistic abilities. However, its underlying
              neural mechanisms remain largely unknown. We studied whether a
              modern artificial neural network trained with ``deep learning''
              methods mimics a central aspect of human sentence processing,
              namely the storing of grammatical number and gender information in
              working memory and its use in long-distance agreement (e.g.,
              capturing the correct number agreement between subject and verb
              when they are separated by other phrases). Although the network, a
              recurrent architecture with Long Short-Term Memory units, was
              solely trained to predict the next word in a large corpus,
              analysis showed the emergence of a very sparse set of specialized
              units that successfully handled local and long-distance syntactic
              agreement for grammatical number. However, the simulations also
              showed that this mechanism does not support full recursion and
              fails with some long-range embedded dependencies. We tested the
              model's predictions in a behavioral experiment where humans
              detected violations in number agreement in sentences with
              systematic variations in the singular/plural status of multiple
              nouns, with or without embedding. Human and model error patterns
              were remarkably similar, showing that the model echoes various
              effects observed in human data. However, a key difference was
              that, with embedded long-range dependencies, humans remained above
              chance level, while the model's systematic errors brought it below
              chance. Overall, our study shows that exploring the ways in which
              modern artificial neural networks process sentences leads to
              precise and testable hypotheses about human linguistic
              performance.",
  month    =  aug,
  year     =  2021,
  keywords = "Grammatical agreement; Language models; Long-range dependencies;
              Recurrent neural networks; Recursion; Relative clauses; Syntactic
              processing",
  doi      = "10.1016/j.cognition.2021.104699",
  pmid     =  33941375,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Lampinen2022-oy,
  title         = "Can language models handle recursively nested grammatical
                   structures? A case study on comparing models and humans",
  author        = "Lampinen, Andrew Kyle",
  journal       = "arXiv [cs.CL]",
  abstract      = "How should we compare the capabilities of language models and
                   humans? Here, I consider a case study: processing of
                   recursively nested grammatical structures. Prior work has
                   suggested that language models cannot handle these structures
                   as reliably as humans can. However, the humans were provided
                   with instructions and training before being evaluated, while
                   the language models were evaluated zero-shot. I therefore
                   attempt to more closely match the evaluation paradigms by
                   providing language models with few-shot prompts. A simple
                   prompt, which contains substantially less content than the
                   human training, allows large language models to consistently
                   outperform the human results. The same prompt even allows
                   extrapolation to more deeply nested conditions than have been
                   tested in humans. Further, a reanalysis of the prior human
                   experiments suggests that the humans may not perform above
                   chance at the difficult structures initially. These results
                   suggest that large language models can in fact process
                   recursively nested grammatical structures comparably to
                   humans. This case study highlights how discrepancies in the
                   quantity of experiment-specific context can confound
                   comparisons of language models and humans. I use this case
                   study to reflect on the broader challenge of comparing human
                   and model capabilities, and to suggest that there is an
                   important difference between evaluating cognitive models of a
                   specific phenomenon and evaluating broadly-trained models.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.15303"
}

@ARTICLE{Josselyn2020-xn,
  title    = "Memory engrams: Recalling the past and imagining the future",
  author   = "Josselyn, Sheena A and Tonegawa, Susumu",
  journal  = "Science",
  volume   =  367,
  number   =  6473,
  abstract = "In 1904, Richard Semon introduced the term ``engram'' to describe
              the neural substrate for storing memories. An experience, Semon
              proposed, activates a subset of cells that undergo off-line,
              persistent chemical and/or physical changes to become an engram.
              Subsequent reactivation of this engram induces memory retrieval.
              Although Semon's contributions were largely ignored in his
              lifetime, new technologies that allow researchers to image and
              manipulate the brain at the level of individual neurons has
              reinvigorated engram research. We review recent progress in
              studying engrams, including an evaluation of evidence for the
              existence of engrams, the importance of intrinsic excitability and
              synaptic plasticity in engrams, and the lifetime of an engram.
              Together, these findings are beginning to define an engram as the
              basic unit of memory.",
  month    =  jan,
  year     =  2020,
  doi      = "10.1126/science.aaw4325",
  pmc      = "PMC7577560",
  pmid     =  31896692,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@ARTICLE{Scoville1957-dh,
  title    = "Loss of recent memory after bilateral hippocampal lesions",
  author   = "Scoville, W B and Milner, B",
  journal  = "Journal of neurology, neurosurgery, and psychiatry",
  volume   =  20,
  number   =  1,
  pages    = "11--21",
  month    =  feb,
  year     =  1957,
  keywords = "MEMORY; TEMPORAL LOBE/surgery",
  doi      = "10.1136/jnnp.20.1.11",
  pmc      = "PMC497229",
  pmid     =  13406589,
  issn     = "0022-3050",
  language = "en"
}

@ARTICLE{Whittington2021-sj,
  title         = "Relating transformers to models and neural representations of
                   the hippocampal formation",
  author        = "Whittington, James C R and Warren, Joseph and Behrens,
                   Timothy E J",
  journal       = "arXiv [cs.NE]",
  abstract      = "Many deep neural network architectures loosely based on brain
                   networks have recently been shown to replicate neural firing
                   patterns observed in the brain. One of the most exciting and
                   promising novel architectures, the Transformer neural
                   network, was developed without the brain in mind. In this
                   work, we show that transformers, when equipped with recurrent
                   position encodings, replicate the precisely tuned spatial
                   representations of the hippocampal formation; most notably
                   place and grid cells. Furthermore, we show that this result
                   is no surprise since it is closely related to current
                   hippocampal models from neuroscience. We additionally show
                   the transformer version offers dramatic performance gains
                   over the neuroscience version. This work continues to bind
                   computations of artificial and brain networks, offers a novel
                   understanding of the hippocampal-cortical interaction, and
                   suggests how wider cortical areas may perform complex tasks
                   beyond current neuroscience models such as language
                   comprehension.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "2112.04035"
}

@ARTICLE{Goldstone1998-ne,
  title    = "Perceptual learning",
  author   = "Goldstone, R L",
  journal  = "Annual review of psychology",
  volume   =  49,
  pages    = "585--612",
  abstract = "Perceptual learning involves relatively long-lasting changes to an
              organism's perceptual system that improve its ability to respond
              to its environment. Four mechanisms of perceptual learning are
              discussed: attention weighting, imprinting, differentiation, and
              unitization. By attention weighting, perception becomes adapted to
              tasks and environments by increasing the attention paid to
              important dimensions and features. By imprinting, receptors are
              developed that are specialized for stimuli or parts of stimuli. By
              differentiation, stimuli that were once indistinguishable become
              psychologically separated. By unitization, tasks that originally
              required detection of several parts are accomplished by detecting
              a single constructed unit representing a complex configuration.
              Research from cognitive psychology, psychophysics, neuroscience,
              expert/novice differences, development, computer science, and
              cross-cultural differences is described that relates to these
              mechanisms. The locus, limits, and applications of perceptual
              learning are also discussed.",
  year     =  1998,
  doi      = "10.1146/annurev.psych.49.1.585",
  pmid     =  9496632,
  issn     = "0066-4308",
  language = "en"
}

@ARTICLE{Barlow1989-pm,
  title     = "Unsupervised learning",
  author    = "Barlow, H B",
  journal   = "Neural computation",
  publisher = "MIT Press - Journals",
  volume    =  1,
  number    =  3,
  pages     = "295--311",
  abstract  = "What use can the brain make of the massive flow of sensory
               information that occurs without any associated rewards or
               punishments? This question is reviewed in the light of
               connectionist models of unsupervised learning and some older
               ideas, namely the cognitive maps and working models of Tolman and
               Craik, and the idea that redundancy is important for
               understanding perception (Attneave 1954), the physiology of
               sensory pathways (Barlow 1959), and pattern recognition (Watanabe
               1960). It is argued that (1) The redundancy of sensory messages
               provides the knowledge incorporated in the maps or models. (2)
               Some of this knowledge can be obtained by observations of mean,
               variance, and covariance of sensory messages, and perhaps also by
               a method called “minimum entropy coding.” (3) Such knowledge may
               be incorporated in a model of “what usually happens” with which
               incoming messages are automatically compared, enabling unexpected
               discrepancies to be immediately identified. (4) Knowledge of the
               sort incorporated into such a filter is a necessary prerequisite
               of ordinary learning, and a representation whose elements are
               independent makes it possible to form associations with logical
               functions of the elements, not just with the elements themselves.",
  month     =  sep,
  year      =  1989,
  doi       = "10.1162/neco.1989.1.3.295",
  issn      = "0899-7667,1530-888X",
  language  = "en"
}

@ARTICLE{Aslin2012-rt,
  title    = "Statistical learning: From acquiring specific items to forming
              general rules",
  author   = "Aslin, Richard N and Newport, Elissa L",
  journal  = "Current directions in psychological science",
  volume   =  21,
  number   =  3,
  pages    = "170--176",
  abstract = "Statistical learning is a rapid and robust mechanism that enables
              adults and infants to extract patterns of stimulation embedded in
              both language and visual domains. Importantly, statistical
              learning operates implicitly, without instruction, through mere
              exposure to a set of input stimuli. However, much of what learners
              must acquire about a structured domain consists of principles or
              rules that can be applied to novel inputs. Although it has been
              claimed that statistical learning and rule learning are separate
              mechanisms, here we review evidence and provide a unifying
              perspective that argues for a single mechanism of statistical
              learning that accounts for both the learning of the input stimuli
              and the generalization to novel instances. The balance between
              instance-learning and generalization is based on two factors: the
              strength of perceptual biases that highlight structural
              regularities, and the consistency of unique versus overlapping
              contexts in the input.",
  month    =  jun,
  year     =  2012,
  keywords = "generalization; infants; rule learning; statistical learning",
  doi      = "10.1177/0963721412436806",
  pmc      = "PMC3758750",
  pmid     =  24000273,
  issn     = "0963-7214",
  language = "en"
}

@ARTICLE{Krathwohl2002-bb,
  title     = "A revision of bloom's taxonomy: An overview",
  author    = "Krathwohl, David R",
  journal   = "Theory into practice",
  publisher = "Informa UK Limited",
  address   = "New York, United States, Columbus",
  volume    =  41,
  number    =  4,
  pages     = "212--218",
  abstract  = "From One Dimension to Two Dimensions Objectives that describe
               intended learning outcomes as the result of instruction are
               usually framed in terms of (a) some subject matter content and
               (b) a description of what is to be done with or to that content.
               [...]statements of objectives typically consist of a noun or noun
               phrase-the subject matter content-and a verb or verb phrase-the
               cognitive process(es). [...]any objective could be classified in
               the Taxonomy Table in one or more cells that correspond with the
               intersection of the columns) appropriate for categorizing the
               verbs) and the rows) appropriate for categorizing the nouns) or
               noun phrase(s). Analyze, of course, would be 4. Since both
               categories of cognitive processes are likely to be involved (with
               students being expected to analyze before they create), we would
               place this objective in two cells of the Taxonomy Table: B4,
               Analyze Conceptual Knowledge, and B6, Create [based on]
               Conceptual Knowledge (see Figure 1). In addition to showing what
               was included, the Taxonomy Table also suggests what might have
               been but wasn't. [...]in Figure 2, the two blank bottom rows
               raise questions about whether there might have been procedural or
               metacognitive knowledge objectives that could have been included.",
  month     =  nov,
  year      =  2002,
  doi       = "10.1207/s15430421tip4104\_2",
  issn      = "0040-5841,1543-0421",
  language  = "en"
}

@ARTICLE{Ho2022-iy,
  title    = "People construct simplified mental representations to plan",
  author   = "Ho, Mark K and Abel, David and Correa, Carlos G and Littman,
              Michael L and Cohen, Jonathan D and Griffiths, Thomas L",
  journal  = "Nature",
  volume   =  606,
  number   =  7912,
  pages    = "129--136",
  abstract = "One of the most striking features of human cognition is the
              ability to plan. Two aspects of human planning stand out-its
              efficiency and flexibility. Efficiency is especially impressive
              because plans must often be made in complex environments, and yet
              people successfully plan solutions to many everyday problems
              despite having limited cognitive resources1-3. Standard accounts
              in psychology, economics and artificial intelligence have
              suggested that human planning succeeds because people have a
              complete representation of a task and then use heuristics to plan
              future actions in that representation4-11. However, this approach
              generally assumes that task representations are fixed. Here we
              propose that task representations can be controlled and that such
              control provides opportunities to quickly simplify problems and
              more easily reason about them. We propose a computational account
              of this simplification process and, in a series of preregistered
              behavioural experiments, show that it is subject to online
              cognitive control12-14 and that people optimally balance the
              complexity of a task representation and its utility for planning
              and acting. These results demonstrate how strategically perceiving
              and conceiving problems facilitates the effective use of limited
              cognitive resources.",
  month    =  jun,
  year     =  2022,
  doi      = "10.1038/s41586-022-04743-9",
  pmc      =  5111694,
  pmid     =  35589843,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Carta2023-ng,
  title         = "Grounding Large Language Models in Interactive Environments
                   with Online Reinforcement Learning",
  author        = "Carta, Thomas and Romac, Clément and Wolf, Thomas and
                   Lamprier, Sylvain and Sigaud, Olivier and Oudeyer,
                   Pierre-Yves",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent works successfully leveraged Large Language Models'
                   (LLM) abilities to capture abstract knowledge about world's
                   physics to solve decision-making problems. Yet, the alignment
                   between LLMs' knowledge and the environment can be wrong and
                   limit functional competence due to lack of grounding. In this
                   paper, we study an approach to achieve this alignment through
                   functional grounding: we consider an agent using an LLM as a
                   policy that is progressively updated as the agent interacts
                   with the environment, leveraging online Reinforcement
                   Learning to improve its performance to solve goals. Using an
                   interactive textual environment designed to study
                   higher-level forms of functional grounding, and a set of
                   spatial and navigation tasks, we study several scientific
                   questions: 1) Can LLMs boost sample efficiency for online
                   learning of various RL tasks? 2) How can it boost different
                   forms of generalization? 3) What is the impact of online
                   learning? We study these questions by functionally grounding
                   several variants (size, architecture) of FLAN-T5.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.02662"
}

@ARTICLE{Koh2023-oc,
  title         = "Grounding Language Models to Images for Multimodal Generation",
  author        = "Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel",
  journal       = "arXiv [cs.CL]",
  abstract      = "We propose an efficient method to ground pretrained text-only
                   language models to the visual domain, enabling them to
                   process and generate arbitrarily interleaved image-and-text
                   data. Our method leverages the abilities of language models
                   learnt from large scale text-only pretraining, such as
                   in-context learning and free-form text generation. We keep
                   the language model frozen, and finetune input and output
                   linear layers to enable cross-modality interactions. This
                   allows our model to process arbitrarily interleaved
                   image-and-text inputs, and generate free-form text
                   interleaved with retrieved images. We achieve strong
                   zero-shot performance on grounded tasks such as contextual
                   image retrieval and multimodal dialogue, and showcase
                   compelling interactive abilities. Our approach works with any
                   off-the-shelf language model and paves the way towards an
                   effective, general solution for leveraging pretrained
                   language models in visually grounded settings.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2301.13823"
}

@ARTICLE{Fedor2017-kh,
  title    = "Cognitive Architecture with Evolutionary Dynamics Solves Insight
              Problem",
  author   = "Fedor, Anna and Zachar, István and Szilágyi, András and Öllinger,
              Michael and de Vladar, Harold P and Szathmáry, Eörs",
  journal  = "Frontiers in psychology",
  volume   =  8,
  pages    =  427,
  abstract = "In this paper, we show that a neurally implemented a cognitive
              architecture with evolutionary dynamics can solve the four-tree
              problem. Our model, called Darwinian Neurodynamics, assumes that
              the unconscious mechanism of problem solving during insight tasks
              is a Darwinian process. It is based on the evolution of patterns
              that represent candidate solutions to a problem, and are stored
              and reproduced by a population of attractor networks. In our first
              experiment, we used human data as a benchmark and showed that the
              model behaves comparably to humans: it shows an improvement in
              performance if it is pretrained and primed appropriately, just
              like human participants in Kershaw et al. (2013)'s experiment. In
              the second experiment, we further investigated the effects of
              pretraining and priming in a two-by-two design and found a
              beginner's luck type of effect: solution rate was highest in the
              condition that was primed, but not pretrained with patterns
              relevant for the task. In the third experiment, we showed that
              deficits in computational capacity and learning abilities
              decreased the performance of the model, as expected. We conclude
              that Darwinian Neurodynamics is a promising model of human problem
              solving that deserves further investigation.",
  month    =  mar,
  year     =  2017,
  keywords = "Darwinian Neurodynamics; attractor networks; evolutionary search;
              four-tree problem; insight",
  doi      = "10.3389/fpsyg.2017.00427",
  pmc      = "PMC5370243",
  pmid     =  28405191,
  issn     = "1664-1078",
  language = "en"
}

@ARTICLE{Yang2012-if,
  title    = "Critical period for acoustic preference in mice",
  author   = "Yang, Eun-Jin and Lin, Eric W and Hensch, Takao K",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   = "109 Suppl 2",
  number   = "Suppl 2",
  pages    = "17213--17220",
  abstract = "Preference behaviors are often established during early life, but
              the underlying neural circuit mechanisms remain unknown. Adapting
              a unique nesting behavior assay, we confirmed a ``critical
              period'' for developing music preference in C57BL/6 mice. Early
              music exposure between postnatal days 15 and 24 reversed their
              innate bias for silent shelter, which typically could not be
              altered in adulthood. Instead, exposing adult mice treated acutely
              with valproic acid or carrying a targeted deletion of the Nogo
              receptor (NgR(-/-)) unmasked a strong plasticity of preference
              consistent with a reopening of the critical period as seen in
              other systems. Imaging of cFos expression revealed a prominent
              neuronal activation in response to the exposed music in the
              prelimbic and infralimbic medial prefrontal cortex only under
              conditions of open plasticity. Neither behavioral changes nor
              selective medial prefrontal cortex activation was observed in
              response to pure tone exposure, indicating a music-specific
              effect. Open-field center crossings were increased concomitant
              with shifts in music preference, suggesting a potential anxiolytic
              effect. Thus, music may offer both a unique window into the
              emotional state of mice and a potentially efficient assay for
              molecular ``brakes'' on critical period plasticity common to
              sensory and higher order brain areas.",
  month    =  oct,
  year     =  2012,
  doi      = "10.1073/pnas.1200705109",
  pmc      = "PMC3477391",
  pmid     =  23045690,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Firestone2020-rp,
  title    = "Performance vs. competence in human-machine comparisons",
  author   = "Firestone, Chaz",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  117,
  number   =  43,
  pages    = "26562--26571",
  abstract = "Does the human mind resemble the machines that can behave like it?
              Biologically inspired machine-learning systems approach
              ``human-level'' accuracy in an astounding variety of domains, and
              even predict human brain activity-raising the exciting possibility
              that such systems represent the world like we do. However, even
              seemingly intelligent machines fail in strange and ``unhumanlike''
              ways, threatening their status as models of our minds. How can we
              know when human-machine behavioral differences reflect deep
              disparities in their underlying capacities, vs. when such failures
              are only superficial or peripheral? This article draws on a
              foundational insight from cognitive science-the distinction
              between performance and competence-to encourage ``species-fair''
              comparisons between humans and machines. The
              performance/competence distinction urges us to consider whether
              the failure of a system to behave as ideally hypothesized, or the
              failure of one creature to behave like another, arises not because
              the system lacks the relevant knowledge or internal capacities
              (``competence''), but instead because of superficial constraints
              on demonstrating that knowledge (``performance''). I argue that
              this distinction has been neglected by research comparing human
              and machine behavior, and that it should be essential to any such
              comparison. Focusing on the domain of image classification, I
              identify three factors contributing to the species-fairness of
              human-machine comparisons, extracted from recent work that equates
              such constraints. Species-fair comparisons level the playing field
              between natural and artificial intelligence, so that we can
              separate more superficial differences from those that may be deep
              and enduring.",
  month    =  oct,
  year     =  2020,
  keywords = "artificial intelligence; cognition; deep learning; development;
              perception",
  doi      = "10.1073/pnas.1905334117",
  pmc      = "PMC7604508",
  pmid     =  33051296,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@ARTICLE{Lu2021-on,
  title         = "Pretrained Transformers as Universal Computation Engines",
  author        = "Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch,
                   Igor",
  journal       = "arXiv [cs.LG]",
  abstract      = "We investigate the capability of a transformer pretrained on
                   natural language to generalize to other modalities with
                   minimal finetuning -- in particular, without finetuning of
                   the self-attention and feedforward layers of the residual
                   blocks. We consider such a model, which we call a Frozen
                   Pretrained Transformer (FPT), and study finetuning it on a
                   variety of sequence classification tasks spanning numerical
                   computation, vision, and protein fold prediction. In contrast
                   to prior works which investigate finetuning on the same
                   modality as the pretraining dataset, we show that pretraining
                   on natural language can improve performance and compute
                   efficiency on non-language downstream tasks. Additionally, we
                   perform an analysis of the architecture, comparing the
                   performance of a random initialized transformer to a random
                   LSTM. Combining the two insights, we find language-pretrained
                   transformers can obtain strong performance on a variety of
                   non-language tasks.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2103.05247"
}

@ARTICLE{Adaptive_Agent_Team2023-vt,
  title         = "Human-Timescale Adaptation in an Open-Ended Task Space",
  author        = "{Adaptive Agent Team} and Bauer, Jakob and Baumli, Kate and
                   Baveja, Satinder and Behbahani, Feryal and Bhoopchand,
                   Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael and
                   Clay, Natalie and Collister, Adrian and Dasagi, Vibhavari and
                   Gonzalez, Lucy and Gregor, Karol and Hughes, Edward and
                   Kashem, Sheleem and Loks-Thompson, Maria and Openshaw, Hannah
                   and Parker-Holder, Jack and Pathak, Shreya and Perez-Nieves,
                   Nicolas and Rakicevic, Nemanja and Rocktäschel, Tim and
                   Schroecker, Yannick and Sygnowski, Jakub and Tuyls, Karl and
                   York, Sarah and Zacherl, Alexander and Zhang, Lei",
  journal       = "arXiv [cs.LG]",
  abstract      = "Foundation models have shown impressive adaptation and
                   scalability in supervised and self-supervised learning
                   problems, but so far these successes have not fully
                   translated to reinforcement learning (RL). In this work, we
                   demonstrate that training an RL agent at scale leads to a
                   general in-context learning algorithm that can adapt to
                   open-ended novel embodied 3D problems as quickly as humans.
                   In a vast space of held-out environment dynamics, our
                   adaptive agent (AdA) displays on-the-fly hypothesis-driven
                   exploration, efficient exploitation of acquired knowledge,
                   and can successfully be prompted with first-person
                   demonstrations. Adaptation emerges from three ingredients:
                   (1) meta-reinforcement learning across a vast, smooth and
                   diverse task distribution, (2) a policy parameterised as a
                   large-scale attention-based memory architecture, and (3) an
                   effective automated curriculum that prioritises tasks at the
                   frontier of an agent's capabilities. We demonstrate
                   characteristic scaling laws with respect to network size,
                   memory length, and richness of the training task
                   distribution. We believe our results lay the foundation for
                   increasingly general and adaptive RL agents that perform well
                   across ever-larger open-ended domains.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2301.07608"
}

@MISC{noauthor_undated-vu,
  title        = "3-second-naive-physics-manifesto.pdf",
  howpublished = "\url{http://www.cs.unibo.it/~nuzzoles/courses/intelligenza-artificiale/exam/3-second-naive-physics-manifesto.pdf}"
}

@ARTICLE{Brooks1991-sg,
  title    = "Intelligence without representation",
  author   = "Brooks, Rodney A",
  journal  = "Artificial intelligence",
  volume   =  47,
  number   =  1,
  pages    = "139--159",
  abstract = "Artificial intelligence research has foundered on the issue of
              representation. When intelligence is approached in an incremental
              manner, with strict reliance on interfacing to the real world
              through perception and action, reliance on representation
              disappears. In this paper we outline our approach to incrementally
              building complete intelligent Creatures. The fundamental
              decomposition of the intelligent system is not into independent
              information processing units which must interface with each other
              via representations. Instead, the intelligent system is decomposed
              into independent and parallel activity producers which all
              interface directly to the world through perception and action,
              rather than interface to each other particularly much. The notions
              of central and peripheral systems evaporate—everything is both
              central and peripheral. Based on these principles we have built a
              very successful series of mobile robots which operate without
              supervision as Creatures in standard office environments.",
  month    =  jan,
  year     =  1991,
  doi      = "10.1016/0004-3702(91)90053-M",
  issn     = "0004-3702"
}

@ARTICLE{Hinton2002-on,
  title    = "Training products of experts by minimizing contrastive divergence",
  author   = "Hinton, Geoffrey E",
  journal  = "Neural computation",
  volume   =  14,
  number   =  8,
  pages    = "1771--1800",
  abstract = "It is possible to combine multiple latent-variable models of the
              same data by multiplying their probability distributions together
              and then renormalizing. This way of combining individual
              ``expert'' models makes it hard to generate samples from the
              combined model but easy to infer the values of the latent
              variables of each expert, because the combination rule ensures
              that the latent variables of different experts are conditionally
              independent when given the data. A product of experts (PoE) is
              therefore an interesting candidate for a perceptual system in
              which rapid inference is vital and generation is unnecessary.
              Training a PoE by maximizing the likelihood of the data is
              difficult because it is hard even to approximate the derivatives
              of the renormalization term in the combination rule. Fortunately,
              a PoE can be trained using a different objective function called
              ``contrastive divergence'' whose derivatives with regard to the
              parameters can be approximated accurately and efficiently.
              Examples are presented of contrastive divergence learning using
              several types of expert on several types of data.",
  month    =  aug,
  year     =  2002,
  doi      = "10.1162/089976602760128018",
  pmid     =  12180402,
  issn     = "0899-7667",
  language = "en"
}

@ARTICLE{Wolpert1996-yw,
  title     = "The existence of a priori distinctions between learning
               algorithms",
  author    = "Wolpert, David H",
  journal   = "Neural computation",
  publisher = "MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA
               journals-info …",
  volume    =  8,
  number    =  7,
  pages     = "1391--1420",
  year      =  1996,
  issn      = "0899-7667"
}

@ARTICLE{Wolpert1996-ky,
  title     = "The Lack of A Priori Distinctions Between Learning Algorithms",
  author    = "Wolpert, David H",
  journal   = "Neural computation",
  publisher = "MIT Press",
  volume    =  8,
  number    =  7,
  pages     = "1341--1390",
  month     =  oct,
  year      =  1996,
  doi       = "10.1162/neco.1996.8.7.1341",
  issn      = "0899-7667"
}

@ARTICLE{Zelikman2022-xw,
  title         = "{STaR}: Bootstrapping Reasoning With Reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with a
                   few rationale examples; if the generated answers are wrong,
                   try again to generate a rationale given the correct answer;
                   fine-tune on all the rationales that ultimately yielded
                   correct answers; repeat. We show that STaR significantly
                   improves performance on multiple datasets compared to a model
                   fine-tuned to directly predict final answers, and performs
                   comparably to fine-tuning a 30$\times$ larger
                   state-of-the-art language model on CommensenseQA. Thus, STaR
                   lets a model improve itself by learning from its own
                   generated reasoning.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2203.14465"
}

@ARTICLE{Shridhar2020-qs,
  title         = "{ALFWorld}: Aligning Text and Embodied Environments for
                   Interactive Learning",
  author        = "Shridhar, Mohit and Yuan, Xingdi and Côté, Marc-Alexandre and
                   Bisk, Yonatan and Trischler, Adam and Hausknecht, Matthew",
  journal       = "arXiv [cs.CL]",
  abstract      = "Given a simple request like Put a washed apple in the kitchen
                   fridge, humans can reason in purely abstract terms by
                   imagining action sequences and scoring their likelihood of
                   success, prototypicality, and efficiency, all without moving
                   a muscle. Once we see the kitchen in question, we can update
                   our abstract plans to fit the scene. Embodied agents require
                   the same abilities, but existing work does not yet provide
                   the infrastructure necessary for both reasoning abstractly
                   and executing concretely. We address this limitation by
                   introducing ALFWorld, a simulator that enables agents to
                   learn abstract, text based policies in TextWorld (C\^ot\'e et
                   al., 2018) and then execute goals from the ALFRED benchmark
                   (Shridhar et al., 2020) in a rich visual environment.
                   ALFWorld enables the creation of a new BUTLER agent whose
                   abstract knowledge, learned in TextWorld, corresponds
                   directly to concrete, visually grounded actions. In turn, as
                   we demonstrate empirically, this fosters better agent
                   generalization than training only in the visually grounded
                   environment. BUTLER's simple, modular design factors the
                   problem to allow researchers to focus on models for improving
                   every piece of the pipeline (language understanding,
                   planning, navigation, and visual scene understanding).",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.03768"
}

@ARTICLE{Mialon2023-jz,
  title         = "Augmented Language Models: a Survey",
  author        = "Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and
                   Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu,
                   Roberta and Rozière, Baptiste and Schick, Timo and
                   Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and
                   LeCun, Yann and Scialom, Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "This survey reviews works in which language models (LMs) are
                   augmented with reasoning skills and the ability to use tools.
                   The former is defined as decomposing a potentially complex
                   task into simpler subtasks while the latter consists in
                   calling external modules such as a code interpreter. LMs can
                   leverage these augmentations separately or in combination via
                   heuristics, or learn to do so from demonstrations. While
                   adhering to a standard missing tokens prediction objective,
                   such augmented LMs can use various, possibly non-parametric
                   external modules to expand their context processing ability,
                   thus departing from the pure language modeling paradigm. We
                   therefore refer to them as Augmented Language Models (ALMs).
                   The missing token objective allows ALMs to learn to reason,
                   use tools, and even act, while still performing standard
                   natural language tasks and even outperforming most regular
                   LMs on several benchmarks. In this work, after reviewing
                   current advance in ALMs, we conclude that this new research
                   direction has the potential to address common limitations of
                   traditional LMs such as interpretability, consistency, and
                   scalability issues.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.07842"
}

@ARTICLE{Wei2022-mg,
  title         = "Chain-of-Thought Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought prompting,
                   where a few chain of thought demonstrations are provided as
                   exemplars in prompting. Experiments on three large language
                   models show that chain of thought prompting improves
                   performance on a range of arithmetic, commonsense, and
                   symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math word
                   problems, surpassing even finetuned GPT-3 with a verifier.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.11903"
}

@ARTICLE{Nye2021-dw,
  title         = "Show Your Work: Scratchpads for Intermediate Computation with
                   Language Models",
  author        = "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy
                   and Michalewski, Henryk and Austin, Jacob and Bieber, David
                   and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and
                   Luan, David and Sutton, Charles and Odena, Augustus",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large pre-trained language models perform remarkably well on
                   tasks that can be done ``in one pass'', such as generating
                   realistic text or synthesizing computer programs. However,
                   they struggle with tasks that require unbounded multi-step
                   computation, such as adding integers or executing programs.
                   Surprisingly, we find that these same models are able to
                   perform complex multi-step computations -- even in the
                   few-shot regime -- when asked to perform the operation ``step
                   by step'', showing the results of intermediate computations.
                   In particular, we train transformers to perform multi-step
                   computations by asking them to emit intermediate computation
                   steps into a ``scratchpad''. On a series of increasingly
                   complex tasks ranging from long addition to the execution of
                   arbitrary programs, we show that scratchpads dramatically
                   improve the ability of language models to perform multi-step
                   computations.",
  month         =  nov,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2112.00114"
}

@BOOK{Newell1972-xp,
  title    = "Human problem solving",
  author   = "Newell, A and Simon, H A",
  abstract = "… This research conducted by Allen Newell and Herbert A. … The
              main concern in this study is given to the integrated activities
              that constitute problem solving and ignores the …",
  year     =  1972
}

@ARTICLE{Rescorla1972-fu,
  title     = "A theory of Pavlovian conditioning: Variations in the
               effectiveness of reinforcement and non-reinforcement",
  author    = "Rescorla, Robert A",
  journal   = "Classical conditioning, Current research and theory",
  publisher = "Appleton-Century-Crofts",
  volume    =  2,
  pages     = "64--69",
  abstract  = "A theory of Pavlovian conditioning : Variations in the
               effectiveness of reinforcement and non - reinforcement | CiNii
               Research … A theory of Pavlovian conditioning : Variations in the
               effectiveness of reinforcement and non - reinforcement …
               Classical conditioning , Current research and theory 2 64-69,
               1972 …",
  year      =  1972
}

@INCOLLECTION{Gureckis2015-eh,
  title     = "Reinforcement learning: A computational perspective",
  author    = "Gureckis, Todd and Love, B C",
  booktitle = "Oxford handbook of computational and mathematical psychology",
  publisher = "Oxford University Press",
  year      =  2015
}

@ARTICLE{Rescorla1988-hl,
  title    = "Pavlovian conditioning: It's not what you think it is",
  author   = "Rescorla, Robert A",
  journal  = "The American psychologist",
  volume   =  43,
  number   =  3,
  pages    = "151--160",
  abstract = "Current thinking about Pavlovian conditioning differs
              substantially from that of 20 years ago. Yet the changes that have
              taken place remain poorly appreciated by psychologists generally.
              Traditional descriptions of conditioning as the acquired ability
              of one stimulus to evoke the original response to another because
              of their pairing are shown to be inadequate. They fail to
              characterize adequately the circumstances producing learning, the
              content of that learning, or the manner in which that learning
              influences performance. Instead, conditioning is now described as
              the learning of relations among events so as to allow the organism
              to represent its environment. Within this framework, the study of
              Pavlovian conditioning continues to be an intellectually active
              area, full of new discoveries and information relevant to other
              areas of psychology. (PsycINFO Database Record (c) 2016 APA, all
              rights reserved)",
  month    =  mar,
  year     =  1988,
  doi      = "10.1037/0003-066X.43.3.151",
  issn     = "0003-066X"
}

@ARTICLE{Wang2010-xb,
  title    = "On the cognitive process of human problem solving",
  author   = "Wang, Yingxu and Chiew, Vincent",
  journal  = "Cognitive systems research",
  volume   =  11,
  number   =  1,
  pages    = "81--92",
  abstract = "One of the fundamental human cognitive processes is problem
              solving. As a higher-layer cognitive process, problem solving
              interacts with many other cognitive processes such as abstraction,
              searching, learning, decision making, inference, analysis, and
              synthesis on the basis of internal knowledge representation by the
              object–attribute-relation (OAR) model. Problem solving is a
              cognitive process of the brain that searches a solution for a
              given problem or finds a path to reach a given goal. When a
              problem object is identified, problem solving can be perceived as
              a search process in the memory space for finding a relationship
              between a set of solution goals and a set of alternative paths.
              This paper presents both a cognitive model and a mathematical
              model of the problem solving process. The cognitive structures of
              the brain and the mechanisms of internal knowledge representation
              behind the cognitive process of problem solving are explained. The
              cognitive process is formally described using real-time process
              algebra (RTPA) and concept algebra. This work is a part of the
              cognitive computing project that designed to reveal and simulate
              the fundamental mechanisms and processes of the brain according to
              Wang’s layered reference model of the brain (LRMB), which is
              expected to lead to the development of future generation
              methodologies for cognitive computing and novel cognitive
              computers that are capable of think, learn, and perceive.",
  month    =  mar,
  year     =  2010,
  keywords = "Cognitive informatics; Cognitive computing; Brain informatics;
              Computational intelligence; Reference model of the brain;
              Cognitive processes; Problem solving; Mathematical model; Concept
              algebra; RTPA",
  doi      = "10.1016/j.cogsys.2008.08.003",
  issn     = "1389-0417"
}

@ARTICLE{Courville2006-en,
  title    = "Bayesian theories of conditioning in a changing world",
  author   = "Courville, Aaron C and Daw, Nathaniel D and Touretzky, David S",
  journal  = "Trends in cognitive sciences",
  volume   =  10,
  number   =  7,
  pages    = "294--300",
  abstract = "The recent flowering of Bayesian approaches invites the
              re-examination of classic issues in behavior, even in areas as
              venerable as Pavlovian conditioning. A statistical account can
              offer a new, principled interpretation of behavior, and previous
              experiments and theories can inform many unexplored aspects of the
              Bayesian enterprise. Here we consider one such issue: the finding
              that surprising events provoke animals to learn faster. We suggest
              that, in a statistical account of conditioning, surprise signals
              change and therefore uncertainty and the need for new learning. We
              discuss inference in a world that changes and show how
              experimental results involving surprise can be interpreted from
              this perspective, and also how, thus understood, these phenomena
              help constrain statistical theories of animal and human learning.",
  month    =  jul,
  year     =  2006,
  doi      = "10.1016/j.tics.2006.05.004",
  pmid     =  16793323,
  issn     = "1364-6613",
  language = "en"
}

@INCOLLECTION{Daw2014-xf,
  title     = "Chapter 16 - Advanced Reinforcement Learning",
  author    = "Daw, Nathaniel D",
  editor    = "Glimcher, Paul W and Fehr, Ernst",
  booktitle = "Neuroeconomics (Second Edition)",
  publisher = "Academic Press",
  address   = "San Diego",
  pages     = "299--320",
  abstract  = "This chapter reviews issues of current research in reinforcement
               learning theories and their neural substrates. We consider how
               the formal constructs of states, actions, and rewards that these
               theories describe can be understood to map onto counterparts
               experienced by biological organisms learning in the real world.
               In each case, this correspondence involves significant
               difficulties. However, elaborated theoretical accounts from
               computer science clarify, in each case, how to extend these
               theories to more realistic circumstances while still preserving
               the core prediction error-driven learning mechanism that has been
               prominent in neuroeconomic accounts.",
  month     =  jan,
  year      =  2014,
  keywords  = "Dopamine; Hierarchical reinforcement learning; Reinforcement
               learning; Uncertainty",
  doi       = "10.1016/B978-0-12-416008-8.00016-4",
  isbn      =  9780124160088
}

@ARTICLE{Niv2008-ki,
  title    = "Dialogues on prediction errors",
  author   = "Niv, Yael and Schoenbaum, Geoffrey",
  journal  = "Trends in cognitive sciences",
  volume   =  12,
  number   =  7,
  pages    = "265--272",
  abstract = "The recognition that computational ideas from reinforcement
              learning are relevant to the study of neural circuits has taken
              the cognitive neuroscience community by storm. A central tenet of
              these models is that discrepancies between actual and expected
              outcomes can be used for learning. Neural correlates of such
              prediction-error signals have been observed now in midbrain
              dopaminergic neurons, striatum, amygdala and even prefrontal
              cortex, and models incorporating prediction errors have been
              invoked to explain complex phenomena such as the transition from
              goal-directed to habitual behavior. Yet, like any revolution, the
              fast-paced progress has left an uneven understanding in its wake.
              Here, we provide answers to ten simple questions about prediction
              errors, with the aim of exposing both the strengths and the
              limitations of this active area of neuroscience research.",
  month    =  jul,
  year     =  2008,
  doi      = "10.1016/j.tics.2008.03.006",
  pmid     =  18567531,
  issn     = "1364-6613",
  language = "en"
}

@ARTICLE{Daw2006-xl,
  title    = "Cortical substrates for exploratory decisions in humans",
  author   = "Daw, Nathaniel D and O'Doherty, John P and Dayan, Peter and
              Seymour, Ben and Dolan, Raymond J",
  journal  = "Nature",
  volume   =  441,
  number   =  7095,
  pages    = "876--879",
  abstract = "Decision making in an uncertain environment poses a conflict
              between the opposing demands of gathering and exploiting
              information. In a classic illustration of this
              'exploration-exploitation' dilemma, a gambler choosing between
              multiple slot machines balances the desire to select what seems,
              on the basis of accumulated experience, the richest option,
              against the desire to choose a less familiar option that might
              turn out more advantageous (and thereby provide information for
              improving future decisions). Far from representing idle curiosity,
              such exploration is often critical for organisms to discover how
              best to harvest resources such as food and water. In appetitive
              choice, substantial experimental evidence, underpinned by
              computational reinforcement learning (RL) theory, indicates that a
              dopaminergic, striatal and medial prefrontal network mediates
              learning to exploit. In contrast, although exploration has been
              well studied from both theoretical and ethological perspectives,
              its neural substrates are much less clear. Here we show, in a
              gambling task, that human subjects' choices can be characterized
              by a computationally well-regarded strategy for addressing the
              explore/exploit dilemma. Furthermore, using this characterization
              to classify decisions as exploratory or exploitative, we employ
              functional magnetic resonance imaging to show that the frontopolar
              cortex and intraparietal sulcus are preferentially active during
              exploratory decisions. In contrast, regions of striatum and
              ventromedial prefrontal cortex exhibit activity characteristic of
              an involvement in value-based exploitative decision making. The
              results suggest a model of action selection under uncertainty that
              involves switching between exploratory and exploitative
              behavioural modes, and provide a computationally precise
              characterization of the contribution of key decision-related brain
              systems to each of these functions.",
  month    =  jun,
  year     =  2006,
  doi      = "10.1038/nature04766",
  pmc      = "PMC2635947",
  pmid     =  16778890,
  issn     = "0028-0836,1476-4687",
  language = "en"
}

@ARTICLE{Sutton1988-ad,
  title     = "Learning to predict by the methods of temporal differences",
  author    = "Sutton, Richard S",
  journal   = "Machine learning",
  publisher = "Springer",
  volume    =  3,
  number    =  1,
  pages     = "9--44",
  abstract  = "This article introduces a class of incremental learning
               procedures specialized for prediction-that is, for using past
               experience with an incompletely known system to predict its
               future behavior. Whereas conventional prediction-learning methods
               assign credit by means of the difference between predicted and
               actual outcomes, the new methods assign credit by means of the
               difference between temporally successive predictions. Although
               such temporal-difference methods have been used in Samuel's
               checker player, Holland's bucket brigade, and the author's
               Adaptive Heuristic Critic, they have remained poorly understood.
               Here we prove their convergence and optimality for special cases
               and relate them to supervised-learning methods. For most
               real-world prediction problems, temporal-difference methods
               require less memory and less peak computation than conventional
               methods and they produce more accurate predictions. We argue that
               most problems to which supervised learning is currently applied
               are really prediction problems of the sort to which
               temporal-difference methods can be applied to advantage.",
  month     =  aug,
  year      =  1988,
  doi       = "10.1007/BF00115009",
  issn      = "0885-6125,1573-0565"
}

@ARTICLE{Hinton2022-lf,
  title         = "The Forward-Forward Algorithm: Some Preliminary
                   Investigations",
  author        = "Hinton, Geoffrey",
  journal       = "arXiv [cs.LG]",
  abstract      = "The aim of this paper is to introduce a new learning
                   procedure for neural networks and to demonstrate that it
                   works well enough on a few small problems to be worth further
                   investigation. The Forward-Forward algorithm replaces the
                   forward and backward passes of backpropagation by two forward
                   passes, one with positive (i.e. real) data and the other with
                   negative data which could be generated by the network itself.
                   Each layer has its own objective function which is simply to
                   have high goodness for positive data and low goodness for
                   negative data. The sum of the squared activities in a layer
                   can be used as the goodness but there are many other
                   possibilities, including minus the sum of the squared
                   activities. If the positive and negative passes could be
                   separated in time, the negative passes could be done offline,
                   which would make the learning much simpler in the positive
                   pass and allow video to be pipelined through the network
                   without ever storing activities or stopping to propagate
                   derivatives.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2212.13345"
}

@ARTICLE{Batchelder2012-xv,
  title    = "Insight Problem Solving: A Critical Examination of the Possibility
              of Formal Theory",
  author   = "Batchelder, William H and Alexander, Gregory E",
  journal  = "The Journal of Problem Solving",
  volume   =  5,
  number   =  1,
  pages    =  6,
  abstract = "This paper provides a critical examination of the current state
              and future possibility of formal cognitive theory for insight
              problem solving and its associated “aha!” experience. Insight
              problems are contrasted with move problems, which have been
              formally defined and studied extensively by cognitive
              psychologists since the pioneering work of Alan Newell and Herbert
              Simon. To facilitate our discussion, a number of classical
              brainteasers are presented along with their solutions and some
              conclusions derived from observing the behavior of many students
              trying to solve them. Some of these problems are interesting in
              their own right, and many of them have not been discussed before
              in the psychological literature. The main purpose of presenting
              the brainteasers is to assist in discussing the status of formal
              cognitive theory for insight problem solving, which is argued to
              be considerably weaker than that found in other areas of higher
              cognition such as human memory, decision-making, categorization,
              and perception. We discuss theoretical barriers that have plagued
              the development of successful formal theory for insight problem
              solving. A few suggestions are made that might serve to advance
              the field.",
  year     =  2012,
  doi      = "10.7771/1932-6246.1143",
  issn     = "1932-6246"
}

@ARTICLE{Lowe2023-jf,
  title         = "Regularised neural networks mimic human insight",
  author        = "Löwe, Anika T and Touzo, Léo and Muhle-Karbe, Paul S and
                   Saxe, Andrew M and Summerfield, Christopher and Schuck,
                   Nicolas W",
  journal       = "arXiv [cs.AI]",
  abstract      = "Humans sometimes show sudden improvements in task performance
                   that have been linked to moments of insight. Such
                   insight-related performance improvements appear special
                   because they are preceded by an extended period of impasse,
                   are unusually abrupt, and occur only in some, but not all,
                   learners. Here, we ask whether insight-like behaviour also
                   occurs in artificial neural networks trained with gradient
                   descent algorithms. We compared learning dynamics in humans
                   and regularised neural networks in a perceptual decision task
                   that provided a hidden opportunity which allowed to solve the
                   task more efficiently. We show that humans tend to discover
                   this regularity through insight, rather than gradually.
                   Notably, neural networks with regularised gate modulation
                   closely mimicked behavioural characteristics of human
                   insights, exhibiting delay of insight, suddenness and
                   selective occurrence. Analyses of network learning dynamics
                   revealed that insight-like behaviour crucially depended on
                   noise added to gradient updates, and was preceded by ``silent
                   knowledge'' that is initially suppressed by regularised
                   (attentional) gating. This suggests that insights can arise
                   naturally from gradual learning, where they reflect the
                   combined influences of noise, attentional gating and
                   regularisation.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2302.11351"
}

@ARTICLE{Phattanasri2007-qt,
  title     = "The Dynamics of Associative Learning in Evolved Model Circuits",
  author    = "Phattanasri, Phattanard and Chiel, Hillel J and Beer, Randall D",
  journal   = "Adaptive behavior",
  publisher = "SAGE Publications Ltd STM",
  volume    =  15,
  number    =  4,
  pages     = "377--396",
  abstract  = "In this article, we evolve and analyze continuous-time recurrent
               neural networks capable of associating the smells of different
               foods with edibility or inedibility in different environments.
               First, we present an in-depth analysis of this task, highlighting
               the evolutionary challenges it poses and how these challenges
               informed our experimental design. Next, we describe the evolution
               of nonplastic neural circuits that can solve this food edibility
               learning problem. We then show that the dynamics of the best
               evolved nonplastic circuits instantiate finite state machines
               that capture the combinatorial structure of this task. Finally,
               we demonstrate that successful circuits with Hebbian synaptic
               plasticity can also be evolved, but that such circuits do not
               utilize their synaptic plasticity in a traditional way.",
  month     =  dec,
  year      =  2007,
  doi       = "10.1177/1059712307084688",
  issn      = "1059-7123"
}

@ARTICLE{Mikulik2020-iu,
  title         = "Meta-trained agents implement Bayes-optimal agents",
  author        = "Mikulik, Vladimir and Delétang, Grégoire and McGrath, Tom and
                   Genewein, Tim and Martic, Miljan and Legg, Shane and Ortega,
                   Pedro A",
  journal       = "arXiv [cs.AI]",
  abstract      = "Memory-based meta-learning is a powerful technique to build
                   agents that adapt fast to any task within a target
                   distribution. A previous theoretical study has argued that
                   this remarkable performance is because the meta-training
                   protocol incentivises agents to behave Bayes-optimally. We
                   empirically investigate this claim on a number of prediction
                   and bandit tasks. Inspired by ideas from theoretical computer
                   science, we show that meta-learned and Bayes-optimal agents
                   not only behave alike, but they even share a similar
                   computational structure, in the sense that one agent system
                   can approximately simulate the other. Furthermore, we show
                   that Bayes-optimal agents are fixed points of the
                   meta-learning dynamics. Our results suggest that memory-based
                   meta-learning might serve as a general technique for
                   numerically approximating Bayes-optimal agents - that is,
                   even for task distributions for which we currently don't
                   possess tractable models.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2010.11223"
}

@ARTICLE{Schultz1997-ti,
  title    = "A neural substrate of prediction and reward",
  author   = "Schultz, W and Dayan, P and Montague, P R",
  journal  = "Science",
  volume   =  275,
  number   =  5306,
  pages    = "1593--1599",
  abstract = "The capacity to predict future events permits a creature to
              detect, model, and manipulate the causal structure of its
              interactions with its environment. Behavioral experiments suggest
              that learning is driven by changes in the expectations about
              future salient events such as rewards and punishments.
              Physiological work has recently complemented these studies by
              identifying dopaminergic neurons in the primate whose fluctuating
              output apparently signals changes or errors in the predictions of
              future salient and rewarding events. Taken together, these
              findings can be understood through quantitative theories of
              adaptive optimizing control.",
  month    =  mar,
  year     =  1997,
  doi      = "10.1126/science.275.5306.1593",
  pmid     =  9054347,
  issn     = "0036-8075",
  language = "en"
}

@ARTICLE{Gershman2010-km,
  title    = "Context, learning, and extinction",
  author   = "Gershman, Samuel J and Blei, David M and Niv, Yael",
  journal  = "Psychological review",
  volume   =  117,
  number   =  1,
  pages    = "197--209",
  abstract = "A. Redish et al. (2007) proposed a reinforcement learning model of
              context-dependent learning and extinction in conditioning
              experiments, using the idea of ``state classification'' to
              categorize new observations into states. In the current article,
              the authors propose an interpretation of this idea in terms of
              normative statistical inference. They focus on renewal and latent
              inhibition, 2 conditioning paradigms in which contextual
              manipulations have been studied extensively, and show that online
              Bayesian inference within a model that assumes an unbounded number
              of latent causes can characterize a diverse set of behavioral
              results from such manipulations, some of which pose problems for
              the model of Redish et al. Moreover, in both paradigms, context
              dependence is absent in younger animals, or if hippocampal lesions
              are made prior to training. The authors suggest an explanation in
              terms of a restricted capacity to infer new causes.",
  month    =  jan,
  year     =  2010,
  doi      = "10.1037/a0017808",
  pmid     =  20063968,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@ARTICLE{Griffiths2011-yk,
  title   = "The Indian Buffet Process: An Introduction and Review",
  author  = "Griffiths, Thomas L and Ghahramani, Zoubin",
  journal = "Journal of machine learning research: JMLR",
  volume  =  12,
  number  =  32,
  pages   = "1185--1224",
  year    =  2011,
  issn    = "1532-4435,1533-7928"
}

@ARTICLE{Wang2022-uc,
  title    = "Finding Structure in One Child's Linguistic Experience",
  author   = "Wang, Wentao and Vong, Wai Keen and Kim, Najoung and Lake, Brenden
              M",
  abstract = "Neural network models have recently made striking progress in
              natural language processing, but they are typically trained on
              orders of magnitude more language input than children receive.
              What can these neural networks, which are primarily distributional
              learners, learn from a naturalistic subset of a single child's
              experience? We examine this question using a recent longitudinal
              dataset collected from a single child, consisting of egocentric
              visual data paired with text transcripts. We train both
              language-only and vision-and-language neural networks and analyze
              the linguistic knowledge they acquire. In parallel with findings
              from Elman's (1990) seminal work, the neural networks form
              emergent clusters of words corresponding to syntactic (nouns,
              transitive and intransitive verbs) and semantic categories (e.g.,
              animals and clothing), based solely on one child's linguistic
              input. The networks also acquire sensitivity to acceptability
              contrasts from linguistic phenomena such as determiner-noun
              agreement and argument structure. We find that incorporating
              visual information produces an incremental gain in predicting
              words in context, especially for syntactic categories that are
              comparatively more easily grounded such as nouns and verbs, but
              the underlying linguistic representations are not fundamentally
              altered. Our findings demonstrate which kinds of linguistic
              knowledge are learnable from a snapshot of a single child's real
              developmental experience, and which kinds may benefit from
              stronger inductive biases or richer sources of data.",
  month    =  dec,
  year     =  2022,
  doi      = "10.31234/osf.io/85k3y"
}

@ARTICLE{Nikolaus2022-qh,
  title     = "Learning English with Peppa Pig",
  author    = "Nikolaus, Mitja and Alishahi, Afra and Chrupała, Grzegorz",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  10,
  pages     = "922--936",
  abstract  = "Abstract Recent computational models of the acquisition of spoken
               language via grounding in perception exploit associations between
               spoken and visual modalities and learn to represent speech and
               visual data in a joint vector space. A major unresolved issue
               from the point of ecological validity is the training data,
               typically consisting of images or videos paired with spoken
               descriptions of what is depicted. Such a setup guarantees an
               unrealistically strong correlation between speech and the visual
               data. In the real world the coupling between the linguistic and
               the visual modality is loose, and often confounded by
               correlations with non-semantic aspects of the speech signal. Here
               we address this shortcoming by using a dataset based on the
               children’s cartoon Peppa Pig. We train a simple bi-modal
               architecture on the portion of the data consisting of dialog
               between characters, and evaluate on segments containing
               descriptive narrations. Despite the weak and confounded signal in
               this training data, our model succeeds at learning aspects of the
               visual semantics of spoken language.",
  month     =  sep,
  year      =  2022,
  doi       = "10.1162/tacl\_a\_00498",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Ghavamzadeh2016-fs,
  title         = "Bayesian Reinforcement Learning: A Survey",
  author        = "Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and
                   Tamar, Aviv",
  journal       = "arXiv [cs.AI]",
  abstract      = "Bayesian methods for machine learning have been widely
                   investigated, yielding principled methods for incorporating
                   prior information into inference algorithms. In this survey,
                   we provide an in-depth review of the role of Bayesian methods
                   for the reinforcement learning (RL) paradigm. The major
                   incentives for incorporating Bayesian reasoning in RL are: 1)
                   it provides an elegant approach to action-selection
                   (exploration/exploitation) as a function of the uncertainty
                   in learning; and 2) it provides a machinery to incorporate
                   prior knowledge into the algorithms. We first discuss models
                   and methods for Bayesian inference in the simple single-step
                   Bandit model. We then review the extensive recent literature
                   on Bayesian methods for model-based RL, where prior
                   information can be expressed on the parameters of the Markov
                   model. We also present Bayesian methods for model-free RL,
                   where priors are expressed over the value function or policy
                   class. The objective of the paper is to provide a
                   comprehensive survey on Bayesian RL algorithms and their
                   theoretical and empirical properties.",
  month         =  sep,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1609.04436",
  doi           = "10.1561/2200000049"
}

@ARTICLE{Austerweil2013-uj,
  title    = "A nonparametric Bayesian framework for constructing flexible
              feature representations",
  author   = "Austerweil, Joseph L and Griffiths, Thomas L",
  journal  = "Psychological review",
  volume   =  120,
  number   =  4,
  pages    = "817--851",
  abstract = "Representations are a key explanatory device used by cognitive
              psychologists to account for human behavior. Understanding the
              effects of context and experience on the representations people
              use is essential, because if two people encode the same stimulus
              using different representations, their response to that stimulus
              may be different. We present a computational framework that can be
              used to define models that flexibly construct feature
              representations (where by a feature we mean a part of the image of
              an object) for a set of observed objects, based on nonparametric
              Bayesian statistics. Austerweil and Griffiths (2011) presented an
              initial model constructed in this framework that captures how the
              distribution of parts affects the features people use to represent
              a set of objects. We build on this work in three ways. First,
              although people use features that can be transformed on each
              observation (e.g., translate on the retinal image), many existing
              feature learning models can only recognize features that are not
              transformed (occur identically each time). Consequently, we extend
              the initial model to infer features that are invariant over a set
              of transformations, and learn different structures of dependence
              between feature transformations. Second, we compare two possible
              methods for capturing the manner that categorization affects
              feature representations. Finally, we present a model that learns
              features incrementally, capturing an effect of the order of object
              presentation on the features people learn. We conclude by
              considering the implications and limitations of our empirical and
              theoretical results.",
  month    =  oct,
  year     =  2013,
  doi      = "10.1037/a0034194",
  pmid     =  24219850,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@ARTICLE{Huang2022-mi,
  title         = "Inner Monologue: Embodied Reasoning through Planning with
                   Language Models",
  author        = "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris
                   and Liang, Jacky and Florence, Pete and Zeng, Andy and
                   Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and
                   Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu,
                   Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian",
  journal       = "arXiv [cs.RO]",
  abstract      = "Recent works have shown how the reasoning capabilities of
                   Large Language Models (LLMs) can be applied to domains beyond
                   natural language processing, such as planning and interaction
                   for robots. These embodied problems require an agent to
                   understand many semantic aspects of the world: the repertoire
                   of skills available, how these skills influence the world,
                   and how changes to the world map back to the language. LLMs
                   planning in embodied environments need to consider not just
                   what skills to do, but also how and when to do them - answers
                   that change over time in response to the agent's own choices.
                   In this work, we investigate to what extent LLMs used in such
                   embodied contexts can reason over sources of feedback
                   provided through natural language, without any additional
                   training. We propose that by leveraging environment feedback,
                   LLMs are able to form an inner monologue that allows them to
                   more richly process and plan in robotic control scenarios. We
                   investigate a variety of sources of feedback, such as success
                   detection, scene description, and human interaction. We find
                   that closed-loop language feedback significantly improves
                   high-level instruction completion on three domains, including
                   simulated and real table top rearrangement tasks and
                   long-horizon mobile manipulation tasks in a kitchen
                   environment in the real world.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.RO",
  eprint        = "2207.05608"
}

@ARTICLE{Dickinson1997-ix,
  title     = "Actions and habits: the development of behavioural autonomy",
  author    = "Dickinson, A and Weiskrantz, Lawrence",
  journal   = "Philosophical transactions of the Royal Society of London. Series
               B, Biological sciences",
  publisher = "Royal Society",
  volume    =  308,
  number    =  1135,
  pages     = "67--78",
  month     =  jan,
  year      =  1997,
  doi       = "10.1098/rstb.1985.0010",
  issn      = "0962-8436"
}

@ARTICLE{Herrnstein1970-qr,
  title    = "On the law of effect",
  author   = "Herrnstein, R J",
  journal  = "Journal of the experimental analysis of behavior",
  volume   =  13,
  number   =  2,
  pages    = "243--266",
  abstract = "Experiments on single, multiple, and concurrent schedules of
              reinforcement find various correlations between the rate of
              responding and the rate or magnitude of reinforcement. For
              concurrent schedules (i.e., simultaneous choice procedures), there
              is matching between the relative frequencies of responding and
              reinforcement; for multiple schedules (i.e., successive
              discrimination procedures), there are contrast effects between
              responding in each component and reinforcement in the others; and
              for single schedules, there are a host of increasing monotonic
              relations between the rate of responding and the rate of
              reinforcement. All these results, plus several others, can be
              accounted for by a coherent system of equations, the most general
              of which states that the absolute rate of any response is
              proportional to its associated relative reinforcement.",
  month    =  mar,
  year     =  1970,
  doi      = "10.1901/jeab.1970.13-243",
  pmc      = "PMC1333768",
  pmid     =  16811440,
  issn     = "0022-5002",
  language = "en"
}

@ARTICLE{Piantadosi2012-xc,
  title     = "Bootstrapping in a language of thought: a formal model of
               numerical concept learning",
  author    = "Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  123,
  number    =  2,
  pages     = "199--217",
  abstract  = "In acquiring number words, children exhibit a qualitative leap in
               which they transition from understanding a few number words, to
               possessing a rich system of interrelated numerical concepts. We
               present a computational framework for understanding this
               inductive leap as the consequence of statistical inference over a
               sufficiently powerful representational system. We provide an
               implemented model that is powerful enough to learn number word
               meanings and other related conceptual systems from naturalistic
               data. The model shows that bootstrapping can be made
               computationally and philosophically well-founded as a theory of
               number learning. Our approach demonstrates how learners may
               combine core cognitive operations to build sophisticated
               representations during the course of development, and how this
               process explains observed developmental patterns in number word
               learning.",
  month     =  may,
  year      =  2012,
  doi       = "10.1016/j.cognition.2011.11.005",
  pmid      =  22284806,
  issn      = "0010-0277,1873-7838",
  language  = "en"
}

@ARTICLE{Sucholutsky2021-yo,
  title     = "`less than one'-shot learning: Learning {N} classes from {M} <
               {N} samples",
  author    = "Sucholutsky, Ilia and Schonlau, Matthias",
  journal   = "Proceedings of the ... AAAI Conference on Artificial
               Intelligence. AAAI Conference on Artificial Intelligence",
  publisher = "Association for the Advancement of Artificial Intelligence (AAAI)",
  volume    =  35,
  number    =  11,
  pages     = "9739--9746",
  abstract  = "Deep neural networks require large training sets but suffer from
               high computational cost and long training times. Training on much
               smaller training sets while maintaining nearly the same accuracy
               would be very beneficial. In the few-shot learning setting, a
               model must learn a new class given only a small number of samples
               from that class. One-shot learning is an extreme form of few-shot
               learning where the model must learn a new class from a single
               example. We propose the 'less than one'-shot learning task where
               models must learn N new classes given only M",
  month     =  may,
  year      =  2021,
  keywords  = "Multi-class/Multi-label Learning \& Extreme Classification;
               (Deep) Neural Network Learning Theory; Other Foundations of
               Machine Learning",
  doi       = "10.1609/aaai.v35i11.17171",
  issn      = "2159-5399,2374-3468",
  language  = "en"
}

@ARTICLE{Sucholutsky2024-fy,
  title    = "Using Compositionality to Learn Many Categories from Few Examples",
  author   = "Sucholutsky, Ilia and Zhao, Bonan and Griffiths, Tom",
  journal  = "Proceedings of the Annual Meeting of the Cognitive Science Society",
  volume   =  46,
  number   =  0,
  abstract = "Author(s): Sucholutsky, Ilia; Zhao, Bonan; Griffiths, Tom |
              Abstract: Humans have the remarkable ability to learn new
              categories from few examples, but how few examples can we actually
              learn from? Recent studies suggest it may be possible to learn
              more novel concepts than the number of examples. Previous
              approaches to such less-than-one-shot (LO-shot) learning used soft
              labels to provide weighted mappings from each example to multiple
              categories. Unfortunately, people find soft labels unintuitive and
              this approach did not provide plausible, cognitively-grounded
              mechanisms for LO-shot learning at scale. We propose a new
              paradigm that leverages well-established learning strategies:
              reducing complex stimuli to primitives, learning by
              discrimination, and generalizing to novel compositions of
              features. We show that participants can learn 22 categories from
              just 4 examples, shedding light on the mechanisms involved in
              LO-shot learning. Our results provide valuable insights into the
              human ability to learn many categories from limited examples, and
              the strategies people employ to achieve this impressive feat.",
  year     =  2024
}


@ARTICLE{Barbosa2023-cx,
  title    = "A practical guide for studying human behavior in the lab",
  author   = "Barbosa, Joao and Stein, Heike and Zorowitz, Sam and Niv, Yael and
              Summerfield, Christopher and Soto-Faraco, Salvador and Hyafil,
              Alexandre",
  journal  = "Behavior research methods",
  volume   =  55,
  number   =  1,
  pages    = "58--76",
  abstract = "In the last few decades, the field of neuroscience has witnessed
              major technological advances that have allowed researchers to
              measure and control neural activity with great detail. Yet,
              behavioral experiments in humans remain an essential approach to
              investigate the mysteries of the mind. Their relatively modest
              technological and economic requisites make behavioral research an
              attractive and accessible experimental avenue for neuroscientists
              with very diverse backgrounds. However, like any experimental
              enterprise, it has its own inherent challenges that may pose
              practical hurdles, especially to less experienced behavioral
              researchers. Here, we aim at providing a practical guide for a
              steady walk through the workflow of a typical behavioral
              experiment with human subjects. This primer concerns the design of
              an experimental protocol, research ethics, and subject care, as
              well as best practices for data collection, analysis, and sharing.
              The goal is to provide clear instructions for both beginners and
              experienced researchers from diverse backgrounds in planning
              behavioral experiments.",
  month    =  jan,
  year     =  2023,
  keywords = "10 rules; Good practices; Human behavioral experiments; Open
              science; Study design",
  doi      = "10.3758/s13428-022-01793-9",
  pmid     =  35262897,
  issn     = "1554-351X,1554-3528",
  language = "en"
}

@MISC{Wood_undated-zq,
  title        = "Particle filtering for nonparametric Bayesian matrix
                  factorization",
  author       = "Wood, Frank and Griffiths, Thomas L",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2006/file/38ed162a0dbef7b3fe0f628aa08b90e7-Paper.pdf}",
  note         = "Accessed: 2023-3-9"
}

@INPROCEEDINGS{Anderson2018-ox,
  title     = "Deceptive Games",
  author    = "Anderson, Damien and Stephenson, Matthew and Togelius, Julian and
               Salge, Christoph and Levine, John and Renz, Jochen",
  booktitle = "Applications of Evolutionary Computation",
  publisher = "Springer International Publishing",
  pages     = "376--391",
  abstract  = "Deceptive games are games where the reward structure or other
               aspects of the game are designed to lead the agent away from a
               globally optimal policy. While many games are already deceptive
               to some extent, we designed a series of games in the Video Game
               Description Language (VGDL) implementing specific types of
               deception, classified by the cognitive biases they exploit. VGDL
               games can be run in the General Video Game Artificial
               Intelligence (GVGAI) Framework, making it possible to test a
               variety of existing AI agents that have been submitted to the
               GVGAI Competition on these deceptive games. Our results show that
               all tested agents are vulnerable to several kinds of deception,
               but that different agents have different weaknesses. This
               suggests that we can use deception to understand the capabilities
               of a game-playing algorithm, and game-playing algorithms to
               characterize the deception displayed by a game.",
  year      =  2018,
  doi       = "10.1007/978-3-319-77538-8\_26"
}

@ARTICLE{Earle2021-yt,
  title         = "Learning Controllable Content Generators",
  author        = "Earle, Sam and Edwards, Maria and Khalifa, Ahmed and
                   Bontrager, Philip and Togelius, Julian",
  journal       = "arXiv [cs.LG]",
  abstract      = "It has recently been shown that reinforcement learning can be
                   used to train generators capable of producing high-quality
                   game levels, with quality defined in terms of some
                   user-specified heuristic. To ensure that these generators'
                   output is sufficiently diverse (that is, not amounting to the
                   reproduction of a single optimal level configuration), the
                   generation process is constrained such that the initial seed
                   results in some variance in the generator's output. However,
                   this results in a loss of control over the generated content
                   for the human user. We propose to train generators capable of
                   producing controllably diverse output, by making them
                   ``goal-aware.'' To this end, we add conditional inputs
                   representing how close a generator is to some heuristic, and
                   also modify the reward mechanism to incorporate that value.
                   Testing on multiple domains, we show that the resulting level
                   generators are capable of exploring the space of possible
                   levels in a targeted, controllable manner, producing levels
                   of comparable quality as their goal-unaware counterparts,
                   that are diverse along designer-specified dimensions.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2105.02993"
}

@ARTICLE{Open_Ended_Learning_Team2021-at,
  title         = "Open-Ended Learning Leads to Generally Capable Agents",
  author        = "{Open Ended Learning Team} and Stooke, Adam and Mahajan, Anuj
                   and Barros, Catarina and Deck, Charlie and Bauer, Jakob and
                   Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max and
                   Mathieu, Michael and McAleese, Nat and Bradley-Schmieg,
                   Nathalie and Wong, Nathaniel and Porcel, Nicolas and
                   Raileanu, Roberta and Hughes-Fitt, Steph and Dalibard,
                   Valentin and Czarnecki, Wojciech Marian",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this work we create agents that can perform well beyond a
                   single, individual task, that exhibit much wider
                   generalisation of behaviour to a massive, rich space of
                   challenges. We define a universe of tasks within an
                   environment domain and demonstrate the ability to train
                   agents that are generally capable across this vast space and
                   beyond. The environment is natively multi-agent, spanning the
                   continuum of competitive, cooperative, and independent games,
                   which are situated within procedurally generated physical 3D
                   worlds. The resulting space is exceptionally diverse in terms
                   of the challenges posed to agents, and as such, even
                   measuring the learning progress of an agent is an open
                   research problem. We propose an iterative notion of
                   improvement between successive generations of agents, rather
                   than seeking to maximise a singular objective, allowing us to
                   quantify progress despite tasks being incomparable in terms
                   of achievable rewards. We show that through constructing an
                   open-ended learning process, which dynamically changes the
                   training task distributions and training objectives such that
                   the agent never stops learning, we achieve consistent
                   learning of new behaviours. The resulting agent is able to
                   score reward in every one of our humanly solvable evaluation
                   levels, with behaviour generalising to many held-out points
                   in the universe of tasks. Examples of this zero-shot
                   generalisation include good performance on Hide and Seek,
                   Capture the Flag, and Tag. Through analysis and hand-authored
                   probe tasks we characterise the behaviour of our agent, and
                   find interesting emergent heuristic behaviours such as
                   trial-and-error experimentation, simple tool use, option
                   switching, and cooperation. Finally, we demonstrate that the
                   general capabilities of this agent could unlock larger scale
                   transfer of behaviour through cheap finetuning.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.12808"
}

@ARTICLE{Hessel2021-vm,
  title         = "Muesli: Combining Improvements in Policy Optimization",
  author        = "Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez,
                   Arthur and Schmitt, Simon and Sifre, Laurent and Weber,
                   Theophane and Silver, David and van Hasselt, Hado",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose a novel policy update that combines regularized
                   policy optimization with model learning as an auxiliary loss.
                   The update (henceforth Muesli) matches MuZero's
                   state-of-the-art performance on Atari. Notably, Muesli does
                   so without using deep search: it acts directly with a policy
                   network and has computation speed comparable to model-free
                   baselines. The Atari results are complemented by extensive
                   ablations, and by additional results on continuous control
                   and 9x9 Go.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2104.06159"
}

@ARTICLE{Lewis2022-py,
  title         = "Does {CLIP} Bind Concepts? Probing Compositionality in Large
                   Image Models",
  author        = "Lewis, Martha and Yu, Qinan and Merullo, Jack and Pavlick,
                   Ellie",
  journal       = "arXiv [cs.CV]",
  abstract      = "Large-scale models combining text and images have made
                   incredible progress in recent years. However, they can still
                   fail at tasks requiring compositional knowledge, such as
                   correctly picking out a red cube from a picture of multiple
                   shapes. We examine the ability of CLIP (Radford et al.,
                   2021), to caption images requiring compositional knowledge.
                   We implement five compositional language models to probe the
                   kinds of structure that CLIP may be using, and develop a
                   novel training algorithm, Compositional Skipgram for Images
                   (CoSI), to train these models. We look at performance in
                   attribute-based tasks, requiring the identification of a
                   particular combination of attribute and object (such as ``red
                   cube''), and in relational settings, where the spatial
                   relation between two shapes (such as ``cube behind sphere'')
                   must be identified. We find that in some conditions, CLIP is
                   able to learn attribute-object labellings, and to generalize
                   to unseen attribute-object combinations. However, we also see
                   evidence that CLIP is not able to bind features together
                   reliably. Moreover, CLIP is not able to reliably learn
                   relations between objects, whereas some compositional models
                   are able to learn these perfectly. Of the five models we
                   developed, none were able to generalize to unseen relations.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2212.10537"
}

@INCOLLECTION{Tolman1948-cr,
  title     = "Cognitive maps in rats and men",
  author    = "Tolman, E C",
  booktitle = "Image and Environment: Cognitive Mapping and Spatial Behavior",
  publisher = "Taylor and Francis",
  pages     = "27--50",
  abstract  = "I shall devote the body of this paper to a description of
               experiments with rats. But I shall also attempt in a few words at
               the close to indicate the significance of these findings on rats
               for the clinical behavior of men. Most of the rat investigations,
               which I shall report, were carried out in the Berkeley
               laboratory. But I shall also include, occasionally, accounts of
               the behavior of non-Berkeley rats who obviously have misspent
               their lives in out-of-State laboratories. Furthermore, in
               reporting our Berkeley experiments I shall have to omit a very
               great many. The ones I shall talk about were carried out by
               graduate students (or underpaid research assistants) who,
               supposedly, got some of their ideas from me. And a few, though a
               very few, were even carried out by me myself. © 1973 by Taylor \&
               Francis.",
  year      =  1948,
  doi       = "10.4324/9780203789155-11",
  isbn      = "9781351513647,9780202307664"
}

@ARTICLE{Daw2011-kf,
  title    = "Model-based influences on humans' choices and striatal prediction
              errors",
  author   = "Daw, Nathaniel D and Gershman, Samuel J and Seymour, Ben and
              Dayan, Peter and Dolan, Raymond J",
  journal  = "Neuron",
  volume   =  69,
  number   =  6,
  pages    = "1204--1215",
  abstract = "The mesostriatal dopamine system is prominently implicated in
              model-free reinforcement learning, with fMRI BOLD signals in
              ventral striatum notably covarying with model-free prediction
              errors. However, latent learning and devaluation studies show that
              behavior also shows hallmarks of model-based planning, and the
              interaction between model-based and model-free values, prediction
              errors, and preferences is underexplored. We designed a multistep
              decision task in which model-based and model-free influences on
              human choice behavior could be distinguished. By showing that
              choices reflected both influences we could then test the purity of
              the ventral striatal BOLD signal as a model-free report. Contrary
              to expectations, the signal reflected both model-free and
              model-based predictions in proportions matching those that best
              explained choice behavior. These results challenge the notion of a
              separate model-free learner and suggest a more integrated
              computational architecture for high-level human decision-making.",
  month    =  mar,
  year     =  2011,
  doi      = "10.1016/j.neuron.2011.02.027",
  pmc      = "PMC3077926",
  pmid     =  21435563,
  issn     = "0896-6273,1097-4199",
  language = "en"
}

@ARTICLE{Silver2021-ss,
  title    = "Reward is enough",
  author   = "Silver, David and Singh, Satinder and Precup, Doina and Sutton,
              Richard S",
  journal  = "Artificial intelligence",
  volume   =  299,
  pages    =  103535,
  abstract = "In this article we hypothesise that intelligence, and its
              associated abilities, can be understood as subserving the
              maximisation of reward. Accordingly, reward is enough to drive
              behaviour that exhibits abilities studied in natural and
              artificial intelligence, including knowledge, learning,
              perception, social intelligence, language, generalisation and
              imitation. This is in contrast to the view that specialised
              problem formulations are needed for each ability, based on other
              signals or objectives. Furthermore, we suggest that agents that
              learn through trial and error experience to maximise reward could
              learn behaviour that exhibits most if not all of these abilities,
              and therefore that powerful reinforcement learning agents could
              constitute a solution to artificial general intelligence.",
  month    =  oct,
  year     =  2021,
  keywords = "Artificial intelligence; Artificial general intelligence;
              Reinforcement learning; Reward",
  doi      = "10.1016/j.artint.2021.103535",
  issn     = "0004-3702"
}

@ARTICLE{Munos2016-ba,
  title         = "Safe and Efficient Off-Policy Reinforcement Learning",
  author        = "Munos, Rémi and Stepleton, Tom and Harutyunyan, Anna and
                   Bellemare, Marc G",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this work, we take a fresh look at some old and new
                   algorithms for off-policy, return-based reinforcement
                   learning. Expressing these in a common form, we derive a
                   novel algorithm, Retrace($\lambda$), with three desired
                   properties: (1) it has low variance; (2) it safely uses
                   samples collected from any behaviour policy, whatever its
                   degree of ``off-policyness''; and (3) it is efficient as it
                   makes the best use of samples collected from near on-policy
                   behaviour policies. We analyze the contractive nature of the
                   related operator under both off-policy policy evaluation and
                   control settings and derive online sample-based algorithms.
                   We believe this is the first return-based off-policy control
                   algorithm converging a.s. to $Q^*$ without the GLIE
                   assumption (Greedy in the Limit with Infinite Exploration).
                   As a corollary, we prove the convergence of Watkins'
                   Q($\lambda$), which was an open problem since 1989. We
                   illustrate the benefits of Retrace($\lambda$) on a standard
                   suite of Atari 2600 games.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.02647"
}

@ARTICLE{Bhatia2022-dp,
  title    = "Transformer networks of human conceptual knowledge",
  author   = "Bhatia, Sudeep and Richie, Russell",
  journal  = "Psychological review",
  abstract = "We present a computational model capable of simulating aspects of
              human knowledge for thousands of real-world concepts. Our approach
              involves a pretrained transformer network that is further
              fine-tuned on large data sets of participant-generated feature
              norms. We show that such a model can successfully extrapolate from
              its training data, and predict human knowledge for new concepts
              and features. We apply our model to stimuli from 25 previous
              experiments in semantic cognition research and show that it
              reproduces many findings on semantic verification, concept
              typicality, feature distribution, and semantic similarity. We also
              compare our model against several variants, and by doing so,
              establish the model properties that are necessary for good
              prediction. The success of our approach shows how a combination of
              language data and (laboratory-based) psychological data can be
              used to build models with rich world knowledge. Such models can be
              used in the service of new psychological applications, such as the
              modeling of naturalistic semantic verification and knowledge
              retrieval, as well as the modeling of real-world categorization,
              decision-making, and reasoning. (PsycInfo Database Record (c) 2022
              APA, all rights reserved).",
  month    =  oct,
  year     =  2022,
  doi      = "10.1037/rev0000319",
  pmid     =  36301272,
  issn     = "0033-295X,1939-1471",
  language = "en"
}

@INCOLLECTION{Liebana2020-zp,
  title     = "{VGDL} and the {GVGAI} Framework",
  author    = "Liébana, Diego Pérez",
  editor    = "Liébana, Diego Pérez and Lucas, Simon M and Gaina, Raluca D and
               Togelius, Julian and Khalifa, Ahmed and Liu, Jialin",
  booktitle = "General Video Game Artificial Intelligence",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "13--30",
  year      =  2020,
  doi       = "10.1007/978-3-031-02122-0\_2",
  isbn      =  9783031021220
}

@ARTICLE{Suris2023-ah,
  title         = "{ViperGPT}: Visual Inference via Python Execution for
                   Reasoning",
  author        = "Surís, Dídac and Menon, Sachit and Vondrick, Carl",
  journal       = "arXiv [cs.CV]",
  abstract      = "Answering visual queries is a complex task that requires both
                   visual processing and reasoning. End-to-end models, the
                   dominant approach for this task, do not explicitly
                   differentiate between the two, limiting interpretability and
                   generalization. Learning modular programs presents a
                   promising alternative, but has proven challenging due to the
                   difficulty of learning both the programs and modules
                   simultaneously. We introduce ViperGPT, a framework that
                   leverages code-generation models to compose
                   vision-and-language models into subroutines to produce a
                   result for any query. ViperGPT utilizes a provided API to
                   access the available modules, and composes them by generating
                   Python code that is later executed. This simple approach
                   requires no further training, and achieves state-of-the-art
                   results across various complex visual tasks.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2303.08128"
}

@ARTICLE{Blodgett1929-rc,
  title    = "The effect of the introduction of reward upon the maze performance
              of rats",
  author   = "Blodgett, H C",
  journal  = "Publications in psychology. University of California",
  volume   =  4,
  pages    = "113--134",
  abstract = "``The purpose of this investigation was to study the efficiency of
              units of practice when unaccompanied by reward. The method devised
              was that of running two groups of rats through the maze: an
              experimental group which received no reward during the first part
              of learning, but which suddenly had reward introduced in the
              latter part of learning, and a control group which received reward
              throughout the whole of learning. The answer to the question as to
              the efficiency of non-reward units of practice was sought in a
              comparison of the learning curve of the experimental group (both
              before and after the introduction of reward) with that of the
              control group.'' Three mazes were used, two with ordinary blinds
              and one with blinds arranged so that the animal could go two ways
              as well as having as alternatives a long and a short path. ``They
              prevented retracings from one section of the maze to another, they
              were noiseless, and they caused no excitement in the animals.''
              There were 36 rats in each group. The results show that: ``(1)
              Rats run under a non-reward condition learned much more slowly
              than rats run under a reward condition.'' ``(2) Rats previously
              run under a non-reward condition, when suddenly rewarded made a
              great improvement.'' ``(3) During the non-reward period, the rats
              were developing a latent learning of the maze which they were able
              to utilize as soon as reward was introduced.'' ``(5) It was
              demonstrated by the use of the two-path maze that the latent
              learning which was developed under non-reward conditions and was
              made manifest as soon as reward was introduced was not the result
              of any very consistently greater frequency of the correct over the
              incorrect path during the non-reward period.'' Bibliography and
              discussions. (PsycINFO Database Record (c) 2016 APA, all rights
              reserved)",
  year     =  1929
}

@MISC{Mitchell1980-lf,
  title        = "The need for biases in learning generalizations",
  author       = "Mitchell, Tom M",
  publisher    = "Citeseer",
  abstract     = "Learning involves the ability to generalize from past
                  experience in order to deal with new situations that are”
                  related to” this experience. The inductive leaap needed to
                  deal with new situations seems to be possible only under
                  certain biases for choosing one generalization of the
                  situation over another. This paper defines precisely the
                  notion of bias in generalization problems, then shows that
                  biases are necessary for the inductive leap. Classes of
                  justifiable biases are considered, and the relationship
                  between bias and domain-independence is …",
  year         =  1980,
  howpublished = "\url{https://citeseerx.ist.psu.edu/document?repid=rep1\&type=pdf\&doi=6cf35ec34efa592f83e3a1b748aea14957fc784a}",
  note         = "Accessed: 2023-3-28"
}

@ARTICLE{Shepard1987-dl,
  title    = "Toward a universal law of generalization for psychological science",
  author   = "Shepard, R N",
  journal  = "Science",
  volume   =  237,
  number   =  4820,
  pages    = "1317--1323",
  abstract = "A psychological space is established for any set of stimuli by
              determining metric distances between the stimuli such that the
              probability that a response learned to any stimulus will
              generalize to any other is an invariant monotonic function of the
              distance between them. To a good approximation, this probability
              of generalization (i) decays exponentially with this distance, and
              (ii) does so in accordance with one of two metrics, depending on
              the relation between the dimensions along which the stimuli vary.
              These empirical regularities are mathematically derivable from
              universal principles of natural kinds and probabilistic geometry
              that may, through evolutionary internalization, tend to govern the
              behaviors of all sentient organisms.",
  month    =  sep,
  year     =  1987,
  doi      = "10.1126/science.3629243",
  pmid     =  3629243,
  issn     = "0036-8075",
  language = "en"
}

@ARTICLE{Dunsmoor2015-gm,
  title    = "Categories, concepts, and conditioning: how humans generalize fear",
  author   = "Dunsmoor, Joseph E and Murphy, Gregory L",
  journal  = "Trends in cognitive sciences",
  volume   =  19,
  number   =  2,
  pages    = "73--77",
  abstract = "During the past century, Pavlovian conditioning has served as the
              predominant experimental paradigm and theoretical framework to
              understand how humans learn to fear and avoid real or perceived
              dangers. Animal models for translational research offer insight
              into basic behavioral and neurophysiological factors mediating the
              acquisition, expression, inhibition, and generalization of fear.
              However, it is important to consider the limits of traditional
              animal models when applied to humans. Here, we focus on the
              question of how humans generalize fear. We propose that to
              understand fear generalization in humans requires taking into
              account research on higher-level cognition such as category-based
              induction, inferential reasoning, and representation of conceptual
              knowledge. Doing so will open the door for productive avenues of
              new research.",
  month    =  feb,
  year     =  2015,
  doi      = "10.1016/j.tics.2014.12.003",
  pmc      = "PMC4318701",
  pmid     =  25577706,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Li1993-jt,
  title    = "The representation of stimulus familiarity in anterior inferior
              temporal cortex",
  author   = "Li, L and Miller, E K and Desimone, R",
  journal  = "Journal of neurophysiology",
  volume   =  69,
  number   =  6,
  pages    = "1918--1929",
  abstract = "1. The inferior temporal (IT) cortex plays an important role in
              both short- and long-term memory for visual patterns. Most
              previous studies of IT neurons have tested their responses in
              recency memory tasks, which require that the memory lasts only the
              length of a single behavioral trial, which may be 150
              presentations of other stimuli, the maximum tested. For some cells
              the maximum decrement in response occurred for those stimuli that
              initially elicited the largest response. There was no significant
              change in response to stimuli that were already familiar. 5. The
              same cells that showed familiarity effects also showed reduced
              responses to the matching stimuli at the end of each trial,
              compared with the responses to the samples.(ABSTRACT TRUNCATED AT
              400 WORDS)",
  month    =  jun,
  year     =  1993,
  doi      = "10.1152/jn.1993.69.6.1918",
  pmid     =  8350131,
  issn     = "0022-3077",
  language = "en"
}

@ARTICLE{Koutstaal2001-rt,
  title    = "Perceptual specificity in visual object priming: functional
              magnetic resonance imaging evidence for a laterality difference in
              fusiform cortex",
  author   = "Koutstaal, W and Wagner, A D and Rotte, M and Maril, A and
              Buckner, R L and Schacter, D L",
  journal  = "Neuropsychologia",
  volume   =  39,
  number   =  2,
  pages    = "184--199",
  abstract = "Seeing an object on one occasion may facilitate or prime
              processing of the same object if it is later again encountered.
              Such priming may also be found -- but at a reduced level -- for
              different but perceptually similar objects that are alternative
              exemplars or 'tokens' of the initially presented object. We
              explored the neural correlates of this perceptual specificity
              using event-related functional magnetic resonance imaging (fMRI)
              procedures, contrasting neural activity when participants made
              object classification decisions (size judgments) regarding
              previously presented objects (repeated same), alternative
              exemplars of previously presented objects (repeated different), or
              entirely new objects (novel). Many frontal regions (including
              bilateral frontal operculum, bilateral posterior inferior
              frontal/precentral, left anterior inferior frontal, and superior
              frontal cortices) and multiple late visual and posterior regions
              (including middle occipital, fusiform, fusiform-parahippocampal,
              precuneus, and posterior cingulate, all bilaterally), demonstrated
              reduced neural activity for repeated compared to novel objects.
              Greater repetition-induced reductions were observed for same than
              for different exemplars in several of these regions (bilateral
              posterior inferior frontal, right precuneus, bilateral middle
              occipital, bilateral fusiform, bilateral parahippocampal and
              bilateral superior parietal). Additionally, right fusiform
              (occipitotemporal) cortex showed significantly less priming for
              different versus same exemplars than did left fusiform. These
              findings converge with behavioral evidence from divided visual
              field studies and with neuropsychological evidence underscoring
              the key role of right occipitotemporal cortex in processing
              specific visual form information; possible differences in the
              representational-functional role of left fusiform are discussed.",
  year     =  2001,
  doi      = "10.1016/s0028-3932(00)00087-7",
  pmid     =  11163375,
  issn     = "0028-3932",
  language = "en"
}

@ARTICLE{Alayrac2022-ur,
  title     = "Flamingo: a visual language model for few-shot learning",
  author    = "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and
               Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel
               and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm
               and {Others}",
  journal   = "Advances in neural information processing systems",
  publisher = "proceedings.neurips.cc",
  volume    =  35,
  pages     = "23716--23736",
  abstract  = "… Similarly, we demonstrate that the way we train the Flamingo
               models is crucial for … Flamingo on all nine tasks where Flamingo
               does not achieve SotA with few-shot learning. Flamingo …",
  year      =  2022,
  issn      = "1049-5258"
}

@ARTICLE{Davachi2003-ic,
  title    = "Multiple routes to memory: distinct medial temporal lobe processes
              build item and source memories",
  author   = "Davachi, Lila and Mitchell, Jason P and Wagner, Anthony D",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  100,
  number   =  4,
  pages    = "2157--2162",
  abstract = "A central function of memory is to permit an organism to
              distinguish between stimuli that have been previously encountered
              and those that are novel. Although the medial temporal lobe (which
              includes the hippocampus and surrounding perirhinal,
              parahippocampal, and entorhinal cortices) is known to be crucial
              for recognition memory, controversy remains regarding how the
              specific subregions within the medial temporal lobe contribute to
              recognition. We used event-related functional MRI to examine the
              relation between activation in distinct medial temporal lobe
              subregions during memory formation and the ability (i) to later
              recognize an item as previously encountered (item recognition) and
              (ii) to later recollect specific contextual details about the
              prior encounter (source recollection). Encoding activation in
              hippocampus and in posterior parahippocampal cortex predicted
              later source recollection, but was uncorrelated with item
              recognition. In contrast, encoding activation in perirhinal cortex
              predicted later item recognition, but not subsequent source
              recollection. These outcomes suggest that the subregions within
              the medial temporal lobe subserve distinct, but complementary,
              learning mechanisms.",
  month    =  feb,
  year     =  2003,
  doi      = "10.1073/pnas.0337195100",
  pmc      = "PMC149975",
  pmid     =  12578977,
  issn     = "0027-8424",
  language = "en"
}

@ARTICLE{Lisman2017-pf,
  title    = "Viewpoints: how the hippocampus contributes to memory, navigation
              and cognition",
  author   = "Lisman, John and Buzsáki, György and Eichenbaum, Howard and Nadel,
              Lynn and Ranganath, Charan and Redish, A David",
  journal  = "Nature neuroscience",
  volume   =  20,
  number   =  11,
  pages    = "1434--1447",
  abstract = "The hippocampus serves a critical function in memory, navigation,
              and cognition. Nature Neuroscience asked John Lisman to lead a
              group of researchers in a dialog on shared and distinct viewpoints
              on the hippocampus. There has been a long history of studying the
              hippocampus, but recent work has made it possible to study the
              cellular and network basis of defined operations—operations that
              include cognitive processes that have been otherwise difficult to
              study (see Box 1 for useful terminology). These operations deal
              with the context-dependent representation of complex memories, the
              role of mental exploration based on imagined rather than real
              movements, and the use of recalled information for navigation and
              decision-making. The progress that has been made in understanding
              the hippocampus has motivated the study of other brain regions
              that provide hippocampal input or receive hippocampal output; the
              hippocampus is thus serving as a nucleating point for the larger
              goal of understanding the neural codes that allow inter-regional
              communication and more generally, understanding how memory-guided
              behavior is achieved by large scale integration of brain regions.
              In generating a discussion among experts in the study of the
              cognitive processes of the hippocampus, the editors and I have
              posed questions that probe important principles of hippocampal
              function. We hope that the resulting discussion will make clear to
              readers the progress that has been made, while also identifying
              issues where consensus has not yet been achieved and that should
              be pursued in future research. – John Lisman",
  month    =  oct,
  year     =  2017,
  doi      = "10.1038/nn.4661",
  pmc      = "PMC5943637",
  pmid     =  29073641,
  issn     = "1097-6256,1546-1726",
  language = "en"
}

@ARTICLE{Lynch2020-wo,
  title     = "Learning latent plans from play",
  author    = "Lynch, C and Khansari, M and Xiao, T and Kumar, V and {others}",
  journal   = "on robot learning",
  publisher = "proceedings.mlr.press",
  abstract  = "Acquiring a diverse repertoire of general-purpose skills remains
               an open challenge for robotics. In this work, we propose
               self-supervising control on top of human teleoperated play …",
  year      =  2020
}

@ARTICLE{Lynch2020-pj,
  title     = "Language conditioned imitation learning over unstructured data",
  author    = "Lynch, C and Sermanet, P",
  journal   = "arXiv preprint arXiv:2005.07648",
  publisher = "arxiv.org",
  abstract  = "Natural language is perhaps the most flexible and intuitive way
               for humans to communicate tasks to a robot. Prior work in
               imitation learning typically requires each task be specified with
               …",
  year      =  2020,
  eprint    = "2005.07648"
}

@ARTICLE{Camposampiero2023-xn,
  title         = "Visual Abstraction and Reasoning through Language",
  author        = "Camposampiero, Giacomo and Houmard, Loic and Estermann,
                   Benjamin and Mathys, Joël and Wattenhofer, Roger",
  journal       = "arXiv [cs.AI]",
  abstract      = "While Artificial Intelligence (AI) models have achieved human
                   or even superhuman performance in narrowly defined
                   applications, they still struggle to show signs of broader
                   and more flexible intelligence. The Abstraction and Reasoning
                   Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to
                   assess how close AI systems are to human-like cognitive
                   abilities. Most current approaches rely on carefully
                   handcrafted domain-specific languages (DSLs), which are used
                   to brute-force solutions to the tasks present in ARC. In this
                   work, we propose a general framework for solving ARC based on
                   natural language descriptions of the tasks. While not yet
                   beating state-of-the-art DSL models on ARC, we demonstrate
                   the immense potential of our approach hinted at by the
                   ability to solve previously unsolved tasks.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2303.04091"
}

@BOOK{MacKay2003-nl,
  title     = "Information Theory, Inference and Learning Algorithms",
  author    = "MacKay, David J C and Mac Kay, David J",
  publisher = "Cambridge University Press",
  abstract  = "Information theory and inference, often taught separately, are
               here united in one entertaining textbook. These topics lie at the
               heart of many exciting areas of contemporary science and
               engineering - communication, signal processing, data mining,
               machine learning, pattern recognition, computational
               neuroscience, bioinformatics, and cryptography. This textbook
               introduces theory in tandem with applications. Information theory
               is taught alongside practical communication systems, such as
               arithmetic coding for data compression and sparse-graph codes for
               error-correction. A toolbox of inference techniques, including
               message-passing algorithms, Monte Carlo methods, and variational
               approximations, are developed alongside applications of these
               tools to clustering, convolutional codes, independent component
               analysis, and neural networks. The final part of the book
               describes the state of the art in error-correcting codes,
               including low-density parity-check codes, turbo codes, and
               digital fountain codes -- the twenty-first century standards for
               satellite communications, disk drives, and data broadcast. Richly
               illustrated, filled with worked examples and over 400 exercises,
               some with detailed solutions, David MacKay's groundbreaking book
               is ideal for self-learning and for undergraduate or graduate
               courses. Interludes on crosswords, evolution, and sex provide
               entertainment along the way. In sum, this is a textbook on
               information, communication, and coding for a new generation of
               students, and an unparalleled entry point into these subjects for
               professionals in areas as diverse as computational biology,
               financial engineering, and machine learning.",
  month     =  sep,
  year      =  2003,
  isbn      =  9780521642989,
  language  = "en"
}

@ARTICLE{Shiffrin1997-xr,
  title    = "A model for recognition memory: {REM}-retrieving effectively from
              memory",
  author   = "Shiffrin, R M and Steyvers, M",
  journal  = "Psychonomic bulletin \& review",
  volume   =  4,
  number   =  2,
  pages    = "145--166",
  abstract = "A new model of recognition memory is reported. This model is
              placed within, and introduces, a more elaborate theory that is
              being developed to predict the phenomena of explicit and implicit,
              and episodic and generic, memory. The recognition model is applied
              to basic findings, including phenomena that pose problems for
              extant models: the list-strength effect (e.g., Ratcliff, Clark, \&
              Shiffrin, 1990), the mirror effect (e.g., Glanzer \& Adams, 1990),
              and the normal-ROC slope effect (e.g., Ratcliff, McKoon, \&
              Tindall, 1994). The model assumes storage of separate episodic
              images for different words, each image consisting of a vector of
              feature values. Each image is an incomplete and error prone copy
              of the studied vector. For the simplest case, it is possible to
              calculate the probability that a test item is ``old,'' and it is
              assumed that a default ``old'' response is given if this
              probability is greater than .5. It is demonstrated that this model
              and its more complete and realistic versions produce excellent
              qualitative predictions.",
  month    =  jun,
  year     =  1997,
  doi      = "10.3758/BF03209391",
  pmid     =  21331823,
  issn     = "1069-9384",
  language = "en"
}

@ARTICLE{Zelikman2022-nb,
  title         = "{STaR}: Bootstrapping Reasoning With Reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah
                   D",
  journal       = "arXiv [cs.LG]",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with a
                   few rationale examples; if the generated answers are wrong,
                   try again to generate a rationale given the correct answer;
                   fine-tune on all the rationales that ultimately yielded
                   correct answers; repeat. We show that STaR significantly
                   improves performance on multiple datasets compared to a model
                   fine-tuned to directly predict final answers, and performs
                   comparably to fine-tuning a 30$\times$ larger
                   state-of-the-art language model on CommensenseQA. Thus, STaR
                   lets a model improve itself by learning from its own
                   generated reasoning.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2203.14465"
}

@ARTICLE{Ainooson2023-ad,
  title         = "An Approach for Solving Tasks on the Abstract Reasoning
                   Corpus",
  author        = "Ainooson, James and Sanyal, Deepayan and Michelson, Joel P
                   and Yang, Yuan and Kunda, Maithilee",
  journal       = "arXiv [cs.AI]",
  abstract      = "The Abstract Reasoning Corpus (ARC) is an intelligence tests
                   for measuring fluid intelligence in artificial intelligence
                   systems and humans alike. In this paper we present a system
                   for reasoning about and solving ARC tasks. Our system relies
                   on a program synthesis approach that searches a space of
                   potential programs for ones that can solve tasks from the
                   ARC. Programs are in a domain specific language, and in some
                   instances our search algorithm is guided by insights from a
                   corpus of ground truth programs. In particular: We describe
                   an imperative style domain specific language, called Visual
                   Imagery Reasoning Language (VIMRL), for reasoning about tasks
                   in the ARC. We also demonstrate an innovative approach for
                   how large search spaces can be decomposed using special high
                   level functions that determine their own arguments through
                   local searches on a given task item. Finally, we share our
                   results obtained on the publicly available ARC items as well
                   as our system's strong performance on a private test,
                   recently tying for 4th place on the global ARCathon 2022
                   challenge.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2302.09425"
}

@MISC{Vong_undated-ni,
  title        = "Few-shot image classification by generating natural language
                  rules",
  author       = "Vong, Wai Keen and Lake, Brenden M",
  howpublished = "\url{https://openreview.net/pdf?id=BxfpZP2sZq}",
  note         = "Accessed: 2023-4-5"
}

@ARTICLE{Hernandez2022-ow,
  title    = "Natural Language Descriptions of Deep Visual Features",
  author   = "Hernandez, Evan and Schwettmann, Sarah and Bau, David and
              Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob",
  abstract = "Some neurons in deep networks specialize in recognizing highly
              specific perceptual, structural, or semantic features of inputs.
              In computer vision, techniques exist for identifying neurons that
              respond to individual concept categories like colors, textures,
              and object classes. But these techniques are limited in scope,
              labeling only a small subset of neurons and behaviors in any
              network. Is a richer characterization of neuron-level computation
              possible? We introduce a procedure (called MILAN, for mutual
              information-guided linguistic annotation of neurons) that
              automatically labels neurons with open-ended, compositional,
              natural language descriptions. Given a neuron, MILAN generates a
              description by searching for a natural language string that
              maximizes pointwise mutual information with the image regions in
              which the neuron is active. MILAN produces fine-grained
              descriptions that capture categorical, relational, and logical
              structure in learned features. These descriptions obtain high
              agreement with human-generated feature descriptions across a
              diverse set of model architectures and tasks, and can aid in
              understanding and controlling learned models. We highlight three
              applications of natural language neuron descriptions. First, we
              use MILAN for analysis, characterizing the distribution and
              importance of neurons selective for attribute, category, and
              relational information in vision models. Second, we use MILAN for
              auditing, surfacing neurons sensitive to human faces in datasets
              designed to obscure them. Finally, we use MILAN for editing,
              improving robustness in an image classifier by deleting neurons
              sensitive to text features spuriously correlated with class
              labels.",
  month    =  jan,
  year     =  2022
}

@ARTICLE{Austerweil2015-pf,
  title     = "Structure and flexibility in Bayesian models of cognition",
  author    = "Austerweil, Joseph L and Gershman, Samuel J and Tenenbaum, Joshua
               B and Griffiths, Thomas L",
  journal   = "Oxford handbook of computational and mathematical psychology",
  publisher = "Oxford University Press Oxford, UK",
  pages     = "187--208",
  abstract  = "… We discuss nonparametric Bayesian models as a potential answer
               to this … Bayesian models . We then delve into nonparametric
               Bayesian models for three types of hidden structure : …",
  year      =  2015
}

@ARTICLE{Ouyang2022-jj,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.02155"
}

@ARTICLE{Prystawski2023-hp,
  title         = "Why think step-by-step? Reasoning emerges from the locality
                   of experience",
  author        = "Prystawski, Ben and Goodman, Noah D",
  journal       = "arXiv [cs.AI]",
  abstract      = "Humans have a powerful and mysterious capacity to reason. By
                   working through a series of purely mental steps, we can make
                   inferences we would not be capable of making directly --
                   despite that fact that we get no additional data from the
                   world. Similarly, large language models can perform better at
                   complex tasks through chain-of-thought reasoning, where they
                   generate intermediate steps before answering a question. We
                   use language models to investigate the questions of when and
                   why reasoning is helpful, testing the hypothesis that
                   reasoning is effective when training data consisting of local
                   clusters of variables that influence each other strongly.
                   These training conditions enable the chaining of accurate
                   local inferences in order to estimate relationships between
                   variables that were not seen together in training. We train
                   an autoregressive transformer on samples from joint
                   distributions defined by Bayes nets, but only include a
                   subset of all the variables in each sample. We compare
                   language models' ability to match conditional probabilities
                   both with and without intermediate reasoning steps, finding
                   that intermediate steps help only when the training data is
                   locally structured with respect to dependencies between
                   variables. Furthermore, intermediate variables need to be
                   relevant to the relationship between observed information and
                   target inferences. Our results illustrate how the statistical
                   structure of training data drives the effectiveness of
                   reasoning step by step.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2304.03843"
}

@ARTICLE{Park2023-ud,
  title         = "Generative Agents: Interactive Simulacra of Human Behavior",
  author        = "Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and
                   Morris, Meredith Ringel and Liang, Percy and Bernstein,
                   Michael S",
  journal       = "arXiv [cs.HC]",
  abstract      = "Believable proxies of human behavior can empower interactive
                   applications ranging from immersive environments to rehearsal
                   spaces for interpersonal communication to prototyping tools.
                   In this paper, we introduce generative agents--computational
                   software agents that simulate believable human behavior.
                   Generative agents wake up, cook breakfast, and head to work;
                   artists paint, while authors write; they form opinions,
                   notice each other, and initiate conversations; they remember
                   and reflect on days past as they plan the next day. To enable
                   generative agents, we describe an architecture that extends a
                   large language model to store a complete record of the
                   agent's experiences using natural language, synthesize those
                   memories over time into higher-level reflections, and
                   retrieve them dynamically to plan behavior. We instantiate
                   generative agents to populate an interactive sandbox
                   environment inspired by The Sims, where end users can
                   interact with a small town of twenty five agents using
                   natural language. In an evaluation, these generative agents
                   produce believable individual and emergent social behaviors:
                   for example, starting with only a single user-specified
                   notion that one agent wants to throw a Valentine's Day party,
                   the agents autonomously spread invitations to the party over
                   the next two days, make new acquaintances, ask each other out
                   on dates to the party, and coordinate to show up for the
                   party together at the right time. We demonstrate through
                   ablation that the components of our agent
                   architecture--observation, planning, and reflection--each
                   contribute critically to the believability of agent behavior.
                   By fusing large language models with computational,
                   interactive agents, this work introduces architectural and
                   interaction patterns for enabling believable simulations of
                   human behavior.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2304.03442"
}

@ARTICLE{McClelland1995-nl,
  title    = "Why there are complementary learning systems in the hippocampus
              and neocortex: insights from the successes and failures of
              connectionist models of learning and memory",
  author   = "McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall
              C",
  journal  = "Psychological review",
  volume   =  102,
  number   =  3,
  pages    = "419--457",
  abstract = "Damage to the hippocampal system disrupts recent memory but leaves
              remote memory intact. The account presented here suggests that
              memories are first stored via synaptic changes in the hippocampal
              system, that these changes support reinstatement of recent
              memories in the neocortex, that neocortical synapses change a
              little on each reinstatement, and that remote memory is based on
              accumulated neocortical changes. Models that learn via changes to
              connections help explain this organization. These models discover
              the structure in ensembles of items if learning of each item is
              gradual and interleaved with learning about other items. This
              suggests that the neocortex learns slowly to discover the
              structure in ensembles of experiences. The hippocampal system
              permits rapid learning of new items without disrupting this
              structure, and reinstatement of new memories interleaves them with
              others to integrate them into structured neocortical memory
              systems.",
  month    =  jul,
  year     =  1995,
  doi      = "10.1037/0033-295X.102.3.419",
  pmid     =  7624455,
  issn     = "0033-295X",
  language = "en"
}

@ARTICLE{Dasgupta2022-ug,
  title         = "Language models show human-like content effects on reasoning",
  author        = "Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie C
                   Y and Creswell, Antonia and Kumaran, Dharshan and McClelland,
                   James L and Hill, Felix",
  journal       = "arXiv [cs.CL]",
  abstract      = "Abstract reasoning is a key ability for an intelligent
                   system. Large language models achieve above-chance
                   performance on abstract reasoning tasks, but exhibit many
                   imperfections. However, human abstract reasoning is also
                   imperfect, and depends on our knowledge and beliefs about the
                   content of the reasoning problem. For example, humans reason
                   much more reliably about logical rules that are grounded in
                   everyday situations than arbitrary rules about abstract
                   attributes. The training experiences of language models
                   similarly endow them with prior expectations that reflect
                   human knowledge and beliefs. We therefore hypothesized that
                   language models would show human-like content effects on
                   abstract reasoning problems. We explored this hypothesis
                   across three logical reasoning tasks: natural language
                   inference, judging the logical validity of syllogisms, and
                   the Wason selection task (Wason, 1968). We find that state of
                   the art large language models (with 7 or 70 billion
                   parameters; Hoffman et al., 2022) reflect many of the same
                   patterns observed in humans across these tasks -- like
                   humans, models reason more effectively about believable
                   situations than unrealistic or abstract ones. Our findings
                   have implications for understanding both these cognitive
                   effects, and the factors that contribute to language model
                   performance.",
  month         =  jul,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2207.07051"
}

@ARTICLE{Holtzman2021-ms,
  title         = "Surface Form Competition: Why the Highest Probability Answer
                   Isn't Always Right",
  author        = "Holtzman, Ari and West, Peter and Shwartz, Vered and Choi,
                   Yejin and Zettlemoyer, Luke",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models have shown promising results in
                   zero-shot settings (Brown et al.,2020; Radford et al., 2019).
                   For example, they can perform multiple choice tasks simply by
                   conditioning on a question and selecting the answer with the
                   highest probability. However, ranking by string probability
                   can be problematic due to surface form competition-wherein
                   different surface forms compete for probability mass, even if
                   they represent the same underlying concept, e.g. ``computer''
                   and ``PC.'' Since probability mass is finite, this lowers the
                   probability of the correct answer, due to competition from
                   other strings that are valid answers (but not one of the
                   multiple choice options). We introduce Domain Conditional
                   Pointwise Mutual Information, an alternative scoring function
                   that directly compensates for surface form competition by
                   simply reweighing each option according to a term that is
                   proportional to its a priori likelihood within the context of
                   the specific zero-shot task. It achieves consistent gains in
                   zero-shot performance over both calibrated (Zhao et al.,
                   2021) and uncalibrated scoring functions on all GPT-2 and
                   GPT-3 models over a variety of multiple choice datasets.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.08315"
}

@ARTICLE{Ghahramani2013-bu,
  title    = "Bayesian non-parametrics and the probabilistic approach to
              modelling",
  author   = "Ghahramani, Zoubin",
  journal  = "Philosophical transactions. Series A, Mathematical, physical, and
              engineering sciences",
  volume   =  371,
  number   =  1984,
  pages    =  20110553,
  abstract = "Modelling is fundamental to many fields of science and
              engineering. A model can be thought of as a representation of
              possible data one could predict from a system. The probabilistic
              approach to modelling uses probability theory to express all
              aspects of uncertainty in the model. The probabilistic approach is
              synonymous with Bayesian modelling, which simply uses the rules of
              probability theory in order to make predictions, compare
              alternative models, and learn model parameters and structure from
              data. This simple and elegant framework is most powerful when
              coupled with flexible probabilistic models. Flexibility is
              achieved through the use of Bayesian non-parametrics. This article
              provides an overview of probabilistic modelling and an accessible
              survey of some of the main tools in Bayesian non-parametrics. The
              survey covers the use of Bayesian non-parametrics for modelling
              unknown functions, density estimation, clustering, time-series
              modelling, and representing sparsity, hierarchies, and covariance
              structure. More specifically, it gives brief non-technical
              overviews of Gaussian processes, Dirichlet processes, infinite
              hidden Markov models, Indian buffet processes, Kingman's
              coalescent, Dirichlet diffusion trees and Wishart processes.",
  month    =  feb,
  year     =  2013,
  doi      = "10.1098/rsta.2011.0553",
  pmc      = "PMC3538441",
  pmid     =  23277609,
  issn     = "1364-503X",
  language = "en"
}

@ARTICLE{Goldenberg1998-ka,
  title     = "Tool use and mechanical problem solving in apraxia",
  author    = "Goldenberg, G and Hagmann, S",
  journal   = "Neuropsychologia",
  publisher = "Elsevier BV",
  volume    =  36,
  number    =  7,
  pages     = "581--589",
  abstract  = "Moorlaas (1928) proposed that apraxic patients can identify
               objects and can remember the purpose they have been made for but
               do not know the way in which they must be used to achieve that
               purpose. Knowledge about the use of objects and tools can have
               two sources: It can be based on retrieval of instructions of use
               from semantic memory or on a direct inference of function from
               structure. The ability to infer function from structure enables
               subjects to use unfamiliar tools and to detect alternative uses
               of familiar tools. It is the basis of mechanical problem solving.
               The purpose of the present study was to analyze retrieval of
               instruction of use, mechanical problem solving, and actual tool
               use in patients with apraxia due to circumscribed lesions of the
               left hemisphere. For assessing mechanical problem solving we
               developed a test of selection and application of novel tools.
               Access to instruction of use was tested by pantomime of tool use.
               Actual tool use was examined for the same familiar tools. Forty
               two patients with left brain damage (LBD) and aphasia, 22
               patients with right brain damage (RBD) and 22 controls were
               examined. Only LBD patients differed from controls on all tests.
               RBD patients had difficulties with the use but not with the
               selection of novel tools. In LBD patients there was a significant
               correlation between pantomime of tool use and novel tool
               selection but there were single cases who scored in the defective
               range on one of these tests and normally on the other. Analysis
               of LBD patients' lesions suggested that frontal lobe damage does
               not disturb novel tool selection. Only LBD patients who failed on
               pantomime of object use and on novel tool selection committed
               errors in actual use of familiar tools. The finding that
               mechanical problem solving is invariably defective in apraxic
               patients who commit errors with familiar tools is in good accord
               with clinical observations, as the gravity of their errors goes
               beyond what one would expect as a mere sequel of loss of access
               to instruction of use.",
  month     =  jul,
  year      =  1998,
  doi       = "10.1016/s0028-3932(97)00165-6",
  pmid      =  9723930,
  issn      = "0028-3932,1873-3514",
  language  = "en"
}

@ARTICLE{Howard2002-ir,
  title    = "A Distributed Representation of Temporal Context",
  author   = "Howard, Marc W and Kahana, Michael J",
  journal  = "Journal of mathematical psychology",
  volume   =  46,
  number   =  3,
  pages    = "269--299",
  abstract = "The principles of recency and contiguity are two cornerstones of
              the theoretical and empirical analysis of human memory. Recency
              has been alternatively explained by mechanisms of decay,
              displacement, and retroactive interference. Another account of
              recency is based on the idea of variable context (Estes, 1955;
              Mensink \& Raaijmakers, 1989). Such notions are typically cast in
              terms of a randomly fluctuating population of elements reflective
              of subtle changes in the environment or in the subjects' mental
              state. This random context view has recently been incorporated
              into distributed and neural network memory models (Murdock, 1997;
              Murdock, Smith, \& Bai, 2001). Here we propose an alternative
              model. Rather than being driven by random fluctuations, this
              formulation, the temporal context model (TCM), uses retrieval of
              prior contextual states to drive contextual drift. In TCM,
              retrieved context is an inherently asymmetric retrieval cue. This
              allows the model to provide a principled explanation of the
              widespread advantage for forward recalls in free and serial
              recall. Modeling data from single-trial free recall, we
              demonstrate that TCM can simultaneously explain recency and
              contiguity effects across time scales.",
  month    =  jun,
  year     =  2002,
  keywords = "free recall; temporal context; episodic memory",
  doi      = "10.1006/jmps.2001.1388",
  issn     = "0022-2496"
}

@ARTICLE{Kurby2008-dy,
  title    = "Segmentation in the perception and memory of events",
  author   = "Kurby, Christopher A and Zacks, Jeffrey M",
  journal  = "Trends in cognitive sciences",
  volume   =  12,
  number   =  2,
  pages    = "72--79",
  abstract = "People make sense of continuous streams of observed behavior in
              part by segmenting them into events. Event segmentation seems to
              be an ongoing component of everyday perception. Events are
              segmented simultaneously at multiple timescales, and are grouped
              hierarchically. Activity in brain regions including the posterior
              temporal and parietal cortex and lateral frontal cortex increases
              transiently at event boundaries. The parsing of ongoing activity
              into events is related to the updating of working memory, to the
              contents of long-term memory, and to the learning of new
              procedures. Event segmentation might arise as a side effect of an
              adaptive mechanism that integrates information over the recent
              past to improve predictions about the near future.",
  month    =  feb,
  year     =  2008,
  doi      = "10.1016/j.tics.2007.11.004",
  pmc      = "PMC2263140",
  pmid     =  18178125,
  issn     = "1364-6613",
  language = "en"
}

@ARTICLE{Balestriero2023-gs,
  title         = "A Cookbook of Self-Supervised Learning",
  author        = "Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and
                   Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and
                   Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and
                   Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew
                   Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez,
                   Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann
                   and Goldblum, Micah",
  journal       = "arXiv [cs.LG]",
  abstract      = "Self-supervised learning, dubbed the dark matter of
                   intelligence, is a promising path to advance machine
                   learning. Yet, much like cooking, training SSL methods is a
                   delicate art with a high barrier to entry. While many
                   components are familiar, successfully training a SSL method
                   involves a dizzying set of choices from the pretext tasks to
                   training hyper-parameters. Our goal is to lower the barrier
                   to entry into SSL research by laying the foundations and
                   latest SSL recipes in the style of a cookbook. We hope to
                   empower the curious researcher to navigate the terrain of
                   methods, understand the role of the various knobs, and gain
                   the know-how required to explore how delicious SSL can be.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2304.12210"
}

@ARTICLE{Bramley2023-la,
  title    = "Local Search and the Evolution of World Models",
  author   = "Bramley, Neil R and Zhao, Bonan and Quillien, Tadeg and Lucas,
              Christopher G",
  journal  = "Topics in cognitive science",
  abstract = "An open question regarding how people develop their models of the
              world is how new candidates are generated for consideration out of
              infinitely many possibilities. We discuss the role that
              evolutionary mechanisms play in this process. Specifically, we
              argue that when it comes to developing a global world model,
              innovation is necessarily incremental, involving the generation
              and selection among random local mutations and recombinations of
              (parts of) one's current model. We argue that, by narrowing and
              guiding exploration, this feature of cognitive search is what
              allows human learners to discover better theories, without ever
              grappling directly with the problem of finding a ``global
              optimum,'' or best possible world model. We suggest this aspect of
              cognitive processing works analogously to how blind variation and
              selection mechanisms drive biological evolution. We propose
              algorithms developed for program synthesis provide candidate
              mechanisms for how human minds might achieve this. We discuss
              objections and implications of this perspective, finally
              suggesting that a better process-level understanding of how humans
              incrementally explore compositional theory spaces can shed light
              on how we think, and provide explanatory traction on fundamental
              cognitive biases, including anchoring, probability matching, and
              confirmation bias.",
  month    =  oct,
  year     =  2023,
  keywords = "Adaptor grammar; Approximation; Bootstrapping; Concepts;
              Evolution; Inference; Learning; MCMC; Search",
  doi      = "10.1111/tops.12703",
  pmid     =  37850714,
  issn     = "1756-8757,1756-8765",
  language = "en"
}

@ARTICLE{Campbell1960-kj,
  title    = "Blind variation and selective retention in creative thought as in
              other knowledge processes",
  author   = "Campbell, D T",
  journal  = "Psychological review",
  volume   =  67,
  pages    = "380--400",
  month    =  nov,
  year     =  1960,
  keywords = "CREATIVENESS; MEMORY",
  doi      = "10.1037/h0040373",
  pmid     =  13690223,
  issn     = "0033-295X",
  language = "en"
}

@ARTICLE{Simonton1999-cf,
  title     = "Creativity as Blind Variation and Selective Retention: Is the
               Creative Process Darwinian?",
  author    = "Simonton, Dean Keith",
  journal   = "Psychological inquiry",
  publisher = "Taylor \& Francis, Ltd.",
  volume    =  10,
  number    =  4,
  pages     = "309--328",
  abstract  = "[Darwinism provides not only a theory of biological evolution but
               also supplies a more generic process applicable to many phenomena
               in the behavioral sciences. Among these applications is the
               blind-variation and selective-retention model of creativity
               proposed by Campbell (1960). Research over the past 4 decades
               lends even more support to Campbell's model. This support is
               indicated by reviewing the experimental, psychometric, and
               historiometric literature on creativity. Then 4 major objections
               to the Darwinian model are examined (sociocultural determinism,
               individual volition, human rationality, and domain expertise).
               The article concludes by speculating whether the Darwinian model
               may actually subsume all alternative theories of creativity as
               special cases of the larger framework.]",
  year      =  1999,
  issn      = "1047-840X,1532-7965"
}

@ARTICLE{Markant2014-so,
  title    = "Is it better to select or to receive? Learning via active and
              passive hypothesis testing",
  author   = "Markant, Douglas B and Gureckis, Todd M",
  journal  = "Journal of experimental psychology. General",
  volume   =  143,
  number   =  1,
  pages    = "94--122",
  abstract = "People can test hypotheses through either selection or reception.
              In a selection task, the learner actively chooses observations to
              test his or her beliefs, whereas in reception tasks data are
              passively encountered. People routinely use both forms of testing
              in everyday life, but the critical psychological differences
              between selection and reception learning remain poorly understood.
              One hypothesis is that selection learning improves learning
              performance by enhancing generic cognitive processes related to
              motivation, attention, and engagement. Alternatively, we suggest
              that differences between these 2 learning modes derives from a
              hypothesis-dependent sampling bias that is introduced when a
              person collects data to test his or her own individual hypothesis.
              Drawing on influential models of sequential hypothesis-testing
              behavior, we show that such a bias (a) can lead to the collection
              of data that facilitates learning compared with reception learning
              and (b) can be more effective than observing the selections of
              another person. We then report a novel experiment based on a
              popular category learning paradigm that compares reception and
              selection learning. We additionally compare selection learners to
              a set of ``yoked'' participants who viewed the exact same sequence
              of observations under reception conditions. The results revealed
              systematic differences in performance that depended on the
              learner's role in collecting information and the abstract
              structure of the problem.",
  month    =  feb,
  year     =  2014,
  doi      = "10.1037/a0032108",
  pmid     =  23527948,
  issn     = "0096-3445",
  language = "en"
}

@ARTICLE{Ho2019-go,
  title   = "The value of abstraction",
  author  = "Ho, Mark K and Abel, David and Griffiths, Thomas L and Littman,
             Michael L",
  journal = "Current Opinion in Behavioral Sciences",
  volume  =  29,
  pages   = "111--116",
  month   =  oct,
  year    =  2019,
  doi     = "10.1016/j.cobeha.2019.05.001",
  issn    = "2352-1546"
}

@ARTICLE{Elfring2021-vz,
  title    = "Particle Filters: A Hands-On Tutorial",
  author   = "Elfring, Jos and Torta, Elena and van de Molengraft, René",
  journal  = "Sensors",
  volume   =  21,
  number   =  2,
  abstract = "The particle filter was popularized in the early 1990s and has
              been used for solving estimation problems ever since. The standard
              algorithm can be understood and implemented with limited effort
              due to the widespread availability of tutorial material and code
              examples. Extensive research has advanced the standard particle
              filter algorithm to improve its performance and applicability in
              various ways in the years after. As a result, selecting and
              implementing an advanced version of the particle filter that goes
              beyond the standard algorithm and fits a specific estimation
              problem requires either a thorough understanding or reviewing
              large amounts of the literature. The latter can be heavily time
              consuming especially for those with limited hands-on experience.
              Lack of implementation details in theory-oriented papers
              complicates this task even further. The goal of this tutorial is
              facilitating the reader to familiarize themselves with the key
              concepts of advanced particle filter algorithms and to select and
              implement the right particle filter for the estimation problem at
              hand. It acts as a single entry point that provides a theoretical
              overview of the filter, its assumptions and solutions for various
              challenges encountered when applying particle filters. Besides
              that, it includes a running example that demonstrates and
              implements many of the challenges and solutions.",
  month    =  jan,
  year     =  2021,
  keywords = "adaptive; auxiliary; extended Kalman; particle filter; tutorial",
  doi      = "10.3390/s21020438",
  pmc      = "PMC7826670",
  pmid     =  33435468,
  issn     = "1424-8220",
  language = "en"
}

@ARTICLE{Schacter1999-yd,
  title    = "The seven sins of memory. Insights from psychology and cognitive
              neuroscience",
  author   = "Schacter, D L",
  journal  = "The American psychologist",
  volume   =  54,
  number   =  3,
  pages    = "182--203",
  abstract = "Though often reliable, human memory is also fallible. This article
              examines how and why memory can get us into trouble. It is
              suggested that memory's misdeeds can be classified into 7 basic
              ``sins'': transience, absentmindedness, blocking, misattribution,
              suggestibility, bias, and persistence. The first three sins
              involve different types of forgetting, the next three refer to
              different types of distortions, and the final sin concerns
              intrusive recollections that are difficult to forget. Evidence is
              reviewed concerning each of the 7 sins from relevant sectors of
              psychology (cognitive, social, and clinical) and from cognitive
              neuroscience studies that include patients with focal brain damage
              or make use of recently developed neuroimaging techniques.
              Although the 7 sins may appear to reflect flaws in system design,
              it is argued instead that they are by-products of otherwise
              adaptive features of memory.",
  month    =  mar,
  year     =  1999,
  doi      = "10.1037//0003-066x.54.3.182",
  pmid     =  10199218,
  issn     = "0003-066X",
  language = "en"
}

@ARTICLE{Dunlosky2013-tu,
  title    = "Improving Students' Learning With Effective Learning Techniques:
              Promising Directions From Cognitive and Educational Psychology",
  author   = "Dunlosky, John and Rawson, Katherine A and Marsh, Elizabeth J and
              Nathan, Mitchell J and Willingham, Daniel T",
  journal  = "Psychological science in the public interest: a journal of the
              American Psychological Society",
  volume   =  14,
  number   =  1,
  pages    = "4--58",
  abstract = "Many students are being left behind by an educational system that
              some people believe is in crisis. Improving educational outcomes
              will require efforts on many fronts, but a central premise of this
              monograph is that one part of a solution involves helping students
              to better regulate their learning through the use of effective
              learning techniques. Fortunately, cognitive and educational
              psychologists have been developing and evaluating easy-to-use
              learning techniques that could help students achieve their
              learning goals. In this monograph, we discuss 10 learning
              techniques in detail and offer recommendations about their
              relative utility. We selected techniques that were expected to be
              relatively easy to use and hence could be adopted by many
              students. Also, some techniques (e.g., highlighting and rereading)
              were selected because students report relying heavily on them,
              which makes it especially important to examine how well they work.
              The techniques include elaborative interrogation,
              self-explanation, summarization, highlighting (or underlining),
              the keyword mnemonic, imagery use for text learning, rereading,
              practice testing, distributed practice, and interleaved practice.
              To offer recommendations about the relative utility of these
              techniques, we evaluated whether their benefits generalize across
              four categories of variables: learning conditions, student
              characteristics, materials, and criterion tasks. Learning
              conditions include aspects of the learning environment in which
              the technique is implemented, such as whether a student studies
              alone or with a group. Student characteristics include variables
              such as age, ability, and level of prior knowledge. Materials vary
              from simple concepts to mathematical problems to complicated
              science texts. Criterion tasks include different outcome measures
              that are relevant to student achievement, such as those tapping
              memory, problem solving, and comprehension. We attempted to
              provide thorough reviews for each technique, so this monograph is
              rather lengthy. However, we also wrote the monograph in a modular
              fashion, so it is easy to use. In particular, each review is
              divided into the following sections: General description of the
              technique and why it should work How general are the effects of
              this technique? 2a. Learning conditions 2b. Student
              characteristics 2c. Materials 2d. Criterion tasks Effects in
              representative educational contexts Issues for implementation
              Overall assessment The review for each technique can be read
              independently of the others, and particular variables of interest
              can be easily compared across techniques. To foreshadow our final
              recommendations, the techniques vary widely with respect to their
              generalizability and promise for improving student learning.
              Practice testing and distributed practice received high utility
              assessments because they benefit learners of different ages and
              abilities and have been shown to boost students' performance
              across many criterion tasks and even in educational contexts.
              Elaborative interrogation, self-explanation, and interleaved
              practice received moderate utility assessments. The benefits of
              these techniques do generalize across some variables, yet despite
              their promise, they fell short of a high utility assessment
              because the evidence for their efficacy is limited. For instance,
              elaborative interrogation and self-explanation have not been
              adequately evaluated in educational contexts, and the benefits of
              interleaving have just begun to be systematically explored, so the
              ultimate effectiveness of these techniques is currently unknown.
              Nevertheless, the techniques that received moderate-utility
              ratings show enough promise for us to recommend their use in
              appropriate situations, which we describe in detail within the
              review of each technique. Five techniques received a low utility
              assessment: summarization, highlighting, the keyword mnemonic,
              imagery use for text learning, and rereading. These techniques
              were rated as low utility for numerous reasons. Summarization and
              imagery use for text learning have been shown to help some
              students on some criterion tasks, yet the conditions under which
              these techniques produce benefits are limited, and much research
              is still needed to fully explore their overall effectiveness. The
              keyword mnemonic is difficult to implement in some contexts, and
              it appears to benefit students for a limited number of materials
              and for short retention intervals. Most students report rereading
              and highlighting, yet these techniques do not consistently boost
              students' performance, so other techniques should be used in their
              place (e.g., practice testing instead of rereading). Our hope is
              that this monograph will foster improvements in student learning,
              not only by showcasing which learning techniques are likely to
              have the most generalizable effects but also by encouraging
              researchers to continue investigating the most promising
              techniques. Accordingly, in our closing remarks, we discuss some
              issues for how these techniques could be implemented by teachers
              and students, and we highlight directions for future research.",
  month    =  jan,
  year     =  2013,
  doi      = "10.1177/1529100612453266",
  pmid     =  26173288,
  issn     = "1529-1006",
  language = "en"
}

@ARTICLE{Karpicke2012-dl,
  title     = "Retrieval-Based Learning: Active Retrieval Promotes Meaningful
               Learning",
  author    = "Karpicke, Jeffrey D",
  journal   = "Current directions in psychological science",
  publisher = "SAGE Publications Inc",
  volume    =  21,
  number    =  3,
  pages     = "157--163",
  abstract  = "Retrieval is the key process for understanding learning and for
               promoting learning, yet retrieval is not often granted the
               central role it deserves. Learning is typically identified with
               the encoding or construction of knowledge, and retrieval is
               considered merely the assessment of learning that occurred in a
               prior experience. The retrieval-based learning perspective
               outlined here is grounded in the fact that all expressions of
               knowledge involve retrieval and depend on the retrieval cues
               available in a given context. Further, every time a person
               retrieves knowledge, that knowledge is changed, because
               retrieving knowledge improves one?s ability to retrieve it again
               in the future. Practicing retrieval does not merely produce rote,
               transient learning; it produces meaningful, long-term learning.
               Yet retrieval practice is a tool many students lack metacognitive
               awareness of and do not use as often as they should. Active
               retrieval is an effective but undervalued strategy for promoting
               meaningful learning.",
  month     =  jun,
  year      =  2012,
  doi       = "10.1177/0963721412443552",
  issn      = "0963-7214"
}

@ARTICLE{Pathak2017-ow,
  title         = "Curiosity-driven Exploration by Self-supervised Prediction",
  author        = "Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and
                   Darrell, Trevor",
  journal       = "arXiv [cs.LG]",
  abstract      = "In many real-world scenarios, rewards extrinsic to the agent
                   are extremely sparse, or absent altogether. In such cases,
                   curiosity can serve as an intrinsic reward signal to enable
                   the agent to explore its environment and learn skills that
                   might be useful later in its life. We formulate curiosity as
                   the error in an agent's ability to predict the consequence of
                   its own actions in a visual feature space learned by a
                   self-supervised inverse dynamics model. Our formulation
                   scales to high-dimensional continuous state spaces like
                   images, bypasses the difficulties of directly predicting
                   pixels, and, critically, ignores the aspects of the
                   environment that cannot affect the agent. The proposed
                   approach is evaluated in two environments: VizDoom and Super
                   Mario Bros. Three broad settings are investigated: 1) sparse
                   extrinsic reward, where curiosity allows for far fewer
                   interactions with the environment to reach the goal; 2)
                   exploration with no extrinsic reward, where curiosity pushes
                   the agent to explore more efficiently; and 3) generalization
                   to unseen scenarios (e.g. new levels of the same game) where
                   the knowledge gained from earlier experience helps the agent
                   explore new places much faster than starting from scratch.
                   Demo video and code available at
                   https://pathak22.github.io/noreward-rl/",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1705.05363"
}

@ARTICLE{Engstrom2020-xn,
  title         = "Implementation Matters in Deep Policy Gradients: A Case Study
                   on {PPO} and {TRPO}",
  author        = "Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and
                   Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and
                   Madry, Aleksander",
  journal       = "arXiv [cs.LG]",
  abstract      = "We study the roots of algorithmic progress in deep policy
                   gradient algorithms through a case study on two popular
                   algorithms: Proximal Policy Optimization (PPO) and Trust
                   Region Policy Optimization (TRPO). Specifically, we
                   investigate the consequences of ``code-level optimizations:''
                   algorithm augmentations found only in implementations or
                   described as auxiliary details to the core algorithm.
                   Seemingly of secondary importance, such optimizations turn
                   out to have a major impact on agent behavior. Our results
                   show that they (a) are responsible for most of PPO's gain in
                   cumulative reward over TRPO, and (b) fundamentally change how
                   RL methods function. These insights show the difficulty and
                   importance of attributing performance gains in deep
                   reinforcement learning. Code for reproducing our results is
                   available at
                   https://github.com/MadryLab/implementation-matters .",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2005.12729"
}

@ARTICLE{Schulman2017-dt,
  title         = "Proximal Policy Optimization Algorithms",
  author        = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and
                   Radford, Alec and Klimov, Oleg",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose a new family of policy gradient methods for
                   reinforcement learning, which alternate between sampling data
                   through interaction with the environment, and optimizing a
                   ``surrogate'' objective function using stochastic gradient
                   ascent. Whereas standard policy gradient methods perform one
                   gradient update per data sample, we propose a novel objective
                   function that enables multiple epochs of minibatch updates.
                   The new methods, which we call proximal policy optimization
                   (PPO), have some of the benefits of trust region policy
                   optimization (TRPO), but they are much simpler to implement,
                   more general, and have better sample complexity
                   (empirically). Our experiments test PPO on a collection of
                   benchmark tasks, including simulated robotic locomotion and
                   Atari game playing, and we show that PPO outperforms other
                   online policy gradient methods, and overall strikes a
                   favorable balance between sample complexity, simplicity, and
                   wall-time.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1707.06347"
}

@ARTICLE{Whitehead1990-xz,
  title     = "Active Perception and Reinforcement Learning",
  author    = "Whitehead, Steven D and Ballard, Dana H",
  journal   = "Neural computation",
  publisher = "MIT Press",
  volume    =  2,
  number    =  4,
  pages     = "409--419",
  abstract  = "This paper considers adaptive control architectures that
               integrate active sensorimotor systems with decision systems based
               on reinforcement learning. One unavoidable consequence of active
               perception is that the agent's internal representation often
               confounds external world states. We call this phenomenon
               perceptual aliasing and show that it destabilizes existing
               reinforcement learning algorithms with respect to the optimal
               decision policy. A new decision system that overcomes these
               difficulties is described. The system incorporates a perceptual
               subcycle within the overall decision cycle and uses a modified
               learning algorithm to suppress the effects of perceptual
               aliasing. The result is a control architecture that learns not
               only how to solve a task but also where to focus its attention in
               order to collect necessary sensory information.",
  month     =  dec,
  year      =  1990,
  doi       = "10.1162/neco.1990.2.4.409",
  issn      = "0899-7667"
}

@INPROCEEDINGS{Kar2011-or,
  title     = "Bandit problems in networks: Asymptotically efficient distributed
               allocation rules",
  author    = "Kar, Soummya and Poor, H Vincent and Cui, Shuguang",
  booktitle = "2011 50th IEEE Conference on Decision and Control and European
               Control Conference",
  pages     = "1771--1778",
  abstract  = "This paper studies the multi-agent bandit problem in a
               distributed networked setting. The setting considered assumes
               only one bandit (the major bandit) has accessible reward
               information from its samples, whereas the rest (the minor
               bandits) have unobservable rewards. Under the assumption that the
               minor bandits are aware of the sampling pattern of the major
               bandit (but with no direct access to its rewards), a lower bound
               on the expected average network regret is obtained. The lower
               bound resembles the logarithmic optimal regret attained in single
               (classical) bandit problems, but in addition is shown to scale
               down with the number of agents. A collaborative and adaptive
               distributed allocation rule DA is proposed and is shown to
               achieve the lower bound on the expected average regret for a
               connected inter-bandit communication network. In particular, it
               is shown that under the DA allocation rule, the minor bandits
               attain sub-logarithmic expected regrets as opposed to logarithmic
               in the single agent setting.",
  month     =  dec,
  year      =  2011,
  keywords  = "Resource management;Decision making;Symmetric
               matrices;Vectors;Collaboration;Random variables;Density
               measurement;Networked Bandit Problems;Distributed Allocation
               Rules;Asymptotically Efficient;Partially Observable Rewards",
  doi       = "10.1109/CDC.2011.6160719",
  issn      = "0743-1546"
}

@ARTICLE{Webb2022-ao,
  title         = "Emergent Analogical Reasoning in Large Language Models",
  author        = "Webb, Taylor and Holyoak, Keith J and Lu, Hongjing",
  journal       = "arXiv [cs.AI]",
  abstract      = "The recent advent of large language models has reinvigorated
                   debate over whether human cognitive capacities might emerge
                   in such generic models given sufficient training data. Of
                   particular interest is the ability of these models to reason
                   about novel problems zero-shot, without any direct training.
                   In human cognition, this capacity is closely tied to an
                   ability to reason by analogy. Here, we performed a direct
                   comparison between human reasoners and a large language model
                   (the text-davinci-003 variant of GPT-3) on a range of
                   analogical tasks, including a novel text-based matrix
                   reasoning task closely modeled on Raven's Progressive
                   Matrices. We found that GPT-3 displayed a surprisingly strong
                   capacity for abstract pattern induction, matching or even
                   surpassing human capabilities in most settings. Our results
                   indicate that large language models such as GPT-3 have
                   acquired an emergent ability to find zero-shot solutions to a
                   broad range of analogy problems.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2212.09196"
}

@ARTICLE{Chen2023-lu,
  title         = "Teaching Large Language Models to Self-Debug",
  author        = "Chen, Xinyun and Lin, Maxwell and Schärli, Nathanael and
                   Zhou, Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have achieved impressive
                   performance on code generation. However, for complex
                   programming tasks, generating the correct solution in one go
                   becomes challenging, thus some prior works have designed
                   program repair approaches to improve code generation
                   performance. In this work, we propose Self-Debugging, which
                   teaches a large language model to debug its predicted program
                   via few-shot demonstrations. In particular, we demonstrate
                   that Self-Debugging can teach the large language model to
                   perform rubber duck debugging; i.e., without any feedback on
                   the code correctness or error messages, the model is able to
                   identify its mistakes by explaining the generated code in
                   natural language. Self-Debugging achieves the
                   state-of-the-art performance on several code generation
                   benchmarks, including the Spider dataset for text-to-SQL
                   generation, TransCoder for C++-to-Python translation, and
                   MBPP for text-to-Python generation. On the Spider benchmark
                   where there are no unit tests to verify the correctness of
                   predictions, Self-Debugging with code explanation
                   consistently improves the baseline by 2-3\%, and improves the
                   prediction accuracy on problems of the hardest label by 9\%.
                   On TransCoder and MBPP where unit tests are available,
                   Self-Debugging improves the baseline accuracy by up to 12\%.
                   Meanwhile, by leveraging feedback messages and reusing failed
                   predictions, Self-Debugging notably improves sample
                   efficiency, and can match or outperform baseline models that
                   generate more than 10x candidate programs.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.05128"
}

@INCOLLECTION{Singmann2019-vy,
  title     = "An introduction to mixed models for experimental psychology",
  author    = "Singmann, Henrik and Kellen, David",
  booktitle = "New Methods in Cognitive Psychology",
  publisher = "Routledge",
  edition   = "1st Edition",
  pages     = "4--31",
  abstract  = "This chapter describes a class of statistical model that is able
               to account for most of the cases of nonindependence that are
               typically encountered in psychological experiments, linear
               mixed-effects models, or mixed models for short. It introduces
               the concepts underlying mixed models and how they allow
               accounting for different types of nonindependence that can occur
               in psychological data. The chapter discusses how to set up a
               mixed model and how to perform statistical inference with a mixed
               model. The most important concept for understanding how to
               estimate and how to interpret mixed models is the distinction
               between fixed and random effects. One important characteristic of
               mixed models is that they allow random effects for multiple,
               possibly independent, random effects grouping factors. Mixed
               models are a modern class of statistical models that extend
               regular regression models by including random-effects parameters
               to account for dependencies among related data points.",
  month     =  oct,
  year      =  2019,
  doi       = "10.4324/9780429318405-2",
  isbn      = "9780429318405,9780429318405"
}

@ARTICLE{Forbus1984-zx,
  title     = "Qualitative process theory",
  author    = "Forbus, Kenneth D",
  journal   = "Artificial intelligence",
  publisher = "Elsevier BV",
  volume    =  24,
  number    = "1-3",
  pages     = "85--168",
  abstract  = "Objects move, collide, flow, bend, heat up, cool down, stretch,
               compress, and boil. These and other things that cause changes in
               objects over time are intuitively characterized as processes. To
               understand commonsense physical reasoning and make programs that
               interact with the physical world as well as people do we must
               understand qualitative reasoning about processes, when they will
               occur, their effects, and when they will stop. Qualitative
               process theory defines a simple notion of physical process that
               appears useful as a language in which to write dynamical
               theories. Reasoning about processes also motivates a new
               qualitative representation for quantity in terms of inequalities,
               called the quantity space. This paper describes the basic
               concepts of qualitative process theory, several different kinds
               of reasoning that can be performed with them, and discusses its
               implications for causal reasoning. Several extended examples
               illustrate the utility of the theory, including figuring out that
               a boiler can blow up, that an oscillator with friction will
               eventually stop, and how to say that you can pull with a string,
               but not push with it.",
  month     =  dec,
  year      =  1984,
  doi       = "10.1016/0004-3702(84)90038-9",
  issn      = "0004-3702,1872-7921",
  language  = "en"
}

@INPROCEEDINGS{Liu2022-nh,
  title     = "Mind's Eye: Grounded Language Model Reasoning through Simulation",
  author    = "Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen
               and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai,
               Andrew M",
  booktitle = "The Eleventh International Conference on Learning Representations",
  abstract  = "Successful and effective communication between humans and AI
               relies on a shared experience of the world. By training solely on
               written text, current language models (LMs) miss the grounded
               experience of humans in the real-world---their failure to relate
               language to the physical world causes knowledge to be
               misrepresented and obvious mistakes in their reasoning. We
               present Mind's Eye, a paradigm to ground language model reasoning
               in the physical world. Given a physical reasoning question, we
               use a computational physics engine (DeepMind's MuJoCo) to
               simulate the possible outcomes, and then use the simulation
               results as part of the input, which enables language models to
               perform reasoning. Experiments on 39 tasks in a physics alignment
               benchmark demonstrate that Mind's Eye can improve reasoning
               ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot
               absolute accuracy improvement on average). Smaller language
               models armed with Mind's Eye can obtain similar performance to
               models that are 100x larger. Finally, we confirm the robustness
               of Mind's Eye through ablation studies.",
  month     =  sep,
  year      =  2022
}

@ARTICLE{Dehaene2007-ya,
  title    = "Cultural recycling of cortical maps",
  author   = "Dehaene, Stanislas and Cohen, Laurent",
  journal  = "Neuron",
  volume   =  56,
  number   =  2,
  pages    = "384--398",
  abstract = "Part of human cortex is specialized for cultural domains such as
              reading and arithmetic, whose invention is too recent to have
              influenced the evolution of our species. Representations of letter
              strings and of numbers occupy reproducible locations within
              large-scale macromaps, respectively in the left occipito-temporal
              and bilateral intraparietal cortex. Furthermore, recent fMRI
              studies reveal a systematic architecture within these areas. To
              explain this paradoxical cerebral invariance of cultural maps, we
              propose a neuronal recycling hypothesis, according to which
              cultural inventions invade evolutionarily older brain circuits and
              inherit many of their structural constraints.",
  month    =  oct,
  year     =  2007,
  doi      = "10.1016/j.neuron.2007.10.004",
  pmid     =  17964253,
  issn     = "0896-6273",
  language = "en"
}

@ARTICLE{Bedny2017-as,
  title    = "Evidence from Blindness for a Cognitively Pluripotent Cortex",
  author   = "Bedny, Marina",
  journal  = "Trends in cognitive sciences",
  volume   =  21,
  number   =  9,
  pages    = "637--648",
  abstract = "Cognitive neuroscience seeks to discover how cognitive functions
              are implemented in neural circuits. Studies of plasticity in
              blindness suggest that this mind-brain mapping is highly flexible
              during development. In blindness, 'visual' cortices take on
              higher-cognitive functions, including language and mathematics,
              becoming sensitive to the grammatical structure of spoken
              sentences and the difficulty of math equations. Visual cortex
              activity at rest becomes synchronized with higher-cognitive
              networks. Such repurposing is striking in light of the cognitive
              and evolutionary differences between vision, language, and
              mathematics. We propose that human cortices are cognitively
              pluripotent, that is, capable of assuming a wide range of
              cognitive functions. Specialization is driven by input during
              development, which is itself constrained by connectivity and
              experience. 'The child who methodically adds two numbers from
              right to left, carrying a digit when necessary, may be using the
              same algorithm that is implemented by the wires and transistors of
              the cash register in the neighborhood supermarket…' ▓▓Vision,
              1982, David Marr.",
  month    =  sep,
  year     =  2017,
  keywords = "blindness; development; language; plasticity; visual cortex",
  doi      = "10.1016/j.tics.2017.06.003",
  pmid     =  28821345,
  issn     = "1364-6613,1879-307X",
  language = "en"
}

@ARTICLE{Gomez2019-ci,
  title    = "Extensive childhood experience with Pokémon suggests eccentricity
              drives organization of visual cortex",
  author   = "Gomez, Jesse and Barnett, Michael and Grill-Spector, Kalanit",
  journal  = "Nature human behaviour",
  volume   =  3,
  number   =  6,
  pages    = "611--624",
  abstract = "The functional organization of human high-level visual cortex,
              such as the face- and place-selective regions, is strikingly
              consistent across individuals. An unanswered question in
              neuroscience concerns which dimensions of visual information
              constrain the development and topography of this shared brain
              organization. To answer this question, we used functional magnetic
              resonance imaging to scan a unique group of adults who, as
              children, had extensive visual experience with Pokémon. These
              animal-like, pixelated characters are dissimilar from other
              ecological categories, such as faces and places, along critical
              dimensions (foveal bias, rectilinearity, size, animacy). We show
              not only that adults who have Pokémon experience demonstrate
              distinct distributed cortical responses to Pokémon, but also that
              the experienced retinal eccentricity during childhood can predict
              the locus of Pokémon responses in adulthood. These data
              demonstrate that inherent functional representations in the visual
              cortex-retinal eccentricity-combined with consistent viewing
              behaviour of particular stimuli during childhood result in a
              shared functional topography in adulthood.",
  month    =  jun,
  year     =  2019,
  doi      = "10.1038/s41562-019-0592-8",
  pmc      = "PMC7055538",
  pmid     =  31061489,
  issn     = "2397-3374",
  language = "en"
}

@ARTICLE{Cheyette2024-qr,
  title    = "Response to Difficulty Drives Variation in {IQ} Test Performance",
  author   = "Cheyette, Samuel J and Piantadosi, Steven T",
  journal  = "Open mind : discoveries in cognitive science",
  volume   =  8,
  pages    = "265--277",
  abstract = "In a large (N = 300), pre-registered experiment and data analysis
              model, we find that individual variation in overall performance on
              Raven's Progressive Matrices is substantially driven by
              differential strategizing in the face of difficulty. Some
              participants choose to spend more time on hard problems while
              others choose to spend less and these differences explain about
              42\% of the variance in overall performance. In a data analysis
              jointly predicting participants' reaction times and accuracy on
              each item, we find that the Raven's task captures at most half of
              participants' variation in time-controlled ability (48\%) down to
              almost none (3\%), depending on which notion of ability is
              assumed. Our results highlight the role that confounding factors
              such as motivation play in explaining individuals' differential
              performance in IQ testing.",
  month    =  mar,
  year     =  2024,
  keywords = "Bayesian modeling; IQ; individual differences",
  doi      = "10.1162/opmi\_a\_00127",
  pmc      = "PMC10990577",
  pmid     =  38571527,
  issn     = "2470-2986",
  language = "en"
}

@ARTICLE{Zwaan2006-ge,
  title    = "Seeing, acting, understanding: Motor resonance in language
              comprehension",
  author   = "Zwaan, Rolf A and Taylor, Lawrence J",
  journal  = "Journal of experimental psychology. General",
  volume   =  135,
  number   =  1,
  pages    = "1--11",
  abstract = "Observing actions and understanding sentences about actions
              activates corresponding motor processes in the
              observer-comprehender. In 5 experiments, the authors addressed 2
              novel questions regarding language-based motor resonance. The 1st
              question asks whether visual motion that is associated with an
              action produces motor resonance in sentence comprehension. The 2nd
              question asks whether motor resonance is modulated during sentence
              comprehension. The authors' experiments provide an affirmative
              response to both questions. A rotating visual stimulus affects
              both actual manual rotation and the comprehension of manual
              rotation sentences. Motor resonance is modulated by the linguistic
              input and is a rather immediate and localized phenomenon. The
              results are discussed in the context of theories of action
              observation and mental simulation. (PsycINFO Database Record (c)
              2016 APA, all rights reserved)",
  month    =  feb,
  year     =  2006,
  doi      = "10.1037/0096-3445.135.1.1",
  issn     = "0096-3445,1939-2222"
}

@ARTICLE{Roediger2006-bj,
  title     = "Test-Enhanced Learning: Taking Memory Tests Improves Long-Term
               Retention",
  author    = "Roediger, Henry L and Karpicke, Jeffrey D",
  journal   = "Psychological science",
  publisher = "SAGE Publications Inc",
  volume    =  17,
  number    =  3,
  pages     = "249--255",
  abstract  = "Taking a memory test not only assesses what one knows, but also
               enhances later retention, a phenomenon known as the testing
               effect. We studied this effect with educationally relevant
               materials and investigated whether testing facilitates learning
               only because tests offer an opportunity to restudy material. In
               two experiments, students studied prose passages and took one or
               three immediate free-recall tests, without feedback, or restudied
               the material the same number of times as the students who
               received tests. Students then took a final retention test 5 min,
               2 days, or 1 week later. When the final test was given after 5
               min, repeated studying improved recall relative to repeated
               testing. However, on the delayed tests, prior testing produced
               substantially greater retention than studying, even though
               repeated studying increased students' confidence in their ability
               to remember the material. Testing is a powerful means of
               improving learning, not just assessing it.",
  month     =  mar,
  year      =  2006,
  doi       = "10.1111/j.1467-9280.2006.01693.x",
  issn      = "0956-7976"
}

@ARTICLE{Craik1972-vo,
  title    = "Levels of processing: A framework for memory research",
  author   = "Craik, Fergus I M and Lockhart, Robert S",
  journal  = "Journal of Verbal Learning and Verbal Behavior",
  volume   =  11,
  number   =  6,
  pages    = "671--684",
  abstract = "This paper briefly reviews the evidence for multistore theories of
              memory and points out some difficulties with the approach. An
              alternative framework for human memory research is then outlined
              in terms of depth or levels of processing. Some current data and
              arguments are reexamined in the light of this alternative
              framework and implications for further research considered.",
  month    =  dec,
  year     =  1972,
  doi      = "10.1016/S0022-5371(72)80001-X",
  issn     = "0022-5371"
}

@ARTICLE{Berry1983-fx,
  title     = "Metacognitive experience and transfer of logical reasoning",
  author    = "Berry, Dianne C",
  journal   = "The Quarterly Journal of Experimental Psychology Section A",
  publisher = "Routledge",
  volume    =  35,
  number    =  1,
  pages     = "39--49",
  abstract  = "The experiments examine the influence of metacognitive experience
               on the transfer of logical processes in a problem solving
               setting. Subjects were presented with two versions of Wason's
               (1966) selection task. Although they were able to perform
               successfully on the concrete tasks (following a minimal
               explanation of the correct solution on an initial trial), the
               majority were not able to transfer a successful method to the
               abstract tasks. Verbalization during, or following, the concrete
               tasks produced substantial transfer effects however. It is
               suggested that verbalization may lead to an increased awareness
               of past behaviour, particularly of those aspects necessary for
               successful solution. Department of Experimental Psychology,
               University of Oxford, South Parks Road, Oxford.",
  month     =  feb,
  year      =  1983,
  doi       = "10.1080/14640748308402115",
  issn      = "0272-4987"
}

@BOOK{Ebbinghaus1885-na,
  title     = "Über das Gedächtnis: Untersuchungen zur experimentellen
               Psychologie",
  author    = "Ebbinghaus, Hermann",
  publisher = "Duncker \& Humblot",
  abstract  = "Die Bemühungen, für die mächtigen Hebel der exakten
               Naturforschung, Experiment und Zählung, auch in der Welt der
               psychischen Vorgänge geeignete Angriffspunkte zu gewinnen, sind
               bisher wesentlich auf das groſse Gebiet der Sinnesempfindungen
               und die psychologische Zeitmessung be-schränkt geblieben. Mit den
               Untersuchungen, deren Methode und vorläufige Resultate ich im
               folgenden mitteile, habe ich versucht, einen Schritt weiter in
               das Innere des psychischen Geschehens zu thun und die
               Erscheinungen des …",
  year      =  1885,
  language  = "de"
}

@ARTICLE{Wixted1997-ll,
  title    = "Genuine power curves in forgetting: A quantitative analysis of
              individual subject forgetting functions",
  author   = "Wixted, John T and Ebbesen, Ebbe B",
  journal  = "Memory \& cognition",
  volume   =  25,
  number   =  5,
  pages    = "731--739",
  abstract = "Comments on R. B. Anderson and R. D. Tweney's (1997) article which
              argues that the power law of forgetting may be an artifact of
              arithmetically averaging individual subject forgetting functions
              that are truly exponential in form and that geometric averaging
              would avoid this potential problem. The authors agree that
              researchers should always be cognizant of the possibility of
              averaging artifacts, but they also show that their conclusions
              about the form of forgetting remain unchanged (and goodness-of-fit
              statistics are scarcely affected by) whether arithmetic or
              geometric averaging is used. An analysis of individual subject
              forgetting functions shows that they, too, are described much
              better by a power function than by an exponential. (PsycINFO
              Database Record (c) 2016 APA, all rights reserved)",
  month    =  sep,
  year     =  1997,
  doi      = "10.3758/BF03211316",
  issn     = "0090-502X,1532-5946"
}

@ARTICLE{Karpicke2011-hc,
  title    = "Retrieval practice produces more learning than elaborate studying
              with concept mapping",
  author   = "Karpicke, Jeffrey D and Blunt, Janell R",
  journal  = "Science",
  volume   =  331,
  number   =  6018,
  pages    = "772--775",
  abstract = "Educators rely heavily on learning activities that encourage
              elaborative studying, whereas activities that require students to
              practice retrieving and reconstructing knowledge are used less
              frequently. Here, we show that practicing retrieval produces
              greater gains in meaningful learning than elaborative studying
              with concept mapping. The advantage of retrieval practice
              generalized across texts identical to those commonly found in
              science education. The advantage of retrieval practice was
              observed with test questions that assessed comprehension and
              required students to make inferences. The advantage of retrieval
              practice occurred even when the criterial test involved creating
              concept maps. Our findings support the theory that retrieval
              practice enhances learning by retrieval-specific mechanisms rather
              than by elaborative study processes. Retrieval practice is an
              effective tool to promote conceptual learning about science.
              (PsycINFO Database Record (c) 2018 APA, all rights reserved)",
  month    =  feb,
  year     =  2011,
  doi      = "10.1126/science.1199327",
  issn     = "0036-8075,1095-9203"
}

@ARTICLE{Frankland2004-xd,
  title    = "The involvement of the anterior cingulate cortex in remote
              contextual fear memory",
  author   = "Frankland, Paul W and Bontempi, Bruno and Talton, Lynn E and
              Kaczmarek, Leszek and Silva, Alcino J",
  journal  = "Science",
  volume   =  304,
  number   =  5672,
  pages    = "881--883",
  abstract = "Although the molecular, cellular, and systems mechanisms required
              for initial memory processing have been intensively investigated,
              those underlying permanent memory storage remain elusive. We
              present neuroanatomical, pharmacological, and genetic results
              demonstrating that the anterior cingulate cortex plays a critical
              role in remote memory for contextual fear conditioning. Imaging of
              activity-dependent genes shows that the anterior cingulate is
              activated by remote memory and that this activation is impaired by
              a null alpha-CaMKII mutation that blocks remote memory.
              Accordingly, reversible inactivation of this structure in normal
              mice disrupts remote memory without affecting recent memory.",
  month    =  may,
  year     =  2004,
  doi      = "10.1126/science.1094804",
  pmid     =  15131309,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@ARTICLE{Bontempi1999-og,
  title    = "Time-dependent reorganization of brain circuitry underlying
              long-term memory storage",
  author   = "Bontempi, B and Laurent-Demir, C and Destrade, C and Jaffard, R",
  journal  = "Nature",
  volume   =  400,
  number   =  6745,
  pages    = "671--675",
  abstract = "Retrograde amnesia observed following hippocampal lesions in
              humans and animals is typically temporally graded, with recent
              memory being impaired while remote memories remain intact,
              indicating that the hippocampal formation has a time-limited role
              in memory storage. However, this claim remains controversial
              because studies involving hippocampal lesions tell us nothing
              about the contribution of the hippocampus to memory storage if
              this region was present at the time of memory retrieval. We
              therefore used non-invasive functional brain imaging using
              (14C)2-deoxyglucose uptake to examine how the brain circuitry
              underlying long-term memory storage is reorganized over time in an
              intact brain. Regional metabolic activity in the brain was mapped
              in mice tested at different times for retention of a spatial
              discrimination task. Here we report that increasing the retention
              interval from 5 days to 25 days resulted in both decreased
              hippocampal metabolic activity during retention testing and a loss
              of correlation between hippocampal metabolic activity and memory
              performance. Concomitantly, a recruitment of certain cortical
              areas was observed. These results indicate that there is a
              time-dependent reorganization of the neuronal circuitry underlying
              long-term memory storage, in which a transitory interaction
              between the hippocampal formation and the neocortex would mediate
              the establishment of long-lived cortical memory representations.",
  month    =  aug,
  year     =  1999,
  doi      = "10.1038/23270",
  pmid     =  10458162,
  issn     = "0028-0836",
  language = "en"
}

@ARTICLE{Harnad1987-tn,
  title     = "Categorical perception: The groundwork of cognition",
  editor    = "Harnad, Stevan",
  journal   = "https://psycnet.apa.org › recordhttps://psycnet.apa.org › record",
  publisher = "Cambridge University Press Categorical perception",
  address   = "New York, NY, US",
  volume    =  599,
  abstract  = "``Categorical Perception'' brings together all the known examples
               of categorical perception, from research on humans and animals,
               infants and adults, in all the sense modalities so far
               investigated: hearing, seeing, and touch. The perceptual findings
               are interpreted in terms of the available cognitive and
               neuroscientific theories of how categorical perception is
               accomplished by the brain: Is it inborn? Is it learned? What is
               it that the mind does to the incoming continuous information to
               sort it into the discrete categories we can see, manipulate,
               name, and describe? Work on elementary perceptual and
               psychophysical categories (colors, sounds) is then compared with
               work on higher order categories: objects (tables, chairs),
               patterns, abstract concepts (goodness, truth). From a focus on
               the most thoroughly investigated case of categorical
               perception—speech perception—the book proceeds to an integrative
               view of categorization in general. (PsycINFO Database Record (c)
               2016 APA, all rights reserved)",
  year      =  1987
}

@ARTICLE{Yonelinas2010-bt,
  title    = "Recollection and familiarity: examining controversial assumptions
              and new directions",
  author   = "Yonelinas, Andrew P and Aly, Mariam and Wang, Wei-Chun and Koen,
              Joshua D",
  journal  = "Hippocampus",
  volume   =  20,
  number   =  11,
  pages    = "1178--1194",
  abstract = "It is well accepted that recognition memory reflects the
              contribution of two separable memory retrieval processes, namely
              recollection and familiarity. However, fundamental questions
              remain regarding the functional nature and neural substrates of
              these processes. In this article, we describe a simple
              quantitative model of recognition memory (i.e., the dual-process
              signal detection model) that has been useful in integrating
              findings from a broad range of cognitive studies, and that is now
              being applied in a growing number of neuroscientific
              investigations of memory. The model makes several strong
              assumptions about the behavioral nature and neural substrates of
              recollection and familiarity. A review of the literature indicates
              that these assumptions are generally well supported, but that
              there are clear boundary conditions in which these assumptions
              break down. We argue that these findings provide important
              insights into the operation of the processes underlying
              recognition. Finally, we consider how the dual-process approach
              relates to recent neuroanatomical and computational models and how
              it might be integrated with recent findings concerning the role of
              medial temporal lobe regions in other cognitive functions such as
              novelty detection, perception, implicit memory and short-term
              memory.",
  month    =  nov,
  year     =  2010,
  doi      = "10.1002/hipo.20864",
  pmc      = "PMC4251874",
  pmid     =  20848606,
  issn     = "1050-9631,1098-1063",
  language = "en"
}

@ARTICLE{Nadel2004-xy,
  title    = "The spatial brain",
  author   = "Nadel, Lynn and Hardt, Oliver",
  journal  = "Neuropsychology",
  volume   =  18,
  number   =  3,
  pages    = "473--476",
  abstract = "Themes emerging from the collection of articles in the Special
              Section on Long-Term Spatial Memory include the notion of multiple
              spatial systems, the relation between spatial representations and
              episodic memory, the role of context, and the neural systems
              involved in space. The authors conclude that distinguishing
              between egocentric and allocentric spatial systems makes sense of
              both behavioral and neurobiological data. The special role of the
              hippocampal system in allocentric space, and as a consequence, in
              context, suggests how a spatial system might end up central to the
              ability to remember episodes.",
  month    =  jul,
  year     =  2004,
  doi      = "10.1037/0894-4105.18.3.473",
  pmid     =  15291725,
  issn     = "0894-4105",
  language = "en"
}

@ARTICLE{Cepeda2008-xo,
  title     = "Spacing Effects in Learning: A Temporal Ridgeline of Optimal
               Retention",
  author    = "Cepeda, Nicholas J and Vul, Edward and Rohrer, Doug and Wixted,
               John T and Pashler, Harold",
  journal   = "Psychological science",
  publisher = "SAGE Publications Inc",
  volume    =  19,
  number    =  11,
  pages     = "1095--1102",
  abstract  = "To achieve enduring retention, people must usually study
               information on multiple occasions. How does the timing of study
               events affect retention? Prior research has examined this issue
               only in a spotty fashion, usually with very short time intervals.
               In a study aimed at characterizing spacing effects over
               significant durations, more than 1,350 individuals were taught a
               set of facts and?after a gap of up to 3.5 months?given a review.
               A final test was administered at a further delay of up to 1 year.
               At any given test delay, an increase in the interstudy gap at
               first increased, and then gradually reduced, final test
               performance. The optimal gap increased as test delay increased.
               However, when measured as a proportion of test delay, the optimal
               gap declined from about 20 to 40\% of a 1-week test delay to
               about 5 to 10\% of a 1-year test delay. The interaction of gap
               and test delay implies that many educational practices are highly
               inefficient.",
  month     =  nov,
  year      =  2008,
  doi       = "10.1111/j.1467-9280.2008.02209.x",
  issn      = "0956-7976"
}

@ARTICLE{Koutstaal1998-ta,
  title    = "Post-event review in older and younger adults: improving memory
              accessibility of complex everyday events",
  author   = "Koutstaal, W and Schacter, D L and Johnson, M K and Angell, K E
              and Gross, M S",
  journal  = "Psychology and aging",
  volume   =  13,
  number   =  2,
  pages    = "277--296",
  abstract = "Recalling an event at 1 time often increases the likelihood that
              it will be remembered at a still later time. The authors examined
              the degree to which older and younger adults' memory for everyday
              events that they watched on a videotape was improved by later
              seeing photographs or reading brief verbal descriptions of those
              events. Both older and younger adults recalled more events, in
              greater detail, with than without review. Verbal descriptions
              enhanced later recall to the same degree as reviewing photographs.
              Younger adults generally gained more from review than older adults
              on measures of the absolute number of details recalled and when
              facilitation was assessed relative to a no-review control
              condition, but not when memory for reviewed events was expressed
              as a proportion of each individual's total recall. Post-event
              review has clear potential practical benefits for improving memory
              of older adults.",
  month    =  jun,
  year     =  1998,
  doi      = "10.1037//0882-7974.13.2.277",
  pmid     =  9640588,
  issn     = "0882-7974",
  language = "en"
}

@ARTICLE{Auble1979-fe,
  title     = "Effort toward comprehension: Elaboration or “aha”?",
  author    = "Auble, Pamela M and Franks, Jeffery J and Soraci, Salvatore A",
  journal   = "Memory \& Cognition",
  publisher = "Springer",
  volume    =  7,
  number    =  6,
  pages     = "426--434",
  abstract  = "Auble and Franks (1978) found that a process termed “effort
               toward comprehension” was important in facilitating recall of
               sentences. Four experiments were conducted to further elucidate
               the nature of this process. Two hypotheses were considered:
               (1)Effort toward comprehension involves greater elaboration or
               deeper processing of the sentence; (2)effort toward comprehension
               can be viewed as an “aha” experience (i.e., a state of
               noncomprehension followed by comprehension of the sentence).
               Results indicated that recall was significantly greater for
               subjects in conditions producing “aha” reactions. No support was
               found for the elaboration interpretation of effort toward
               comprehension.",
  month     =  nov,
  year      =  1979,
  doi       = "10.3758/BF03198259",
  issn      = "1532-5946,1532-5946",
  language  = "en"
}

@ARTICLE{Moskvichev2023-ka,
  title         = "The {ConceptARC} Benchmark: Evaluating Understanding and
                   Generalization in the {ARC} Domain",
  author        = "Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell,
                   Melanie",
  journal       = "arXiv [cs.LG]",
  abstract      = "The abilities to form and abstract concepts is key to human
                   intelligence, but such abilities remain lacking in
                   state-of-the-art AI systems. There has been substantial
                   research on conceptual abstraction in AI, particularly using
                   idealized domains such as Raven's Progressive Matrices and
                   Bongard problems, but even when AI systems succeed on such
                   problems, the systems are rarely evaluated in depth to see if
                   they have actually grasped the concepts they are meant to
                   capture. In this paper we describe an in-depth evaluation
                   benchmark for the Abstraction and Reasoning Corpus (ARC), a
                   collection of few-shot abstraction and analogy problems
                   developed by Chollet [2019]. In particular, we describe
                   ConceptARC, a new, publicly available benchmark in the ARC
                   domain that systematically assesses abstraction and
                   generalization abilities on a number of basic spatial and
                   semantic concepts. ConceptARC differs from the original ARC
                   dataset in that it is specifically organized around ``concept
                   groups'' -- sets of problems that focus on specific concepts
                   and that are vary in complexity and level of abstraction. We
                   report results on testing humans on this benchmark as well as
                   three machine solvers: the top two programs from a 2021 ARC
                   competition and OpenAI's GPT-4. Our results show that humans
                   substantially outperform the machine solvers on this
                   benchmark, showing abilities to abstract and generalize
                   concepts that are not yet captured by AI systems. We believe
                   that this benchmark will spur improvements in the development
                   of AI systems for conceptual abstraction and in the effective
                   evaluation of such systems.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2305.07141"
}

@ARTICLE{Atzil2018-lk,
  title     = "Growing a social brain",
  author    = "Atzil, Shir and Gao, Wei and Fradkin, Isaac and Barrett, Lisa
               Feldman",
  journal   = "Nature Human Behaviour",
  publisher = "Nature Publishing Group",
  volume    =  2,
  number    =  9,
  pages     = "624--636",
  abstract  = "It has long been assumed that social animals, such as humans, are
               born with a brain system that has evolved to support social
               affiliation. However, the evidence does not necessarily support
               this assumption. Alternatively, social animals can be defined as
               those who cannot survive alone and rely on members from their
               group to regulate their ongoing physiology (or allostasis). The
               rather simple evolutionary constraint of social dependency for
               survival can be sufficient to make the social environment vitally
               salient, and to provide the ultimate driving force for socially
               crafted brain development and learning. In this Perspective, we
               propose a framework for sociality and specify a set of hypotheses
               on the mechanisms of social development and underlying neural
               systems. The theoretical shift proposed here implies that
               profound human characteristics, including but not limited to
               sociality, are acquired at an early age, while social
               interactions provide key wiring instructions that determine brain
               development. Human infants need a social environment to survive
               as they rely on caregivers to maintain allostasis. This
               Perspective proposes that the need of others to regulate
               physiological changes determines brain development, not only in
               the social domain.",
  month     =  aug,
  year      =  2018,
  doi       = "10.1038/s41562-018-0384-6",
  issn      = "2397-3374,2397-3374",
  language  = "en"
}

@ARTICLE{Barrett2007-hj,
  title    = "Learning about tools in infancy",
  author   = "Barrett, Tracy M and Davis, Evan F and Needham, Amy",
  journal  = "Developmental psychology",
  volume   =  43,
  number   =  2,
  pages    = "352--368",
  abstract = "These experiments explored the role of prior experience in 12- to
              18-month-old infants' tool-directed actions. In Experiment 1,
              infants' use of a familiar tool (spoon) to accomplish a novel task
              (turning on lights inside a box) was examined. Infants tended to
              grasp the spoon by its handle even when doing so made solving the
              task impossible (the bowl did not fit through the hole in the box,
              but the handle did) and even though the experimenter demonstrated
              a bowl-grasp. In contrast, infants used a novel tool flexibly and
              grasped both sides equally often. In Experiment 2, infants
              received training using the novel tool for a particular function;
              3 groups of infants were trained to use the tool differently.
              Later, infants' performance was facilitated on tasks that required
              infants to grasp the part of the tool they were trained to grasp.
              The results suggest that (a) infants' prior experiences with tools
              are important to understanding subsequent tool use, and (b) rather
              than learning about tool function (e.g., hammering), infants learn
              about which part of the tool is meant to be held, at least early
              in their exposure to a novel tool.",
  month    =  mar,
  year     =  2007,
  doi      = "10.1037/0012-1649.43.2.352",
  pmid     =  17352544,
  issn     = "0012-1649",
  language = "en"
}

@ARTICLE{Dennett1987-nj,
  title     = "The intentional stance",
  author    = "Dennett, Daniel C",
  journal   = "https://psycnet.apa.org › record ›
               1987-98612-000https://psycnet.apa.org › record › 1987-98612-000",
  publisher = "The MIT Press The intentional stance.",
  address   = "Cambridge, MA, US",
  volume    =  388,
  abstract  = "The theory of intentionality presented in this book has been
               gradually evolving over about twenty years. While the central
               ideas found rudimentary expression in ``Content and
               Consciousness'' in 1969, it was the publication 1971 of
               ``Intentional Systems'' that initiated the series of articles
               about what I call the intentional stance and the objects one
               discovers from that stance: intentional systems. The first three
               of these articles (Dennett 1971, 1973, 1976b) were reprinted in
               ``Brainstorms'' in 1978, and critics and students often treat
               that book as the canonical, target expression of my view. I soon
               found, however, that the defense of my position was evolving
               further in response to criticism, and so I was driven to compose
               a series of post-``Brainstorms'' essays in which I attempted to
               revise, re-express, and extend my view. (PsycINFO Database Record
               (c) 2016 APA, all rights reserved)",
  year      =  1987
}

@ARTICLE{Zettersten2020-nm,
  title     = "Finding categories through words: More nameable features improve
               category learning",
  author    = "Zettersten, Martin and Lupyan, Gary",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  196,
  number    =  104135,
  pages     =  104135,
  abstract  = "What are the cognitive consequences of having a name for
               something? Having a word for a feature makes it easier to
               communicate about a set of exemplars belonging to the same
               category (e.g., ``the red things''). But might it also make it
               easier to learn the category itself? Here, we provide evidence
               that the ease of learning category distinctions based on simple
               visual features is predicted from the ease of naming those
               features. Across seven experiments, participants learned
               categories composed of colors or shapes that were either easy or
               more difficult to name in English. Holding the category structure
               constant, when the underlying features of the category were easy
               to name, participants were faster and more accurate in learning
               the novel category. These results suggest that compact verbal
               labels may facilitate hypothesis formation during learning: it is
               easier to pose the hypothesis ``it is about redness'' than ``it
               is about that pinkish-purplish color''. Our results have
               consequences for understanding how developmental and
               cross-linguistic differences in a language's vocabulary affect
               category learning and conceptual development.",
  month     =  mar,
  year      =  2020,
  keywords  = "Categorization; Category learning; Hypothesis-testing; Language;
               Nameability; Rule learning",
  doi       = "10.1016/j.cognition.2019.104135",
  pmid      =  31821963,
  issn      = "0010-0277,1873-7838",
  language  = "en"
}

@ARTICLE{Jonas2017-jr,
  title    = "Could a Neuroscientist Understand a Microprocessor?",
  author   = "Jonas, Eric and Kording, Konrad Paul",
  journal  = "PLoS computational biology",
  volume   =  13,
  number   =  1,
  pages    = "e1005268",
  abstract = "There is a popular belief in neuroscience that we are primarily
              data limited, and that producing large, multimodal, and complex
              datasets will, with the help of advanced data analysis algorithms,
              lead to fundamental insights into the way the brain processes
              information. These datasets do not yet exist, and if they did we
              would have no way of evaluating whether or not the
              algorithmically-generated insights were sufficient or even
              correct. To address this, here we take a classical microprocessor
              as a model organism, and use our ability to perform arbitrary
              experiments on it to see if popular data analysis methods from
              neuroscience can elucidate the way it processes information.
              Microprocessors are among those artificial information processing
              systems that are both complex and that we understand at all
              levels, from the overall logical flow, via logical gates, to the
              dynamics of transistors. We show that the approaches reveal
              interesting structure in the data but do not meaningfully describe
              the hierarchy of information processing in the microprocessor.
              This suggests current analytic approaches in neuroscience may fall
              short of producing meaningful understanding of neural systems,
              regardless of the amount of data. Additionally, we argue for
              scientists using complex non-linear dynamical systems with known
              ground truth, such as the microprocessor as a validation platform
              for time-series and structure discovery methods.",
  month    =  jan,
  year     =  2017,
  doi      = "10.1371/journal.pcbi.1005268",
  pmc      = "PMC5230747",
  pmid     =  28081141,
  issn     = "1553-734X,1553-7358",
  language = "en"
}

@ARTICLE{Muthukrishna2019-wq,
  title    = "A problem in theory",
  author   = "Muthukrishna, Michael and Henrich, Joseph",
  journal  = "Nature human behaviour",
  volume   =  3,
  number   =  3,
  pages    = "221--229",
  abstract = "The replication crisis facing the psychological sciences is widely
              regarded as rooted in methodological or statistical shortcomings.
              We argue that a large part of the problem is the lack of a
              cumulative theoretical framework or frameworks. Without an
              overarching theoretical framework that generates hypotheses across
              diverse domains, empirical programs spawn and grow from personal
              intuitions and culturally biased folk theories. By providing ways
              to develop clear predictions, including through the use of formal
              modelling, theoretical frameworks set expectations that determine
              whether a new finding is confirmatory, nicely integrating with
              existing lines of research, or surprising, and therefore requiring
              further replication and scrutiny. Such frameworks also prioritize
              certain research foci, motivate the use diverse empirical
              approaches and, often, provide a natural means to integrate across
              the sciences. Thus, overarching theoretical frameworks pave the
              way toward a more general theory of human behaviour. We illustrate
              one such a theoretical framework: dual inheritance theory.",
  month    =  mar,
  year     =  2019,
  doi      = "10.1038/s41562-018-0522-1",
  pmid     =  30953018,
  issn     = "2397-3374",
  language = "en"
}

@ARTICLE{Allen2023-oj,
  title    = "Using Games to Understand the Mind",
  author   = "Allen, Kelsey R and Brändle, Franziska and Botvinick, Matthew and
              Fan, Judith and Gershman, Samuel J and Gopnik, Alison and
              Griffiths, Thomas L and Hartshorne, Joshua K and Hauser, Tobias U
              and Ho, Mark K and al., Et",
  abstract = "Video games are played by over 2 billion people spread across the
              world population, with both children and adults participating.
              Games have gained popularity as an avenue for studying cognition.
              We believe that studying cognition using games can generate
              progress in psychology and in neuroscience similar to the one that
              has occurred in artificial intelligence research over the past
              decades. Using games to understand the mind enables researchers to
              scale up theories of cognition to more complex settings,
              reverse-engineer human inductive biases, create experiments that
              participants want to take part in, and study learning over long
              time horizons. We describe both the advantages and drawbacks of
              using games relative to standard lab-based experiments, and lay
              out a set of recommendations on how to gain the most from using
              games to study cognition. We hope that this article will lead to a
              wider use of games as experimental paradigms, elevating the
              complexity, robustness, and external validity of research on the
              mind.",
  month    =  feb,
  year     =  2023,
  keywords = "cognition; games; psychology",
  doi      = "10.31234/osf.io/hbsvj"
}

@ARTICLE{Rouault2022-cb,
  title    = "Controllability boosts neural and cognitive signatures of
              changes-of-mind in uncertain environments",
  author   = "Rouault, Marion and Weiss, Aurélien and Lee, Junseok K and
              Drugowitsch, Jan and Chambon, Valerian and Wyart, Valentin",
  journal  = "eLife",
  volume   =  11,
  abstract = "In uncertain environments, seeking information about alternative
              choice options is essential for adaptive learning and
              decision-making. However, information seeking is usually
              confounded with changes-of-mind about the reliability of the
              preferred option. Here, we exploited the fact that information
              seeking requires control over which option to sample to isolate
              its behavioral and neurophysiological signatures. We found that
              changes-of-mind occurring with control require more evidence
              against the current option, are associated with reduced
              confidence, but are nevertheless more likely to be confirmed on
              the next decision. Multimodal neurophysiological recordings showed
              that these changes-of-mind are preceded by stronger activation of
              the dorsal attention network in magnetoencephalography, and
              followed by increased pupil-linked arousal during the presentation
              of decision outcomes. Together, these findings indicate that
              information seeking increases the saliency of evidence perceived
              as the direct consequence of one's own actions.",
  month    =  sep,
  year     =  2022,
  keywords = "confidence; decision-making; exploration; human; inference;
              information seeking; neuroscience",
  doi      = "10.7554/eLife.75038",
  pmc      = "PMC9470160",
  pmid     =  36097814,
  issn     = "2050-084X",
  language = "en"
}

@ARTICLE{Eckstein2023-rg,
  title    = "Predictive and Interpretable: Combining Artificial Neural Networks
              and Classic Cognitive Models to Understand Human Learning and
              Decision Making",
  author   = "Eckstein, Maria K and Summerfield, Christopher and Daw, Nathaniel
              D and Miller, Kevin J",
  journal  = "bioRxiv",
  pages    = "2023.05.17.541226",
  abstract = "Quantitative models of behavior are a fundamental tool in
              cognitive science. Typically, models are hand-crafted to implement
              specific cognitive mechanisms. Such “classic” models are
              interpretable by design, but may provide poor fit to experimental
              data. Artificial neural networks (ANNs), on the contrary, can fit
              arbitrary datasets at the cost of opaque mechanisms. Here, we
              adopt a hybrid approach, combining the predictive power of ANNs
              with the interpretability of classic models. We apply this
              approach to Reinforcement Learning (RL), beginning with classic RL
              models and replacing their components one-by-one with ANNs. We
              find that hybrid models can provide similar fit to fully-general
              ANNs, while retaining the interpretability of classic cognitive
              models: They reveal reward-based learning mechanisms in humans
              that are strikingly similar to classic RL. They also reveal
              mechanisms not contained in classic models, including separate
              rewardblind mechanisms, and the specific memory contents relevant
              to reward-based and reward-blind mechanisms. \#\#\# Competing
              Interest Statement The authors have declared no competing
              interest.",
  month    =  may,
  year     =  2023,
  doi      = "10.1101/2023.05.17.541226",
  language = "en"
}

@ARTICLE{Weiss2021-nw,
  title    = "Interacting with volatile environments stabilizes hidden-state
              inference and its brain signatures",
  author   = "Weiss, Aurélien and Chambon, Valérian and Lee, Junseok K and
              Drugowitsch, Jan and Wyart, Valentin",
  journal  = "Nature communications",
  volume   =  12,
  number   =  1,
  pages    =  2228,
  abstract = "Making accurate decisions in uncertain environments requires
              identifying the generative cause of sensory cues, but also the
              expected outcomes of possible actions. Although both cognitive
              processes can be formalized as Bayesian inference, they are
              commonly studied using different experimental frameworks, making
              their formal comparison difficult. Here, by framing a reversal
              learning task either as cue-based or outcome-based inference, we
              found that humans perceive the same volatile environment as more
              stable when inferring its hidden state by interaction with
              uncertain outcomes than by observation of equally uncertain cues.
              Multivariate patterns of magnetoencephalographic (MEG) activity
              reflected this behavioral difference in the neural interaction
              between inferred beliefs and incoming evidence, an effect
              originating from associative regions in the temporal lobe.
              Together, these findings indicate that the degree of control over
              the sampling of volatile environments shapes human learning and
              decision-making under uncertainty.",
  month    =  apr,
  year     =  2021,
  doi      = "10.1038/s41467-021-22396-6",
  pmc      = "PMC8044147",
  pmid     =  33850124,
  issn     = "2041-1723",
  language = "en"
}

@ARTICLE{Findling2020-rl,
  title    = "Computation noise promotes cognitive resilience to adverse
              conditions during decision-making",
  author   = "Findling, Charles and Wyart, Valentin",
  journal  = "bioRxiv",
  pages    = "2020.06.10.145300",
  abstract = "Random noise in information processing systems is widely seen as
              detrimental to function. But despite the large trial-to-trial
              variability of neural activity and behavior, humans and other
              animals show a remarkable adaptability to unexpected adverse
              events occurring during task execution. This cognitive ability,
              described as constitutive of general intelligence, is missing from
              current artificial intelligence (AI) systems which feature exact
              (noise-free) computations. Here we show that implementing
              computation noise in recurrent neural networks boosts their
              cognitive resilience to a variety of adverse conditions entirely
              unseen during training, in a way that resembles human and animal
              cognition. In contrast to artificial agents with exact
              computations, noisy agents exhibit hallmarks of Bayesian inference
              acquired in a ‘zero-shot’ fashion – without prior experience with
              conditions that require these computations for maximizing rewards.
              We further demonstrate that these cognitive benefits result from
              free-standing regularization of activity patterns in noisy neural
              networks. Together, these findings suggest that intelligence may
              ride on computation noise to promote near-optimal decision-making
              in adverse conditions without any engineered cognitive
              sophistication. \#\#\# Competing Interest Statement The authors
              have declared no competing interest.",
  month    =  jun,
  year     =  2020,
  doi      = "10.1101/2020.06.10.145300",
  language = "en"
}

@ARTICLE{Carr2016-un,
  title     = "Eureka!: What is innovation, how does it develop, and who does
               it?",
  author    = "Carr, Kayleigh and Kendal, Rachel L and Flynn, Emma G",
  journal   = "Child development",
  publisher = "John Wiley \& Sons, Ltd",
  volume    =  87,
  number    =  5,
  pages     = "1505--1519",
  abstract  = "Innovation is not only central to changes in traditional practice
               but arguably responsible for humanity's remarkable success at
               colonizing the earth and diversifying the products, technologies,
               and systems within it. Surprisingly little is known of how this
               integral component of behavioral flexibility develops and the
               factors that are responsible for individual differences therein.
               This review highlights two primary ways in which the process and
               development of innovation may be better understood: By emulating
               the critical advances of animal behavior researchers in examining
               innovation in nonhuman species and establishing a clearer
               conceptualization of what is ``innovation''. A pathway to
               innovation is suggested and an innovation classification system
               offered to aid recognition of its appearance and potential
               cultural contributions.",
  month     =  sep,
  year      =  2016,
  doi       = "10.1111/cdev.12549",
  pmc       = "PMC5053256",
  pmid      =  27241583,
  issn      = "1467-8624,0009-3920",
  language  = "en"
}

@ARTICLE{Cutting2011-nj,
  title     = "Why do children lack the flexibility to innovate tools?",
  author    = "Cutting, Nicola and Apperly, Ian A and Beck, Sarah R",
  journal   = "Journal of experimental child psychology",
  publisher = "Elsevier BV",
  volume    =  109,
  number    =  4,
  pages     = "497--511",
  abstract  = "Despite being proficient tool users, young children have
               surprising difficulty in innovating tools (making novel tools to
               solve problems). Two experiments found that 4- to 7-year-olds had
               difficulty on two tool innovation problems and explored reasons
               for this inflexibility. Experiment 1 (N=51) showed that
               children's performance was unaffected by the need to switch away
               from previously correct strategies. Experiment 2 (N=92) suggested
               that children's difficulty could not easily be explained by task
               pragmatics or permission issues. Both experiments found evidence
               that some children perseverated on a single incorrect strategy,
               but such perseveration was insufficient to explain children's
               tendency not to innovate tools. We suggest that children's
               difficulty lies not with switching, task pragmatics, or
               behavioral perseveration but rather with solving the
               fundamentally ``ill-structured'' nature of tool innovation
               problems.",
  month     =  aug,
  year      =  2011,
  doi       = "10.1016/j.jecp.2011.02.012",
  pmid      =  21420101,
  issn      = "0022-0965,1096-0457",
  language  = "en"
}

@ARTICLE{Beck2011-bo,
  title     = "Making tools isn't child's play",
  author    = "Beck, Sarah R and Apperly, Ian A and Chappell, Jackie and
               Guthrie, Carlie and Cutting, Nicola",
  journal   = "Cognition",
  publisher = "Elsevier BV",
  volume    =  119,
  number    =  2,
  pages     = "301--306",
  abstract  = "Tool making evidences intelligent, flexible thinking. In
               Experiment 1, we confirmed that 4- to 7-year-olds chose a hook
               tool to retrieve a bucket from a tube. In Experiment 2, 3- to
               5-year-olds consistently failed to innovate a simple hook tool.
               Eight-year-olds performed at mature levels. In contrast, making a
               tool following demonstration was easy for even the youngest
               children. In Experiment 3, children's performance did not improve
               given the opportunity to manipulate the objects in a warm-up
               phase. Children's tool innovation lags substantially behind their
               ability to learn how to make tools by observing others.",
  month     =  may,
  year      =  2011,
  doi       = "10.1016/j.cognition.2011.01.003",
  pmid      =  21315325,
  issn      = "0010-0277,1873-7838",
  language  = "en"
}

@ARTICLE{Nielsen2014-lm,
  title     = "Exploring tool innovation: a comparison of Western and Bushman
               children",
  author    = "Nielsen, Mark and Tomaselli, Keyan and Mushin, Ilana and Whiten,
               Andrew",
  journal   = "Journal of experimental child psychology",
  publisher = "Elsevier BV",
  volume    =  126,
  pages     = "384--394",
  abstract  = "A capacity for constructing new tools, or using old tools in new
               ways, to solve novel problems is a core feature of what it means
               to be human. Yet current evidence suggests that young children
               are surprisingly poor at innovating tools. However, all studies
               of tool innovation to date have been conducted with children from
               comparatively privileged Western backgrounds. This raises
               questions as to whether or not previously documented tool
               innovation failure is culturally and economically specific. In
               the current study, thus, we explored the innovation capacities of
               children from Westernized urban backgrounds and from remote
               communities of South African Bushmen. Consistent with past
               research, we found tool innovation to occur at extremely low
               rates and that cultural background had no bearing on this. The
               current study is the first to empirically test tool innovation in
               children from non-Western backgrounds, with our data being
               consistent with the view that despite its key role in human
               evolution, a capacity for innovation in tool making remains
               remarkably undeveloped during early childhood.",
  month     =  oct,
  year      =  2014,
  keywords  = "Cognitive development; Cross-cultural; Cumulative culture;
               Innovation; Social learning; Tool use",
  doi       = "10.1016/j.jecp.2014.05.008",
  pmid      =  25014272,
  issn      = "0022-0965,1096-0457",
  language  = "en"
}

@ARTICLE{Cutting2014-jv,
  title     = "The puzzling difficulty of tool innovation: why can't children
               piece their knowledge together?",
  author    = "Cutting, Nicola and Apperly, Ian A and Chappell, Jackie and Beck,
               Sarah R",
  journal   = "Journal of experimental child psychology",
  publisher = "Elsevier BV",
  volume    =  125,
  pages     = "110--117",
  abstract  = "Tool innovation-designing and making novel tools to solve
               tasks-is extremely difficult for young children. To discover why
               this might be, we highlighted different aspects of tool making to
               children aged 4 to 6 years (N=110). Older children successfully
               innovated the means to make a hook after seeing the pre-made
               target tool only if they had a chance to manipulate the materials
               during a warm-up. Older children who had not manipulated the
               materials and all younger children performed at floor. We
               conclude that children's difficulty is likely to be due to the
               ill-structured nature of tool innovation problems, in which
               components of a solution must be retrieved and coordinated. Older
               children struggled to bring to mind components of the solution
               but could coordinate them, whereas younger children could not
               coordinate components even when explicitly provided.",
  month     =  sep,
  year      =  2014,
  keywords  = "Cognitive Development; Ill-structured problems; Innovation;
               Problem Solving; Social learning; Tools",
  doi       = "10.1016/j.jecp.2013.11.010",
  pmid      =  24530037,
  issn      = "0022-0965,1096-0457",
  language  = "en"
}

@ARTICLE{Nielsen2013-xg,
  title     = "Young children's imitative and innovative behaviour on the
               floating object task: Children's imitative and innovative
               behaviour on the floating object task",
  author    = "Nielsen, Mark",
  journal   = "Infant and child development",
  publisher = "Wiley",
  volume    =  22,
  number    =  1,
  pages     = "44--52",
  abstract  = "Past research has shown that children will copy the actions of
               adults with high fidelity, even actions that are obviously
               causally irrelevant to the modelled outcome. However, this
               phenomenon has always been documented in cases where a clear
               functional outcome has been brought about (e.g. getting a box
               open to retrieve a toy). Here, we demonstrate how young children
               will continue to adopt the precise actions shown to them by an
               adult even if the goal of the actions is not realized during
               modelling. A group of 4-year-old participants were presented with
               a toy lying at the bottom of a Perspex tube. Despite the
               availability of a bottle of water, and unlike the responses of a
               group of adults, few children spontaneously identified the
               solution of pouring the water into the tube to raise the toy up
               the tube where it could be reached. The majority did so, however,
               after seeing an adult demonstrate this approach. Critically,
               children copied the specific method of the adult, even when this
               involved the unnecessary steps of pouring from the bottle into a
               cup before pouring from the cup into the tube. These results
               highlight the value of overimitation to children growing up in a
               world filled with objects whose operating mechanisms are often
               hidden or unclear.",
  month     =  jan,
  year      =  2013,
  keywords  = "imitation; innovation; social learning; tool use; culture",
  doi       = "10.1002/icd.1765",
  issn      = "1522-7219,1522-7227",
  language  = "en"
}

@ARTICLE{Ebel2019-bc,
  title     = "How prior experience and task presentation modulate innovation in
               6-year-old-children",
  author    = "Ebel, Sonja J and Hanus, Daniel and Call, Josep",
  journal   = "Journal of experimental child psychology",
  publisher = "Elsevier BV",
  volume    =  180,
  pages     = "87--103",
  abstract  = "Low innovation rates have been found with children until 6-8
               years of age in tasks that required them to make a tool. Little
               is known about how prior experience and task presentation
               influence innovation rates. In the current study, we investigated
               these aspects in the floating peanut task (FPT), which required
               children to pour water into a vertical tube to retrieve a peanut.
               In three experiments, we varied the amount of plants that
               6-year-olds (N = 256) watered prior to the task (zero, one, or
               five plants), who watered the plants (child or experimenter), and
               the distance and salience of the water source. We expected that
               prior experience with the water would modulate task performance
               by either boosting innovation rates (facilitation effect) or
               reducing them given that children would possibly learn that the
               water was for watering plants (functional fixedness effect). Our
               results indicate robustly low innovation rates in 6-year-olds.
               However, children's performance improved to some extent with
               increased salience of the water source as well as with an
               experimenter-given hint. Due to the low innovation rates in this
               age group, we investigated whether watering plants prior to the
               FPT would influence innovation rates in 7- and 8-year-olds (N =
               33), for which we did not find evidence. We conclude that
               6-year-olds struggle with innovation but that they are more
               likely to innovate if crucial aspects of the task are made more
               salient. Thus, although 6-year-olds can innovate, they require
               more physical and social scaffolding than older children and
               adults.",
  month     =  apr,
  year      =  2019,
  keywords  = "Causal understanding; Floating peanut task; Functional fixedness;
               Innovation; Primates; Prior experience",
  doi       = "10.1016/j.jecp.2018.12.004",
  pmid      =  30639770,
  issn      = "0022-0965,1096-0457",
  language  = "en"
}

@ARTICLE{Gonul2018-dl,
  title     = "The cognitive ontogeny of tool making in children: The role of
               inhibition and hierarchical structuring",
  author    = "Gönül, Gökhan and Takmaz, Ece Kamer and Hohenberger, Annette and
               Corballis, Michael",
  journal   = "Journal of Experimental Child Psychology",
  publisher = "Academic Press",
  volume    =  173,
  pages     = "222--238",
  abstract  = "During the last decade, the ontogeny of tool making has received
               growing attention in the literature on tool-related behaviors.
               However, the cognitive demands underlying tool making are still
               not clearly understood. In this cross-sectional study of 52
               Turkish preschool children from 3 to 6 years of age, the roles of
               executive function (response inhibition), ability to form
               hierarchical representations (hierarchical structuring), and
               social learning were investigated with the hook task previously
               used with children and animals. In this task, children needed to
               bend a pipe cleaner to fetch a small bucket with a sticker out of
               a tall jar. This study replicated earlier findings that
               preschoolers have great difficulty in tool innovation. However,
               social learning facilitates tool making, especially after 5 years
               of age. Capacities to form hierarchical representations and to
               inhibit prepotent responses were significant positive predictors
               of tool making after social learning.",
  series    = "Foundations of linear and generalized linear modelsInterference
               and inhibition in cognitionThe evolution of
               intelligenceStatistical power analysis for the behavioral
               sciencesThe symbolic species: The co-evolution of language and
               the brainThe secret of our success: How culture is driving human
               evolution, domesticating our species, and making us smarterAnimal
               creativity and innovationPiaget, evolution, and developmentThe
               five to seven year shift: The age of reason and responsibilityMan
               the tool-makerDeveloping theories of intention: Social
               understanding and self-controlGeneralized linear mixed models:
               Modern concepts, methods, and applications",
  month     =  sep,
  year      =  2018,
  keywords  = "Hierarchical representation; Ontogeny; Response inhibition; Tool
               innovation; Tool making; Tool manipulation",
  doi       = "10.1016/j.jecp.2018.03.017",
  pmid      =  29747073,
  issn      = "0022-0965,1096-0457",
  language  = "en"
}

@ARTICLE{Chappell2013-qi,
  title     = "The development of tool manufacture in humans: what helps young
               children make innovative tools?",
  author    = "Chappell, Jackie and Cutting, Nicola and Apperly, Ian A and Beck,
               Sarah R",
  journal   = "Philosophical transactions of the Royal Society of London. Series
               B, Biological sciences",
  publisher = "The Royal Society",
  volume    =  368,
  number    =  1630,
  pages     =  20120409,
  abstract  = "We know that even young children are proficient tool users, but
               until recently, little was known about how they make tools. Here,
               we will explore the concepts underlying tool making, and the
               kinds of information and putative cognitive abilities required
               for children to manufacture novel tools. We will review the
               evidence for novel tool manufacture from the comparative
               literature and present a growing body of data from children
               suggesting that innovation of the solution to a problem by making
               a tool is a much more challenging task than previously thought.
               Children's difficulty with these kinds of tasks does not seem to
               be explained by perseveration with unmodified tools, difficulty
               with switching to alternative strategies, task pragmatics or
               issues with permission. Rather, making novel tools (without
               having seen an example of the required tool within the context of
               the task) appears to be hard, because it is an example of an
               'ill-structured problem'. In this type of ill-structured problem,
               the starting conditions and end goal are known, but the
               transformations and/or actions required to get from one to the
               other are not specified. We will discuss the implications of
               these findings for understanding the development of
               problem-solving in humans and other animals.",
  month     =  nov,
  year      =  2013,
  keywords  = "comparative cognition; development; physical cognition; tool
               innovation",
  doi       = "10.1098/rstb.2012.0409",
  pmc       = "PMC4027417",
  pmid      =  24101620,
  issn      = "0962-8436,1471-2970",
  language  = "en"
}

@MISC{Unknown2024-ve,
  title     = "{ARC} Prize 2024",
  publisher = "Kaggle",
  year      =  2024
}

@INPROCEEDINGS{Johansen2019-sv,
  title     = "Video Game Description Language Environment for Unity Machine
               Learning Agents",
  author    = "Johansen, Mads and Pichlmair, Martin and Risi, Sebastian",
  booktitle = "2019 IEEE Conference on Games (CoG)",
  publisher = "IEEE",
  pages     = "1--8",
  abstract  = "This paper introduces UnityVGDL, a port of the Video Game
               Description Language (VGDL) to the widely used Unity game engine.
               Our framework is based on the General Video Game AI (GVGAI)
               competition framework and implements its core ontology, including
               a forward model. It integrates the Unity Machine Learning Agents
               (ML-Agents) toolkit with VGDL to train and run agents in
               VGDL-described games. We compare baseline learning results
               between GVGAI and UnityVGDL across four different games and
               conclude that the Unity port is comparable to the GVGAI
               framework. UnityVGDL is available at:
               https://github.com/pyjamads/UnityVGDL",
  month     =  aug,
  year      =  2019,
  doi       = "10.1109/CIG.2019.8848072",
  isbn      = "9781728118840,9781728118833",
  issn      = "2325-4289,2325-4270"
}

@ARTICLE{Webb2018-wu,
  title    = "Once more with feeling: Normative data for the aha experience in
              insight and noninsight problems",
  author   = "Webb, Margaret E and Little, Daniel R and Cropper, Simon J",
  journal  = "Behavior research methods",
  volume   =  50,
  number   =  5,
  pages    = "2035--2056",
  abstract = "Despite the presumed ability of insight problems to elicit the
              subjective feeling of insight, as well as the use of so-called
              insight problems to investigate this phenomenon for over 100
              years, no research has collected normative data regarding the
              ability of insight problems to actually elicit the feeling of
              insight in a given individual. The work described in this article
              provides an overview of both classic and contemporary problems
              used to examine the construct of insight and presents normative
              data on the success rate, mean time to solution, and mean rating
              of aha experience for each problem and task type. We suggest using
              these data in future work as a reference for selecting problems on
              the basis of their ability to elicit an aha experience.",
  month    =  oct,
  year     =  2018,
  keywords = "Aha experience; Creativity; Insight problem solving",
  doi      = "10.3758/s13428-017-0972-9",
  pmc      =  4636512,
  pmid     =  29052169,
  issn     = "1554-351X,1554-3528",
  language = "en"
}

@ARTICLE{Danek2020-rs,
  title    = "Closing the gap: connecting sudden representational change to the
              subjective Aha! experience in insightful problem solving",
  author   = "Danek, Amory H and Williams, Joshua and Wiley, Jennifer",
  journal  = "Psychological research",
  volume   =  84,
  number   =  1,
  pages    = "111--119",
  abstract = "Two hallmarks of insightful problem solving are thought to be
              suddenness in the emergence of solution due to changes in problem
              representation, and the subjective Aha! experience. Although a
              number of studies have explored the Aha! experience, few studies
              have attempted to measure representational change. Following the
              lead of Durso et al. (Psychol Sci 5(2):94-97, 1994) and Cushen and
              Wiley (Conscious Cognit 21(3):1166-1175, 2012), in this study,
              participants made importance-to-solution ratings throughout their
              solution attempts as a way to assess representational change.
              Participants viewed a set of magic trick videos with the task of
              finding out how each trick worked, and rated six action verbs for
              each trick (including one that implied the correct solution)
              multiple times during solution. They were also asked to indicate
              the extent to which they experienced an Aha! moment. Patterns of
              ratings that showed a sudden change towards a correct solution led
              to stronger Aha! experiences than patterns that showed a more
              incremental change towards a correct solution, or a change towards
              incorrect solutions. The results show a connection between sudden
              changes in problem representations (leading to correct solutions)
              and the subjective appraisal of solutions as an Aha! experience.
              This offers the first empirical support for a close relationship
              between two theoretical constructs that have traditionally been
              assumed to be related to insightful problem solving.",
  month    =  feb,
  year     =  2020,
  doi      = "10.1007/s00426-018-0977-8",
  pmid     =  29349507,
  issn     = "0340-0727,1430-2772",
  language = "en"
}

@ARTICLE{Friston2017-fz,
  title     = "Active inference, curiosity and insight",
  author    = "Friston, Karl J and Lin, Marco and Frith, Christopher D and
               Pezzulo, Giovanni and Hobson, J Allan and Ondobaka, Sasha",
  journal   = "Neural computation",
  publisher = "MIT Press - Journals",
  volume    =  29,
  number    =  10,
  pages     = "2633--2683",
  abstract  = "This article offers a formal account of curiosity and insight in
               terms of active (Bayesian) inference. It deals with the dual
               problem of inferring states of the world and learning its
               statistical structure. In contrast to current trends in machine
               learning (e.g., deep learning), we focus on how people attain
               insight and understanding using just a handful of observations,
               which are solicited through curious behavior. We use simulations
               of abstract rule learning and approximate Bayesian inference to
               show that minimizing (expected) variational free energy leads to
               active sampling of novel contingencies. This epistemic behavior
               closes explanatory gaps in generative models of the world,
               thereby reducing uncertainty and satisfying curiosity. We then
               move from epistemic learning to model selection or structure
               learning to show how abductive processes emerge when agents test
               plausible hypotheses about symmetries (i.e., invariances or
               rules) in their generative models. The ensuing Bayesian model
               reduction evinces mechanisms associated with sleep and has all
               the hallmarks of ``aha'' moments. This formulation moves toward a
               computational account of consciousness in the pre-Cartesian sense
               of sharable knowledge (i.e., con: ``together''; scire: ``to
               know'').",
  month     =  oct,
  year      =  2017,
  doi       = "10.1162/neco\_a\_00999",
  pmid      =  28777724,
  issn      = "0899-7667,1530-888X",
  language  = "en"
}

@ARTICLE{Clark2015-gh,
  title     = "Insight and analysis problem solving in microbes to machines",
  author    = "Clark, Kevin B",
  journal   = "Progress in biophysics and molecular biology",
  publisher = "Elsevier BV",
  volume    =  119,
  number    =  2,
  pages     = "183--193",
  abstract  = "A key feature for obtaining solutions to difficult problems,
               insight is oftentimes vaguely regarded as a special discontinuous
               intellectual process and/or a cognitive restructuring of problem
               representation or goal approach. However, this nearly century-old
               state of art devised by the Gestalt tradition to explain the
               non-analytical or non-trial-and-error, goal-seeking aptitude of
               primate mentality tends to neglect problem-solving capabilities
               of lower animal phyla, Kingdoms other than Animalia, and
               advancing smart computational technologies built from biological,
               artificial, and composite media. Attempting to provide an
               inclusive, precise definition of insight, two major criteria of
               insight, discontinuous processing and problem restructuring, are
               here reframed using terminology and statistical mechanical
               properties of computational complexity classes. Discontinuous
               processing becomes abrupt state transitions in
               algorithmic/heuristic outcomes or in types of
               algorithms/heuristics executed by agents using classical and/or
               quantum computational models. And problem restructuring becomes
               combinatorial reorganization of resources, problem-type
               substitution, and/or exchange of computational models. With
               insight bounded by computational complexity, humans, ciliated
               protozoa, and complex technological networks, for example, show
               insight when restructuring time requirements, combinatorial
               complexity, and problem type to solve polynomial and
               nondeterministic polynomial decision problems. Similar effects
               are expected from other problem types, supporting the idea that
               insight might be an epiphenomenon of analytical problem solving
               and consequently a larger information processing framework. Thus,
               this computational complexity definition of insight improves the
               power, external and internal validity, and reliability of
               operational parameters with which to classify, investigate, and
               produce the phenomenon for computational agents ranging from
               microbes to man-made devices.",
  month     =  nov,
  year      =  2015,
  keywords  = "Cognition; Decision making; Emergent computation; Heuristics;
               Information processing; Intelligence; Natural computing",
  doi       = "10.1016/j.pbiomolbio.2015.08.018",
  pmid      =  26278642,
  issn      = "0079-6107,1873-1732",
  language  = "en"
}

@INPROCEEDINGS{Ha2018-gf,
  title     = "Recurrent world models facilitate policy evolution",
  author    = "Ha, David and Schmidhuber, Jürgen",
  booktitle = "Proceedings of the 32nd International Conference on Neural
               Information Processing Systems",
  publisher = "Curran Associates Inc.",
  address   = "Red Hook, NY, USA",
  pages     = "2455--2467",
  abstract  = "A generative recurrent neural network is quickly trained in an
               unsupervised manner to model popular reinforcement learning
               environments through compressed spatio-temporal representations.
               The world model's extracted features are fed into compact and
               simple policies trained by evolution, achieving state of the art
               results in various environments. We also train our agent entirely
               inside of an environment generated by its own internal world
               model, and transfer this policy back into the actual environment.
               Interactive version of paper: https://worldmodels.github.io",
  series    = "NIPS'18",
  month     =  dec,
  year      =  2018
}

@ARTICLE{Rasch1960-pa,
  title     = "Studies in mathematical psychology: {I}. Probabilistic models for
               some intelligence and attainment tests",
  author    = "Rasch, Georg",
  publisher = "Nielsen \& Lydiche Studies in mathematical psychology",
  address   = "Oxford, England",
  volume    =  184,
  abstract  = "This monograph attempts a new approach for testing construction
               in psychology. A probabilistic model is developed for 3 different
               types of tests. Each model implies 2 types of parameters: a
               ``difficulty'' for each test or item and an ``ability'' for each
               person, independent of which set of tests or items has been
               employed. Both parameters are estimated from the data. Chapters
               1-7 present the basic theory with a minimum of mathematics.
               Chapters 8-10 present in detail the mathematics underlying the
               models. From Psyc Abstracts 36:05:5HB84R. (PsycINFO Database
               Record (c) 2016 APA, all rights reserved)",
  year      =  1960
}

@ARTICLE{Philip_Chalmers2012-qh,
  title    = "mirt: A Multidimensional Item Response Theory Package for the {R}
              Environment",
  author   = "Philip Chalmers, R",
  journal  = "Journal of statistical software",
  volume   =  48,
  pages    = "1--29",
  abstract = "Item response theory (IRT) is widely used in assessment and
              evaluation research to explain how participants respond to item
              level stimuli. Several R packages can be used to estimate the
              parameters in various IRT models, the most flexible being the ltm
              (Rizopoulos 2006), eRm (Mair and Hatzinger 2007), and MCMCpack
              (Martin, Quinn, and Park 2011) packages. However these packages
              have limitations in that ltm and eRm can only analyze
              unidimensional IRT models effectively and the exploratory
              multidimensional extensions available in MCMCpack requires prior
              understanding of Bayesian estimation convergence diagnostics and
              are computationally intensive. Most importantly, multidimensional
              confirmatory item factor analysis methods have not been
              implemented in any R package. The mirt package was created for
              estimating multidimensional item response theory parameters for
              exploratory and confirmatory models by using maximum-likelihood
              meth- ods. The Gauss-Hermite quadrature method used in traditional
              EM estimation (e.g., Bock and Aitkin 1981) is presented for
              exploratory item response models as well as for confirmatory
              bifactor models (Gibbons and Hedeker 1992). Exploratory and
              confirmatory models are estimated by a stochastic algorithm
              described by Cai (2010a,b). Various program comparisons are
              presented and future directions for the package are discussed.",
  month    =  may,
  year     =  2012,
  doi      = "10.18637/jss.v048.i06",
  issn     = "1548-7660,1548-7660",
  language = "en"
}

@ARTICLE{Gemini_Team2023-ym,
  title         = "Gemini: A Family of Highly Capable Multimodal Models",
  author        = "{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and
                   Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and
                   Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and
                   Millican, Katie and Silver, David and Johnson, Melvin and
                   Antonoglou, Ioannis and Schrittwieser, Julian and Glaese,
                   Amelia and Chen, Jilin and Pitler, Emily and Lillicrap,
                   Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy,
                   James and Isard, Michael and Barham, Paul R and Hennigan, Tom
                   and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and
                   Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer,
                   Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub,
                   Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and
                   Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and
                   Kane, Patrick and Chan, Betty and Faruqui, Manaal and
                   Severyn, Aliaksei and Lin, Hanzhao and Li, Yaguang and Cheng,
                   Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia
                   and Sun, Pei and Tran, Dustin and Bagri, Sumit and
                   Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras
                   and Güra, Fabian and Zhou, Hao and Song, Xinying and Boffy,
                   Aurelien and Ganapathy, Harish and Zheng, Steven and Choe,
                   Hyunjeong and Weisz, Ágoston and Zhu, Tao and Lu, Yifeng and
                   Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and
                   Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Al
                   Merey, Majd and Baeuml, Martin and Chen, Zhifeng and El
                   Shafey, Laurent and Zhang, Yujing and Sercinoglu, Olcan and
                   Tucker, George and Piqueras, Enrique and Krikun, Maxim and
                   Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and
                   Roelofs, Becca and White, Anaïs and Andreassen, Anders and
                   von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and
                   Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and
                   Frechette, Alexandre and Smith, Charlotte and Culp, Laura and
                   Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and
                   Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and
                   Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao,
                   Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and
                   Bloniarz, Adam and Rae, Jack W and Lu, Han and Sifre, Laurent
                   and Maggioni, Marcello and Alcober, Fred and Garrette, Dan
                   and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and
                   Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and
                   Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar,
                   Gaurav Singh and Senter, Evan and Chadwick, Martin and
                   Kornakov, Ilya and Attaluri, Nithya and Iturrate, Iñaki and
                   Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy
                   and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad,
                   Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai,
                   Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin,
                   Michael and de Las Casas, Diego and Valter, Dasha and Tao,
                   Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and
                   Reitter, David and Chen, Mianna and Brennan, Jenny and
                   Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita,
                   Gabriela and Labanowski, Jane and Rao, Abhi and Winkler,
                   Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska,
                   Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie
                   and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and
                   Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood,
                   Zoe and Briukhov, Anton and Webson, Albert and Ganapathy,
                   Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang,
                   Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun,
                   Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman,
                   Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy
                   and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and
                   Han, Kehang and Humphreys, Peter and Sellam, Thibault and
                   Bradbury, James and Godbole, Varun and Samangooei, Sina and
                   Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M R
                   and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason
                   and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan,
                   Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam,
                   Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush
                   and Le Paine, Tom and Li, Jian and Li, Yujia and Giang, Minh
                   and Neitz, Alexander and Abbas, Zaheer and York, Sarah and
                   Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and
                   Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaliy
                   and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and
                   Prost, Flavien and He, Luheng and Monteiro, Marianne and
                   Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia,
                   Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de
                   Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and
                   Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and
                   Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and
                   Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan,
                   Devendra and Amplayo, Reinald Kim and Swanson, Craig and
                   Petrova, Dessie and Narayan, Shashi and Guez, Arthur and
                   Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and
                   Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia,
                   Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg
                   and Keeling, James and Georgiev, Petko and Mincu, Diana and
                   Wu, Boxi and Haykal, Salem and Saputro, Rachel and
                   Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and
                   Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and
                   Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and
                   Agrawal, Priyanka and Castro-Ros, Alex and van den Driessche,
                   George and Wang, Tao and Yang, Fan and Chang, Shuo-Yiin and
                   Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang,
                   Guodong and Farhan, Wael and Sharman, Michael and Natsev,
                   Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and
                   Cao, Kris and Shakeri, Siamak and Butterfield, Christina and
                   Chung, Justin and Rubenstein, Paul Kishan and Agrawal,
                   Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc,
                   Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren
                   and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez,
                   Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti,
                   Andrea and Trebacz, Maja and Robinson, Kevin and Katariya,
                   Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan
                   and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and
                   Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and
                   Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee,
                   Lisa and {Music Li} and Kagohara, Thais and Pavagadhi, Jay
                   and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay
                   and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and
                   Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and
                   Besley, James and Chung, Da-Woon and Dozat, Timothy and
                   Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su,
                   Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and
                   Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and
                   Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and
                   Tomasev, Nenad and Xing, Jinwei and Greer, Christina and
                   Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang,
                   Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and
                   Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and
                   Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and
                   Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Le
                   Lan, Charline and Haridasan, Krishna and Marathe, Amit and
                   Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and
                   Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang,
                   Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund,
                   Lars Lowe and Cevey, Sébastien and Gleicher, Zach and
                   Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and
                   Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and
                   Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate
                   and Chang, Michael B and Recasens, Adrià and Caine, Ben and
                   Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and
                   Gergely, Anita and Frye, Justin and Ramasesh, Vinay and
                   Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy,
                   Subhrajit and Dyer, Ethan and Campos, Víctor Campos and
                   Tomala, Alex and Tang, Yunhao and El Badawy, Dalia and White,
                   Elspeth and Mustafa, Basil and Lang, Oran and Jindal,
                   Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles,
                   Sergi and Hemsley, Ross and Thornton, Gregory and Feng,
                   Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker,
                   Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh,
                   Mohammad and Svensson, James and Bileschi, Max and Patil,
                   Piyush and Anand, Ankesh and Ring, Roman and Tsihlas,
                   Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby
                   and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira
                   and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and
                   Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and
                   Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James
                   and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi
                   and Ives, Richard and Hasson, Yana and Noland, Eric and Cao,
                   Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and
                   Sottiaux, Thibault and Paganini, Michela and Lespiau,
                   Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and
                   Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane,
                   Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew
                   and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and
                   Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and
                   Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb
                   and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De
                   Cao, Nicola and Chen, Charlie and Mudgal, Sidharth and
                   Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and
                   Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and
                   Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov,
                   Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran,
                   Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and
                   Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and
                   Kępa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi,
                   Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang,
                   Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara
                   and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie
                   and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and
                   Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou,
                   Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea
                   and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and
                   Toor, Tej and Chen, Tina and Anklin, Valentin and Wang,
                   Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin
                   and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and
                   Kishore, Arun and Adamek, Jakub and Mercado, Tyler and
                   Mallinson, Jonathan and Wandekar, Siddhinita and Cagle,
                   Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser,
                   Clemens and Mukha, Maksim and Sun, Botu and Mohammad,
                   Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani,
                   Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and
                   David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii
                   and Älgmyr, Anton and Lottaz, Timothée and Li, Qi and Yadav,
                   Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh
                   and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and
                   Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and
                   Dai, Zihang and He, Kyle and von Dincklage, Daniel and
                   Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and
                   Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G
                   and Pavan, Kumar Reddy M and Selvan, Aarush and Dektiarev,
                   Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta,
                   Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan
                   Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet,
                   François-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric
                   and Teixeira, Elico and Fritze, Matthew and Bertolini,
                   Francesco and Marinescu, Liana-Eleonora and Bölle, Martin and
                   Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and
                   Chang, Max and Sanders, Jason and Wilson, Roopa and Wu,
                   Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi,
                   Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming
                   and Luong, Thang and Benjamin, Seth and Lee, Jasmine and
                   Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and
                   Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and
                   Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei
                   and Bacon, Geoff and Greene, David and Mirylenka, Daniil and
                   Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and
                   Andermatt, Samuel and Siegler, Patrick and Horn, Ben and
                   Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei
                   ``louis'' and Selvatici, Marco and Silva, Pedro and Wang,
                   Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey
                   and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik
                   and Nguyen, Hung and Donnaile, Noah Ó and Pereira, Sébastien
                   and Friso, Linda and Stambler, Adam and Kurzrok, Adam and
                   Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan,
                   Z J and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and
                   Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel
                   and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi,
                   Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja,
                   Niharika and Saxena, Pranab and Dooley, Dan and Potharaju,
                   Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand
                   and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu,
                   Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and
                   Safranek-Shrader, Chalence and Goodman, Andrew and Kessinger,
                   Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski,
                   Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and
                   Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and
                   Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford,
                   Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru
                   and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis
                   and Li, Chenmei and Chen, Shiyuan and Santo, Niccolò Dal and
                   Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik
                   and Kwak, Chester and Pallavi, L V and Velury, Sarmishta and
                   Choudhury, Himadri and Hall, Jamie and Shah, Premal and
                   Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou,
                   Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur,
                   Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and Ähdel,
                   Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei
                   and Barua, Aditya and Ji, Colin and Park, Ji Ho and
                   Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and
                   Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and
                   Rzadkowski, Wojciech and Macintosh, Fiona and Shagin,
                   Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing
                   and Shah, Pararth and Bi, Yingying and Dankovics, Attila and
                   Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and
                   Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and
                   Chung, Raynald and Yang, Kai and Balani, Nihal and
                   Bražinskas, Arthur and Sozanschi, Andrei and Hayes, Matthew
                   and Alcalde, Héctor Fernández and Makarov, Peter and Chen,
                   Will and Stella, Antonio and Snijders, Liselotte and Mandl,
                   Michael and Kärrman, Ante and Nowak, Paweł and Wu, Xinyi and
                   Dyck, Alex and Vaidyanathan, Krishnan and Raghavender, R and
                   Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and
                   Mittal, Sushil and Udathu, Akhil and Christensen, Janara and
                   Verma, Vishal and Irving, Zach and Santucci, Andreas and
                   Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin
                   and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and
                   Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut
                   and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang,
                   Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang,
                   Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy
                   and Rajashekhar, Vinu and Tumeh, Lara and Ben-David, Eyal and
                   Jain, Rishub and Uesato, Jonathan and Datta, Romina and
                   Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk,
                   Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit
                   and Azzam, Michael and Johnson, Matthew and Paszke, Adam and
                   Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin,
                   Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and
                   Vieillard, Nino and Park, Jane and Zhang, Jiageng and
                   Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and
                   Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei
                   and Evens, Jonathan and Isaac, William and Irving, Geoffrey
                   and Loper, Edward and Fink, Michael and Arkatkar, Isha and
                   Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and
                   Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu,
                   Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto
                   and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman
                   and Palmer, Evan and Suganthan, Paul and Castaño, Alfonso and
                   Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and
                   Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David
                   and Goedeckemeyer, Adrian and Gierke, Willi and Jafari,
                   Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana
                   Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya,
                   Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu,
                   Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey
                   and Cui, Albert and Tian, L I N and Wu, Marcus and Aguilar,
                   Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng,
                   Ginger and Abellan, Elena Allica and Zhang, Mingyang and
                   Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and
                   Repina, Alena and Wu, Xihui and van der Weide, Tom and
                   Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and
                   Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper,
                   Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan
                   and Vijayakumar, Anitha and Andor, Daniel and Valenzuela,
                   Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi
                   and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and
                   Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy
                   and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng,
                   Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou
                   and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga
                   and Keller, Orgad and Reid, David and Finchelstein, Daniel
                   and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and
                   Dadashi, Robert and Gaffney, Colin and Franko, Ken and
                   Bulanova, Anna and Leblond, Rémi and Chung, Shirley and
                   Askham, Harry and Cobo, Luis C and Xu, Kelvin and Fischer,
                   Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris
                   and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and
                   Forbes, Hannah and Banarse, Dylan and Tung, Zora and
                   Omernick, Mark and Bishop, Colton and Sterneck, Rachel and
                   Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno,
                   Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz,
                   Daniel J and Polozov, Alex and Krakovna, Victoria and Brown,
                   Sasha and Bateni, Mohammadhossein and Duan, Dennis and
                   Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist,
                   Matthieu and Girgin, Ser Tan and Li, Hui and Ye, Jiayu and
                   Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp,
                   James and Yew, Christopher and Sinopalnikov, Danila and
                   Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu,
                   Kathy and Miller, David and Sonnerat, Nicolas and Vnukov,
                   Denis and Greig, Rory and Beattie, Jennifer and Caveness,
                   Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy,
                   Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and
                   Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan
                   and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and
                   Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and
                   Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind,
                   Chen and Woodman, Oliver and Carpenter, John and
                   Papamakarios, George and Kemp, Rupert and Kafle, Sushant and
                   Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu,
                   Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton,
                   Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li,
                   Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar
                   and Uria, Benigno and Ko, Yeongil and Knight, Laura and
                   Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi
                   and Li, Yeqing and Levine, Nir and Stolovich, Ariel and
                   Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim,
                   Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie
                   and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann,
                   Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Arora,
                   Sho and Koh, Christy and Yeganeh, Soheil Hassas and Põder,
                   Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian
                   and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu
                   and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and
                   Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li,
                   Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and
                   Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and
                   Choquette-Choo, Christopher A and Li, Yunjie and Lu, T J and
                   Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani
                   and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and
                   Desjardins, Guillaume and Cornero, Marco and Robenek, Brona
                   and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish
                   and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah,
                   Alireza and Rivière, Morgane and Walton, Alanna and Crepy,
                   Clément and Parrish, Alicia and Zhou, Zongwei and Farabet,
                   Clement and Radebaugh, Carey and Srinivasan, Praveen and van
                   der Salm, Claudia and Fidjeland, Andreas and Scellato,
                   Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska,
                   Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom
                   and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex
                   and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and
                   Odoom, Seth and Loher, Lucia and Cotruta, Victor and
                   Yenugula, Madhavi and Grewe, Dominik and Petrushkina,
                   Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky,
                   Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and
                   Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan
                   and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and
                   Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and
                   Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei,
                   Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty,
                   Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak,
                   Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and
                   Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma,
                   Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie
                   and Manyika, James and Amiri, Keyvan and Kim, Yelin and
                   Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni,
                   Nilesh and Madras, David and Guo, Mandy and Waters, Austin
                   and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and
                   Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng
                   and Mansour, Riham and Gelman, Jason and Xu, Yang and
                   Polovets, George and Liu, Ji and Cai, Honglong and Chen,
                   Warren and Sheng, Xianghai and Xue, Emily and Ozair, Sherjil
                   and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop
                   and Wang, Weiren and Wiesinger, Julia and Koukoumidis,
                   Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy,
                   Madhu and Goldenson, Mark and Shah, Parashar and Blake, M K
                   and Yu, Hongkun and Urbanowicz, Anthony and Palomaki,
                   Jennimaria and Fernando, Chrisantha and Durden, Ken and
                   Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and
                   Georgaki, Maria and Raul, Amit and Ruder, Sebastian and
                   Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan,
                   Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker
                   and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and
                   Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley,
                   Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua
                   and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao,
                   Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr,
                   Jean Michel and Preston, Melanie Moranski and Elish,
                   Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and
                   Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M.,
                   Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex
                   and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and
                   Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and
                   Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and
                   Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and
                   Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen,
                   Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila
                   and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer,
                   Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias,
                   Aviel and Lee, Paulina and Listík, Vít and Carlen, Mathias
                   and van de Kerkhof, Jan and Pikus, Marcin and Zaher,
                   Krunoslav and Müller, Paul and Zykova, Sasha and Stefanec,
                   Richard and Gatsko, Vitaly and Hirnschall, Christoph and
                   Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and
                   Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania,
                   Keshav and Katyal, Manish and Gupta, Akshay and Parulekar,
                   Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan
                   and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and
                   Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova,
                   Vera and Ghosh, Abhipso and Limonchik, Ben and Urala,
                   Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and
                   Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and
                   Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and
                   Majmundar, Kushal and Alverson, Michael and Kucharski,
                   Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim
                   and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and
                   Kim, Joseph and Sankar, Swetha and Shah, Vineet and
                   Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben
                   and Weidinger, Laura and Vu, Tu and Subramanya, Amar and
                   Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and
                   Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu,
                   Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol",
  journal       = "arXiv [cs.CL]",
  abstract      = "This report introduces a new family of multimodal models,
                   Gemini, that exhibit remarkable capabilities across image,
                   audio, video, and text understanding. The Gemini family
                   consists of Ultra, Pro, and Nano sizes, suitable for
                   applications ranging from complex reasoning tasks to
                   on-device memory-constrained use-cases. Evaluation on a broad
                   range of benchmarks shows that our most-capable Gemini Ultra
                   model advances the state of the art in 30 of 32 of these
                   benchmarks - notably being the first model to achieve
                   human-expert performance on the well-studied exam benchmark
                   MMLU, and improving the state of the art in every one of the
                   20 multimodal benchmarks we examined. We believe that the new
                   capabilities of the Gemini family in cross-modal reasoning
                   and language understanding will enable a wide variety of use
                   cases. We discuss our approach toward post-training and
                   deploying Gemini models responsibly to users through services
                   including Gemini, Gemini Advanced, Google AI Studio, and
                   Cloud Vertex AI.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.11805"
}

@ARTICLE{Sun2022-cr,
  title    = "Seeing and speaking: How verbal ``description length'' encodes
              visual complexity",
  author   = "Sun, Zekun and Firestone, Chaz",
  journal  = "Journal of experimental psychology. General",
  volume   =  151,
  number   =  1,
  pages    = "82--96",
  abstract = "What is the relationship between complexity in the world and
              complexity in the mind? Intuitively, increasingly complex objects
              and events should give rise to increasingly complex mental
              representations (or perhaps a plateau in complexity after a
              certain point). However, a counterintuitive possibility with roots
              in information theory is an inverted-U-shaped relationship between
              the ``objective'' complexity of some stimulus and the complexity
              of its mental representation, because excessively complex patterns
              might be characterized by surprisingly short computational
              descriptions (e.g., if they are represented as having been
              generated randomly). Here, we demonstrate that this is the case,
              using a novel approach that takes the notion of ``description''
              literally. Subjects saw static and dynamic visual stimuli whose
              objective complexity could be carefully manipulated, and they
              described these stimuli in their own words by giving free-form
              spoken descriptions of them. Across three experiments totaling
              over 10,000 speech clips, spoken descriptions of shapes
              (Experiment 1), dot arrays (Experiment 2), and dynamic motion
              paths (Experiment 3) revealed a striking quadratic relationship
              between the raw complexity of these stimuli and the length of
              their spoken descriptions. In other words, the simplest and most
              complex stimuli received the shortest descriptions, while those
              stimuli with a medium degree of complexity received the longest
              descriptions. Follow-up analyses explored the particular words
              used by subjects, allowing us to further explore how such stimuli
              were represented. We suggest that the mind engages in a kind of
              lossy compression for overly complex stimuli, and we discuss the
              utility of such free-form responses for exploring foundational
              questions about mental representation. (PsycInfo Database Record
              (c) 2022 APA, all rights reserved).",
  month    =  jan,
  year     =  2022,
  doi      = "10.1037/xge0001076",
  pmid     =  34498910,
  issn     = "0096-3445",
  language = "en"
}

@ARTICLE{Macchi2012-bx,
  title     = "Intuitive and analytical processes in insight problem solving: a
               psycho-rhetorical approach to the study of reasoning",
  author    = "Macchi, Laura and Bagassi, Maria",
  journal   = "Mind \& society",
  publisher = "Springer Science and Business Media LLC",
  volume    =  11,
  number    =  1,
  pages     = "53--67",
  abstract  = "Language and thought share a unitary cognitive activity,
               addressed by an interpretative function. This interpretative
               effort reveals the assonance between the attribution of meaning
               to an utterance and the discovery of a solution via restructuring
               in insight problem solving. We suggest a view of complex
               integrated analytical thinking, which assumes that thinking
               processes information in different ways, depending on the
               characteristics of the tasks the subject has to solve, so that
               reasoning results in a stepwise, rule-based process or in a
               widespread activity of search where implicit parallel processes
               are also involved. We investigated the interrelationship between
               language and thought in insight problem solving, in both its
               positive (Experiments 1 and 3) and its negative effects
               (Experiment 2). Our results are discussed in the light of the
               debate on dual processing theories.",
  month     =  jun,
  year      =  2012,
  doi       = "10.1007/s11299-012-0103-3",
  issn      = "1593-7879,1860-1839",
  language  = "en"
}

@ARTICLE{Redko2023-ek,
  title    = "Computational modeling of insight processes and artificial
              cognitive ontogeny",
  author   = "Red'ko, Vladimir G and Samsonovich, Alexei V and Klimov, Valentin
              V",
  journal  = "Cognitive systems research",
  volume   =  78,
  pages    = "71--86",
  abstract = "Many approaches were proposed to model complex socially emotional
              behavior in virtual actors, such as intelligent tutors, creative
              assistants, or team partners. They still lack the ‘magic’ of
              human-level cognition: systems built for one paradigm appear
              clueless outside of its boundary. Natural cognitive systems, on
              the other hand, can adapt to unexpected environments and
              paradigms. To capture the robustness of natural cognitive
              development, a new approach is proposed here that enables the
              formation of new higher cognitive abilities in a model system,
              embedded in an unexpected environment. This is achieved based on
              the naturally developing grounding of innate abstract constructs
              (schemas). The mechanism producing this binding is that of
              creative insight. In this study, principles of insight processes
              borrowed from psychology are formalized and adapted for computer
              modeling. To do this, several examples of insight phenomena at
              different evolutionary levels and in different species are
              analyzed before the model is formulated based on the dual process
              theory, the signal model of insight, and the eBICA cognitive
              architecture framework. Results of its computer simulations prove
              the concept. One specific finding is that the accumulation of
              activation during the incubation period increases creative
              abilities of the system. It is argued that the proposed approach
              can explain a range of facts and mysteries associated with the
              human cognitive ontogeny and can provide the basis for a
              self-sustained evolution of future Artificial Intelligence.",
  month    =  mar,
  year     =  2023,
  keywords = "Computational creativity; Insight; Dual process theory; Cognitive
              architectures; Symbol grounding; Socially emotional intelligence",
  doi      = "10.1016/j.cogsys.2022.12.004",
  issn     = "1389-0417"
}

@ARTICLE{Berckley2023-uh,
  title     = "Making learning visible: Observable correlates of the Aha! Moment
               when moving from surface to deep thinking",
  author    = "Berckley, Jay and Hattie, John",
  journal   = "The Journal of creative behavior",
  publisher = "Wiley",
  volume    =  57,
  number    =  3,
  pages     = "439--449",
  abstract  = "ABSTRACTThe purpose of this study is to provide reliable evidence
               of the observable correlates of the Aha! moment and what this
               means for moving from surface to deep thinking. The study
               analyses facial expressions and body language from video
               recordings of students solving problems, followed by interviews,
               focus groups, and surveys. The problem‐solving involved asking
               students to solve word problems to generate an Aha! moment and
               seeing how their expressions changed (or not) when they
               ascertained the correct answer. Findings showed clear facial,
               emotional, and body language changes when the problems were
               solved. Implications for teaching are discussed.",
  month     =  sep,
  year      =  2023,
  doi       = "10.1002/jocb.589",
  issn      = "0022-0175,2162-6057",
  language  = "en"
}

@ARTICLE{Ji2022-ex,
  title         = "Abstract Visual Reasoning with Tangram Shapes",
  author        = "Ji, Anya and Kojima, Noriyuki and Rush, Noah and Suhr, Alane
                   and Vong, Wai Keen and Hawkins, Robert D and Artzi, Yoav",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce KiloGram, a resource for studying abstract
                   visual reasoning in humans and machines. Drawing on the
                   history of tangram puzzles as stimuli in cognitive science,
                   we build a richly annotated dataset that, with >1k distinct
                   stimuli, is orders of magnitude larger and more diverse than
                   prior resources. It is both visually and linguistically
                   richer, moving beyond whole shape descriptions to include
                   segmentation maps and part labels. We use this resource to
                   evaluate the abstract visual reasoning capacities of recent
                   multi-modal models. We observe that pre-trained weights
                   demonstrate limited abstract reasoning, which dramatically
                   improves with fine-tuning. We also observe that explicitly
                   describing parts aids abstract reasoning for both humans and
                   models, especially when jointly encoding the linguistic and
                   visual inputs. KiloGram is available at
                   https://lil.nlp.cornell.edu/kilogram .",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2211.16492"
}

@INCOLLECTION{Langley1988-oy,
  title     = "A computational model of scientific insight",
  author    = "Langley, Pat and Jones, Randolph",
  editor    = "Sternberg, Robert J",
  booktitle = "The nature of creativity: Contemporary psychological perspectives
               (pp",
  publisher = "Cambridge University Press, x",
  address   = "New York, NY, US",
  volume    =  454,
  pages     = "177--201",
  abstract  = "advantages of cognitive simulation problem space hypothesis
               phenomenon of scientific insight / Hadamard's theory / Ohlsson's
               restructuring theory / Simon's theory of familiarization and
               selective forgetting research on reasoning by analogy /
               Dreistadt's analogy-based theory of insight / Gentner's structure
               mapping theory / Winston's theory of analogy / Carbonell's theory
               of derivational analogy alternative theory of insight process
               models and behavioral descriptions task of theory formation
               indexing, spreading activation, and retrieval elaboration through
               derivation predictions of the theory (PsycINFO Database Record
               (c) 2016 APA, all rights reserved)",
  year      =  1988
}

@ARTICLE{Tuckute2024-ho,
  title     = "Language in brains, minds, and machines",
  author    = "Tuckute, Greta and Kanwisher, Nancy and Fedorenko, Evelina",
  journal   = "Annual review of neuroscience",
  publisher = "Annual Reviews",
  abstract  = "It has long been argued that only humans could produce and
               understand language. But now, for the first time, artificial
               language models (LMs) achieve this feat. Here we survey the new
               purchase LMs are providing on the question of how language is
               implemented in the brain. We discuss why, a priori, LMs might be
               expected to share similarities with the human language system. We
               then summarize evidence that LMs represent linguistic information
               similarly enough to humans to enable relatively accurate brain
               encoding and decoding during language processing. Finally, we
               examine which LM properties-their architecture, task performance,
               or training-are critical for capturing human neural responses to
               language and review studies using LMs as in silico model
               organisms for testing hypotheses about language. These ongoing
               investigations bring us closer to understanding the
               representations and processes that underlie our ability to
               comprehend sentences and express thoughts in language.",
  month     =  apr,
  year      =  2024,
  doi       = "10.1146/annurev-neuro-120623-101142",
  pmid      =  38669478,
  issn      = "0147-006X,1545-4126",
  language  = "en"
}

@ARTICLE{Lupyan2012-tj,
  title    = "Linguistically modulated perception and cognition: the
              label-feedback hypothesis",
  author   = "Lupyan, Gary",
  journal  = "Frontiers in psychology",
  volume   =  3,
  pages    =  54,
  abstract = "How does language impact cognition and perception? A growing
              number of studies show that language, and specifically the
              practice of labeling, can exert extremely rapid and pervasive
              effects on putatively non-verbal processes such as categorization,
              visual discrimination, and even simply detecting the presence of a
              stimulus. Progress on the empirical front, however, has not been
              accompanied by progress in understanding the mechanisms by which
              language affects these processes. One puzzle is how effects of
              language can be both deep, in the sense of affecting even basic
              visual processes, and yet vulnerable to manipulations such as
              verbal interference, which can sometimes nullify effects of
              language. In this paper, I review some of the evidence for effects
              of language on cognition and perception, showing that performance
              on tasks that have been presumed to be non-verbal is rapidly
              modulated by language. I argue that a clearer understanding of the
              relationship between language and cognition can be achieved by
              rejecting the distinction between verbal and non-verbal
              representations and by adopting a framework in which language
              modulates ongoing cognitive and perceptual processing in a
              flexible and task-dependent manner.",
  month    =  mar,
  year     =  2012,
  keywords = "Whorf; categorization; labels; language and thought; linguistic
              relativity; perception; top-down effects",
  doi      = "10.3389/fpsyg.2012.00054",
  pmc      = "PMC3297074",
  pmid     =  22408629,
  issn     = "1664-1078",
  language = "en"
}

@ARTICLE{LeCun_undated-ui,
  title    = "Convolutional networks for images, speech, and time series",
  author   = "LeCun, Y and Bengio, Y",
  journal  = "Citeseer",
  abstract = "The ability of multilayer back-propagation networks to learn
              complex, high-dimensional, nonlinear mappings from large
              collections of examples makes them obvious candidates for …"
}

@ARTICLE{Bellemare2012-cr,
  title         = "The Arcade Learning Environment: An Evaluation Platform for
                   General Agents",
  author        = "Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and
                   Bowling, Michael",
  journal       = "arXiv [cs.AI]",
  abstract      = "In this article we introduce the Arcade Learning Environment
                   (ALE): both a challenge problem and a platform and
                   methodology for evaluating the development of general,
                   domain-independent AI technology. ALE provides an interface
                   to hundreds of Atari 2600 game environments, each one
                   different, interesting, and designed to be a challenge for
                   human players. ALE presents significant research challenges
                   for reinforcement learning, model learning, model-based
                   planning, imitation learning, transfer learning, and
                   intrinsic motivation. Most importantly, it provides a
                   rigorous testbed for evaluating and comparing approaches to
                   these problems. We illustrate the promise of ALE by
                   developing and benchmarking domain-independent agents
                   designed using well-established AI techniques for both
                   reinforcement learning and planning. In doing so, we also
                   propose an evaluation methodology made possible by ALE,
                   reporting empirical results on over 55 different games. All
                   of the software, including the benchmark agents, is publicly
                   available.",
  month         =  jul,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1207.4708"
}

@ARTICLE{Schulman2015-at,
  title         = "High-Dimensional Continuous Control Using Generalized
                   Advantage Estimation",
  author        = "Schulman, John and Moritz, Philipp and Levine, Sergey and
                   Jordan, Michael and Abbeel, Pieter",
  journal       = "arXiv [cs.LG]",
  abstract      = "Policy gradient methods are an appealing approach in
                   reinforcement learning because they directly optimize the
                   cumulative reward and can straightforwardly be used with
                   nonlinear function approximators such as neural networks. The
                   two main challenges are the large number of samples typically
                   required, and the difficulty of obtaining stable and steady
                   improvement despite the nonstationarity of the incoming data.
                   We address the first challenge by using value functions to
                   substantially reduce the variance of policy gradient
                   estimates at the cost of some bias, with an
                   exponentially-weighted estimator of the advantage function
                   that is analogous to TD(lambda). We address the second
                   challenge by using trust region optimization procedure for
                   both the policy and the value function, which are represented
                   by neural networks. Our approach yields strong empirical
                   results on highly challenging 3D locomotion tasks, learning
                   running gaits for bipedal and quadrupedal simulated robots,
                   and learning a policy for getting the biped to stand up from
                   starting out lying on the ground. In contrast to a body of
                   prior work that uses hand-crafted policy representations, our
                   neural network policies map directly from raw kinematics to
                   joint torques. Our algorithm is fully model-free, and the
                   amount of simulated experience required for the learning
                   tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1506.02438"
}

@ARTICLE{Hornik1989-dl,
  title    = "Multilayer feedforward networks are universal approximators",
  author   = "Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert",
  journal  = "Neural networks: the official journal of the International Neural
              Network Society",
  volume   =  2,
  number   =  5,
  pages    = "359--366",
  abstract = "This paper rigorously establishes that standard multilayer
              feedforward networks with as few as one hidden layer using
              arbitrary squashing functions are capable of approximating any
              Borel measurable function from one finite dimensional space to
              another to any desired degree of accuracy, provided sufficiently
              many hidden units are available. In this sense, multilayer
              feedforward networks are a class of universal approximators.",
  month    =  jan,
  year     =  1989,
  keywords = "Feedforward networks; Universal approximation; Mapping networks;
              Network representation capability; Stone-Weierstrass Theorem;
              Squashing functions; Sigma-Pi networks; Back-propagation networks",
  doi      = "10.1016/0893-6080(89)90020-8",
  issn     = "0893-6080"
}

@ARTICLE{Silvia2019-hu,
  title    = "Curiosity and Motivation",
  author   = "Silvia, Paul J",
  abstract = "Abstract. Curiosity is an old, intriguing, and vexing construct in
              the psychology of motivation. This chapter reviews the major
              strands of thought on curio",
  month    =  aug,
  year     =  2019,
  doi      = "10.1093/oxfordhb/9780190666453.013.9"
}

@ARTICLE{Vieillard2019-je,
  title         = "Deep Conservative Policy Iteration",
  author        = "Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu",
  journal       = "arXiv [cs.LG]",
  abstract      = "Conservative Policy Iteration (CPI) is a founding algorithm
                   of Approximate Dynamic Programming (ADP). Its core principle
                   is to stabilize greediness through stochastic mixtures of
                   consecutive policies. It comes with strong theoretical
                   guarantees, and inspired approaches in deep Reinforcement
                   Learning (RL). However, CPI itself has rarely been
                   implemented, never with neural networks, and only
                   experimented on toy problems. In this paper, we show how CPI
                   can be practically combined with deep RL with discrete
                   actions. We also introduce adaptive mixture rates inspired by
                   the theory. We experiment thoroughly the resulting algorithm
                   on the simple Cartpole problem, and validate the proposed
                   method on a representative subset of Atari games. Overall,
                   this work suggests that revisiting classic ADP may lead to
                   improved and more stable deep RL algorithms.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1906.09784"
}

@ARTICLE{Juliani2018-ov,
  title         = "Unity: A General Platform for Intelligent Agents",
  author        = "Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin
                   and Cohen, Andrew and Harper, Jonathan and Elion, Chris and
                   Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan
                   and Lange, Danny",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent advances in artificial intelligence have been driven
                   by the presence of increasingly realistic and complex
                   simulated environments. However, many of the existing
                   environments provide either unrealistic visuals, inaccurate
                   physics, low task complexity, restricted agent perspective,
                   or a limited capacity for interaction among artificial
                   agents. Furthermore, many platforms lack the ability to
                   flexibly configure the simulation, making the simulated
                   environment a black-box from the perspective of the learning
                   system. In this work, we propose a novel taxonomy of existing
                   simulation platforms and discuss the highest level class of
                   general platforms which enable the development of learning
                   environments that are rich in visual, physical, task, and
                   social complexity. We argue that modern game engines are
                   uniquely suited to act as general platforms and as a case
                   study examine the Unity engine and open source Unity
                   ML-Agents Toolkit. We then survey the research enabled by
                   Unity and the Unity ML-Agents Toolkit, discussing the kinds
                   of research a flexible, interactive and easily configurable
                   general platform can facilitate.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1809.02627"
}

@ARTICLE{Butt2024-ck,
  title         = "{CodeIt}: Self-Improving Language Models with Prioritized
                   Hindsight Replay",
  author        = "Butt, Natasha and Manczak, Blazej and Wiggers, Auke and
                   Rainone, Corrado and Zhang, David and Defferrard, Michaël and
                   Cohen, Taco",
  journal       = "arXiv [cs.AI]",
  abstract      = "Large language models are increasingly solving tasks that are
                   commonly believed to require human-level reasoning ability.
                   However, these models still perform very poorly on benchmarks
                   of general intelligence such as the Abstraction and Reasoning
                   Corpus (ARC). In this paper, we approach ARC as a
                   programming-by-examples problem, and introduce a novel and
                   scalable method for language model self-improvement called
                   Code Iteration (CodeIt). Our method iterates between 1)
                   program sampling and hindsight relabeling, and 2) learning
                   from prioritized experience replay. By relabeling the goal of
                   an episode (i.e., the target program output given input) to
                   the realized output produced by the sampled program, our
                   method effectively deals with the extreme sparsity of rewards
                   in program synthesis. Applying CodeIt to the ARC dataset, we
                   demonstrate that prioritized hindsight replay, along with
                   pre-training and data-augmentation, leads to successful
                   inter-task generalization. CodeIt is the first neuro-symbolic
                   approach that scales to the full ARC evaluation dataset. Our
                   method solves 15\% of ARC evaluation tasks, achieving
                   state-of-the-art performance and outperforming existing
                   neural and symbolic baselines.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2402.04858"
}

@ARTICLE{Ha2018-gn,
  title         = "Recurrent World Models Facilitate Policy Evolution",
  author        = "Ha, David and Schmidhuber, Jürgen",
  journal       = "arXiv [cs.LG]",
  abstract      = "A generative recurrent neural network is quickly trained in
                   an unsupervised manner to model popular reinforcement
                   learning environments through compressed spatio-temporal
                   representations. The world model's extracted features are fed
                   into compact and simple policies trained by evolution,
                   achieving state of the art results in various environments.
                   We also train our agent entirely inside of an environment
                   generated by its own internal world model, and transfer this
                   policy back into the actual environment. Interactive version
                   of paper at https://worldmodels.github.io",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1809.01999"
}

@ARTICLE{Hall-McMaster2024-zs,
  title    = "Neural Prioritisation of Past Solutions Supports Generalisation",
  author   = "Hall-McMaster, Sam and Tomov, Momchil S and Gershman, Samuel J and
              Schuck, Nicolas W",
  journal  = "bioRxiv",
  pages    = "2024.06.10.598294",
  abstract = "Generalisation from past experience is an important feature of
              intelligent systems. When faced with a new task, efficient
              generalisation can be achieved by evaluating solutions to earlier
              tasks as candidates for reuse. Consistent with this idea, we found
              that human participants (n=40) learned optimal solutions to a set
              of training tasks and continued to reuse them on novel test tasks.
              Corresponding functional magnetic resonance imaging data showed
              that optimal solutions from the training tasks were represented on
              test tasks in occipitotemporal and dorsolateral prefrontal cortex.
              These findings suggest that humans evaluate and generalise
              successful past solutions when attempting to solve new tasks.
              \#\#\# Competing Interest Statement The authors have declared no
              competing interest.",
  month    =  jun,
  year     =  2024,
  doi      = "10.1101/2024.06.10.598294",
  language = "en"
}

@ARTICLE{Gendron2023-gk,
  title         = "Large Language Models Are Not Strong Abstract Reasoners",
  author        = "Gendron, Gaël and Bao, Qiming and Witbrock, Michael and
                   Dobbie, Gillian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models have shown tremendous performance on a
                   large variety of natural language processing tasks, ranging
                   from text comprehension to common sense reasoning. However,
                   the mechanisms responsible for this success remain opaque,
                   and it is unclear whether LLMs can achieve human-like
                   cognitive capabilities or whether these models are still
                   fundamentally circumscribed. Abstract reasoning is a
                   fundamental task for cognition, consisting of finding and
                   applying a general pattern from few data. Evaluating deep
                   neural architectures on this task could give insight into
                   their potential limitations regarding reasoning and their
                   broad generalisation abilities, yet this is currently an
                   under-explored area. In this paper, we introduce a new
                   benchmark for evaluating language models beyond memorization
                   on abstract reasoning tasks. We perform extensive evaluations
                   of state-of-the-art LLMs, showing that they currently achieve
                   very limited performance in contrast with other natural
                   language tasks, even when applying techniques that have been
                   shown to improve performance on other NLP tasks. We argue
                   that guiding LLM generation to follow causal paths could help
                   improve the generalisation and reasoning abilities of LLMs.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.19555"
}

@ARTICLE{Lupyan2013-ed,
  title    = "The difficulties of executing simple algorithms: why brains make
              mistakes computers don't",
  author   = "Lupyan, Gary",
  journal  = "Cognition",
  volume   =  129,
  number   =  3,
  pages    = "615--636",
  abstract = "It is shown that educated adults routinely make errors in placing
              stimuli into familiar, well-defined categories such as triangle
              and odd number. Scalene triangles are often rejected as instances
              of triangles and 798 is categorized by some as an odd number.
              These patterns are observed both in timed and untimed tasks, hold
              for people who can fully express the necessary and sufficient
              conditions for category membership, and for individuals with
              varying levels of education. A sizeable minority of people believe
              that 400 is more even than 798 and that an equilateral triangle is
              the most ``trianglest'' of triangles. Such beliefs predict how
              people instantiate other categories with necessary and sufficient
              conditions, e.g., grandmother. I argue that the distributed and
              graded nature of mental representations means that human
              algorithms, unlike conventional computer algorithms, only
              approximate rule-based classification and never fully abstract
              from the specifics of the input. This input-sensitivity is
              critical to obtaining the kind of cognitive flexibility at which
              humans excel, but comes at the cost of generally poor abilities to
              perform context-free computations. If human algorithms cannot be
              trusted to produce unfuzzy representations of odd numbers,
              triangles, and grandmothers, the idea that they can be trusted to
              do the heavy lifting of moment-to-moment cognition that is
              inherent in the metaphor of mind as digital computer still common
              in cognitive science, needs to be seriously reconsidered.",
  month    =  dec,
  year     =  2013,
  keywords = "Categorization; Concepts; Distributed representations; Human
              algorithms; Inference; Prototypes",
  doi      = "10.1016/j.cognition.2013.08.015",
  pmid     =  24156803,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@ARTICLE{Wei2022-th,
  title         = "Emergent Abilities of Large Language Models",
  author        = "Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin
                   and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani
                   and Bosma, Maarten and Zhou, Denny and Metzler, Donald and
                   Chi, Ed H and Hashimoto, Tatsunori and Vinyals, Oriol and
                   Liang, Percy and Dean, Jeff and Fedus, William",
  journal       = "arXiv [cs.CL]",
  abstract      = "Scaling up language models has been shown to predictably
                   improve performance and sample efficiency on a wide range of
                   downstream tasks. This paper instead discusses an
                   unpredictable phenomenon that we refer to as emergent
                   abilities of large language models. We consider an ability to
                   be emergent if it is not present in smaller models but is
                   present in larger models. Thus, emergent abilities cannot be
                   predicted simply by extrapolating the performance of smaller
                   models. The existence of such emergence implies that
                   additional scaling could further expand the range of
                   capabilities of language models.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2206.07682"
}

@ARTICLE{Bhagavatula2019-yk,
  title         = "Abductive Commonsense Reasoning",
  author        = "Bhagavatula, Chandra and Le Bras, Ronan and Malaviya,
                   Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and
                   Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-Tau and
                   Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Abductive reasoning is inference to the most plausible
                   explanation. For example, if Jenny finds her house in a mess
                   when she returns from work, and remembers that she left a
                   window open, she can hypothesize that a thief broke into her
                   house and caused the mess, as the most plausible explanation.
                   While abduction has long been considered to be at the core of
                   how people interpret and read between the lines in natural
                   language (Hobbs et al., 1988), there has been relatively
                   little research in support of abductive natural language
                   inference and generation. We present the first study that
                   investigates the viability of language-based abductive
                   reasoning. We introduce a challenge dataset, ART, that
                   consists of over 20k commonsense narrative contexts and 200k
                   explanations. Based on this dataset, we conceptualize two new
                   tasks -- (i) Abductive NLI: a multiple-choice question
                   answering task for choosing the more likely explanation, and
                   (ii) Abductive NLG: a conditional generation task for
                   explaining given observations in natural language. On
                   Abductive NLI, the best model achieves 68.9\% accuracy, well
                   below human performance of 91.4\%. On Abductive NLG, the
                   current best language generators struggle even more, as they
                   lack reasoning capabilities that are trivial for humans. Our
                   analysis leads to new insights into the types of reasoning
                   that deep pre-trained language models fail to
                   perform--despite their strong performance on the related but
                   more narrowly defined task of entailment NLI--pointing to
                   interesting avenues for future research.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.05739"
}

@ARTICLE{Wang2019-zd,
  title         = "Does It Make Sense? And Why? A Pilot Study for Sense Making
                   and Explanation",
  author        = "Wang, Cunxiang and Liang, Shuailong and Zhang, Yue and Li,
                   Xiaonan and Gao, Tian",
  journal       = "arXiv [cs.AI]",
  abstract      = "Introducing common sense to natural language understanding
                   systems has received increasing research attention. It
                   remains a fundamental question on how to evaluate whether a
                   system has a sense making capability. Existing benchmarks
                   measures commonsense knowledge indirectly and without
                   explanation. In this paper, we release a benchmark to
                   directly test whether a system can differentiate natural
                   language statements that make sense from those that do not
                   make sense. In addition, a system is asked to identify the
                   most crucial reason why a statement does not make sense. We
                   evaluate models trained over large-scale language modeling
                   tasks as well as human performance, showing that there are
                   different challenges for system sense making.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1906.00363"
}

@ARTICLE{Frankfurt1958-wz,
  title     = "Peirce's Notion of Abduction",
  author    = "Frankfurt, Harry G",
  journal   = "The journal of philosophy",
  publisher = "Journal of Philosophy, Inc.",
  volume    =  55,
  number    =  14,
  pages     = "593--597",
  year      =  1958,
  doi       = "10.2307/2021966",
  issn      = "0022-362X"
}

@ARTICLE{Nye2019-yp,
  title         = "Learning to infer program sketches",
  author        = "Nye, Maxwell and Hewitt, Luke and Tenenbaum, Joshua and
                   Solar-Lezama, Armando",
  journal       = "arXiv [cs.AI]",
  abstract      = "Our goal is to build systems which write code automatically
                   from the kinds of specifications humans can most easily
                   provide, such as examples and natural language instruction.
                   The key idea of this work is that a flexible combination of
                   pattern recognition and explicit reasoning can be used to
                   solve these complex programming problems. We propose a method
                   for dynamically integrating these types of information. Our
                   novel intermediate representation and training algorithm
                   allow a program synthesis system to learn, without direct
                   supervision, when to rely on pattern recognition and when to
                   perform symbolic search. Our model matches the memorization
                   and generalization performance of neural synthesis and
                   symbolic search, respectively, and achieves state-of-the-art
                   performance on a dataset of simple English
                   description-to-code programming problems.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1902.06349"
}

@ARTICLE{Goldenberg2009-is,
  title     = "The neural basis of tool use",
  author    = "Goldenberg, G and Spatt, J",
  journal   = "Brain: a journal of neurology",
  publisher = "Oxford University Press (OUP)",
  volume    =  132,
  number    = "Pt 6",
  pages     = "1645--1655",
  abstract  = "Misuse of tools and objects by patients with left brain damage is
               generally recognized as a manifestation of apraxia, caused by
               parietal lobe damage. The use of tools and objects can, however,
               be subdivided in several components. The purpose of our study was
               to find out which of these are dependent on parietal lobe
               function. Thirty-eight patients with left brain damage and
               aphasia were examined using tests to assess the retrieval of
               functional knowledge from semantic memory (Functional
               Associations), mechanical problem solving (Novel Tools) and use
               of everyday tools and objects (Common Tools). Voxel-wise analysis
               of magnetic resonance images revealed two regions where lesions
               had a significant impact on the test results. One extended
               rostrally from the central region and ventrally through the
               middle frontal cortex to the dorsal margin of the inferior
               frontal gyrus. The other reached dorsally and caudally from the
               supramarginal gyrus, through the inferior, to superior parietal
               lobe. Whereas the frontal lesions had an adverse influence on all
               experimental tests as well as on the subtests of the Aachen
               Aphasia test, parietal lesions impaired Novel and Common Tools,
               but did not have an adverse effect on the Functional Associates.
               An association between Functional Associations and temporal
               lesions became apparent when patients with only a selective
               deficit in the test were considered, but did not show up in the
               whole group analysis. The parietal influence was as strong for
               the selection as for the use of either novel or common tools,
               although choice of appropriate manual configuration and movements
               was more important for use than for selection. We conclude that
               the contribution of the parietal lobe to tool use concerns
               general principles of tool use rather than knowledge about the
               prototypical use of common tools and objects, and the
               comprehension of mechanical interactions of the tool with other
               tools, recipients or material rather than the selection of grip
               formation and manual movements.",
  month     =  jun,
  year      =  2009,
  doi       = "10.1093/brain/awp080",
  pmid      =  19351777,
  issn      = "0006-8950,1460-2156",
  language  = "en"
}

@ARTICLE{Reynaud2016-fb,
  title     = "On the neurocognitive origins of human tool use : A critical
               review of neuroimaging data",
  author    = "Reynaud, Emanuelle and Lesourd, Mathieu and Navarro, Jordan and
               Osiurak, François",
  journal   = "Neuroscience and biobehavioral reviews",
  publisher = "Elsevier BV",
  volume    =  64,
  pages     = "421--437",
  abstract  = "Since more than a century, neuropsychological models have assumed
               that the left inferior parietal cortex is central to tool use by
               storing manipulation knowledge (the manipulation-based approach).
               Interestingly, recent neuropsychological evidence indicates that
               the left inferior parietal cortex might rather support the
               ability to reason about physical object properties (the
               reasoning-based approach). Historically, these two approaches
               have been developed from data obtained in left brain-damaged
               patients. This review is the first one to (1) give an overview of
               the two aforementioned approaches and (2) reanalyze functional
               neuroimaging data of the past decade to examine their
               predictions. Globally, we demonstrate that the left inferior
               parietal cortex is involved in the understanding of tool-use
               actions, providing support for the reasoning-based approach. We
               also discuss the functional involvement of the different regions
               of the tool-use brain network (left supramarginal gyrus, left
               intraparietal sulcus, left posterior temporal cortex). Our
               findings open promising avenues for future research on the
               neurocognitive basis of human tool use.",
  month     =  may,
  year      =  2016,
  keywords  = "Inferior parietal cortex; Manipulation knowledge; Mechanical
               knowledge; Meta-analysis; NeuroImaging; Tool use",
  doi       = "10.1016/j.neubiorev.2016.03.009",
  pmid      =  26976352,
  issn      = "0149-7634,1873-7528",
  language  = "en"
}

@ARTICLE{Gallivan2013-nr,
  title     = "Decoding the neural mechanisms of human tool use",
  author    = "Gallivan, Jason P and McLean, D Adam and Valyear, Kenneth F and
               Culham, Jody C",
  journal   = "eLife",
  publisher = "eLife Sciences Publications, Ltd",
  volume    =  2,
  pages     = "e00425",
  abstract  = "Sophisticated tool use is a defining characteristic of the
               primate species but how is it supported by the brain,
               particularly the human brain? Here we show, using functional MRI
               and pattern classification methods, that tool use is subserved by
               multiple distributed action-centred neural representations that
               are both shared with and distinct from those of the hand. In
               areas of frontoparietal cortex we found a common representation
               for planned hand- and tool-related actions. In contrast, in
               parietal and occipitotemporal regions implicated in hand actions
               and body perception we found that coding remained selectively
               linked to upcoming actions of the hand whereas in parietal and
               occipitotemporal regions implicated in tool-related processing
               the coding remained selectively linked to upcoming actions of the
               tool. The highly specialized and hierarchical nature of this
               coding suggests that hand- and tool-related actions are
               represented separately at earlier levels of sensorimotor
               processing before becoming integrated in frontoparietal cortex.
               DOI:http://dx.doi.org/10.7554/eLife.00425.001.",
  month     =  may,
  year      =  2013,
  keywords  = "Human; action; fMRI; intentions; motor; perception; tool use",
  doi       = "10.7554/eLife.00425",
  pmc       = "PMC3667577",
  pmid      =  23741616,
  issn      = "2050-084X",
  language  = "en"
}

@ARTICLE{LeGris2024-od,
  title         = "{H}-{ARC}: A robust estimate of human performance on the
                   Abstraction and Reasoning Corpus benchmark",
  author        = "LeGris, Solim and Vong, Wai Keen and Lake, Brenden M and
                   Gureckis, Todd M",
  journal       = "arXiv [cs.AI]",
  abstract      = "The Abstraction and Reasoning Corpus (ARC) is a visual
                   program synthesis benchmark designed to test challenging
                   out-of-distribution generalization in humans and machines.
                   Since 2019, limited progress has been observed on the
                   challenge using existing artificial intelligence methods.
                   Comparing human and machine performance is important for the
                   validity of the benchmark. While previous work explored how
                   well humans can solve tasks from the ARC benchmark, they
                   either did so using only a subset of tasks from the original
                   dataset, or from variants of ARC, and therefore only provided
                   a tentative estimate of human performance. In this work, we
                   obtain a more robust estimate of human performance by
                   evaluating 1729 humans on the full set of 400 training and
                   400 evaluation tasks from the original ARC problem set. We
                   estimate that average human performance lies between 73.3\%
                   and 77.2\% correct with a reported empirical average of
                   76.2\% on the training set, and between 55.9\% and 68.9\%
                   correct with a reported empirical average of 64.2\% on the
                   public evaluation set. However, we also find that 790 out of
                   the 800 tasks were solvable by at least one person in three
                   attempts, suggesting that the vast majority of the publicly
                   available ARC tasks are in principle solvable by typical
                   crowd-workers recruited over the internet. Notably, while
                   these numbers are slightly lower than earlier estimates,
                   human performance still greatly exceeds current
                   state-of-the-art approaches for solving ARC. To facilitate
                   research on ARC, we publicly release our dataset, called
                   H-ARC (human-ARC), which includes all of the submissions and
                   action traces from human participants.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2409.01374"
}

@ARTICLE{Zhou2024-jf,
  title    = "Compositional diversity in visual concept learning",
  author   = "Zhou, Yanli and Feinman, Reuben and Lake, Brenden M",
  journal  = "Cognition",
  volume   =  244,
  pages    =  105711,
  abstract = "Humans leverage compositionality to efficiently learn new
              concepts, understanding how familiar parts can combine together to
              form novel objects. In contrast, popular computer vision models
              struggle to make the same types of inferences, requiring more data
              and generalizing less flexibly than people do. Here, we study
              these distinctively human abilities across a range of different
              types of visual composition, examining how people classify and
              generate ``alien figures'' with rich relational structure. We also
              develop a Bayesian program induction model which searches for the
              best programs for generating the candidate visual figures,
              utilizing a large program space containing different compositional
              mechanisms and abstractions. In few shot classification tasks, we
              find that people and the program induction model can make a range
              of meaningful compositional generalizations, with the model
              providing a strong account of the experimental data as well as
              interpretable parameters that reveal human assumptions about the
              factors invariant to category membership (here, to rotation and
              changing part attachment). In few shot generation tasks, both
              people and the models are able to construct compelling novel
              examples, with people behaving in additional structured ways
              beyond the model capabilities, e.g. making choices that complete a
              set or reconfigure existing parts in new ways. To capture these
              additional behavioral patterns, we develop an alternative model
              based on neuro-symbolic program induction: this model also
              composes new concepts from existing parts yet, distinctively, it
              utilizes neural network modules to capture residual statistical
              structure. Together, our behavioral and computational findings
              show how people and models can produce a variety of compositional
              behavior when classifying and generating visual objects.",
  month    =  mar,
  year     =  2024,
  keywords = "Bayesian inference; Compositionality; Concept learning; Few-shot
              learning; Neuro-symbolic models; Visual learning",
  doi      = "10.1016/j.cognition.2023.105711",
  pmid     =  38224649,
  issn     = "0010-0277,1873-7838",
  language = "en"
}

@INPROCEEDINGS{Ellis2021-fx,
  title     = "{DreamCoder}: bootstrapping inductive program synthesis with
               wake-sleep library learning",
  author    = "Ellis, Kevin and Wong, Catherine and Nye, Maxwell and
               Sablé-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and
               Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B",
  booktitle = "Proceedings of the 42nd ACM SIGPLAN International Conference on
               Programming Language Design and Implementation",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jun,
  year      =  2021,
  doi       = "10.1145/3453483.3454080",
  isbn      =  9781450383912,
  language  = "en"
}

@ARTICLE{Michel2024-vq,
  title     = "When visual metacognition fails: widespread anosognosia for
               visual deficits",
  author    = "Michel, Matthias and Gao, Yi and Mazor, Matan and Kletenik,
               Isaiah and Rahnev, Dobromir",
  journal   = "Trends in cognitive sciences",
  publisher = "Elsevier BV",
  abstract  = "Anosognosia for visual deficits - cases where significant visual
               deficits go unnoticed - challenges the view that our own
               conscious experiences are what we know best. We review these
               widespread and striking failures of awareness. Anosognosia can
               occur with total blindness, visual abnormalities induced by brain
               lesions, and eye diseases. We show that anosognosia for visual
               deficits is surprisingly widespread. Building on previous
               accounts, we introduce a framework showing how apparently
               disparate forms of anosognosia fit together. The central idea is
               that, to notice a deficit, individuals need to form expectations
               about normal vision, compare expectations and visual input, and
               judge any mismatch at the metacognitive level. A failure in any
               of these three steps may lead to unawareness of visual deficits.",
  month     =  sep,
  year      =  2024,
  keywords  = "anosognosia; brain lesions; perceptual decision-making; visual
               metacognition",
  doi       = "10.1016/j.tics.2024.09.003",
  pmid      =  39353838,
  issn      = "1364-6613,1879-307X",
  language  = "en"
}

@ARTICLE{Rule2018-oy,
  title       = "Learning list concepts through program induction",
  author      = "Rule, Joshua and Schulz, Eric and Piantadosi, Steven T and
                 Tenenbaum, Joshua B",
  institution = "bioRxiv",
  abstract    = "AbstractHumans master complex systems of interrelated concepts
                 like mathematics and natural language. Previous work suggests
                 learning these systems relies on iteratively and directly
                 revising a language-like conceptual representation. We
                 introduce and assess a novel concept learning paradigm called
                 Martha’s Magical Machines that captures complex relationships
                 between concepts. We model human concept learning in this
                 paradigm as a search in the space of term rewriting systems,
                 previously developed as an abstract model of computation. Our
                 model accurately predicts that participants learn some
                 transformations more easily than others and that they learn
                 harder concepts more easily using a bootstrapping curriculum
                 focused on their compositional parts. Our results suggest that
                 term rewriting systems may be a useful model of human
                 conceptual representations.",
  month       =  may,
  year        =  2018,
  doi         = "10.1101/321505"
}

@ARTICLE{Chollet2019-di,
  title         = "On the {measure} of {intelligence}",
  author        = "Chollet, François",
  journal       = "arXiv [cs.AI]",
  number        = "arXiv:1911.01547",
  abstract      = "To make deliberate progress towards more intelligent and more
                   human-like artificial systems, we need to be following an
                   appropriate feedback signal: we need to be able to define and
                   evaluate intelligence in a way that enables comparisons
                   between two systems, as well as comparisons with humans. Over
                   the past hundred years, there has been an abundance of
                   attempts to define and measure intelligence, across both the
                   fields of psychology and AI. We summarize and critically
                   assess these definitions and evaluation approaches, while
                   making apparent the two historical conceptions of
                   intelligence that have implicitly guided them. We note that
                   in practice, the contemporary AI community still gravitates
                   towards benchmarking intelligence by comparing the skill
                   exhibited by AIs and humans at specific tasks such as board
                   games and video games. We argue that solely measuring skill
                   at any given task falls short of measuring intelligence,
                   because skill is heavily modulated by prior knowledge and
                   experience: unlimited priors or unlimited training data allow
                   experimenters to ``buy'' arbitrary levels of skills for a
                   system, in a way that masks the system's own generalization
                   power. We then articulate a new formal definition of
                   intelligence based on Algorithmic Information Theory,
                   describing intelligence as skill-acquisition efficiency and
                   highlighting the concepts of scope, generalization
                   difficulty, priors, and experience. Using this definition, we
                   propose a set of guidelines for what a general AI benchmark
                   should look like. Finally, we present a benchmark closely
                   following these guidelines, the Abstraction and Reasoning
                   Corpus (ARC), built upon an explicit set of priors designed
                   to be as close as possible to innate human priors. We argue
                   that ARC can be used to measure a human-like form of general
                   fluid intelligence and that it enables fair general
                   intelligence comparisons between AI systems and humans.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1911.01547",
  keywords      = "Computer Science - Artificial Intelligence"
}

@ARTICLE{Johnson2021-mw,
  title         = "Fast and flexible: {Human} program induction in abstract
                   reasoning tasks",
  author        = "Johnson, Aysja and Vong, Wai Keen and Lake, Brenden M and
                   Gureckis, Todd M",
  journal       = "arXiv [cs.HC]",
  abstract      = "The Abstraction and Reasoning Corpus (ARC) is a challenging
                   program induction dataset that was recently proposed by
                   Chollet (2019). Here, we report the first set of results
                   collected from a behavioral study of humans solving a subset
                   of tasks from ARC (40 out of 1000). Although this subset of
                   tasks contains considerable variation, our results showed
                   that humans were able to infer the underlying program and
                   generate the correct test output for a novel test input
                   example, with an average of 80\% of tasks solved per
                   participant, and with 65\% of tasks being solved by more than
                   80\% of participants. Additionally, we find interesting
                   patterns of behavioral consistency and variability within the
                   action sequences during the generation process, the natural
                   language descriptions to describe the transformations for
                   each task, and the errors people made. Our findings suggest
                   that people can quickly and reliably determine the relevant
                   features and properties of a task to compose a correct
                   solution. Future modeling work could incorporate these
                   findings, potentially by connecting the natural language
                   descriptions we collected here to the underlying semantics of
                   ARC.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2103.05823",
  keywords      = "Computer Science - Artificial Intelligence,Computer Science -
                   Human-Computer Interaction,Computer Science - Machine
                   Learning"
}

@ARTICLE{Lake2019-dt,
  title         = "Compositional generalization through meta
                   sequence-to-sequence learning",
  author        = "Lake, Brenden M",
  journal       = "arXiv [cs.CL]",
  abstract      = "People can learn a new concept and use it compositionally,
                   understanding how to ``blicket twice'' after learning how to
                   ``blicket.'' In contrast, powerful sequence-to-sequence
                   (seq2seq) neural networks fail such tests of
                   compositionality, especially when composing new concepts
                   together with existing concepts. In this paper, I show how
                   memory-augmented neural networks can be trained to generalize
                   compositionally through meta seq2seq learning. In this
                   approach, models train on a series of seq2seq problems to
                   acquire the compositional skills needed to solve new seq2seq
                   problems. Meta se2seq learning solves several of the SCAN
                   tests for compositional learning and can learn to apply
                   implicit rules to variables.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.05381",
  keywords      = "Computer Science - Artificial Intelligence,Computer Science -
                   Computation and Language,Computer Science - Machine Learning"
}

@INPROCEEDINGS{Lake2012-qw,
  title     = "Concept learning as motor program induction: {A} large-scale
               empirical study",
  author    = "Lake, Brenden and Salakhutdinov, Ruslan and Tenenbaum, Joshua",
  booktitle = "Proceedings of the Annual Meeting of the Cognitive Science
               Society",
  volume    =  34,
  pages     =  6,
  abstract  = "Human concept learning is particularly impressive in two
               respects: the internal structure of concepts can be
               representationally rich, and yet the very same concepts can also
               be learned from just a few examples. Several decades of research
               have dramatically advanced our understanding of these two aspects
               of concepts. While the richness and speed of concept learning are
               most often studied in isolation, the power of human concepts may
               be best explained through their synthesis. This paper presents a
               large-scale empirical study of one-shot concept learning,
               suggesting that rich generative knowledge in the form of a motor
               program can be induced from just a single example of a novel
               concept. Participants were asked to draw novel handwritten
               characters given a reference form, and we recorded the motor data
               used for production. Multiple drawers of the same character not
               only produced visually similar drawings, but they also showed a
               striking correspondence in their strokes, as measured by their
               number, shape, order, and direction. This suggests that
               participants can infer a rich motorbased concept from a single
               example. We also show that the motor programs induced by
               individual subjects provide a powerful basis for one-shot
               classification, yielding far higher accuracy than
               state-of-the-art pattern recognition methods based on just the
               visual form.",
  year      =  2012
}

@ARTICLE{Allen2020-tf,
  title    = "Rapid trial-and-error learning with simulation supports flexible
              tool use and physical reasoning",
  author   = "Allen, Kelsey R and Smith, Kevin A and Tenenbaum, Joshua B",
  journal  = "Proceedings of the National Academy of Sciences of the United
              States of America",
  volume   =  117,
  number   =  47,
  pages    = "29302--29310",
  abstract = "Many animals, and an increasing number of artificial agents,
              display sophisticated capabilities to perceive and manipulate
              objects. But human beings remain distinctive in their capacity for
              flexible, creative tool use-using objects in new ways to act on
              the world, achieve a goal, or solve a problem. To study this type
              of general physical problem solving, we introduce the Virtual
              Tools game. In this game, people solve a large range of
              challenging physical puzzles in just a handful of attempts. We
              propose that the flexibility of human physical problem solving
              rests on an ability to imagine the effects of hypothesized
              actions, while the efficiency of human search arises from rich
              action priors which are updated via observations of the world. We
              instantiate these components in the ``sample, simulate, update''
              (SSUP) model and show that it captures human performance across 30
              levels of the Virtual Tools game. More broadly, this model
              provides a mechanism for explaining how people condense general
              physical knowledge into actionable, task-specific plans to achieve
              flexible and efficient physical problem solving.",
  month    =  nov,
  year     =  2020,
  keywords = "intuitive physics; physical problem solving; tool use",
  doi      = "10.1073/pnas.1912341117",
  pmc      = "PMC7703630",
  pmid     =  33229515,
  issn     = "0027-8424,1091-6490",
  language = "en"
}

@MISC{Andreas2017-kq,
  title         = "Learning with {{Latent Language}}",
  author        = "Andreas, Jacob and Klein, Dan and Levine, Sergey",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:1711.00482",
  abstract      = "The named concepts and compositional operators present in
                   natural language provide a rich source of information about
                   the kinds of abstractions humans use to navigate the world.
                   Can this linguistic background knowledge improve the
                   generality and efficiency of learned classifiers and control
                   policies? This paper aims to show that using the space of
                   natural language strings as a parameter space is an effective
                   way to capture natural task structure. In a pretraining
                   phase, we learn a language interpretation model that
                   transforms inputs (e.g. images) into outputs (e.g. labels)
                   given natural language descriptions. To learn a new concept
                   (e.g. a classifier), we search directly in the space of
                   descriptions to minimize the interpreter's loss on training
                   examples. Crucially, our models do not require language data
                   to learn these concepts: language is used only in pretraining
                   to impose structure on subsequent learning. Results on image
                   classification, text editing, and reinforcement learning show
                   that, in all settings, models with a linguistic
                   parameterization outperform those without.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs",
  eprint        = "1711.00482",
  keywords      = "Computer Science - Computation and Language,Computer Science
                   - Neural and Evolutionary Computing"
}

@UNPUBLISHED{Andrychowicz2016-uq,
  title    = "Learning to Learn by Gradient Descent by Gradient Descent",
  author   = "Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and
              Hoffman, Matthew W and Pfau, David and Schaul, Tom and
              Shillingford, Brendan and de Freitas, Nando",
  abstract = "The move from hand-designed features to learned features in
              machine learning has been wildly successful. In spite of this,
              optimization algorithms are still designed by hand. In this paper
              we show how the design of an optimization algorithm can be cast as
              a learning problem, allowing the algorithm to learn to exploit
              structure in the problems of interest in an automatic way. Our
              learned algorithms, implemented by LSTMs, outperform generic,
              hand-designed competitors on the tasks for which they are trained,
              and also generalize well to new tasks with similar structure. We
              demonstrate this on a number of tasks, including simple convex
              problems, training neural networks, and styling images with neural
              art.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Machine Learning,Computer Science - Neural and
              Evolutionary Computing"
}

@INCOLLECTION{Ashby2011-au,
  title     = "{{COVIS}}",
  author    = "Ashby, F Gregory and Paul, Erick J and Maddox, W Todd",
  editor    = "Pothos, Emmanuel M and Wills, Andy J",
  booktitle = "Formal {{Approaches}} in {{Categorization}}",
  publisher = "Cambridge University Press",
  address   = "Cambridge",
  pages     = "65--87",
  abstract  = "The COVIS model of category learning assumes separate rule-based
               and procedural-learning categorization systems that compete for
               access to response production. The rule-based system selects and
               tests simple verbalizable hypotheses about category membership.
               The procedurallearning system gradually associates categorization
               responses with regions of perceptual space via reinforcement
               learning.",
  year      =  2011,
  doi       = "10.1017/CBO9780511921322.004",
  isbn      =  9780511921322
}

@ARTICLE{Bengio2013-nf,
  title    = "Representation {{Learning}}: {{A Review}} and {{New Perspectives}}",
  author   = "Bengio, Y and Courville, A and Vincent, P",
  journal  = "IEEE transactions on pattern analysis and machine intelligence",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical connections
              between representation learning, density estimation, and manifold
              learning.",
  month    =  aug,
  year     =  2013,
  keywords = "Abstracts,autoencoder,Boltzmann machine,Deep learning,Feature
              extraction,feature learning,Learning systems,Machine
              learning,Manifolds,neural nets,Neural networks,representation
              learning,Speech recognition,unsupervised learning",
  doi      = "10.1109/TPAMI.2013.50",
  issn     = "0162-8828,2160-9292"
}

@ARTICLE{Bonner2018-ii,
  title    = "Computational Mechanisms Underlying Cortical Responses to the
              Affordance Properties of Visual Scenes",
  author   = "Bonner, Michael F and Epstein, Russell A",
  editor   = "Einhäuser, Wolfgang",
  journal  = "PLoS computational biology",
  volume   =  14,
  number   =  4,
  pages    = "e1006111",
  abstract = "Biologically inspired deep convolutional neural networks (CNNs),
              trained for computer vision tasks, have been found to predict
              cortical responses with remarkable accuracy. However, the internal
              operations of these models remain poorly understood, and the
              factors that account for their success are unknown. Here we
              develop a set of techniques for using CNNs to gain insights into
              the computational mechanisms underlying cortical responses. We
              focused on responses in the occipital place area (OPA), a
              scene-selective region of dorsal occipitoparietal cortex. In a
              previous study, we showed that fMRI activation patterns in the OPA
              contain information about the navigational affordances of scenes;
              that is, information about where one can and cannot move within
              the immediate environment. We hypothesized that this affordance
              information could be extracted using a set of purely feedforward
              computations. To test this idea, we examined a deep CNN with a
              feedforward architecture that had been previously trained for
              scene classification. We found that responses in the CNN to scene
              images were highly predictive of fMRI responses in the OPA.
              Moreover the CNN accounted for the portion of OPA variance
              relating to the navigational affordances of scenes. The CNN could
              thus serve as an image-computable candidate model of
              affordance-related responses in the OPA. We then ran a series of
              in silico experiments on this model to gain insights into its
              internal operations. These analyses showed that the computation of
              affordance-related features relied heavily on visual information
              at high-spatial frequencies and cardinal orientations, both of
              which have previously been identified as lowlevel stimulus
              preferences of scene-selective visual cortex. These computations
              also exhibited a strong preference for information in the lower
              visual field, which is consistent with known retinotopic biases in
              the OPA. Visualizations of feature selectivity within the CNN
              suggested that affordance-based responses encoded features that
              define the layout of the spatial environment, such as
              boundary-defining junctions and large extended surfaces. Together,
              these results map the sensory functions of the OPA onto a fully
              quantitative model that provides insights into its visual
              computations. More broadly, they advance integrative techniques
              for understanding visual cortex across multiple level of analysis:
              from the identification of cortical sensory functions to the
              modeling of their underlying algorithms.",
  month    =  apr,
  year     =  2018,
  doi      = "10.1371/journal.pcbi.1006111",
  issn     = "1553-734X,1553-7358"
}

@ARTICLE{Bonner2021-zd,
  title    = "Object Representations in the Human Brain Reflect the
              Co-Occurrence Statistics of Vision and Language",
  author   = "Bonner, Michael F and Epstein, Russell A",
  journal  = "Nature communications",
  volume   =  12,
  number   =  1,
  pages    =  4081,
  abstract = "Abstract A central regularity of visual perception is the
              co-occurrence of objects in the natural environment. Here we use
              machine learning and fMRI to test the hypothesis that object
              co-occurrence statistics are encoded in the human visual system
              and elicited by the perception of individual objects. We
              identified low-dimensional representations that capture the latent
              statistical structure of object co-occurrence in real-world
              scenes, and we mapped these statistical representations onto
              voxel-wise fMRI responses during object viewing. We found that
              cortical responses to single objects were predicted by the
              statistical ensembles in which they typically occur, and that this
              link between objects and their visual contexts was made most
              strongly in parahippocampal cortex, overlapping with the anterior
              portion of scene-selective parahippocampal place area. In
              contrast, a language-based statistical model of the co-occurrence
              of object names in written text predicted responses in neighboring
              regions of object-selective visual cortex. Together, these
              findings show that the sensory coding of objects in the human
              brain reflects the latent statistics of object context in visual
              and linguistic experience.",
  month    =  dec,
  year     =  2021,
  doi      = "10.1038/s41467-021-24368-2",
  issn     = "2041-1723"
}

@ARTICLE{Botvinick2017-xr,
  title    = "Building Machines That Learn and Think for Themselves",
  author   = "Botvinick, Matthew and Barrett, David G T and Battaglia, Peter and
              de Freitas, Nando and Kumaran, Darshan and Leibo, Joel Z and
              Lillicrap, Timothy and Modayil, Joseph and Mohamed, Shakir and
              Rabinowitz, Neil C and Rezende, Danilo J and Santoro, Adam and
              Schaul, Tom and Summerfield, Christopher and Wayne, Greg and
              Weber, Theophane and Wierstra, Daan and Legg, Shane and Hassabis,
              Demis",
  journal  = "The Behavioral and brain sciences",
  volume   =  40,
  pages    = "e255",
  abstract = "We agree with Lake and colleagues on their list of ‘key
              ingredients’ for building humanlike intelligence, including the
              idea that model-based reasoning is essential. However, we favor an
              approach that centers on one additional ingredient: autonomy. In
              particular, we aim toward agents that can both build and exploit
              their own internal models, with minimal human hand-engineering. We
              believe an approach centered on autonomous learning has the
              greatest chance of success as we scale toward real-world
              complexity, tackling domains for which ready-made formal models
              are not available. Here we survey several important examples of
              the progress that has been made toward building autonomous agents
              with humanlike abilities, and highlight some outstanding
              challenges.",
  year     =  2017,
  doi      = "10.1017/S0140525X17000048",
  issn     = "0140-525X,1469-1825"
}

@ARTICLE{Bourlard1988-if,
  title    = "Auto-Association by Multilayer Perceptrons and Singular Value
              Decomposition",
  author   = "Bourlard, H and Kamp, Y",
  journal  = "Biological cybernetics",
  volume   =  59,
  number   = "4-5",
  pages    = "291--294",
  abstract = "The multilayer perceptron, when working in auto-association mode,
              is sometimes considered as an interesting candidate to perform
              data compression or dimensionality reduction of the feature space
              in information processing applications. The present paper shows
              that, for auto-association, the nonlinearities of the hidden units
              are useless and that the optimal parameter values can be derived
              directly by purely linear techniques relying on singular value
              decomposition and low rank matrix approximation, similar in spirit
              to the well-known Karhunen-Lo6ve transform. This approach appears
              thus as an efficient alternative to the general error
              back-propagation algorithm commonly used for training multilayer
              perceptrons. Moreover, it also gives a clear interpretation of the
              r61e of the different parameters.",
  month    =  sep,
  year     =  1988,
  doi      = "10.1007/BF00332918",
  issn     = "0340-1200,1432-0770"
}

@ARTICLE{Carey2004-ya,
  title   = "Bootstrapping \& the Origin of Concepts",
  author  = "Carey, Susan",
  journal = "Daedalus",
  volume  =  133,
  number  =  1,
  pages   = "59--68",
  month   =  jan,
  year    =  2004,
  doi     = "10.1162/001152604772746701",
  issn    = "0011-5266,1548-6192"
}

@ARTICLE{Carey1978-gm,
  title  = "Acquiring a {{Single New Word}}",
  author = "Carey, Susan and Bartlett, Elsa",
  volume =  15,
  pages  =  14,
  year   =  1978
}

@ARTICLE{Cassimatis_undated-lu,
  title    = "Artificial {{Intelligence}} and {{Cognitive Science Have}} the
              {{Same Problem}}",
  author   = "Cassimatis, Nicholas L",
  pages    =  6,
  abstract = "Cognitive scientists attempting to explain human intelligence
              share a puzzle with artificial intelligence researchers aiming to
              create computers that exhibit humanlevel intelligence: how can a
              system composed of relatively unintelligent parts (such as neurons
              or transistors) behave intelligently? I argue that although
              cognitive science has made significant progress towards many of
              its goals, that solving the puzzle of intelligence requires
              special standards and methods in addition to those already
              employed in cognitive science. To promote such research, I suggest
              creating a subfield within cognitive science called intelligence
              science and propose some guidelines for research addressing the
              intelligence puzzle."
}

@ARTICLE{Chomsky1956-aj,
  title    = "Three Models for the Description of Language",
  author   = "Chomsky, N",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "113--124",
  abstract = "We investigate several conceptions of linguistic structure to
              determine whether or not they can provide simple and ``revealing''
              grammars that generate all of the sentences of English and only
              these. We find that no finite-state Markov process that produces
              symbols with transition from state to state can serve as an
              English grammar. Furthermore, the particular subclass of such
              processes that producen-order statistical approximations to
              English do not come closer, with increasingn, to matching the
              output of an English grammar. We formalize-the notions of ``phrase
              structure'' and show that this gives us a method for describing
              language which is essentially more powerful, though still
              representable as a rather elementary type of finite-state process.
              Nevertheless, it is successful only when limited to a small subset
              of simple sentences. We study the formal properties of a set of
              grammatical transformations that carry sentences with phrase
              structure into new sentences with derived phrase structure,
              showing that transformational grammars are processes of the same
              elementary type as phrase-structure grammars; that the grammar of
              English is materially simplified if phrase structure description
              is limited to a kernel of simple sentences from which all other
              sentences are constructed by repeated transformations; and that
              this view of linguistic structure gives a certain insight into the
              use and understanding of language.",
  month    =  sep,
  year     =  1956,
  keywords = "Impedance matching,Kernel,Laboratories,Markov processes,Natural
              languages,Research and development,Testing",
  doi      = "10.1109/TIT.1956.1056813",
  issn     = "2168-2712"
}

@ARTICLE{Choromanska_undated-tg,
  title  = "The {{Loss Surfaces}} of {{Multilayer Networks}}",
  author = "Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous,
            Gerard Ben and LeCun, Yann",
  pages  =  13
}

@UNPUBLISHED{Darwiche2017-sr,
  title    = "Human-{{Level Intelligence}} or {{Animal-Like Abilities}}?",
  author   = "Darwiche, Adnan",
  abstract = "The vision systems of the eagle and the snake outperform
              everything that we can make in the laboratory, but snakes and
              eagles cannot build an eyeglass or a telescope or a microscope.
              (Judea Pearl)",
  month    =  jul,
  year     =  2017,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computers and Society,Computer Science - Machine
              Learning,Statistics - Machine Learning"
}

@MISC{De_Raedt2020-hk,
  title         = "From {{Statistical Relational}} to {{Neuro-Symbolic
                   Artificial Intelligence}}",
  author        = "De Raedt, Luc and Dumančić, Sebastijan and Manhaeve, Robin
                   and Marra, Giuseppe",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2003.08316",
  abstract      = "Neuro-symbolic and statistical relational artificial
                   intelligence both integrate frameworks for learning with
                   logical reasoning. This survey identifies several parallels
                   across seven different dimensions between these two fields.
                   These cannot only be used to characterize and position
                   neuro-symbolic artificial intelligence approaches but also to
                   identify a number of directions for further research.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs",
  eprint        = "2003.08316",
  keywords      = "Computer Science - Artificial Intelligence",
  doi           = "10.48550/arXiv.2003.08316"
}

@ARTICLE{Dekker2022-lm,
  title     = "Determinants of Human Compositional Generalization",
  author    = "Dekker, Ronald Boris and Otto, Fabian and Summerfield,
               Christopher",
  publisher = "PsyArXiv",
  abstract  = "Generalisation (or transfer) is the ability to repurpose
               knowledge in novel settings. It is often asserted that
               generalisation is an important ingredient of human intelligence,
               but its extent, nature and determinants have proved
               controversial. Here, we re-examine this question with a new
               paradigm that formalises the transfer learning problem as one of
               recomposing existing functions to solve unseen problems. We find
               that people can generalise compositionally in ways that are
               elusive for standard neural networks, and that human
               generalisation benefits from training regimes in which items are
               axis-aligned and temporally correlated. We describe a neural
               network model based around a Hebbian gating process which can
               capture how human generalisation benefits from different training
               curricula. We additionally find that adult humans tend to learn
               composable functions asynchronously, exhibiting discontinuities
               in learning that resemble those seen in child development.",
  month     =  mar,
  year      =  2022,
  keywords  = "Cognitive Psychology,Computational Neuroscience,Concepts and
               Categories,generalisation,Learning,neural
               network,Neuroscience,Social and Behavioral Sciences",
  doi       = "10.31234/osf.io/qnpw6"
}

@ARTICLE{Dwivedi2021-zk,
  title     = "Unveiling Functions of the Visual Cortex Using Task-Specific Deep
               Neural Networks",
  author    = "Dwivedi, Kshitij and Bonner, Michael F and Cichy, Radoslaw Martin
               and Roig, Gemma",
  journal   = "PLoS computational biology",
  publisher = "Public Library of Science",
  volume    =  17,
  number    =  8,
  pages     = "e1009267",
  abstract  = "The human visual cortex enables visual perception through a
               cascade of hierarchical computations in cortical regions with
               distinct functionalities. Here, we introduce an AI-driven
               approach to discover the functional mapping of the visual cortex.
               We related human brain responses to scene images measured with
               functional MRI (fMRI) systematically to a diverse set of deep
               neural networks (DNNs) optimized to perform different scene
               perception tasks. We found a structured mapping between DNN tasks
               and brain regions along the ventral and dorsal visual streams.
               Low-level visual tasks mapped onto early brain regions,
               3-dimensional scene perception tasks mapped onto the dorsal
               stream, and semantic tasks mapped onto the ventral stream. This
               mapping was of high fidelity, with more than 60\% of the
               explainable variance in nine key regions being explained.
               Together, our results provide a novel functional mapping of the
               human visual cortex and demonstrate the power of the
               computational approach.",
  month     =  aug,
  year      =  2021,
  keywords  = "Functional magnetic resonance imaging,Linear regression
               analysis,Neural networks,Permutation,Semantics,Sensory
               perception,Vision,Visual cortex",
  doi       = "10.1371/journal.pcbi.1009267",
  issn      = "1553-734X,1553-7358"
}

@MISC{Ellis2020-xn,
  title         = "{{DreamCoder}}: {{Growing}} Generalizable, Interpretable
                   Knowledge with Wake-Sleep {{Bayesian}} Program Learning",
  author        = "Ellis, Kevin and Wong, Catherine and Nye, Maxwell and
                   Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and
                   Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua
                   B",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2006.08381",
  abstract      = "Expert problem-solving is driven by powerful languages for
                   thinking about problems and their solutions. Acquiring
                   expertise means learning these languages -- systems of
                   concepts, alongside the skills to use them. We present
                   DreamCoder, a system that learns to solve problems by writing
                   programs. It builds expertise by creating programming
                   languages for expressing domain concepts, together with
                   neural networks to guide the search for programs within these
                   languages. A ``wake-sleep'' learning algorithm alternately
                   extends the language with new symbolic abstractions and
                   trains the neural network on imagined and replayed problems.
                   DreamCoder solves both classic inductive programming tasks
                   and creative tasks such as drawing pictures and building
                   scenes. It rediscovers the basics of modern functional
                   programming, vector algebra and classical physics, including
                   Newton's and Coulomb's laws. Concepts are built
                   compositionally from those learned earlier, yielding
                   multi-layered symbolic representations that are interpretable
                   and transferrable to new tasks, while still growing scalably
                   and flexibly with experience.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs",
  eprint        = "2006.08381",
  keywords      = "Computer Science - Artificial Intelligence,Computer Science -
                   Machine Learning"
}

@BOOK{Epstein2009-te,
  title     = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  publisher = "Springer Netherlands",
  address   = "Dordrecht",
  year      =  2009,
  doi       = "10.1007/978-1-4020-6710-5",
  isbn      = "9781402096242,9781402067105"
}

@ARTICLE{Erickson2011-on,
  title   = "Exercise Training Increases Size of Hippocampus and Improves Memory",
  author  = "Erickson, K I and Voss, M W and Prakash, R S and Basak, C and
             Szabo, A and Chaddock, L and Kim, J S and Heo, S and Alves, H and
             White, S M and Wojcicki, T R and Mailey, E and Vieira, V J and
             Martin, S A and Pence, B D and Woods, J A and McAuley, E and
             Kramer, A F",
  journal = "Proceedings of the National Academy of Sciences",
  volume  =  108,
  number  =  7,
  pages   = "3017--3022",
  month   =  feb,
  year    =  2011,
  doi     = "10.1073/pnas.1015950108",
  issn    = "0027-8424,1091-6490"
}

@UNPUBLISHED{Feinman2021-mh,
  title    = "Learning {{Task-General Representations}} with {{Generative
              Neuro-Symbolic Modeling}}",
  author   = "Feinman, Reuben and Lake, Brenden M",
  abstract = "People can learn rich, general-purpose conceptual representations
              from only raw perceptual inputs. Current machine learning
              approaches fall well short of these human standards, although
              different modeling traditions often have complementary strengths.
              Symbolic models can capture the compositional and causal knowledge
              that enables flexible generalization, but they struggle to learn
              from raw inputs, relying on strong abstractions and simplifying
              assumptions. Neural network models can learn directly from raw
              data, but they struggle to capture compositional and causal
              structure and typically must retrain to tackle new tasks. We bring
              together these two traditions to learn generative models of
              concepts that capture rich compositional and causal structure,
              while learning from raw data. We develop a generative
              neuro-symbolic (GNS) model of handwritten character concepts that
              uses the control flow of a probabilistic program, coupled with
              symbolic stroke primitives and a symbolic image renderer, to
              represent the causal and compositional processes by which
              characters are formed. The distributions of parts (strokes), and
              correlations between parts, are modeled with neural network
              subroutines, allowing the model to learn directly from raw data
              and express nonparametric statistical relationships. We apply our
              model to the Omniglot challenge of human-level concept learning,
              using a background set of alphabets to learn an expressive prior
              distribution over character drawings. In a subsequent evaluation,
              our GNS model uses probabilistic inference to learn rich
              conceptual representations from a single training image that
              generalize to 4 unique tasks, succeeding where previous work has
              fallen short.",
  month    =  jan,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning"
}

@ARTICLE{Fodor1988-gq,
  title    = "Connectionism and Cognitive Architecture: {{A}} Critical Analysis",
  author   = "Fodor, Jerry A and Pylyshyn, Zenon W",
  journal  = "Cognition",
  volume   =  28,
  number   =  1,
  pages    = "3--71",
  abstract = "This paper explores differences between Connectionist proposals
              for cognitive architecture and the sorts of models that have
              traditionally been assumed in cognitive science. We claim that the
              major distinction is that, while both Connectionist and Classical
              architectures postulate representational mental states, the latter
              but not the former are committed to a symbol-level of
              representation, or to a ‘language of thought’: i.e., to
              representational states that have combinatorial syntactic and
              semantic structure. Several arguments for combinatorial structure
              in mental representations are then reviewed. These include
              arguments based on the ‘systematicity’ of mental representation:
              i.e., on the fact that cognitive capacities always exhibit certain
              symmetries, so that the ability to entertain a given thought
              implies the ability to entertain thoughts with semantically
              related contents. We claim that such arguments make a powerful
              case that mind/brain architecture is not Connectionist at the
              cognitive level. We then consider the possibility that
              Connectionism may provide an account of the neural (or ‘abstract
              neurological’) structures in which Classical cognitive
              architecture is implemented. We survey a number of the standard
              arguments that have been offered in favor of Connectionism, and
              conclude that they are coherent only on this interpretation.
              Résumé Cet articleétudie les différences entre modèles
              connectionistes et modèles classiques de la structure cognitive.
              Nous pensons que, bien que les deux types de modèles stipulent
              l'existence d'états mentaux représentationnels, la différence
              essentielle est que seuls les modèles classiques requièrent
              l'existence d'un niveau de représentation symbolique—un “langage
              de la pensée”—, c'est-à-dire d'états représentationnels possédant
              une structure syntaxique et sémantique. Nous examinons ensuite
              différents arguments qui militent en faveur de l'existence de
              représentations mentales ayant ces propriétés. Certains de ces
              arguments reposent sur la “systématicité” des représentations
              mentales, c'est-à-dire sur le fait que les capacités cognitives
              exhibent toujours certaines symétries, de sorte que la
              capacitéd'entretenir certaines pensées implique la
              capacitéd'entretenir d'autres pensées apparentées par leur contenu
              sémantique. Nous pensons que ces arguments montrent de manière
              convainquante que l'architecture de l'esprit/du cerveau n'est pas
              connectioniste au niveau cognitif. Nous nous demandons ensuite
              s'il est possible d'interpréter le connectionisme comme une
              analyse des structures neuronales (ou des structures neurologiques
              “abstraites”) dans lesquelles est réalisée l'architecture
              cognitive classique. Nous examinons plusieurs des arguments
              avancés habituellement en défense du connectionisme, et en
              concluons que ceux-ci n'ont de sens que dans cette interprétation.",
  year     =  1988,
  doi      = "10.1016/0010-0277(88)90031-5",
  issn     = "0010-0277"
}

@ARTICLE{Fontana2010-ie,
  title   = "Extending {{Healthy Life Span--From Yeast}} to {{Humans}}",
  author  = "Fontana, L and Partridge, L and Longo, V D",
  journal = "Science",
  volume  =  328,
  number  =  5976,
  pages   = "321--326",
  month   =  apr,
  year    =  2010,
  doi     = "10.1126/science.1172539",
  issn    = "0036-8075,1095-9203"
}

@INPROCEEDINGS{Gal2016-il,
  title     = "Dropout as a {{Bayesian Approximation}}: {{Representing Model
               Uncertainty}} in {{Deep Learning}}",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
  booktitle = "Proceedings of The 33rd International Conference on Machine
               Learning",
  publisher = "PMLR",
  address   = "New York, New York, USA",
  volume    =  48,
  pages     = "1050--1059",
  abstract  = "Deep learning tools have gained tremendous attention in applied
               machine learning. However such tools for regression and
               classification do not capture model uncertainty. In comparison,
               Bayesian models offer a mathematically grounded framework to
               reason about model uncertainty, but usually come with a
               prohibitive computational cost. In this paper we develop a new
               theoretical framework casting dropout training in deep neural
               networks (NNs) as approximate Bayesian inference in deep Gaussian
               processes. A direct result of this theory gives us tools to model
               uncertainty with dropout NNs – extracting information from
               existing models that has been thrown away so far. This mitigates
               the problem of representing uncertainty in deep learning without
               sacrificing either computational complexity or test accuracy. We
               perform an extensive study of the properties of dropout’s
               uncertainty. Various network architectures and non-linearities
               are assessed on tasks of regression and classification, using
               MNIST as an example. We show a considerable improvement in
               predictive log-likelihood and RMSE compared to existing
               state-of-the-art methods, and finish by using dropout’s
               uncertainty in deep reinforcement learning.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2016
}

@ARTICLE{Gandhi2021-hw,
  title         = "Baby {{Intuitions Benchmark}} ({{BIB}}): {{Discerning}} the
                   Goals, Preferences, and Actions of Others",
  author        = "Gandhi, Kanishk and Stojnic, Gala and Lake, Brenden M and
                   Dillon, Moira R",
  journal       = "CoRR",
  volume        = "abs/2102.11938",
  year          =  2021,
  archivePrefix = "arXiv",
  eprint        = "2102.11938",
  keywords      = "Computer Science - Artificial Intelligence,Computer Science -
                   Machine Learning"
}

@ARTICLE{Gershman2015-pa,
  title    = "Computational Rationality: {{A}} Converging Paradigm for
              Intelligence in Brains, Minds, and Machines",
  author   = "Gershman, Samuel J and Horvitz, Eric J and Tenenbaum, Joshua B",
  journal  = "Science",
  volume   =  349,
  number   =  6245,
  pages    = "273--278",
  abstract = "After growing up together, and mostly growing apart in the second
              half of the 20th century, the fields of artificial intelligence
              (AI), cognitive science, and neuroscience are reconverging on a
              shared view of the computational foundations of intelligence that
              promotes valuable cross-disciplinary exchanges on questions,
              methods, and results. We chart advances over the past several
              decades that address challenges of perception and action under
              uncertainty through the lens of computation. Advances include the
              development of representations and inferential procedures for
              large-scale probabilistic inference and machinery for enabling
              reflection and decisions about tradeoffs in effort, precision, and
              timeliness of computations. These tools are deployed toward the
              goal of computational rationality: identifying decisions with
              highest expected utility, while taking into consideration the
              costs of computation in complex real-world problems in which most
              relevant calculations can only be approximated. We highlight key
              concepts with examples that show the potential for interchange
              between computer science, cognitive science, and neuroscience.",
  month    =  jul,
  year     =  2015,
  doi      = "10.1126/science.aac6076",
  issn     = "0036-8075,1095-9203"
}

@ARTICLE{Ghahramani2015-ko,
  title     = "Probabilistic Machine Learning and Artificial Intelligence",
  author    = "Ghahramani, Zoubin",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "452--459",
  abstract  = "How can a machine learn from experience? Probabilistic modelling
               provides a framework for understanding what learning is, and has
               therefore emerged as one of the principal theoretical and
               practical approaches for designing machines that learn from data
               acquired through experience. The probabilistic framework, which
               describes how to represent and manipulate uncertainty about
               models and predictions, has a central role in scientific data
               analysis, machine learning, robotics, cognitive science and
               artificial intelligence. This Review provides an introduction to
               this framework, and discusses some of the state-of-the-art
               advances in the field, namely, probabilistic programming,
               Bayesian optimization, data compression and automatic model
               discovery.",
  month     =  may,
  year      =  2015,
  doi       = "10.1038/nature14541",
  issn      = "0028-0836,1476-4687"
}

@ARTICLE{Girshick_undated-ee,
  title    = "Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and
              {{Semantic Segmentation}}",
  author   = "Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik,
              Jitendra",
  pages    =  8,
  abstract = "Object detection performance, as measured on the canonical PASCAL
              VOC dataset, has plateaued in the last few years. The
              best-performing methods are complex ensemble systems that
              typically combine multiple low-level image features with
              high-level context. In this paper, we propose a simple and
              scalable detection algorithm that improves mean average precision
              (mAP) by more than 30\% relative to the previous best result on
              VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key
              insights: (1) one can apply high-capacity convolutional neural
              networks (CNNs) to bottom-up region proposals in order to localize
              and segment objects and (2) when labeled training data is scarce,
              supervised pre-training for an auxiliary task, followed by
              domain-specific fine-tuning, yields a significant performance
              boost. Since we combine region proposals with CNNs, we call our
              method R-CNN: Regions with CNN features. We also present
              experiments that provide insight into what the network learns,
              revealing a rich hierarchy of image features. Source code for the
              complete system is available at http://www.cs.berkeley.edu/
              ˜rbg/rcnn."
}

@ARTICLE{Goodman2008-ee,
  title    = "A {{Rational Analysis}} of {{Rule-Based Concept Learning}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Feldman, Jacob and
              Griffiths, Thomas L",
  journal  = "Cognitive science",
  volume   =  32,
  number   =  1,
  pages    = "108--154",
  abstract = "This article proposes a new model of human concept learning that
              provides a rational analysis of learning feature-based concepts.
              This model is built upon Bayesian inference for a grammatically
              structured hypothesis space—a concept language of logical rules.
              This article compares the model predictions to human
              generalization judgments in several well-known category learning
              experiments, and finds good agreement for both average and
              individual participant generalizations. This article further
              investigates judgments for a broad set of 7-feature concepts—a
              more natural setting in several ways—and again finds that the
              model explains human performance.",
  month    =  jan,
  year     =  2008,
  doi      = "10.1080/03640210701802071",
  issn     = "0364-0213"
}

@ARTICLE{Goodman2014-kq,
  title  = "Concepts in a {{Probabilistic Language}} of {{Thought}}",
  author = "Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, Tobias",
  pages  =  25,
  year   =  2014
}

@UNPUBLISHED{Goyal2020-od,
  title    = "Recurrent {{Independent Mechanisms}}",
  author   = "Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani,
              Shagun and Levine, Sergey and Bengio, Yoshua and Schölkopf,
              Bernhard",
  abstract = "Learning modular structures which reflect the dynamics of the
              environment can lead to better generalization and robustness to
              changes which only affect a few of the underlying causes. We
              propose Recurrent Independent Mechanisms (RIMs), a new recurrent
              architecture in which multiple groups of recurrent cells operate
              with nearly independent transition dynamics, communicate only
              sparingly through the bottleneck of attention, and are only
              updated at time steps where they are most relevant. We show that
              this leads to specialization amongst the RIMs, which in turn
              allows for dramatically improved generalization on tasks where
              some factors of variation differ systematically between training
              and evaluation.",
  month    =  nov,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning"
}

@MISC{Goyal2021-ki,
  title         = "Inductive {{Biases}} for {{Deep Learning}} of {{Higher-Level
                   Cognition}}",
  author        = "Goyal, Anirudh and Bengio, Yoshua",
  journal       = "arXiv [cs, stat]",
  publisher     = "arXiv",
  number        = "arXiv:2011.15091",
  abstract      = "A fascinating hypothesis is that human and animal
                   intelligence could be explained by a few principles (rather
                   than an encyclopedic list of heuristics). If that hypothesis
                   was correct, we could more easily both understand our own
                   intelligence and build intelligent machines. Just like in
                   physics, the principles themselves would not be sufficient to
                   predict the behavior of complex systems like brains, and
                   substantial computation might be needed to simulate
                   human-like intelligence. This hypothesis would suggest that
                   studying the kind of inductive biases that humans and animals
                   exploit could help both clarify these principles and provide
                   inspiration for AI research and neuroscience theories. Deep
                   learning already exploits several key inductive biases, and
                   this work considers a larger list, focusing on those which
                   concern mostly higher-level and sequential conscious
                   processing. The objective of clarifying these particular
                   principles is that they could potentially help us build AI
                   systems benefiting from humans’ abilities in terms of
                   flexible out-of-distribution and systematic generalization,
                   which is currently an area where a large gap exists between
                   state-of-the-art machine learning and human intelligence.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs, stat",
  eprint        = "2011.15091",
  keywords      = "Computer Science - Artificial Intelligence,Computer Science -
                   Machine Learning,Statistics - Machine Learning"
}

@UNPUBLISHED{Greff2020-gr,
  title    = "On the {{Binding Problem}} in {{Artificial Neural Networks}}",
  author   = "Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber, Jürgen",
  abstract = "Contemporary neural networks still fall short of human-level
              generalization, which extends far beyond our direct experiences.
              In this paper, we argue that the underlying cause for this
              shortcoming is their inability to dynamically and flexibly bind
              information that is distributed throughout the network. This
              binding problem affects their capacity to acquire a compositional
              understanding of the world in terms of symbol-like entities (like
              objects), which is crucial for generalizing in predictable and
              systematic ways. To address this issue, we propose a unifying
              framework that revolves around forming meaningful entities from
              unstructured sensory inputs (segregation), maintaining this
              separation of information at a representational level
              (representation), and using these entities to construct new
              inferences, predictions, and behaviors (composition). Our analysis
              draws inspiration from a wealth of research in neuroscience and
              cognitive psychology, and surveys relevant mechanisms from the
              machine learning literature, to help identify a combination of
              inductive biases that allow symbolic information processing to
              emerge naturally in neural networks. We believe that a
              compositional approach to AI, in terms of grounded symbol-like
              representations, is of fundamental importance for realizing
              human-level generalization, and we hope that this paper may
              contribute towards that goal as a reference and inspiration.",
  month    =  dec,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,I.2.6"
}

@ARTICLE{Griffiths2010-wc,
  title   = "Probabilistic Models of Cognition: Exploring Representations and
             Inductive Biases",
  author  = "Griffiths, Thomas L and Chater, Nick and Kemp, Charles and Perfors,
             Amy and Tenenbaum, Joshua B",
  journal = "Trends in cognitive sciences",
  volume  =  14,
  number  =  8,
  pages   = "357--364",
  month   =  aug,
  year    =  2010,
  doi     = "10.1016/j.tics.2010.05.004",
  issn    = "1364-6613"
}

@UNPUBLISHED{Grosse2012-pf,
  title    = "Exploiting Compositionality to Explore a Large Space of Model
              Structures",
  author   = "Grosse, Roger and Salakhutdinov, Ruslan R and Freeman, William T
              and Tenenbaum, Joshua B",
  abstract = "The recent proliferation of richly structured probabilistic models
              raises the question of how to automatically determine an
              appropriate model for a dataset. We investigate this question for
              a space of matrix decomposition models which can express a variety
              of widely used models from unsupervised learning. To enable model
              selection, we organize these models into a context-free grammar
              which generates a wide variety of structures through the
              compositional application of a few simple rules. We use our
              grammar to generically and efficiently infer latent components and
              estimate predictive likelihood for nearly 2500 structures using a
              small toolbox of reusable algorithms. Using a greedy search over
              our grammar, we automatically choose the decomposition structure
              from raw data by evaluating only a small fraction of all models.
              The proposed method typically finds the correct structure for
              synthetic data and backs off gracefully to simpler models under
              heavy noise. It learns sensible structures for datasets as diverse
              as image patches, motion capture, 20 Questions, and U.S. Senate
              votes, all using exactly the same code.",
  month    =  oct,
  year     =  2012,
  keywords = "Computer Science - Machine Learning,Statistics - Machine Learning"
}

@ARTICLE{Gureckis2012-xz,
  title    = "Self-{{Directed Learning}}: {{A Cognitive}} and {{Computational
              Perspective}}",
  author   = "Gureckis, Todd M and Markant, Douglas B",
  journal  = "Perspectives on psychological science: a journal of the
              Association for Psychological Science",
  volume   =  7,
  number   =  5,
  pages    = "464--481",
  abstract = "A widely advocated idea in education is that people learn better
              when the flow of experience is under their control (i.e., learning
              is self-directed). However, the reasons why volitional control
              might result in superior acquisition and the limits to such
              advantages remain poorly understood. In this article, we review
              the issue from both a cognitive and computational perspective. On
              the cognitive side, self-directed learning allows individuals to
              focus effort on useful information they do not yet possess, can
              expose information that is inaccessible via passive observation,
              and may enhance the encoding and retention of materials. On the
              computational side, the development of efficient “active learning”
              algorithms that can select their own training data is an emerging
              research topic in machine learning. This review argues that recent
              advances in these related fields may offer a fresh theoretical
              perspective on how people gather information to support their own
              learning.",
  month    =  sep,
  year     =  2012,
  keywords = "active learning,intervention-based causal learning,machine
              learning,self-directed learning,self-regulated study",
  doi      = "10.1177/1745691612454304",
  issn     = "1745-6916,1745-6924"
}

@INCOLLECTION{Hauser2004-go,
  title     = "Evolutionary and Developmental Foundations of Human Knowledge",
  author    = "Hauser, Marc D and Spelke, Elizabeth",
  editor    = "Gazzaniga, Michael S",
  booktitle = "The {{Cognitive Neurosciences Iii}}",
  publisher = "MIT Press",
  year      =  2004
}

@ARTICLE{Hawkins2019-jn,
  title   = "A {{Framework}} for {{Intelligence}} and {{Cortical Function
             Based}} on {{Grid Cells}} in the {{Neocortex}}",
  author  = "Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott
             and Ahmad, Subutai",
  journal = "Frontiers in neural circuits",
  volume  =  12,
  pages   =  121,
  month   =  jan,
  year    =  2019,
  doi     = "10.3389/fncir.2018.00121",
  issn    = "1662-5110"
}

@INPROCEEDINGS{He2016-if,
  title     = "Deep {{Residual Learning}} for {{Image Recognition}}",
  author    = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  booktitle = "2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
               Recognition}} ({{CVPR}})",
  publisher = "IEEE",
  address   = "Las Vegas, NV, USA",
  pages     = "770--778",
  abstract  = "Deeper neural networks are more difficult to train. We present a
               residual learning framework to ease the training of networks that
               are substantially deeper than those used previously. We
               explicitly reformulate the layers as learning residual functions
               with reference to the layer inputs, instead of learning
               unreferenced functions. We provide comprehensive empirical
               evidence showing that these residual networks are easier to
               optimize, and can gain accuracy from considerably increased
               depth. On the ImageNet dataset we evaluate residual nets with a
               depth of up to 152 layers—8× deeper than VGG nets [40] but still
               having lower complexity. An ensemble of these residual nets
               achieves 3.57\% error on the ImageNet test set. This result won
               the 1st place on the ILSVRC 2015 classification task. We also
               present analysis on CIFAR-10 with 100 and 1000 layers.",
  month     =  jun,
  year      =  2016,
  doi       = "10.1109/CVPR.2016.90",
  isbn      =  9781467388511
}

@ARTICLE{Hill2019-uj,
  title    = "{{BDNF}}, Endurance Activity, and Mechanisms Underlying the
              Evolution of Hominin Brains",
  author   = "Hill, Tyler and Polk, John D",
  journal  = "American journal of physical anthropology",
  volume   =  168,
  number   = "S67",
  pages    = "47--62",
  abstract = "Objectives As a complex, polygenic trait, brain size has likely
              been influenced by a range of direct and indirect selection
              pressures for both cognitive and non-cognitive functions and
              capabilities. It has been hypothesized that hominin brain
              expansion was, in part, a correlated response to selection acting
              on aerobic capacity (Raichlen \& Polk, 2013). According to this
              hypothesis, selection for aerobic capacity increased the activity
              of various signaling molecules, including those involved in brain
              growth. One key molecule is brain-derived neurotrophic factor
              (BDNF), a protein that regulates neuronal development, survival,
              and plasticity in mammals. This review updates, partially tests,
              and expands Raichlen and Polk's (2013) hypothesis by evaluating
              evidence for BDNF as a mediator of brain size. Discussion We
              contend that selection for endurance capabilities in a hot climate
              favored changes to muscle composition, mitochondrial dynamics and
              increased energy budget through pathways involving regulation of
              PGC-1α and MEF2 genes, both of which promote BDNF activity. In
              addition, the evolution of hairlessness and the skin's
              thermoregulatory response provide other molecular pathways that
              promote both BDNF activity and neurotransmitter synthesis. We
              discuss how these pathways contributed to the evolution of brain
              size and function in human evolution and propose avenues for
              future research. Our results support Raichlen and Polk's
              contention that selection for non-cognitive functions has direct
              mechanistic linkages to the evolution of brain size in hominins.",
  year     =  2019,
  keywords = "BDNF,brain
              growth,exercise,MEF2,neurotrophins,PGC-1α,thermoregulation",
  doi      = "10.1002/ajpa.23762",
  issn     = "0002-9483,1096-8644"
}

@ARTICLE{Hung2018-fi,
  title     = "Effect of {{Acute Exercise Mode}} on {{Serum Brain-Derived
               Neurotrophic Factor}} ({{BDNF}}) and {{Task Switching
               Performance}}",
  author    = "Hung, Chiao-Ling and Tseng, Jun-Wei and Chao, Hsiao-Han and Hung,
               Tsung-Min and Wang, Ho-Seng",
  journal   = "Journal of clinical medicine research",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  10,
  pages     =  301,
  abstract  = "Previous studies have consistently reported a positive effect of
               acute exercise on cognition, particularly on executive function.
               However, most studies have focused on aerobic and resistant forms
               of exercise. The purpose of this study was to compare the effect
               of ‘open-skill’ with ‘closed-skill’ exercise (defined in terms of
               the predictability of the performing environment) on
               brain-derived neurotrophic factor (BDNF) production and task
               switching performance. Twenty young adult males participated in
               both closed (running) and open (badminton) skill exercise
               sessions in a counterbalanced order on separate days. The
               exercise sessions consisted of 5 min of warm up exercises
               followed by 30 min of running or badminton. The exercise
               intensity was set at 60\% (±5\%) of the heart rate reserve level
               (HRR) with HR being monitored by a wireless heart rate monitor.
               Blood samples were taken and participation in a task-switching
               paradigm occurred before and after each exercise session. Results
               showed no differences in serum BDNF or task-switching performance
               at the pre-test stage, however, badminton exercise resulted in
               significantly higher serum BDNF levels (a proxy for levels of
               BDNF in the brain) and near significant smaller global switching
               costs relative to running. This study has provided preliminary
               evidence in support the relative benefits of open-skills
               exercises on BDNF and executive function.",
  month     =  oct,
  year      =  2018,
  keywords  = "closed-skill,executive function,open-skill,switch cost",
  doi       = "10.3390/jcm7100301",
  issn      = "1918-3003"
}

@ARTICLE{Japkowicz2000-wi,
  title    = "Nonlinear {{Autoassociation Is Not Equivalent}} to {{PCA}}",
  author   = "Japkowicz, Nathalie and Hanson, Stephen José and Gluck, Mark A",
  journal  = "Neural computation",
  volume   =  12,
  number   =  3,
  pages    = "531--545",
  abstract = "A common misperception within the neural network community is that
              even with nonlinearities in their hidden layer, autoassociators
              trained with backpropagation are equivalent to linear methods such
              as principal component analysis (PCA). Our purpose is to
              demonstrate that nonlinear autoassociators actually behave
              differently from linear methods and that they can outperform these
              methods when used for latent extraction, projection, and
              classification. While linear autoassociators emulate PCA, and thus
              exhibit a flat or unimodal reconstruction error surface,
              autoassociators with nonlinearities in their hidden layer learn
              domains by building error reconstruction surfaces that, depending
              on the task, contain multiple local valleys. This interpolation
              bias allows nonlinear autoassociators to represent appropriate
              classifications of nonlinear multimodal domains, in contrast to
              linear autoassociators, which are inappropriate for such tasks. In
              fact, autoassociators with hidden unit nonlinearities can be shown
              to perform nonlinear classification and nonlinear recognition.",
  month    =  mar,
  year     =  2000,
  doi      = "10.1162/089976600300015691",
  issn     = "0899-7667"
}

@ARTICLE{Kemp2009-oz,
  title    = "Structured Statistical Models of Inductive Reasoning",
  author   = "Kemp, Charles and Tenenbaum, Joshua B",
  journal  = "Psychological review",
  volume   =  116,
  number   =  1,
  pages    = "20--58",
  abstract = "Everyday inductive inferences are often guided by rich background
              knowledge. Formal models of induction should aim to incorporate
              this knowledge and should explain how different kinds of knowledge
              lead to the distinctive patterns of reasoning found in different
              inductive contexts. This article presents a Bayesian framework
              that attempts to meet both goals and describe 4 applications of
              the framework: a taxonomic model, a spatial model, a threshold
              model, and a causal model. Each model makes probabilistic
              inferences about the extensions of novel properties, but the
              priors for the 4 models are defined over different kinds of
              structures that capture different relationships between the
              categories in a domain. The framework therefore shows how
              statistical inference can operate over structured background
              knowledge, and the authors argue that this interaction between
              structure and statistics is critical for explaining the power and
              flexibility of human reasoning.",
  year     =  2009,
  doi      = "10.1037/a0014282",
  issn     = "0033-295X,1939-1471"
}

@ARTICLE{Kruschke1993-eg,
  title    = "Human {{Category Learning}}: {{Implications}} for
              {{Backpropagation Models}}",
  author   = "Kruschke, John K",
  journal  = "Connection science",
  volume   =  5,
  number   =  1,
  pages    = "3--36",
  abstract = "Backpropagation (Rumelhart et al., 1986a) was proposed as a
              general learning algorithm for multi-layer perceptrons. This a n d
              e demonstrates chat a standard version of backprop fails to attend
              selectively to input dimensions in the same way as humans, suffers
              catastrophic forgetting of previously learned associations when
              novel exemplars are [rained, and can be overly sensitive to linear
              categoy boundaries. Another connecrionist model, A L C O V E
              (Krwchke 1990, 1992), does nor suffer those failures. Previous
              researchers identified these problems; the present article repons
              quantitative fits of the models to new human learning data. A L C
              O V E can be functionally approximated by a network that uses
              linear-sigmoid hidden nodes, like standard backprop. Ir is argued
              that models of human category learning should incorporate
              quasi-local representations and dimensional artention learning, as
              well as error-driuen learning, to address simulraneously all three
              phenomena.",
  month    =  jan,
  year     =  1993,
  doi      = "10.1080/09540099308915683",
  issn     = "0954-0091,1360-0494"
}

@ARTICLE{Lake2015-ap,
  title   = "Human-Level Concept Learning through Probabilistic Program
             Induction",
  author  = "Lake, B M and Salakhutdinov, R and Tenenbaum, J B",
  journal = "Science",
  volume  =  350,
  number  =  6266,
  pages   = "1332--1338",
  month   =  dec,
  year    =  2015,
  doi     = "10.1126/science.aab3050",
  issn    = "0036-8075,1095-9203"
}

@UNPUBLISHED{Lake2016-zm,
  title    = "Building {{Machines That Learn}} and {{Think Like People}}",
  author   = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and
              Gershman, Samuel J",
  abstract = "Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people.
              Many advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats
              humans in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we
              argue that these machines should (a) build causal models of the
              world that support explanation and understanding, rather than
              merely solving pattern recognition problems; (b) ground learning
              in intuitive theories of physics and psychology, to support and
              enrich the knowledge that is learned; and (c) harness
              compositionality and learning-to-learn to rapidly acquire and
              generalize knowledge to new tasks and situations. We suggest
              concrete challenges and promising routes towards these goals that
              can combine the strengths of recent neural network advances with
              more structured cognitive models.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science - Machine
              Learning,Computer Science - Neural and Evolutionary
              Computing,Statistics - Machine Learning"
}

@ARTICLE{Lake2018-gk,
  title    = "The {{Emergence}} of {{Organizing Structure}} in {{Conceptual
              Representation}}",
  author   = "Lake, Brenden M and Lawrence, Neil D and Tenenbaum, Joshua B",
  journal  = "Cognitive science",
  volume   =  42,
  number   = "S3",
  pages    = "809--832",
  abstract = "Both scientists and children make important structural
              discoveries, yet their computational underpinnings are not well
              understood. Structure discovery has previously been formalized as
              probabilistic inference about the right structural form—where form
              could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum,
              2008). Although this approach can learn intuitive organizations,
              including a tree for animals and a ring for the color circle, it
              assumes a strong inductive bias that considers only these
              particular forms, and each form is explicitly provided as initial
              knowledge. Here we introduce a new computational model of how
              organizing structure can be discovered, utilizing a broad
              hypothesis space with a preference for sparse connectivity. Given
              that the inductive bias is more general, the model's initial
              knowledge shows little qualitative resemblance to some of the
              discoveries it supports. As a consequence, the model can also
              learn complex structures for domains that lack intuitive
              description, as well as predict human property induction judgments
              without explicit structural forms. By allowing form to emerge from
              sparsity, our approach clarifies how both the richness and
              flexibility of human conceptual organization can coexist.",
  year     =  2018,
  keywords = "Bayesian modeling,Sparsity,Structure discovery,Unsupervised
              learning",
  doi      = "10.1111/cogs.12580",
  issn     = "0364-0213,1551-6709"
}

@UNPUBLISHED{Lake2019-ng,
  title    = "Human Few-Shot Learning of Compositional Instructions",
  author   = "Lake, Brenden M and Linzen, Tal and Baroni, Marco",
  abstract = "People learn in fast and flexible ways that have not been emulated
              by machines. Once a person learns a new verb “dax,” he or she can
              effortlessly understand how to “dax twice,” “walk and dax,” or
              “dax vigorously.” There have been striking recent improvements in
              machine learning for natural language processing, yet the best
              algorithms require vast amounts of experience and struggle to
              generalize new concepts in compositional ways. To better
              understand these distinctively human abilities, we study the
              compositional skills of people through languagelike instruction
              learning tasks. Our results show that people can learn and use
              novel functional concepts from very few examples (few-shot
              learning), successfully applying familiar functions to novel
              inputs. People can also compose concepts in complex ways that go
              beyond the provided demonstrations. Two additional experiments
              examined the assumptions and inductive biases that people make
              when solving these tasks, revealing three biases: mutual
              exclusivity, one-to-one mappings, and iconic concatenation. We
              discuss the implications for cognitive modeling and the potential
              for building machines with more human-like language learning
              capabilities.",
  month    =  may,
  year     =  2019,
  keywords = "Computer Science - Computation and Language"
}

@ARTICLE{Lake2019-zq,
  title   = "The {{Omniglot}} Challenge: A 3-Year Progress Report",
  author  = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  journal = "Current Opinion in Behavioral Sciences",
  volume  =  29,
  pages   = "97--104",
  month   =  oct,
  year    =  2019,
  doi     = "10.1016/j.cobeha.2019.04.007",
  issn    = "2352-1546"
}

@ARTICLE{Lake2021-tz,
  title     = "Word Meaning in Minds and Machines",
  author    = "Lake, Brenden M and Murphy, Gregory L",
  journal   = "Psychological review",
  publisher = "US: American Psychological Association",
  year      =  2021,
  doi       = "10.1037/rev0000297",
  issn      = "0033-295X,1939-1471"
}

@ARTICLE{Lake2020-wr,
  title    = "People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few
              Examples}}",
  author   = "Lake, Brenden M and Piantadosi, Steven T",
  journal  = "Computational Brain \& Behavior",
  volume   =  3,
  number   =  1,
  pages    = "54--65",
  abstract = "Machine learning has made major advances in categorizing objects
              in images, yet the best algorithms miss important aspects of how
              people learn and think about categories. People can learn richer
              concepts from fewer examples, including causal models that explain
              how members of a category are formed. Here, we explore the limits
              of this human ability to infer causal “programs”—latent generating
              processes with nontrivial algorithmic properties—from one, two, or
              three visual examples. People were asked to extrapolate the
              programs in several ways, for both classifying and generating new
              examples. As a theory of these inductive abilities, we present a
              Bayesian program learning model that searches the space of
              programs for the best explanation of the observations. Although
              variable, people’s judgments are broadly consistent with the model
              and inconsistent with several alternatives, including a pretrained
              deep neural network for object recognition, indicating that people
              can learn and reason with rich algorithmic abstractions from
              sparse input data.",
  month    =  mar,
  year     =  2020,
  doi      = "10.1007/s42113-019-00053-y",
  issn     = "2522-0861,2522-087X"
}

@ARTICLE{Landau1988-oy,
  title   = "The Importance of Shape in Early Lexical Learning",
  author  = "Landau, Barbara and Smith, Linda B and Jones, Susan S",
  journal = "Cognitive development",
  volume  =  3,
  number  =  3,
  pages   = "299--321",
  month   =  jul,
  year    =  1988,
  doi     = "10.1016/0885-2014(88)90014-7",
  issn    = "0885-2014"
}

@ARTICLE{LeCun2015-hm,
  title     = "Deep Learning",
  author    = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "436--444",
  abstract  = "Deep learning allows computational models that are composed of
               multiple processing layers to learn representations of data with
               multiple levels of abstraction. These methods have dramatically
               improved the state-of-the-art in speech recognition, visual
               object recognition, object detection and many other domains such
               as drug discovery and genomics. Deep learning discovers intricate
               structure in large data sets by using the backpropagation
               algorithm to indicate how a machine should change its internal
               parameters that are used to compute the representation in each
               layer from the representation in the previous layer. Deep
               convolutional nets have brought about breakthroughs in processing
               images, video, speech and audio, whereas recurrent nets have
               shone light on sequential data such as text and speech.",
  month     =  may,
  year      =  2015,
  doi       = "10.1038/nature14539",
  issn      = "0028-0836,1476-4687"
}

@INPROCEEDINGS{Lerer2016-kb,
  title     = "Learning {{Physical Intuition}} of {{Block Towers}} by
               {{Example}}",
  author    = "Lerer, Adam and Gross, Sam and Fergus, Rob",
  booktitle = "International {{Conference}} on {{Machine Learning}}",
  publisher = "PMLR",
  pages     = "430--438",
  month     =  jun,
  year      =  2016
}

@ARTICLE{Levine2014-bg,
  title    = "Low {{Protein Intake Is Associated}} with a {{Major Reduction}} in
              {{IGF-1}}, {{Cancer}}, and {{Overall Mortality}} in the 65 and
              {{Younger}} but {{Not Older Population}}",
  author   = "Levine, Morgan E and Suarez, Jorge A and Brandhorst, Sebastian and
              Balasubramanian, Priya and Cheng, Chia-Wei and Madia, Federica and
              Fontana, Luigi and Mirisola, Mario G and Guevara-Aguirre, Jaime
              and Wan, Junxiang and Passarino, Giuseppe and Kennedy, Brian K and
              Wei, Min and Cohen, Pinchas and Crimmins, Eileen M and Longo,
              Valter D",
  journal  = "Cell metabolism",
  volume   =  19,
  number   =  3,
  pages    = "407--417",
  abstract = "Mice and humans with growth hormone receptor/ IGF-1 deficiencies
              display major reductions in agerelated diseases. Because protein
              restriction reduces GHR-IGF-1 activity, we examined links between
              protein intake and mortality. Respondents aged 50–65 reporting
              high protein intake had a 75\% increase in overall mortality and a
              4-fold increase in cancer death risk during the following 18
              years. These associations were either abolished or attenuated if
              the proteins were plant derived. Conversely, high protein intake
              was associated with reduced cancer and overall mortality in
              respondents over 65, but a 5-fold increase in diabetes mortality
              across all ages. Mouse studies confirmed the effect of high
              protein intake and GHR-IGF-1 signaling on the incidence and
              progression of breast and melanoma tumors, but also the
              detrimental effects of a low protein diet in the very old. These
              results suggest that low protein intake during middle age followed
              by moderate to high protein consumption in old adults may optimize
              healthspan and longevity.",
  month    =  mar,
  year     =  2014,
  doi      = "10.1016/j.cmet.2014.02.006",
  issn     = "1550-4131"
}

@ARTICLE{Lloyd2014-ua,
  title    = "Automatic {{Construction}} and {{Natural-Language Description}} of
              {{Nonparametric Regression Models}}",
  author   = "Lloyd, James and Duvenaud, David and Grosse, Roger and Tenenbaum,
              Joshua and Ghahramani, Zoubin",
  journal  = "Proceedings of the ... AAAI Conference on Artificial Intelligence.
              AAAI Conference on Artificial Intelligence",
  volume   =  28,
  number   =  1,
  abstract = "This paper presents the beginnings of an automatic statistician,
              focusing on regression problems. Our system explores an open-ended
              space of statistical models to discover a good explanation of a
              data set, and then produces a detailed report with figures and
              natural-language text. Our approach treats unknown regression
              functions nonparametrically using Gaussian processes, which has
              two important consequences. First, Gaussian processes can model
              functions in terms of high-level properties (e.g. smoothness,
              trends, periodicity, changepoints). Taken together with the
              compositional structure of our language of models this allows us
              to automatically describe functions in simple terms. Second, the
              use of flexible nonparametric models and a rich language for
              composing them in an open-ended manner also results in
              state-of-the-art extrapolation performance evaluated over 13 real
              time series data sets from various domains.",
  month    =  jun,
  year     =  2014,
  keywords = "Regression",
  issn     = "2159-5399,2374-3468"
}

@INCOLLECTION{Lupyan2021-sj,
  title     = "Does {{Vocabulary Help Structure}} the {{Mind}}?",
  author    = "Lupyan, Gary and Zettersten, Martin",
  booktitle = "Minnesota {{Symposia}} on {{Child Psychology}}",
  publisher = "John Wiley \& Sons, Ltd",
  pages     = "160--199",
  abstract  = "The idea that language shapes thinking seemed plausible when
               scientists were in the dark about how thinking works. This
               chapter describes several mechanisms by which the words of a
               language can help structure knowledge and navigate cognitive
               problems. It is because thought and language seem so closely
               linked that language is so often used as a window to thought. The
               cognitive priority view faces two serious problems. The first is
               accounting for the cross-linguistic diversity of vocabularies.
               The second problem is the problem of origin. The chapter
               discusses new data aimed at both collecting verbal complexity
               measures independently from the original Bongard problems
               themselves, and collecting more objective measures of solution
               accuracy. It provides further evidence for the idea that
               easier-to-name visual features are more likely to be used by
               people when judging visual similarity.",
  year      =  2021,
  keywords  = "Bongard problems,cognitive priority,cross-linguistic
               diversity,name visual features,verbal complexity,visual
               similarity,vocabularies",
  doi       = "10.1002/9781119684527.ch6",
  isbn      =  9781119684527
}

@ARTICLE{Mahowald1991-jj,
  title   = "The {{Silicon Retina}}",
  author  = "Mahowald, Misha A and Mead, Carver",
  journal = "Scientific American",
  pages   =  9,
  year    =  1991,
  issn    = "0036-8733"
}

@UNPUBLISHED{Mao2019-ht,
  title    = "The {{Neuro-Symbolic Concept Learner}}: {{Interpreting Scenes}},
              {{Words}}, and {{Sentences From Natural Supervision}}",
  author   = "Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum,
              Joshua B and Wu, Jiajun",
  abstract = "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model
              that learns visual concepts, words, and semantic parsing of
              sentences without explicit supervision on any of them; instead,
              our model learns by simply looking at images and reading paired
              questions and answers. Our model builds an object-based scene
              representation and translates sentences into executable, symbolic
              programs. To bridge the learning of two modules, we use a
              neuro-symbolic reasoning module that executes these programs on
              the latent scene representation. Analogical to human concept
              learning, the perception module learns visual concepts based on
              the language description of the object being referred to.
              Meanwhile, the learned visual concepts facilitate learning new
              words and parsing new sentences. We use curriculum learning to
              guide the searching over the large compositional space of images
              and language. Extensive experiments demonstrate the accuracy and
              efficiency of our model on learning visual concepts, word
              representations, and semantic parsing of sentences. Further, our
              method allows easy generalization to new object attributes,
              compositions, language concepts, scenes and questions, and even
              new program domains. It also empowers applications including
              visual question answering and bidirectional image-text retrieval.",
  month    =  apr,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Computer Vision and
              Pattern Recognition,Computer Science - Machine Learning"
}

@INCOLLECTION{Minsky2019-mb,
  title     = "A {{Framework For Representing Knowledge}}",
  author    = "Minsky, M",
  booktitle = "A {{Framework For Representing Knowledge}}",
  publisher = "De Gruyter",
  pages     = "1--25",
  abstract  = "A Framework For Representing Knowledge was published in Frame
               Conceptions and Text Understanding on page 1.",
  month     =  jul,
  year      =  2019,
  doi       = "10.1515/9783110858778-003",
  isbn      =  9783110858778
}

@BOOK{Minsky1988-bi,
  title     = "Perceptrons: Expanded Edition",
  author    = "Minsky, Marvin L and Papert, Seymour A",
  publisher = "MIT press",
  year      =  1988,
  isbn      =  9780262631112
}

@ARTICLE{Mnih2015-vd,
  title     = "Human-Level Control through Deep Reinforcement Learning",
  author    = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
               Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves,
               Alex and Riedmiller, Martin and Fidjeland, Andreas K and
               Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
               Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
               Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  518,
  number    =  7540,
  pages     = "529--533",
  abstract  = "An artificial agent is developed that learns to play a diverse
               range of classic Atari 2600 computer games directly from sensory
               experience, achieving a performance comparable to that of an
               expert human player; this work paves the way to building
               general-purpose learning algorithms that bridge the divide
               between perception and action.",
  month     =  feb,
  year      =  2015,
  doi       = "10.1038/nature14236",
  issn      = "0028-0836,1476-4687"
}

@ARTICLE{Newell1956-lm,
  title    = "The Logic Theory {Machine–{{A}}} Complex Information Processing
              System",
  author   = "Newell, A and Simon, H",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "61--79",
  abstract = "In this paper we describe a complex information processing system,
              which we call the logic theory machine, that is capable of
              discovering proofs for theorems in symbolic logic. This system, in
              contrast to the systematic algorithms that are ordinarily employed
              in computation, relies heavily on heuristic methods similar to
              those that have been observed in . human problem solving activity.
              The specification is written in a formal language, of the nature
              of a pseudo-code, that is suitable for coding for digital
              computers. However, the present paper is concerned exclusively
              with specification of the system, and not with its realization in
              a computer. The logic theory machine is part of a program of
              research to understand complex information processing systems by
              specifying and synthesizing a substantial variety of such systems
              for empirical study.",
  month    =  sep,
  year     =  1956,
  keywords = "Automatic programming,Formal languages,Heuristic
              algorithms,Humans,Information analysis,Information
              processing,Logic,Pattern recognition,Problem-solving",
  doi      = "10.1109/TIT.1956.1056797",
  issn     = "2168-2712"
}

@ARTICLE{Nobandegani_undated-cc,
  title    = "Example {{Generation Under Constraints Using Cascade Correlation
              Neural Nets}}",
  author   = "Nobandegani, Ardavan S and Shultz, Thomas R",
  pages    =  6,
  abstract = "Humans not only can effortlessly imagine a wide range of novel
              instances and scenarios when prompted (e.g., a new shirt), but
              more remarkably, they can adequately generate examples which
              satisfy a given set of constraints (e.g., a new, dotted, pink
              shirt). Recently, Nobandegani and Shultz (2017) proposed a
              framework which permits converting deterministic, discriminative
              neural nets into probabilistic generative models. In this work, we
              formally show that an extension of this framework allows for
              generating examples under a wide range of constraints.
              Furthermore, we show that this framework is consistent with
              developmental findings on children’s generative abilities, and can
              account for a developmental shift in infants’ probabilistic
              learning and reasoning. We discuss the importance of integrating
              Bayesian and connectionist approaches to computational
              developmental psychology, and how our work contributes to that
              research."
}

@ARTICLE{Nye2020-np,
  title    = "Learning compositional rules via neural program synthesis",
  author   = "Nye, Maxwell and Solar-Lezama, Armando and Tenenbaum, Josh and
              Lake, Brenden M",
  journal  = "Advances in neural information processing systems",
  volume   =  33,
  pages    = "10832--10842",
  abstract = "Many aspects of human reasoning, including language, require
              learning rules from very little data. Humans can do this, often
              learning systematic rules from very few examples, and combining
              these rules to form compositional rule-based systems. Current
              neural architectures, on the other hand, often fail to generalize
              in a compositional manner, especially when evaluated in ways that
              vary systematically from training. In this work, we present a
              neuro-symbolic model which learns entire rule systems from a small
              set of examples. Instead of directly predicting outputs from
              inputs, we train our model to induce the explicit system of rules
              governing a set of previously seen examples, drawing upon
              techniques from the neural program synthesis literature. Our
              rule-synthesis approach outperforms neural meta-learning
              techniques in three domains: an artificial instruction-learning
              domain used to evaluate human learning, the SCAN challenge
              datasets, and learning rule-based translations of number words
              into integers for a wide range of human languages.",
  year     =  2020,
  issn     = "1049-5258"
}

@UNPUBLISHED{Peterson2016-lj,
  title    = "Adapting {{Deep Network Features}} to {{Capture Psychological
              Representations}}",
  author   = "Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L",
  abstract = "Deep neural networks have become increasingly successful at
              solving classic perception problems such as object recognition,
              semantic segmentation, and scene understanding, often reaching or
              surpassing human-level accuracy. This success is due in part to
              the ability of DNNs to learn useful representations of
              high-dimensional inputs, a problem that humans must also solve. We
              examine the relationship between the representations learned by
              these networks and human psychological representations recovered
              from similarity judgments. We find that deep features learned in
              service of object classification account for a significant amount
              of the variance in human similarity judgments for a set of animal
              images. However, these features do not capture some qualitative
              distinctions that are a key part of human representations. To
              remedy this, we develop a method for adapting deep features to
              align with human similarity judgments, resulting in image
              representations that can potentially be used to extend the scope
              of psychological experiments.",
  month    =  aug,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science - Neural
              and Evolutionary Computing"
}

@ARTICLE{Piantadosi2016-na,
  title     = "The Logical Primitives of Thought: {{Empirical}} Foundations for
               Compositional Cognitive Models",
  author    = "Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D",
  journal   = "Psychological review",
  publisher = "US: American Psychological Association",
  volume    =  123,
  number    =  4,
  pages     =  392,
  year      =  2016,
  doi       = "10.1037/a0039980",
  issn      = "0033-295X,1939-1471"
}

@ARTICLE{Piantadosi2021-ie,
  title    = "The {{Computational Origin}} of {{Representation}}",
  author   = "Piantadosi, Steven T",
  journal  = "Minds and Machines",
  volume   =  31,
  number   =  1,
  pages    = "1--58",
  abstract = "Each of our theories of mental representation provides some
              insight into how the mind works. However, these insights often
              seem incompatible, as the debates between symbolic, dynamical,
              emergentist, sub-symbolic, and grounded approaches to cognition
              attest. Mental representations—whatever they are—must share many
              features with each of our theories of representation, and yet
              there are few hypotheses about how a synthesis could be possible.
              Here, I develop a theory of the underpinnings of symbolic
              cognition that shows how sub-symbolic dynamics may give rise to
              higher-level cognitive representations of structures, systems of
              knowledge, and algorithmic processes. This theory implements a
              version of conceptual role semantics by positing an internal
              universal representation language in which learners may create
              mental models to capture dynamics they observe in the world. The
              theory formalizes one account of how truly novel conceptual
              content may arise, allowing us to explain how even elementary
              logical and computational operations may be learned from a more
              primitive basis. I provide an implementation that learns to
              represent a variety of structures, including logic, number,
              kinship trees, regular languages, context-free languages, domains
              of theories like magnetism, dominance hierarchies, list
              structures, quantification, and computational primitives like
              repetition, reversal, and recursion. This account is based on
              simple discrete dynamical processes that could be implemented in a
              variety of different physical or biological systems. In
              particular, I describe how the required dynamics can be directly
              implemented in a connectionist framework. The resulting theory
              provides an “assembly language” for cognition, where high-level
              theories of symbolic computation can be implemented in simple
              dynamics that themselves could be encoded in biologically
              plausible systems.",
  month    =  mar,
  year     =  2021,
  doi      = "10.1007/s11023-020-09540-9",
  issn     = "0924-6495,1572-8641"
}

@ARTICLE{Piantadosi2016-fe,
  title    = "Four {{Problems Solved}} by the {{Probabilistic Language}} of
              {{Thought}}",
  author   = "Piantadosi, Steven T and Jacobs, Robert A",
  journal  = "Current directions in psychological science",
  volume   =  25,
  number   =  1,
  pages    = "54--59",
  abstract = "We argue for the advantages of the probabilistic language of
              thought (pLOT), a recently emerging approach to modeling human
              cognition. Work using this framework demonstrates how the pLOT (a)
              refines the debate between symbols and statistics in cognitive
              modeling, (b) permits theories that draw on insights from both
              nativist and empiricist approaches, (c) explains the origins of
              novel and complex computational concepts, and (d) provides a
              framework for abstraction that can link sensation and conception.
              In each of these areas, the pLOT provides a productive middle
              ground between historical divides in cognitive psychology,
              pointing to a promising way forward for the field.",
  month    =  feb,
  year     =  2016,
  doi      = "10.1177/0963721415609581",
  issn     = "0963-7214,1467-8721"
}

@ARTICLE{Piloto2022-yi,
  title    = "Intuitive Physics Learning in a Deep-Learning Model Inspired by
              Developmental Psychology",
  author   = "Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and
              Botvinick, Matthew",
  journal  = "Nature Human Behaviour",
  abstract = "Abstract ‘Intuitive physics’ enables our pragmatic engagement with
              the physical world and forms a key component of ‘common sense’
              aspects of thought. Current artificial intelligence systems pale
              in their understanding of intuitive physics, in comparison to even
              very young children. Here we address this gap between humans and
              machines by drawing on the field of developmental psychology.
              First, we introduce and open-source a machine-learning dataset
              designed to evaluate conceptual understanding of intuitive
              physics, adopting the violation-of-expectation (VoE) paradigm from
              developmental psychology. Second, we build a deep-learning system
              that learns intuitive physics directly from visual data, inspired
              by studies of visual cognition in children. We demonstrate that
              our model can learn a diverse set of physical concepts, which
              depends critically on object-level representations, consistent
              with findings from developmental psychology. We consider the
              implications of these results both for AI and for research on
              human cognition.",
  month    =  jul,
  year     =  2022,
  doi      = "10.1038/s41562-022-01394-8",
  issn     = "2397-3374"
}

@ARTICLE{Pitt_undated-sp,
  title    = "Exact {{Number Concepts Are Limited}} to the {{Verbal Count
              Range}}",
  author   = "Pitt, Benjamin and Gibson, Edward and Piantadosi, Steven T",
  pages    =  11,
  abstract = "Previous findings suggest that mentally representing exact numbers
              larger than four depends on a verbal count routine (e.g., “one,
              two, three . . .”). However, these findings are controversial
              because they rely on comparisons across radically different
              languages and cultures. We tested the role of language in number
              concepts within a single population—the Tsimane’ of Bolivia—in
              which knowledge of number words varies across individual adults.
              We used a novel data-analysis model to quantify the point at which
              participants (N = 30) switched from exact to approximate number
              representations during a simple numerical matching task. The
              results show that these behavioral switch points were bounded by
              participants’ verbal count ranges; their representations of exact
              cardinalities were limited to the number words they knew. Beyond
              that range, they resorted to numerical approximation. These
              results resolve competing accounts of previous findings and
              provide unambiguous evidence that large exact number concepts are
              enabled by language."
}

@ARTICLE{Premack1997-aj,
  title    = "Infants {{Attribute Value}}± to the {{Goal-Directed Actions}} of
              {{Self-propelled Objects}}",
  author   = "Premack, David and Premack, Ann James",
  journal  = "Journal of cognitive neuroscience",
  volume   =  9,
  number   =  6,
  pages    = "848--856",
  abstract = "Motion is a fundamental source of information for basic human
              interpretations; it is basic to the fundamental concept of
              causality and, the present model argues, equally basic to the
              fundamental concept of intentionality.The model is based on two
              main assumptions: When an infant perceives an object (1) moving
              spontaneously and (2) displaying goaldirected action, it will
              interpret the object as intentional and assign to it the unique
              properties of the psychological domain. The key property tested
              was: Do infants attribute value to interactions between
              intentional objects using criteria specified by the model?We
              showed infants (average age 52 weeks) computer-generated
              animations of spontaneously moving “balls,” using looking time in
              a standard habituation/dishabituation paradigm. In two positive
              interactions, one ball either “caressed” another, or “helped” it
              achieve its goal; whereas in two negative interactions, one ball
              either “hit“ another, or “prevented” it from achieving its goal.
              In keeping with predictions of the model, when transferred to a
              negative condition, infants who had been habituated on a positive
              condition showed greater dishabituation than those habituated on a
              negative condition. The results could not be easily explained by
              the similarity relations among the animations depicting the
              interactions.The results suggest that well before the age when the
              child can ascribe mental states or has a “theory of mind,” it
              recognizes the goals of self-propelled objects and attributes
              value to the interactions between them.",
  month    =  nov,
  year     =  1997,
  doi      = "10.1162/jocn.1997.9.6.848",
  issn     = "0898-929X"
}

@UNPUBLISHED{Radford2021-jz,
  title    = "Learning {{Transferable Visual Models From Natural Language
              Supervision}}",
  author   = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
              Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
              and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
              Krueger, Gretchen and Sutskever, Ilya",
  abstract = "State-of-the-art computer vision systems are trained to predict a
              fixed set of predetermined object categories. This restricted form
              of supervision limits their generality and usability since
              additional labeled data is needed to specify any other visual
              concept. Learning directly from raw text about images is a
              promising alternative which leverages a much broader source of
              supervision. We demonstrate that the simple pre-training task of
              predicting which caption goes with which image is an efficient and
              scalable way to learn SOTA image representations from scratch on a
              dataset of 400 million (image, text) pairs collected from the
              internet. After pre-training, natural language is used to
              reference learned visual concepts (or describe new ones) enabling
              zero-shot transfer of the model to downstream tasks. We study the
              performance of this approach by benchmarking on over 30 different
              existing computer vision datasets, spanning tasks such as OCR,
              action recognition in videos, geo-localization, and many types of
              fine-grained object classification. The model transfers
              non-trivially to most tasks and is often competitive with a fully
              supervised baseline without the need for any dataset specific
              training. For instance, we match the accuracy of the original
              ResNet-50 on ImageNet zero-shot without needing to use any of the
              1.28 million training examples it was trained on. We release our
              code and pre-trained model weights at
              https://github.com/OpenAI/CLIP.",
  month    =  feb,
  year     =  2021,
  keywords = "Computer Science - Computer Vision and Pattern
              Recognition,Computer Science - Machine Learning"
}

@UNPUBLISHED{Rezende2016-ge,
  title    = "One-{{Shot Generalization}} in {{Deep Generative Models}}",
  author   = "Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo and
              Gregor, Karol and Wierstra, Daan",
  abstract = "Humans have an impressive ability to reason about new concepts and
              experiences from just a single example. In particular, humans have
              an ability for one-shot generalization: an ability to encounter a
              new concept, understand its structure, and then be able to
              generate compelling alternative variations of the concept. We
              develop machine learning systems with this important capacity by
              developing new deep generative models, models that combine the
              representational power of deep learning with the inferential power
              of Bayesian reasoning. We develop a class of sequential generative
              models that are built on the principles of feedback and attention.
              These two characteristics lead to generative models that are among
              the state-of-the art in density estimation and image generation.
              We demonstrate the one-shot generalization ability of our models
              using three tasks: unconditional sampling, generating new
              exemplars of a given concept, and generating new exemplars of a
              family of concepts. In all cases our models are able to generate
              compelling and diverse samples—having seen new examples just
              once—providing an important class of general-purpose models for
              one-shot machine learning.",
  month    =  may,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning"
}

@ARTICLE{Rich2018-zj,
  title     = "The Limits of Learning: {{Exploration}}, Generalization, and the
               Development of Learning Traps",
  author    = "Rich, Alexander S and Gureckis, Todd M",
  journal   = "Journal of experimental psychology. General",
  publisher = "American Psychological Association",
  address   = "US",
  volume    =  147,
  number    =  11,
  pages     = "1553--1570",
  abstract  = "Learning usually improves the accuracy of beliefs through the
               accumulation of experience. But are there limits to learning that
               prevent us from accurately understanding our world? In this
               article we investigate the concept of a “learning trap”—the
               formation of a stable false belief even with extensive
               experience. Our review highlights how these traps develop through
               the interaction of learning and decision making in unknown
               environments. We further document a particularly pernicious
               learning trap driven by selective attention, a mechanism often
               assumed to facilitate learning in complex environments. Using
               computer simulation, we demonstrate the key attributes of the
               agent and environment that lead to this new type of learning
               trap. Then, in a series of experiments we present evidence that
               people robustly fall into this trap, even in the presence of
               various interventions predicted to meliorate it. These results
               highlight a fundamental limit to learning and adaptive behavior
               that impacts individuals, organizations, animals, and machines.
               (PsycInfo Database Record (c) 2020 APA, all rights reserved)",
  year      =  2018,
  keywords  = "Decision Making,Development,Environment,False
               Beliefs,Generalization (Learning),Learning,Learning
               Environment,Selective Attention",
  doi       = "10.1037/xge0000466",
  issn      = "0096-3445,1939-2222"
}

@BOOK{Rojas2013-pv,
  title     = "Neural Networks: A Systematic Introduction",
  author    = "Rojas, Raúl",
  publisher = "Springer Science \& Business Media",
  year      =  2013,
  isbn      =  9783642610684
}

@ARTICLE{Rosenblatt1958-kl,
  title   = "The Perceptron: {{A}} Probabilistic Model for Information Storage
             and Organization in the Brain",
  author  = "Rosenblatt, F",
  journal = "Psychological review",
  volume  =  65,
  number  =  6,
  pages   = "386--408",
  year    =  1958,
  doi     = "10.1037/h0042519",
  issn    = "0033-295X,1939-1471"
}

@ARTICLE{Ruegsegger2017-yh,
  title     = "Running from {{Disease}}: {{Molecular Mechanisms Associating
               Dopamine}} and {{Leptin Signaling}} in the {{Brain}} with
               {{Physical Inactivity}}, {{Obesity}}, and {{Type}} 2 {{Diabetes}}",
  author    = "Ruegsegger, Gregory N and Booth, Frank W",
  journal   = "Frontiers in endocrinology",
  publisher = "Frontiers",
  volume    =  0,
  abstract  = "Physical inactivity is a primary contributor to diseases such as
               obesity, cardiovascular disease, and Type 2 diabetes.
               Accelerometry data suggest that a majority of U.S. adults fail to
               perform substantial levels of physical activity needed to improve
               health. Thus, understanding the molecular factors that stimulate
               physical activity, and physical inactivity, is imperative for the
               development of strategies to reduce sedentary behavior and in
               turn prevent chronic disease. Despite many of the well-known
               health benefits of physical activity being described, little is
               known about genetic and biological factors that may influence
               this complex behavior. The mesolimbic dopamine system regulates
               motivating and rewarding behavior as well as motor movement.
               Here, we present data supporting the hypothesis that obesity may
               mechanistically lower voluntary physical activity levels via
               dopamine dysregulation. In doing so, we review data that suggests
               mesolimbic dopamine activity is a strong contributor to voluntary
               physical activity behavior. We also summarize findings suggesting
               that obesity leads to central dopaminergic dysfunction, which in
               turn contributes to reductions in physical activity that often
               accompany obesity. Additionally, we highlight examples in which
               central leptin activity influences physical activity levels in a
               dopamine-dependent manner. Future elucidation of these mechanisms
               will help support strategies to increase physical activity levels
               in obese patients and prevent diseases caused by physical
               inactivity.",
  year      =  2017,
  keywords  = "Dopamine,Leptin,Motivation,Nucleus Accumbens,physical
               activity,physical inactivity",
  doi       = "10.3389/fendo.2017.00109",
  issn      = "1664-2392"
}

@TECHREPORT{Rule2018-zm,
  title       = "Learning List Concepts through Program Induction",
  author      = "Rule, Joshua and Schulz, Eric and Piantadosi, Steven T and
                 Tenenbaum, Joshua B",
  institution = "Animal Behavior and Cognition",
  abstract    = "Humans master complex systems of interrelated concepts like
                 mathematics and natural language. Previous work suggests
                 learning these systems relies on iteratively and directly
                 revising a language-like conceptual representation. We
                 introduce and assess a novel concept learning paradigm called
                 Martha’s Magical Machines that captures complex relationships
                 between concepts. We model human concept learning in this
                 paradigm as a search in the space of term rewriting systems,
                 previously developed as an abstract model of computation. Our
                 model accurately predicts that participants learn some
                 transformations more easily than others and that they learn
                 harder concepts more easily using a bootstrapping curriculum
                 focused on their compositional parts. Our results suggest that
                 term rewriting systems may be a useful model of human
                 conceptual representations.",
  month       =  may,
  year        =  2018,
  doi         = "10.1101/321505"
}

@ARTICLE{Rule2020-db,
  title   = "The {{Child}} as {{Hacker}}",
  author  = "Rule, Joshua S and Tenenbaum, Joshua B and Piantadosi, Steven T",
  journal = "Trends in cognitive sciences",
  volume  =  24,
  number  =  11,
  pages   = "900--915",
  month   =  nov,
  year    =  2020,
  doi     = "10.1016/j.tics.2020.07.005",
  issn    = "1364-6613"
}

@UNPUBLISHED{Scherrer2021-wg,
  title    = "Learning {{Neural Causal Models}} with {{Active Interventions}}",
  author   = "Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and Goyal,
              Anirudh and Schwab, Patrick and Schölkopf, Bernhard and Mozer,
              Michael C and Bengio, Yoshua and Bauer, Stefan and Ke, Nan
              Rosemary",
  abstract = "Discovering causal structures from data is a challenging inference
              problem of fundamental importance in all areas of science. The
              appealing scaling properties of neural networks have recently led
              to a surge of interest in differentiable neural network-based
              methods for learning causal structures from data. So far
              differentiable causal discovery has focused on static datasets of
              observational or interventional origin. In this work, we introduce
              an active intervention-targeting mechanism which enables a quick
              identification of the underlying causal structure of the
              data-generating process. Our method significantly reduces the
              required number of interactions compared with random intervention
              targeting and is applicable for both discrete and continuous
              optimization formulations of learning the underlying directed
              acyclic graph (DAG) from data. We examine the proposed method
              across a wide range of settings and demonstrate superior
              performance on multiple benchmarks from simulated to real-world
              data.",
  month    =  sep,
  year     =  2021,
  keywords = "Computer Science - Machine Learning,Statistics - Machine Learning"
}

@UNPUBLISHED{Shultz2021-hq,
  title    = "A {{Computational Model}} of {{Infant Learning}} and {{Reasoning}}
              with {{Probabilities}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments reveal that 6- to 12-month-old infants can
              learn probabilities and reason with them. In this work, we present
              a novel computational system called Neural Probability Learner and
              Sampler (NPLS) that learns and reasons with probabilities,
              providing a computationally sufficient mechanism to explain infant
              probabilistic learning and inference. In 24 computer simulations,
              NPLS simulations show how probability distributions can emerge
              naturally from neural-network learning of event sequences,
              providing a novel explanation of infant probabilistic learning and
              reasoning. Three mathematical proofs show how and why NPLS
              simulates the infant results so accurately. The results are
              situated in relation to seven other active research lines. This
              work provides an effective way to integrate Bayesian and
              neural-network approaches to cognition.",
  month    =  jun,
  year     =  2021,
  keywords = "Quantitative Biology - Neurons and Cognition"
}

@ARTICLE{Shultz_undated-cv,
  title    = "Probability {{Without Counting}} and {{Dividing}}: {{A Fresh
              Computational Perspective}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  pages    =  7,
  abstract = "Recent experiments show that preverbal infants can reason
              probabilistically. This raises a deep puzzle because infants lack
              the counting and dividing abilities presumably required to compute
              probabilities. In the standard way of computing probabilities,
              they would have to count or accurately estimate large frequencies
              and divide those values by their total. Here, we present a novel
              neural-network model that learns and uses probability
              distributions without explicit counting or dividing. Probability
              distributions emerge naturally from neural-network learning of
              event sequences, providing a computationally sufficient
              explanation of how infants could succeed at probabilistic
              reasoning. Several alternative explanations are discussed and
              ruled out. Our work bears on several other active literatures, and
              it suggests an effective way to integrate Bayesian and
              neural-network approaches to cognition."
}

@TECHREPORT{Smolensky2022-ts,
  title       = "Neurocompositional Computing in Human and Machine Intelligence:
                 {{A}} Tutorial",
  author      = "Smolensky, Paul and McCoy, R Thomas and Fernandez, Roland and
                 Goldrick, Matthew and Gao, Jianfeng",
  institution = "Microsoft",
  abstract    = "The past decade has produced a revolution in Artificial
                 Intelligence (AI), after a half-century of AI repeatedly
                 failing to meet expectations. What explains the dramatic change
                 from 20th-century to 21st-century AI, and how can remaining
                 limitations of current AI be overcome? Until now, the widely
                 accepted narrative has attributed the recent progress in AI to
                 technical engineering advances that have yielded massive
                 increases in the quantity of computational resources and
                 training data available to support statistical learning in deep
                 artificial neural networks. Although these quantitative
                 engineering innovations are important, here we show that the
                 latest advances in AI are not solely due to quantitative
                 increases in computing power but also qualitative changes in
                 how that computing power is deployed. These qualitative changes
                 have brought about a new type of computing that we call
                 neurocompositional computing. In neurocompositional computing,
                 neural networks exploit two scientific principles that
                 contemporary theory in cognitive science maintains are
                 simultaneously necessary to enable human-level cognition. The
                 Compositionality Principle asserts that encodings of complex
                 information are structures that are systematically composed
                 from simpler structured encodings. The Continuity Principle
                 states that the encoding and processing of information is
                 formalized with real numbers that vary continuously. These
                 principles have seemed irreconcilable until the recent
                 mathematical discovery that compositionality can be realized
                 not only through the traditional discrete methods of symbolic
                 computing, well developed in 20th-century AI, but also through
                 novel forms of continuous neural computing—neurocompositional
                 computing. The unprecedented progress of 21st-century AI has
                 resulted from the use of limited—first-generation—forms of
                 neurocompositional computing. We show that the new techniques
                 now being deployed in second-generation neurocompositional
                 computing create AI systems that are not only more robust and
                 accurate than current systems, but also more
                 comprehensible—making it possible to diagnose errors in, and
                 exert human control over, artificial neural networks through
                 interpretation of their internal states and direct intervention
                 upon those states. Note: This tutorial is intended for those
                 new to this topic, and does not assume familiarity with
                 cognitive science, AI, or deep learning. Appendices provide
                 more advanced material. Each figure, and the associated box
                 explaining it, provides an exposition, illustration, or further
                 details of a main point of the paper; in order to make these
                 figures relatively self-contained, it has sometimes been
                 necessary to repeat some material from the text. For a brief
                 introduction and additional development of some of this
                 material see “Neurocompositional computing: From the central
                 paradox of cognition to a new generation of ai systems”
                 (arXiv:2205.01128; to appear, AI Magazine)",
  month       =  may,
  year        =  2022
}

@ARTICLE{Spelke2007-uu,
  title    = "Core Knowledge",
  author   = "Spelke, Elizabeth S and Kinzler, Katherine D",
  journal  = "Developmental science",
  volume   =  10,
  number   =  1,
  pages    = "89--96",
  abstract = "Human cognition is founded, in part, on four systems for
              representing objects, actions, number, and space. It may be based,
              as well, on a fifth system for representing social partners. Each
              system has deep roots in human phylogeny and ontogeny, and it
              guides and shapes the mental lives of adults. Converging research
              on human infants, non-human primates, children and adults in
              diverse cultures can aid both understanding of these systems and
              attempts to overcome their limits.",
  year     =  2007,
  doi      = "10.1111/j.1467-7687.2007.00569.x",
  issn     = "1363-755X,1467-7687"
}

@INPROCEEDINGS{Stuhlmuller2010-ka,
  title     = "Learning Structured Generative Concepts",
  author    = "Stuhlmuller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D",
  publisher = "Cognitive Science Society",
  year      =  2010,
  isbn      =  9781617388903
}

@ARTICLE{Tenenbaum2011-ud,
  title    = "How to grow a mind: statistics, structure, and abstraction",
  author   = "Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and
              Goodman, Noah D",
  journal  = "Science",
  volume   =  331,
  number   =  6022,
  pages    = "1279--1285",
  abstract = "In coming to understand the world-in learning concepts, acquiring
              language, and grasping causal relations-our minds make inferences
              that appear to go far beyond the data available. How do we do it?
              This review describes recent approaches to reverse-engineering
              human learning and cognitive development and, in parallel,
              engineering more humanlike machine learning systems. Computational
              models that perform probabilistic inference over hierarchies of
              flexibly structured representations can address some of the
              deepest questions about the nature and origins of human thought:
              How does abstract knowledge guide learning and reasoning from
              sparse data? What forms does our knowledge take, across different
              domains and tasks? And how is that abstract knowledge itself
              acquired?",
  month    =  mar,
  year     =  2011,
  doi      = "10.1126/science.1192788",
  pmid     =  21393536,
  issn     = "0036-8075,1095-9203",
  language = "en"
}

@INCOLLECTION{Turing2009-sc,
  title     = "Computing {{Machinery}} and {{Intelligence}}",
  author    = "Turing, Alan M",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  booktitle = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  publisher = "Springer Netherlands",
  address   = "Dordrecht",
  pages     = "23--65",
  abstract  = "I propose to consider the question, “Can machines think?”♣ This
               should begin with definitions of the meaning of the terms
               “machine” and “think”. The definitions might be framed so as to
               reflect so far as possible the normal use of the words, but this
               attitude is dangerous. If the meaning of the words “machine” and
               “think” are to be found by examining how they are commonly used
               it is difficult to escape the conclusion that the meaning and the
               answer to the question, “Can machines think?” is to be sought in
               a statistical survey such as a Gallup poll.",
  year      =  2009,
  keywords  = "Computing Machinery,Digital Computer,Performance Capacity,Real
               Robot,Turing Machine",
  doi       = "10.1007/978-1-4020-6710-5\_3",
  isbn      =  9781402067105
}

@ARTICLE{Vaswani_undated-fb,
  title    = "Attention Is {{All}} You {{Need}}",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and
              Polosukhin, Illia",
  pages    =  11,
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks that include an encoder
              and a decoder. The best performing models also connect the encoder
              and decoder through an attention mechanism. We propose a new
              simple network architecture, the Transformer, based solely on
              attention mechanisms, dispensing with recurrence and convolutions
              entirely. Experiments on two machine translation tasks show these
              models to be superior in quality while being more parallelizable
              and requiring significantly less time to train. Our model achieves
              28.4 BLEU on the WMT 2014 Englishto-German translation task,
              improving over the existing best results, including ensembles, by
              over 2 BLEU. On the WMT 2014 English-to-French translation task,
              our model establishes a new single-model state-of-the-art BLEU
              score of 41.0 after training for 3.5 days on eight GPUs, a small
              fraction of the training costs of the best models from the
              literature."
}

@ARTICLE{Watkins1992-pk,
  title     = "{Q}-Learning",
  author    = "Watkins, Christopher Jch and Dayan, Peter",
  journal   = "Machine learning",
  publisher = "Springer",
  volume    =  8,
  number    = "3-4",
  pages     = "279--292",
  year      =  1992,
  issn      = "0885-6125"
}

@ARTICLE{Wellman_undated-aj,
  title  = "Cognitive {{Development}}: {{Foundational Theories}} of {{Core
            Domains}}",
  author = "Wellman, Henry M and Gelman, Susan A",
  pages  =  39
}

@INBOOK{2021-fc,
  title     = "Perceptron",
  author    = "{Wikipedia}",
  booktitle = "Wikipedia",
  abstract  = "In machine learning, the perceptron is an algorithm for
               supervised learning of binary classifiers. A binary classifier is
               a function which can decide whether or not an input, represented
               by a vector of numbers, belongs to some specific class. It is a
               type of linear classifier, i.e. a classification algorithm that
               makes its predictions based on a linear predictor function
               combining a set of weights with the feature vector.",
  month     =  aug,
  year      =  2021
}

@MISC{noauthor_undated-by,
  title        = "Readings in Cognitive Science : A Perspective from Psychology
                  and Artificial Intelligence | {{McGill University Library}}",
  howpublished = "\url{https://mcgill.on.worldcat.org/v2/oclc/555237322}",
  note         = "Accessed: 2021-9-17"
}

@MISC{noauthor_undated-xy,
  title        = "Efficient Inverse Graphics in Biological Face Processing |
                  {{Science Advances}}",
  howpublished = "\url{https://advances.sciencemag.org/content/6/10/eaax5979/tab-pdf?\_\_cf\_chl\_jschl\_tk\_\_=7edc4ca4dbf68389fcff61bcb4f9b2933065972f-1626234016-0-AUXGIGlyGqNMVHJhjRyQlw9DOb7\_Nsxa5\_\_OpC2K69M9D8q3ft1l8LC648BjbyZssjf1qXDRh9heKB23bAUqhMchMuBuTHxr1gBzgLl0YZz-fvX\_BbIfEdkuGqgS2wIQwcCLXUhX7Vuu9lepXXTiDRdqgoBR\_3HgolzgHQ6iOR88SOt\_HkomiuuG19xpy9eHwbtkXyxVeuqVct2S5FKj2c\_gF7JoxpOsDzMQ2ClPLRcttkLxGd5Y2NSg0hSEDqyY9X-MJK6yri2xtNvCQoUyFH2cOstnm8cMcp0PKaq91OAXSh\_A3uCIJLJF8Dvy5m58BYllfEUROmN2Gs2l0Mvt-pdW5ClIo4ZFzQ\_JlY8SX5yrdk3Nv7rREduZqg\_ZKj\_rSXsnQccxZkIUOnWnfk3HWWQJqVOfzh9HFpMYsr2jkFs-ljn7ZmKqdkoo5QRPi1XbSg}",
  note         = "Accessed: 2021-7-14"
}

@ARTICLE{Baars2007-hu,
  title    = "An Architectural Model of Conscious and Unconscious Brain
              Functions: {{Global Workspace Theory}} and {{IDA}}",
  author   = "Baars, Bernard J and Franklin, Stan",
  journal  = "Neural networks: the official journal of the International Neural
              Network Society",
  volume   =  20,
  number   =  9,
  pages    = "955--961",
  abstract = "While neural net models have been developed to a high degree of
              sophistication, they have some drawbacks at a more integrative,
              “architectural” level of analysis. We describe a “hybrid”
              cognitive architecture that is implementable in neuronal nets, and
              which has uniform brainlike features, including activation-passing
              and highly distributed “codelets,” implementable as small-scale
              neural nets. Empirically, this cognitive architecture accounts
              qualitatively for the data described by Baars’ Global Workspace
              Theory (GWT), and Franklin’s LIDA architecture, including
              state-of-the-art models of conscious contents in action-planning,
              Baddeley-style Working Memory, and working models of episodic and
              semantic longterm memory. These terms are defined both
              conceptually and empirically for the current theoretical domain.
              The resulting architecture meets four desirable goals for a
              unified theory of cognition: practical workability, autonomous
              agency, a plausible role for conscious cognition, and
              translatability into plausible neural terms. It also generates
              testable predictions, both empirical and computational.",
  series   = "Brain and Consciousness",
  month    =  nov,
  year     =  2007,
  keywords = "Cognitive architecture,Conscious cognition,Global workspace
              theory,LIDA architecture",
  doi      = "10.1016/j.neunet.2007.09.013",
  issn     = "0893-6080"
}
