@MISC{noauthor_undated-xy,
  title        = "Efficient Inverse Graphics in Biological Face Processing |
                  {{Science Advances}}",
  howpublished = "\url{https://advances.sciencemag.org/content/6/10/eaax5979/tab-pdf?__cf_chl_jschl_tk__=7edc4ca4dbf68389fcff61bcb4f9b2933065972f-1626234016-0-AUXGIGlyGqNMVHJhjRyQlw9DOb7_Nsxa5__OpC2K69M9D8q3ft1l8LC648BjbyZssjf1qXDRh9heKB23bAUqhMchMuBuTHxr1gBzgLl0YZz-fvX_BbIfEdkuGqgS2wIQwcCLXUhX7Vuu9lepXXTiDRdqgoBR_3HgolzgHQ6iOR88SOt_HkomiuuG19xpy9eHwbtkXyxVeuqVct2S5FKj2c_gF7JoxpOsDzMQ2ClPLRcttkLxGd5Y2NSg0hSEDqyY9X-MJK6yri2xtNvCQoUyFH2cOstnm8cMcp0PKaq91OAXSh_A3uCIJLJF8Dvy5m58BYllfEUROmN2Gs2l0Mvt-pdW5ClIo4ZFzQ_JlY8SX5yrdk3Nv7rREduZqg_ZKj_rSXsnQccxZkIUOnWnfk3HWWQJqVOfzh9HFpMYsr2jkFs-ljn7ZmKqdkoo5QRPi1XbSg}",
  note         = "Accessed: 2021-7-14",
  keywords     = "comp-cog-sci"
}

@UNPUBLISHED{Grosse2012-pf,
  title    = "Exploiting Compositionality to Explore a Large Space of Model
              Structures",
  author   = "Grosse, Roger and Salakhutdinov, Ruslan R and Freeman, William T
              and Tenenbaum, Joshua B",
  abstract = "The recent proliferation of richly structured probabilistic
              models raises the question of how to automatically determine an
              appropriate model for a dataset. We investigate this question for
              a space of matrix decomposition models which can express a
              variety of widely used models from unsupervised learning. To
              enable model selection, we organize these models into a
              context-free grammar which generates a wide variety of structures
              through the compositional application of a few simple rules. We
              use our grammar to generically and efficiently infer latent
              components and estimate predictive likelihood for nearly 2500
              structures using a small toolbox of reusable algorithms. Using a
              greedy search over our grammar, we automatically choose the
              decomposition structure from raw data by evaluating only a small
              fraction of all models. The proposed method typically finds the
              correct structure for synthetic data and backs off gracefully to
              simpler models under heavy noise. It learns sensible structures
              for datasets as diverse as image patches, motion capture, 20
              Questions, and U.S. Senate votes, all using exactly the same
              code.",
  month    =  oct,
  year     =  2012,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@UNPUBLISHED{Rezende2016-ge,
  title    = "{One-{{Shot} Generalization}} in {{Deep Generative Models}}",
  author   = "Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo
              and Gregor, Karol and Wierstra, Daan",
  abstract = "Humans have an impressive ability to reason about new concepts
              and experiences from just a single example. In particular, humans
              have an ability for one-shot generalization: an ability to
              encounter a new concept, understand its structure, and then be
              able to generate compelling alternative variations of the
              concept. We develop machine learning systems with this important
              capacity by developing new deep generative models, models that
              combine the representational power of deep learning with the
              inferential power of Bayesian reasoning. We develop a class of
              sequential generative models that are built on the principles of
              feedback and attention. These two characteristics lead to
              generative models that are among the state-of-the art in density
              estimation and image generation. We demonstrate the one-shot
              generalization ability of our models using three tasks:
              unconditional sampling, generating new exemplars of a given
              concept, and generating new exemplars of a family of concepts. In
              all cases our models are able to generate compelling and diverse
              samples---having seen new examples just once---providing an
              important class of general-purpose models for one-shot machine
              learning.",
  month    =  may,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;machine-learning"
}

@UNPUBLISHED{Lake2016-zm,
  title    = "Building {{Machines That Learn}} and {{Think Like People}}",
  author   = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and
              Gershman, Samuel J",
  abstract = "Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people.
              Many advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats
              humans in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we
              argue that these machines should (a) build causal models of the
              world that support explanation and understanding, rather than
              merely solving pattern recognition problems; (b) ground learning
              in intuitive theories of physics and psychology, to support and
              enrich the knowledge that is learned; and (c) harness
              compositionality and learning-to-learn to rapidly acquire and
              generalize knowledge to new tasks and situations. We suggest
              concrete challenges and promising routes towards these goals that
              can combine the strengths of recent neural network advances with
              more structured cognitive models.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,Statistics - Machine Learning;read;comp-cog-sci;l\&m
              final"
}

@ARTICLE{Munos2016-ba,
  title         = "Safe and Efficient {Off-Policy} Reinforcement Learning",
  author        = "Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and
                   Bellemare, Marc G",
  abstract      = "In this work, we take a fresh look at some old and new
                   algorithms for off-policy, return-based reinforcement
                   learning. Expressing these in a common form, we derive a
                   novel algorithm, Retrace($\lambda$), with three desired
                   properties: (1) it has low variance; (2) it safely uses
                   samples collected from any behaviour policy, whatever its
                   degree of ``off-policyness''; and (3) it is efficient as it
                   makes the best use of samples collected from near on-policy
                   behaviour policies. We analyze the contractive nature of the
                   related operator under both off-policy policy evaluation and
                   control settings and derive online sample-based algorithms.
                   We believe this is the first return-based off-policy control
                   algorithm converging a.s. to $Q^*$ without the GLIE
                   assumption (Greedy in the Limit with Infinite Exploration).
                   As a corollary, we prove the convergence of Watkins'
                   Q($\lambda$), which was an open problem since 1989. We
                   illustrate the benefits of Retrace($\lambda$) on a standard
                   suite of Atari 2600 games.",
  month         =  jun,
  year          =  2016,
  keywords      = "project 1;RL",
  archivePrefix = "arXiv",
  eprint        = "1606.02647",
  primaryClass  = "cs.LG",
  arxivid       = "1606.02647"
}

@UNPUBLISHED{Andrychowicz2016-uq,
  title    = "Learning to Learn by Gradient Descent by Gradient Descent",
  author   = "Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and
              Hoffman, Matthew W and Pfau, David and Schaul, Tom and
              Shillingford, Brendan and de Freitas, Nando",
  abstract = "The move from hand-designed features to learned features in
              machine learning has been wildly successful. In spite of this,
              optimization algorithms are still designed by hand. In this paper
              we show how the design of an optimization algorithm can be cast
              as a learning problem, allowing the algorithm to learn to exploit
              structure in the problems of interest in an automatic way. Our
              learned algorithms, implemented by LSTMs, outperform generic,
              hand-designed competitors on the tasks for which they are
              trained, and also generalize well to new tasks with similar
              structure. We demonstrate this on a number of tasks, including
              simple convex problems, training neural networks, and styling
              images with neural art.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Machine Learning,Computer Science - Neural and
              Evolutionary Computing;comp-cog-sci;machine-learning"
}

@UNPUBLISHED{Peterson2016-lj,
  title    = "Adapting {{Deep Network Features}} to {{Capture Psychological
              Representations}}",
  author   = "Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L",
  abstract = "Deep neural networks have become increasingly successful at
              solving classic perception problems such as object recognition,
              semantic segmentation, and scene understanding, often reaching or
              surpassing human-level accuracy. This success is due in part to
              the ability of DNNs to learn useful representations of
              high-dimensional inputs, a problem that humans must also solve.
              We examine the relationship between the representations learned
              by these networks and human psychological representations
              recovered from similarity judgments. We find that deep features
              learned in service of object classification account for a
              significant amount of the variance in human similarity judgments
              for a set of animal images. However, these features do not
              capture some qualitative distinctions that are a key part of
              human representations. To remedy this, we develop a method for
              adapting deep features to align with human similarity judgments,
              resulting in image representations that can potentially be used
              to extend the scope of psychological experiments.",
  month    =  aug,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science - Neural
              and Evolutionary Computing;read;comp-cog-sci;ccm2023"
}

@ARTICLE{Ghavamzadeh2016-fs,
  title         = "Bayesian Reinforcement Learning: A Survey",
  author        = "Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle
                   and Tamar, Aviv",
  abstract      = "Bayesian methods for machine learning have been widely
                   investigated, yielding principled methods for incorporating
                   prior information into inference algorithms. In this survey,
                   we provide an in-depth review of the role of Bayesian
                   methods for the reinforcement learning (RL) paradigm. The
                   major incentives for incorporating Bayesian reasoning in RL
                   are: 1) it provides an elegant approach to action-selection
                   (exploration/exploitation) as a function of the uncertainty
                   in learning; and 2) it provides a machinery to incorporate
                   prior knowledge into the algorithms. We first discuss models
                   and methods for Bayesian inference in the simple single-step
                   Bandit model. We then review the extensive recent literature
                   on Bayesian methods for model-based RL, where prior
                   information can be expressed on the parameters of the Markov
                   model. We also present Bayesian methods for model-free RL,
                   where priors are expressed over the value function or policy
                   class. The objective of the paper is to provide a
                   comprehensive survey on Bayesian RL algorithms and their
                   theoretical and empirical properties.",
  month         =  sep,
  year          =  2016,
  keywords      = "project 2",
  archivePrefix = "arXiv",
  eprint        = "1609.04436",
  primaryClass  = "cs.AI",
  arxivid       = "1609.04436",
  doi           = "10.1561/2200000049"
}

@ARTICLE{Chang2016-qi,
  title         = "A Compositional {Object-Based} Approach to Learning Physical
                   Dynamics",
  author        = "Chang, Michael B and Ullman, Tomer and Torralba, Antonio and
                   Tenenbaum, Joshua B",
  abstract      = "We present the Neural Physics Engine (NPE), a framework for
                   learning simulators of intuitive physics that naturally
                   generalize across variable object count and different scene
                   configurations. We propose a factorization of a physical
                   scene into composable object-based representations and a
                   neural network architecture whose compositional structure
                   factorizes object dynamics into pairwise interactions. Like
                   a symbolic physics engine, the NPE is endowed with generic
                   notions of objects and their interactions; realized as a
                   neural network, it can be trained via stochastic gradient
                   descent to adapt to specific object properties and dynamics
                   of different worlds. We evaluate the efficacy of our
                   approach on simple rigid body dynamics in two-dimensional
                   worlds. By comparing to less structured architectures, we
                   show that the NPE's compositional representation of the
                   structure in physical interactions improves its ability to
                   predict movement, generalize across variable object count
                   and different scene configurations, and infer latent
                   properties of objects such as mass.",
  month         =  dec,
  year          =  2016,
  keywords      = "skimmed;project 2",
  archivePrefix = "arXiv",
  eprint        = "1612.00341",
  primaryClass  = "cs.AI",
  arxivid       = "1612.00341"
}

@UNPUBLISHED{Darwiche2017-sr,
  title    = "{Human-{{Level} Intelligence}} or {{{Animal-Like} Abilities}}?",
  author   = "Darwiche, Adnan",
  abstract = "The vision systems of the eagle and the snake outperform
              everything that we can make in the laboratory, but snakes and
              eagles cannot build an eyeglass or a telescope or a microscope.
              (Judea Pearl)",
  month    =  jul,
  year     =  2017,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computers and Society,Computer Science - Machine
              Learning,Statistics - Machine Learning;comp-cog-sci"
}

@MISC{Andreas2017-kq,
  title         = "Learning with {{Latent Language}}",
  author        = "Andreas, Jacob and Klein, Dan and Levine, Sergey",
  abstract      = "The named concepts and compositional operators present in
                   natural language provide a rich source of information about
                   the kinds of abstractions humans use to navigate the world.
                   Can this linguistic background knowledge improve the
                   generality and efficiency of learned classifiers and control
                   policies? This paper aims to show that using the space of
                   natural language strings as a parameter space is an
                   effective way to capture natural task structure. In a
                   pretraining phase, we learn a language interpretation model
                   that transforms inputs (e.g. images) into outputs (e.g.
                   labels) given natural language descriptions. To learn a new
                   concept (e.g. a classifier), we search directly in the space
                   of descriptions to minimize the interpreter's loss on
                   training examples. Crucially, our models do not require
                   language data to learn these concepts: language is used only
                   in pretraining to impose structure on subsequent learning.
                   Results on image classification, text editing, and
                   reinforcement learning show that, in all settings, models
                   with a linguistic parameterization outperform those without.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:1711.00482",
  month         =  nov,
  year          =  2017,
  keywords      = "Computer Science - Computation and Language,Computer Science
                   - Neural and Evolutionary Computing;read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "1711.00482",
  primaryClass  = "cs",
  arxivid       = "1711.00482"
}

@ARTICLE{Valkov2018-sq,
  title         = "{HOUDINI}: Lifelong Learning as Program Synthesis",
  author        = "Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and
                   Sutton, Charles and Chaudhuri, Swarat",
  abstract      = "We present a neurosymbolic framework for the lifelong
                   learning of algorithmic tasks that mix perception and
                   procedural reasoning. Reusing high-level concepts across
                   domains and learning complex procedures are key challenges
                   in lifelong learning. We show that a program synthesis
                   approach that combines gradient descent with combinatorial
                   search over programs can be a more effective response to
                   these challenges than purely neural methods. Our framework,
                   called HOUDINI, represents neural networks as strongly
                   typed, differentiable functional programs that use symbolic
                   higher-order combinators to compose a library of neural
                   functions. Our learning algorithm consists of: (1) a
                   symbolic program synthesizer that performs a type-directed
                   search over parameterized programs, and decides on the
                   library functions to reuse, and the architectures to combine
                   them, while learning a sequence of tasks; and (2) a neural
                   module that trains these programs using stochastic gradient
                   descent. We evaluate HOUDINI on three benchmarks that
                   combine perception with the algorithmic tasks of counting,
                   summing, and shortest-path computation. Our experiments show
                   that HOUDINI transfers high-level concepts more effectively
                   than traditional transfer learning and progressive neural
                   networks, and that the typed representation of networks
                   significantly accelerates the search.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  eprint        = "1804.00218",
  primaryClass  = "cs.LG",
  arxivid       = "1804.00218"
}

@UNPUBLISHED{Lake2019-ng,
  title    = "Human {Few-Shot} Learning of Compositional Instructions",
  author   = "Lake, Brenden M and Linzen, Tal and Baroni, Marco",
  abstract = "People learn in fast and flexible ways that have not been
              emulated by machines. Once a person learns a new verb ``dax,'' he
              or she can effortlessly understand how to ``dax twice,'' ``walk
              and dax,'' or ``dax vigorously.'' There have been striking recent
              improvements in machine learning for natural language processing,
              yet the best algorithms require vast amounts of experience and
              struggle to generalize new concepts in compositional ways. To
              better understand these distinctively human abilities, we study
              the compositional skills of people through languagelike
              instruction learning tasks. Our results show that people can
              learn and use novel functional concepts from very few examples
              (few-shot learning), successfully applying familiar functions to
              novel inputs. People can also compose concepts in complex ways
              that go beyond the provided demonstrations. Two additional
              experiments examined the assumptions and inductive biases that
              people make when solving these tasks, revealing three biases:
              mutual exclusivity, one-to-one mappings, and iconic
              concatenation. We discuss the implications for cognitive modeling
              and the potential for building machines with more human-like
              language learning capabilities.",
  month    =  may,
  year     =  2019,
  keywords = "Computer Science - Computation and Language;comp-cog-sci"
}

@UNPUBLISHED{Mao2019-ht,
  title    = "The {{{Neuro-Symbolic} Concept Learner}}: {{Interpreting
              Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}",
  author   = "Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum,
              Joshua B and Wu, Jiajun",
  abstract = "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model
              that learns visual concepts, words, and semantic parsing of
              sentences without explicit supervision on any of them; instead,
              our model learns by simply looking at images and reading paired
              questions and answers. Our model builds an object-based scene
              representation and translates sentences into executable, symbolic
              programs. To bridge the learning of two modules, we use a
              neuro-symbolic reasoning module that executes these programs on
              the latent scene representation. Analogical to human concept
              learning, the perception module learns visual concepts based on
              the language description of the object being referred to.
              Meanwhile, the learned visual concepts facilitate learning new
              words and parsing new sentences. We use curriculum learning to
              guide the searching over the large compositional space of images
              and language. Extensive experiments demonstrate the accuracy and
              efficiency of our model on learning visual concepts, word
              representations, and semantic parsing of sentences. Further, our
              method allows easy generalization to new object attributes,
              compositions, language concepts, scenes and questions, and even
              new program domains. It also empowers applications including
              visual question answering and bidirectional image-text retrieval.",
  month    =  apr,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Computer Vision and
              Pattern Recognition,Computer Science - Machine
              Learning;skimmed;comp-cog-sci"
}

@UNPUBLISHED{Lake2019-dt,
  title    = "Compositional Generalization through Meta {Sequence-to-Sequence}
              Learning",
  author   = "Lake, Brenden M",
  abstract = "People can learn a new concept and use it compositionally,
              understanding how to ``blicket twice'' after learning how to
              ``blicket.'' In contrast, powerful sequence-tosequence (seq2seq)
              neural networks fail such tests of compositionality, especially
              when composing new concepts together with existing concepts. In
              this paper, I show how memory-augmented neural networks can be
              trained to generalize compositionally through meta seq2seq
              learning. In this approach, models train on a series of seq2seq
              problems to acquire the compositional skills needed to solve new
              seq2seq problems. Meta se2seq learning solves several of the SCAN
              tests for compositional learning and can learn to apply implicit
              rules to variables.",
  month    =  oct,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Machine
              Learning;comp-cog-sci"
}

@ARTICLE{Bakhtin2019-hv,
  title         = "{PHYRE}: A New Benchmark for Physical Reasoning",
  author        = "Bakhtin, Anton and van der Maaten, Laurens and Johnson,
                   Justin and Gustafson, Laura and Girshick, Ross",
  abstract      = "Understanding and reasoning about physics is an important
                   ability of intelligent agents. We develop the PHYRE
                   benchmark for physical reasoning that contains a set of
                   simple classical mechanics puzzles in a 2D physical
                   environment. The benchmark is designed to encourage the
                   development of learning algorithms that are sample-efficient
                   and generalize well across puzzles. We test several modern
                   learning algorithms on PHYRE and find that these algorithms
                   fall short in solving the puzzles efficiently. We expect
                   that PHYRE will encourage the development of novel
                   sample-efficient agents that learn efficient but useful
                   models of physics. For code and to play PHYRE for yourself,
                   please visit https://player.phyre.ai.",
  month         =  aug,
  year          =  2019,
  keywords      = "read;project 2",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1908.05656",
  primaryClass  = "cs.LG",
  arxivid       = "1908.05656"
}

@UNPUBLISHED{Goyal2020-od,
  title    = "Recurrent {{Independent Mechanisms}}",
  author   = "Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani,
              Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf,
              Bernhard",
  abstract = "Learning modular structures which reflect the dynamics of the
              environment can lead to better generalization and robustness to
              changes which only affect a few of the underlying causes. We
              propose Recurrent Independent Mechanisms (RIMs), a new recurrent
              architecture in which multiple groups of recurrent cells operate
              with nearly independent transition dynamics, communicate only
              sparingly through the bottleneck of attention, and are only
              updated at time steps where they are most relevant. We show that
              this leads to specialization amongst the RIMs, which in turn
              allows for dramatically improved generalization on tasks where
              some factors of variation differ systematically between training
              and evaluation.",
  month    =  nov,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Yi2019-lo,
  title         = "{CLEVRER}: {CoLlision} Events for Video {REpresentation} and
                   Reasoning",
  author        = "Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet
                   and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B",
  abstract      = "The ability to reason about temporal and causal events from
                   videos lies at the core of human intelligence. Most video
                   reasoning benchmarks, however, focus on pattern recognition
                   from complex visual and language input, instead of on causal
                   structure. We study the complementary problem, exploring the
                   temporal and causal structures behind videos of objects with
                   simple visual appearance. To this end, we introduce the
                   CoLlision Events for Video REpresentation and Reasoning
                   (CLEVRER), a diagnostic video dataset for systematic
                   evaluation of computational models on a wide range of
                   reasoning tasks. Motivated by the theory of human casual
                   judgment, CLEVRER includes four types of questions:
                   descriptive (e.g., ``what color''), explanatory (``what is
                   responsible for''), predictive (``what will happen next''),
                   and counterfactual (``what if''). We evaluate various
                   state-of-the-art models for visual reasoning on our
                   benchmark. While these models thrive on the perception-based
                   task (descriptive), they perform poorly on the causal tasks
                   (explanatory, predictive and counterfactual), suggesting
                   that a principled approach for causal reasoning should
                   incorporate the capability of both perceiving complex visual
                   and language inputs, and understanding the underlying
                   dynamics and causal relations. We also study an oracle model
                   that explicitly combines these components via symbolic
                   representations.",
  month         =  oct,
  year          =  2019,
  keywords      = "project 2",
  archivePrefix = "arXiv",
  eprint        = "1910.01442",
  primaryClass  = "cs.CV",
  arxivid       = "1910.01442"
}

@ARTICLE{Raffel2019-jw,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   {Text-to-Text} Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of
                   transfer learning techniques for NLP by introducing a
                   unified framework that converts all text-based language
                   problems into a text-to-text format. Our systematic study
                   compares pre-training objectives, architectures, unlabeled
                   data sets, transfer approaches, and other factors on dozens
                   of language understanding tasks. By combining the insights
                   from our exploration with scale and our new ``Colossal Clean
                   Crawled Corpus'', we achieve state-of-the-art results on
                   many benchmarks covering summarization, question answering,
                   text classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "1910.10683",
  primaryClass  = "cs.LG",
  arxivid       = "1910.10683"
}

@MISC{Chollet2019-di,
  title         = "On the {{Measure}} of {{Intelligence}}",
  author        = "Chollet, Fran{\c c}ois",
  abstract      = "To make deliberate progress towards more intelligent and
                   more human-like artificial systems, we need to be following
                   an appropriate feedback signal: we need to be able to define
                   and evaluate intelligence in a way that enables comparisons
                   between two systems, as well as comparisons with humans.
                   Over the past hundred years, there has been an abundance of
                   attempts to define and measure intelligence, across both the
                   fields of psychology and AI. We summarize and critically
                   assess these definitions and evaluation approaches, while
                   making apparent the two historical conceptions of
                   intelligence that have implicitly guided them. We note that
                   in practice, the contemporary AI community still gravitates
                   towards benchmarking intelligence by comparing the skill
                   exhibited by AIs and humans at specific tasks, such as board
                   games and video games. We argue that solely measuring skill
                   at any given task falls short of measuring intelligence,
                   because skill is heavily modulated by prior knowledge and
                   experience: unlimited priors or unlimited training data
                   allow experimenters to ``buy'' arbitrary levels of skills
                   for a system, in a way that masks the system's own
                   generalization power. We then articulate a new formal
                   definition of intelligence based on Algorithmic Information
                   Theory, describing intelligence as skill-acquisition
                   efficiency and highlighting the concepts of scope,
                   generalization difficulty, priors, and experience, as
                   critical pieces to be accounted for in characterizing
                   intelligent systems. Using this definition, we propose a set
                   of guidelines for what a general AI benchmark should look
                   like. Finally, we present a new benchmark closely following
                   these guidelines, the Abstraction and Reasoning Corpus
                   (ARC), built upon an explicit set of priors designed to be
                   as close as possible to innate human priors. We argue that
                   ARC can be used to measure a human-like form of general
                   fluid intelligence and that it enables fair general
                   intelligence comparisons between AI systems and humans.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:1911.01547",
  month         =  nov,
  year          =  2019,
  keywords      = "Computer Science - Artificial
                   Intelligence;read;comp-cog-sci;ARC Project",
  archivePrefix = "arXiv",
  eprint        = "1911.01547",
  primaryClass  = "cs",
  arxivid       = "1911.01547"
}

@ARTICLE{Gordon2020-qy,
  title         = "Compressing {BERT}: Studying the Effects of Weight Pruning
                   on Transfer Learning",
  author        = "Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas",
  abstract      = "Pre-trained universal feature extractors, such as BERT for
                   natural language processing and VGG for computer vision,
                   have become effective methods for improving deep learning
                   models without requiring more labeled data. While effective,
                   feature extractors like BERT may be prohibitively large for
                   some deployment scenarios. We explore weight pruning for
                   BERT and ask: how does compression during pre-training
                   affect transfer learning? We find that pruning affects
                   transfer learning in three broad regimes. Low levels of
                   pruning (30-40\%) do not affect pre-training loss or
                   transfer to downstream tasks at all. Medium levels of
                   pruning increase the pre-training loss and prevent useful
                   pre-training information from being transferred to
                   downstream tasks. High levels of pruning additionally
                   prevent models from fitting downstream datasets, leading to
                   further degradation. Finally, we observe that fine-tuning
                   BERT on a specific task does not improve its prunability. We
                   conclude that BERT can be pruned once during pre-training
                   rather than separately for each task without affecting
                   performance.",
  month         =  feb,
  year          =  2020,
  keywords      = "read;nlp;pruning-llm",
  archivePrefix = "arXiv",
  eprint        = "2002.08307",
  primaryClass  = "cs.CL",
  arxivid       = "2002.08307"
}

@ARTICLE{Landgren2020-ri,
  title         = "Distributed Cooperative Decision Making in Multi-agent
                   Multi-armed Bandits",
  author        = "Landgren, Peter and Srivastava, Vaibhav and Leonard, Naomi
                   Ehrich",
  abstract      = "We study a distributed decision-making problem in which
                   multiple agents face the same multi-armed bandit (MAB), and
                   each agent makes sequential choices among arms to maximize
                   its own individual reward. The agents cooperate by sharing
                   their estimates over a fixed communication graph. We
                   consider an unconstrained reward model in which two or more
                   agents can choose the same arm and collect independent
                   rewards. And we consider a constrained reward model in which
                   agents that choose the same arm at the same time receive no
                   reward. We design a dynamic, consensus-based, distributed
                   estimation algorithm for cooperative estimation of mean
                   rewards at each arm. We leverage the estimates from this
                   algorithm to develop two distributed algorithms: coop-UCB2
                   and coop-UCB2-selective-learning, for the unconstrained and
                   constrained reward models, respectively. We show that both
                   algorithms achieve group performance close to the
                   performance of a centralized fusion center. Further, we
                   investigate the influence of the communication graph
                   structure on performance. We propose a novel graph
                   explore-exploit index that predicts the relative performance
                   of groups in terms of the communication graph, and we
                   propose a novel nodal explore-exploit centrality index that
                   predicts the relative performance of agents in terms of the
                   agent locations in the communication graph.",
  month         =  mar,
  year          =  2020,
  keywords      = "ccm-project",
  archivePrefix = "arXiv",
  eprint        = "2003.01312",
  primaryClass  = "math.OC",
  arxivid       = "2003.01312"
}

@MISC{De_Raedt2020-hk,
  title         = "From {{Statistical Relational}} to {{{Neuro-Symbolic}
                   Artificial Intelligence}}",
  author        = "De Raedt, Luc and Duman{\v c}i{\'c}, Sebastijan and
                   Manhaeve, Robin and Marra, Giuseppe",
  abstract      = "Neuro-symbolic and statistical relational artificial
                   intelligence both integrate frameworks for learning with
                   logical reasoning. This survey identifies several parallels
                   across seven different dimensions between these two fields.
                   These cannot only be used to characterize and position
                   neuro-symbolic artificial intelligence approaches but also
                   to identify a number of directions for further research.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2003.08316",
  month         =  mar,
  year          =  2020,
  keywords      = "Computer Science - Artificial Intelligence;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2003.08316",
  primaryClass  = "cs",
  arxivid       = "2003.08316",
  doi           = "10.48550/arXiv.2003.08316"
}

@ARTICLE{Ribeiro2020-xl,
  title         = "Beyond Accuracy: Behavioral Testing of {NLP} models with
                   {CheckList}",
  author        = "Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos
                   and Singh, Sameer",
  abstract      = "Although measuring held-out accuracy has been the primary
                   approach to evaluate generalization, it often overestimates
                   the performance of NLP models, while alternative approaches
                   for evaluating models either focus on individual tasks or on
                   specific behaviors. Inspired by principles of behavioral
                   testing in software engineering, we introduce CheckList, a
                   task-agnostic methodology for testing NLP models. CheckList
                   includes a matrix of general linguistic capabilities and
                   test types that facilitate comprehensive test ideation, as
                   well as a software tool to generate a large and diverse
                   number of test cases quickly. We illustrate the utility of
                   CheckList with tests for three tasks, identifying critical
                   failures in both commercial and state-of-art models. In a
                   user study, a team responsible for a commercial sentiment
                   analysis model found new and actionable bugs in an
                   extensively tested model. In another user study, NLP
                   practitioners with CheckList created twice as many tests,
                   and found almost three times as many bugs as users without
                   it.",
  month         =  may,
  year          =  2020,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "2005.04118",
  primaryClass  = "cs.CL",
  arxivid       = "2005.04118"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lynch2020-pj,
  title     = "Language conditioned imitation learning over unstructured data",
  author    = "Lynch, C and Sermanet, P",
  abstract  = "Natural language is perhaps the most flexible and intuitive way
               for humans to communicate tasks to a robot. Prior work in
               imitation learning typically requires each task be specified
               with â€¦",
  journal   = "arXiv preprint arXiv:2005.07648",
  publisher = "arxiv.org",
  year      =  2020,
  keywords  = "ARC Project",
  arxivid   = "2005.07648"
}

@ARTICLE{Brown2020-gd,
  title         = "Language Models are {Few-Shot} Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",
  abstract      = "Recent work has demonstrated substantial gains on many NLP
                   tasks and benchmarks by pre-training on a large corpus of
                   text followed by fine-tuning on a specific task. While
                   typically task-agnostic in architecture, this method still
                   requires task-specific fine-tuning datasets of thousands or
                   tens of thousands of examples. By contrast, humans can
                   generally perform a new language task from only a few
                   examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we
                   show that scaling up language models greatly improves
                   task-agnostic, few-shot performance, sometimes even reaching
                   competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive
                   language model with 175 billion parameters, 10x more than
                   any previous non-sparse language model, and test its
                   performance in the few-shot setting. For all tasks, GPT-3 is
                   applied without any gradient updates or fine-tuning, with
                   tasks and few-shot demonstrations specified purely via text
                   interaction with the model. GPT-3 achieves strong
                   performance on many NLP datasets, including translation,
                   question-answering, and cloze tasks, as well as several
                   tasks that require on-the-fly reasoning or domain
                   adaptation, such as unscrambling words, using a novel word
                   in a sentence, or performing 3-digit arithmetic. At the same
                   time, we also identify some datasets where GPT-3's few-shot
                   learning still struggles, as well as some datasets where
                   GPT-3 faces methodological issues related to training on
                   large web corpora. Finally, we find that GPT-3 can generate
                   samples of news articles which human evaluators have
                   difficulty distinguishing from articles written by humans.
                   We discuss broader societal impacts of this finding and of
                   GPT-3 in general.",
  month         =  may,
  year          =  2020,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "2005.14165",
  primaryClass  = "cs.CL",
  arxivid       = "2005.14165"
}

@ARTICLE{Hasani2020-ve,
  title         = "Liquid Time-constant Networks",
  author        = "Hasani, Ramin and Lechner, Mathias and Amini, Alexander and
                   Rus, Daniela and Grosu, Radu",
  abstract      = "We introduce a new class of time-continuous recurrent neural
                   network models. Instead of declaring a learning system's
                   dynamics by implicit nonlinearities, we construct networks
                   of linear first-order dynamical systems modulated via
                   nonlinear interlinked gates. The resulting models represent
                   dynamical systems with varying (i.e., liquid) time-constants
                   coupled to their hidden state, with outputs being computed
                   by numerical differential equation solvers. These neural
                   networks exhibit stable and bounded behavior, yield superior
                   expressivity within the family of neural ordinary
                   differential equations, and give rise to improved
                   performance on time-series prediction tasks. To demonstrate
                   these properties, we first take a theoretical approach to
                   find bounds over their dynamics and compute their expressive
                   power by the trajectory length measure in latent trajectory
                   space. We then conduct a series of time-series prediction
                   experiments to manifest the approximation capability of
                   Liquid Time-Constant Networks (LTCs) compared to classical
                   and modern RNNs. Code and data are available at
                   https://github.com/raminmh/liquid\_time\_constant\_networks",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  eprint        = "2006.04439",
  primaryClass  = "cs.LG",
  arxivid       = "2006.04439"
}

@MISC{Ellis2020-xn,
  title         = "{{{DreamCoder}}}: {{Growing}} Generalizable, Interpretable
                   Knowledge with {Wake-Sleep} {{Bayesian}} Program Learning",
  author        = "Ellis, Kevin and Wong, Catherine and Nye, Maxwell and
                   Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and
                   Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua
                   B",
  abstract      = "Expert problem-solving is driven by powerful languages for
                   thinking about problems and their solutions. Acquiring
                   expertise means learning these languages -- systems of
                   concepts, alongside the skills to use them. We present
                   DreamCoder, a system that learns to solve problems by
                   writing programs. It builds expertise by creating
                   programming languages for expressing domain concepts,
                   together with neural networks to guide the search for
                   programs within these languages. A ``wake-sleep'' learning
                   algorithm alternately extends the language with new symbolic
                   abstractions and trains the neural network on imagined and
                   replayed problems. DreamCoder solves both classic inductive
                   programming tasks and creative tasks such as drawing
                   pictures and building scenes. It rediscovers the basics of
                   modern functional programming, vector algebra and classical
                   physics, including Newton's and Coulomb's laws. Concepts are
                   built compositionally from those learned earlier, yielding
                   multi-layered symbolic representations that are
                   interpretable and transferrable to new tasks, while still
                   growing scalably and flexibly with experience.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2006.08381",
  month         =  jun,
  year          =  2020,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning;read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2006.08381",
  primaryClass  = "cs",
  arxivid       = "2006.08381"
}

@UNPUBLISHED{Feinman2021-mh,
  title    = "Learning {{{Task-General} Representations}} with {{Generative
              {Neuro-Symbolic} Modeling}}",
  author   = "Feinman, Reuben and Lake, Brenden M",
  abstract = "People can learn rich, general-purpose conceptual representations
              from only raw perceptual inputs. Current machine learning
              approaches fall well short of these human standards, although
              different modeling traditions often have complementary strengths.
              Symbolic models can capture the compositional and causal
              knowledge that enables flexible generalization, but they struggle
              to learn from raw inputs, relying on strong abstractions and
              simplifying assumptions. Neural network models can learn directly
              from raw data, but they struggle to capture compositional and
              causal structure and typically must retrain to tackle new tasks.
              We bring together these two traditions to learn generative models
              of concepts that capture rich compositional and causal structure,
              while learning from raw data. We develop a generative
              neuro-symbolic (GNS) model of handwritten character concepts that
              uses the control flow of a probabilistic program, coupled with
              symbolic stroke primitives and a symbolic image renderer, to
              represent the causal and compositional processes by which
              characters are formed. The distributions of parts (strokes), and
              correlations between parts, are modeled with neural network
              subroutines, allowing the model to learn directly from raw data
              and express nonparametric statistical relationships. We apply our
              model to the Omniglot challenge of human-level concept learning,
              using a background set of alphabets to learn an expressive prior
              distribution over character drawings. In a subsequent evaluation,
              our GNS model uses probabilistic inference to learn rich
              conceptual representations from a single training image that
              generalize to 4 unique tasks, succeeding where previous work has
              fallen short.",
  month    =  jan,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Thomas_McCoy2020-zy,
  title         = "Universal linguistic inductive biases via meta-learning",
  author        = "Thomas McCoy, R and Grant, Erin and Smolensky, Paul and
                   Griffiths, Thomas L and Linzen, Tal",
  abstract      = "How do learners acquire languages from the limited data
                   available to them? This process must involve some inductive
                   biases - factors that affect how a learner generalizes - but
                   it is unclear which inductive biases can explain observed
                   patterns in language acquisition. To facilitate
                   computational modeling aimed at addressing this question, we
                   introduce a framework for giving particular linguistic
                   inductive biases to a neural network model; such a model can
                   then be used to empirically explore the effects of those
                   inductive biases. This framework disentangles universal
                   inductive biases, which are encoded in the initial values of
                   a neural network's parameters, from non-universal factors,
                   which the neural network must learn from data in a given
                   language. The initial state that encodes the inductive
                   biases is found with meta-learning, a technique through
                   which a model discovers how to acquire new languages more
                   easily via exposure to many possible languages. By
                   controlling the properties of the languages that are used
                   during meta-learning, we can control the inductive biases
                   that meta-learning imparts. We demonstrate this framework
                   with a case study based on syllable structure. First, we
                   specify the inductive biases that we intend to give our
                   model, and then we translate those inductive biases into a
                   space of languages from which a model can meta-learn.
                   Finally, using existing analysis techniques, we verify that
                   our approach has imparted the linguistic inductive biases
                   that it was intended to impart.",
  month         =  jun,
  year          =  2020,
  keywords      = "read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2006.16324",
  primaryClass  = "cs.CL",
  arxivid       = "2006.16324"
}

@ARTICLE{Shridhar2020-qs,
  title         = "{ALFWorld}: Aligning Text and Embodied Environments for
                   Interactive Learning",
  author        = "Shridhar, Mohit and Yuan, Xingdi and C{\^o}t{\'e},
                   Marc-Alexandre and Bisk, Yonatan and Trischler, Adam and
                   Hausknecht, Matthew",
  abstract      = "Given a simple request like Put a washed apple in the
                   kitchen fridge, humans can reason in purely abstract terms
                   by imagining action sequences and scoring their likelihood
                   of success, prototypicality, and efficiency, all without
                   moving a muscle. Once we see the kitchen in question, we can
                   update our abstract plans to fit the scene. Embodied agents
                   require the same abilities, but existing work does not yet
                   provide the infrastructure necessary for both reasoning
                   abstractly and executing concretely. We address this
                   limitation by introducing ALFWorld, a simulator that enables
                   agents to learn abstract, text based policies in TextWorld
                   (C\textbackslash^ot\textbackslash'e et al., 2018) and then
                   execute goals from the ALFRED benchmark (Shridhar et al.,
                   2020) in a rich visual environment. ALFWorld enables the
                   creation of a new BUTLER agent whose abstract knowledge,
                   learned in TextWorld, corresponds directly to concrete,
                   visually grounded actions. In turn, as we demonstrate
                   empirically, this fosters better agent generalization than
                   training only in the visually grounded environment. BUTLER's
                   simple, modular design factors the problem to allow
                   researchers to focus on models for improving every piece of
                   the pipeline (language understanding, planning, navigation,
                   and visual scene understanding).",
  month         =  oct,
  year          =  2020,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2010.03768",
  primaryClass  = "cs.CL",
  arxivid       = "2010.03768"
}

@ARTICLE{Mikulik2020-iu,
  title         = "Meta-trained agents implement Bayes-optimal agents",
  author        = "Mikulik, Vladimir and Del{\'e}tang, Gr{\'e}goire and
                   McGrath, Tom and Genewein, Tim and Martic, Miljan and Legg,
                   Shane and Ortega, Pedro A",
  abstract      = "Memory-based meta-learning is a powerful technique to build
                   agents that adapt fast to any task within a target
                   distribution. A previous theoretical study has argued that
                   this remarkable performance is because the meta-training
                   protocol incentivises agents to behave Bayes-optimally. We
                   empirically investigate this claim on a number of prediction
                   and bandit tasks. Inspired by ideas from theoretical
                   computer science, we show that meta-learned and
                   Bayes-optimal agents not only behave alike, but they even
                   share a similar computational structure, in the sense that
                   one agent system can approximately simulate the other.
                   Furthermore, we show that Bayes-optimal agents are fixed
                   points of the meta-learning dynamics. Our results suggest
                   that memory-based meta-learning might serve as a general
                   technique for numerically approximating Bayes-optimal agents
                   - that is, even for task distributions for which we
                   currently don't possess tractable models.",
  month         =  oct,
  year          =  2020,
  keywords      = "project 1;RL",
  archivePrefix = "arXiv",
  eprint        = "2010.11223",
  primaryClass  = "cs.AI",
  arxivid       = "2010.11223"
}

@MISC{Goyal2021-ki,
  title         = "Inductive {{Biases}} for {{Deep Learning}} of
                   {{{Higher-Level} Cognition}}",
  author        = "Goyal, Anirudh and Bengio, Yoshua",
  abstract      = "A fascinating hypothesis is that human and animal
                   intelligence could be explained by a few principles (rather
                   than an encyclopedic list of heuristics). If that hypothesis
                   was correct, we could more easily both understand our own
                   intelligence and build intelligent machines. Just like in
                   physics, the principles themselves would not be sufficient
                   to predict the behavior of complex systems like brains, and
                   substantial computation might be needed to simulate
                   human-like intelligence. This hypothesis would suggest that
                   studying the kind of inductive biases that humans and
                   animals exploit could help both clarify these principles and
                   provide inspiration for AI research and neuroscience
                   theories. Deep learning already exploits several key
                   inductive biases, and this work considers a larger list,
                   focusing on those which concern mostly higher-level and
                   sequential conscious processing. The objective of clarifying
                   these particular principles is that they could potentially
                   help us build AI systems benefiting from humans' abilities
                   in terms of flexible out-of-distribution and systematic
                   generalization, which is currently an area where a large gap
                   exists between state-of-the-art machine learning and human
                   intelligence.",
  journal       = "arXiv [cs, stat]",
  publisher     = "arXiv",
  number        = "arXiv:2011.15091",
  month         =  feb,
  year          =  2021,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning,Statistics - Machine
                   Learning;skimmed;comp-cog-sci;machine-learning;l\&m final",
  archivePrefix = "arXiv",
  eprint        = "2011.15091",
  primaryClass  = "cs, stat",
  arxivid       = "2011.15091"
}

@UNPUBLISHED{Greff2020-gr,
  title    = "On the {{Binding Problem}} in {{Artificial Neural Networks}}",
  author   = "Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber,
              J{\"u}rgen",
  abstract = "Contemporary neural networks still fall short of human-level
              generalization, which extends far beyond our direct experiences.
              In this paper, we argue that the underlying cause for this
              shortcoming is their inability to dynamically and flexibly bind
              information that is distributed throughout the network. This
              binding problem affects their capacity to acquire a compositional
              understanding of the world in terms of symbol-like entities (like
              objects), which is crucial for generalizing in predictable and
              systematic ways. To address this issue, we propose a unifying
              framework that revolves around forming meaningful entities from
              unstructured sensory inputs (segregation), maintaining this
              separation of information at a representational level
              (representation), and using these entities to construct new
              inferences, predictions, and behaviors (composition). Our
              analysis draws inspiration from a wealth of research in
              neuroscience and cognitive psychology, and surveys relevant
              mechanisms from the machine learning literature, to help identify
              a combination of inductive biases that allow symbolic information
              processing to emerge naturally in neural networks. We believe
              that a compositional approach to AI, in terms of grounded
              symbol-like representations, is of fundamental importance for
              realizing human-level generalization, and we hope that this paper
              may contribute towards that goal as a reference and inspiration.",
  month    =  dec,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,I.2.6;comp-cog-sci"
}

@ARTICLE{Gandhi2021-hw,
  title         = "Baby {{Intuitions Benchmark}} ({{{BIB}})}: {{Discerning}}
                   the Goals, Preferences, and Actions of Others",
  author        = "Gandhi, Kanishk and Stojnic, Gala and Lake, Brenden M and
                   Dillon, Moira R",
  journal       = "CoRR",
  volume        = "abs/2102.11938",
  year          =  2021,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2102.11938",
  arxivid       = "2102.11938"
}

@UNPUBLISHED{Radford2021-jz,
  title    = "Learning {{Transferable Visual Models From Natural Language
              Supervision}}",
  author   = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
              Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
              and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
              Krueger, Gretchen and Sutskever, Ilya",
  abstract = "State-of-the-art computer vision systems are trained to predict a
              fixed set of predetermined object categories. This restricted
              form of supervision limits their generality and usability since
              additional labeled data is needed to specify any other visual
              concept. Learning directly from raw text about images is a
              promising alternative which leverages a much broader source of
              supervision. We demonstrate that the simple pre-training task of
              predicting which caption goes with which image is an efficient
              and scalable way to learn SOTA image representations from scratch
              on a dataset of 400 million (image, text) pairs collected from
              the internet. After pre-training, natural language is used to
              reference learned visual concepts (or describe new ones) enabling
              zero-shot transfer of the model to downstream tasks. We study the
              performance of this approach by benchmarking on over 30 different
              existing computer vision datasets, spanning tasks such as OCR,
              action recognition in videos, geo-localization, and many types of
              fine-grained object classification. The model transfers
              non-trivially to most tasks and is often competitive with a fully
              supervised baseline without the need for any dataset specific
              training. For instance, we match the accuracy of the original
              ResNet-50 on ImageNet zero-shot without needing to use any of the
              1.28 million training examples it was trained on. We release our
              code and pre-trained model weights at
              https://github.com/OpenAI/CLIP.",
  month    =  feb,
  year     =  2021,
  keywords = "Computer Science - Computer Vision and Pattern
              Recognition,Computer Science - Machine
              Learning;skimmed;machine-learning;compling-cogsci2023;nlp"
}

@ARTICLE{Goyal2021-dr,
  title         = "Neural Production Systems: Learning {Rule-Governed} Visual
                   Dynamics",
  author        = "Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and
                   Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas
                   and Mozer, Michael and Bengio, Yoshua",
  abstract      = "Visual environments are structured, consisting of distinct
                   objects or entities. These entities have properties -- both
                   visible and latent -- that determine the manner in which
                   they interact with one another. To partition images into
                   entities, deep-learning researchers have proposed structural
                   inductive biases such as slot-based architectures. To model
                   interactions among entities, equivariant graph neural nets
                   (GNNs) are used, but these are not particularly well suited
                   to the task for two reasons. First, GNNs do not predispose
                   interactions to be sparse, as relationships among
                   independent entities are likely to be. Second, GNNs do not
                   factorize knowledge about interactions in an
                   entity-conditional manner. As an alternative, we take
                   inspiration from cognitive science and resurrect a classic
                   approach, production systems, which consist of a set of rule
                   templates that are applied by binding placeholder variables
                   in the rules to specific entities. Rules are scored on their
                   match to entities, and the best fitting rules are applied to
                   update entity properties. In a series of experiments, we
                   demonstrate that this architecture achieves a flexible,
                   dynamic flow of control and serves to factorize
                   entity-specific and rule-based information. This
                   disentangling of knowledge achieves robust future-state
                   prediction in rich visual environments, outperforming
                   state-of-the-art methods using GNNs, and allows for the
                   extrapolation from simple (few object) environments to more
                   complex environments.",
  month         =  mar,
  year          =  2021,
  keywords      = "compling-cogsci2023;project 1",
  archivePrefix = "arXiv",
  eprint        = "2103.01937",
  primaryClass  = "cs.AI",
  arxivid       = "2103.01937"
}

@ARTICLE{Lu2021-on,
  title         = "Pretrained Transformers as Universal Computation Engines",
  author        = "Lu, Kevin and Grover, Aditya and Abbeel, Pieter and
                   Mordatch, Igor",
  abstract      = "We investigate the capability of a transformer pretrained on
                   natural language to generalize to other modalities with
                   minimal finetuning -- in particular, without finetuning of
                   the self-attention and feedforward layers of the residual
                   blocks. We consider such a model, which we call a Frozen
                   Pretrained Transformer (FPT), and study finetuning it on a
                   variety of sequence classification tasks spanning numerical
                   computation, vision, and protein fold prediction. In
                   contrast to prior works which investigate finetuning on the
                   same modality as the pretraining dataset, we show that
                   pretraining on natural language can improve performance and
                   compute efficiency on non-language downstream tasks.
                   Additionally, we perform an analysis of the architecture,
                   comparing the performance of a random initialized
                   transformer to a random LSTM. Combining the two insights, we
                   find language-pretrained transformers can obtain strong
                   performance on a variety of non-language tasks.",
  month         =  mar,
  year          =  2021,
  keywords      = "ARC Project;comp-cog-sci;machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2103.05247",
  primaryClass  = "cs.LG",
  arxivid       = "2103.05247"
}

@UNPUBLISHED{Johnson2021-mw,
  title    = "Fast and Flexible: {{Human}} Program Induction in Abstract
              Reasoning Tasks",
  author   = "Johnson, Aysja and Vong, Wai Keen and Lake, Brenden M and
              Gureckis, Todd M",
  abstract = "The Abstraction and Reasoning Corpus (ARC) is a challenging
              program induction dataset that was recently proposed by Chollet
              (2019). Here, we report the first set of results collected from a
              behavioral study of humans solving a subset of tasks from ARC (40
              out of 1000). Although this subset of tasks contains considerable
              variation, our results showed that humans were able to infer the
              underlying program and generate the correct test output for a
              novel test input example, with an average of 80\% of tasks solved
              per participant, and with 65\% of tasks being solved by more than
              80\% of participants. Additionally, we find interesting patterns
              of behavioral consistency and variability within the action
              sequences during the generation process, the natural language
              descriptions to describe the transformations for each task, and
              the errors people made. Our findings suggest that people can
              quickly and reliably determine the relevant features and
              properties of a task to compose a correct solution. Future
              modeling work could incorporate these findings, potentially by
              connecting the natural language descriptions we collected here to
              the underlying semantics of ARC.",
  month    =  mar,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Human-Computer Interaction,Computer Science - Machine
              Learning;read;comp-cog-sci;ARC Project"
}

@ARTICLE{Hessel2021-vm,
  title         = "Muesli: Combining Improvements in Policy Optimization",
  author        = "Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez,
                   Arthur and Schmitt, Simon and Sifre, Laurent and Weber,
                   Theophane and Silver, David and van Hasselt, Hado",
  abstract      = "We propose a novel policy update that combines regularized
                   policy optimization with model learning as an auxiliary
                   loss. The update (henceforth Muesli) matches MuZero's
                   state-of-the-art performance on Atari. Notably, Muesli does
                   so without using deep search: it acts directly with a policy
                   network and has computation speed comparable to model-free
                   baselines. The Atari results are complemented by extensive
                   ablations, and by additional results on continuous control
                   and 9x9 Go.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  eprint        = "2104.06159",
  primaryClass  = "cs.LG",
  arxivid       = "2104.06159"
}

@ARTICLE{Holtzman2021-ms,
  title         = "Surface Form Competition: Why the Highest Probability Answer
                   Isn't Always Right",
  author        = "Holtzman, Ari and West, Peter and Shwartz, Vered and Choi,
                   Yejin and Zettlemoyer, Luke",
  abstract      = "Large language models have shown promising results in
                   zero-shot settings (Brown et al.,2020; Radford et al.,
                   2019). For example, they can perform multiple choice tasks
                   simply by conditioning on a question and selecting the
                   answer with the highest probability. However, ranking by
                   string probability can be problematic due to surface form
                   competition-wherein different surface forms compete for
                   probability mass, even if they represent the same underlying
                   concept, e.g. ``computer'' and ``PC.'' Since probability
                   mass is finite, this lowers the probability of the correct
                   answer, due to competition from other strings that are valid
                   answers (but not one of the multiple choice options). We
                   introduce Domain Conditional Pointwise Mutual Information,
                   an alternative scoring function that directly compensates
                   for surface form competition by simply reweighing each
                   option according to a term that is proportional to its a
                   priori likelihood within the context of the specific
                   zero-shot task. It achieves consistent gains in zero-shot
                   performance over both calibrated (Zhao et al., 2021) and
                   uncalibrated scoring functions on all GPT-2 and GPT-3 models
                   over a variety of multiple choice datasets.",
  month         =  apr,
  year          =  2021,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2104.08315",
  primaryClass  = "cs.CL",
  arxivid       = "2104.08315"
}

@ARTICLE{Earle2021-yt,
  title         = "Learning Controllable Content Generators",
  author        = "Earle, Sam and Edwards, Maria and Khalifa, Ahmed and
                   Bontrager, Philip and Togelius, Julian",
  abstract      = "It has recently been shown that reinforcement learning can
                   be used to train generators capable of producing
                   high-quality game levels, with quality defined in terms of
                   some user-specified heuristic. To ensure that these
                   generators' output is sufficiently diverse (that is, not
                   amounting to the reproduction of a single optimal level
                   configuration), the generation process is constrained such
                   that the initial seed results in some variance in the
                   generator's output. However, this results in a loss of
                   control over the generated content for the human user. We
                   propose to train generators capable of producing
                   controllably diverse output, by making them ``goal-aware.''
                   To this end, we add conditional inputs representing how
                   close a generator is to some heuristic, and also modify the
                   reward mechanism to incorporate that value. Testing on
                   multiple domains, we show that the resulting level
                   generators are capable of exploring the space of possible
                   levels in a targeted, controllable manner, producing levels
                   of comparable quality as their goal-unaware counterparts,
                   that are diverse along designer-specified dimensions.",
  month         =  may,
  year          =  2021,
  keywords      = "project 1;RL",
  archivePrefix = "arXiv",
  eprint        = "2105.02993",
  primaryClass  = "cs.LG",
  arxivid       = "2105.02993"
}

@ARTICLE{Acquaviva2021-ru,
  title         = "Communicating Natural Programs to Humans and Machines",
  author        = "Acquaviva, Samuel and Pu, Yewen and Kryven, Marta and
                   Sechopoulos, Theodoros and Wong, Catherine and Ecanow,
                   Gabrielle E and Nye, Maxwell and Tessler, Michael Henry and
                   Tenenbaum, Joshua B",
  abstract      = "The Abstraction and Reasoning Corpus (ARC) is a set of
                   procedural tasks that tests an agent's ability to flexibly
                   solve novel problems. While most ARC tasks are easy for
                   humans, they are challenging for state-of-the-art AI. What
                   makes building intelligent systems that can generalize to
                   novel situations such as ARC difficult? We posit that the
                   answer might be found by studying the difference of
                   \textbackslashemph\{language\}: While humans readily
                   generate and interpret instructions in a general language,
                   computer systems are shackled to a narrow domain-specific
                   language that they can precisely execute. We present LARC,
                   the \textbackslashtextit\{Language-complete ARC\}: a
                   collection of natural language descriptions by a group of
                   human participants who instruct each other on how to solve
                   ARC tasks using language alone, which contains successful
                   instructions for 88\% of the ARC tasks. We analyze the
                   collected instructions as `natural programs', finding that
                   while they resemble computer programs, they are distinct in
                   two ways: First, they contain a wide range of primitives;
                   Second, they frequently leverage communicative strategies
                   beyond directly executable codes. We demonstrate that these
                   two distinctions prevent current program synthesis
                   techniques from leveraging LARC to its full potential, and
                   give concrete suggestions on how to build the
                   next-generation program synthesizers.",
  month         =  jun,
  year          =  2021,
  keywords      = "read;ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2106.07824",
  primaryClass  = "cs.AI",
  arxivid       = "2106.07824"
}

@ARTICLE{Vorbach2021-nz,
  title    = "Causal Navigation by Continuous-time Neural Networks",
  author   = "Vorbach, Charles J and Hasani, Ramin M and Amini, Alexander and
              Lechner, Mathias and Rus, D",
  abstract = "The results demonstrate that causal continuous-time deep models
              can perform robust navigation tasks, where advanced recurrent
              models fail, and learn complex causal control representations
              directly from raw visual inputs and scale to solve a variety of
              tasks using imitation learning. Imitation learning enables
              high-fidelity, vision-based learning of policies within rich,
              photorealistic environments. However, such techniques often rely
              on traditional discrete-time neural models and face difficulties
              in generalizing to domain shifts by failing to account for the
              causal relationships between the agent and the environment. In
              this paper, we propose a theoretical and experimental framework
              for learning causal representations using continuous-time neural
              networks, specifically over their discrete-time counterparts. We
              evaluate our method in the context of visual-control learning of
              drones over a series of complex tasks, ranging from short- and
              long-term navigation, to chasing static and dynamic objects
              through photorealistic environments. Our results demonstrate that
              causal continuous-time deep models can perform robust navigation
              tasks, where advanced recurrent models fail. These models learn
              complex causal control representations directly from raw visual
              inputs and scale to solve a variety of tasks using imitation
              learning.",
  journal  = "Adv. Neural Inf. Process. Syst.",
  year     =  2021,
  language = "en",
  issn     = "1049-5258",
  arxivid  = "2106.08314"
}

@UNPUBLISHED{Shultz2021-hq,
  title    = "A {{Computational Model}} of {{Infant Learning}} and
              {{Reasoning}} with {{Probabilities}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments reveal that 6- to 12-month-old infants can
              learn probabilities and reason with them. In this work, we
              present a novel computational system called Neural Probability
              Learner and Sampler (NPLS) that learns and reasons with
              probabilities, providing a computationally sufficient mechanism
              to explain infant probabilistic learning and inference. In 24
              computer simulations, NPLS simulations show how probability
              distributions can emerge naturally from neural-network learning
              of event sequences, providing a novel explanation of infant
              probabilistic learning and reasoning. Three mathematical proofs
              show how and why NPLS simulates the infant results so accurately.
              The results are situated in relation to seven other active
              research lines. This work provides an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  month    =  jun,
  year     =  2021,
  keywords = "Quantitative Biology - Neurons and
              Cognition;comp-cog-sci;development"
}

@ARTICLE{Vysogorets2021-wi,
  title         = "Connectivity Matters: Neural Network Pruning Through the
                   Lens of Effective Sparsity",
  author        = "Vysogorets, Artem and Kempe, Julia",
  abstract      = "Neural network pruning is a fruitful area of research with
                   surging interest in high sparsity regimes. Benchmarking in
                   this domain heavily relies on faithful representation of the
                   sparsity of subnetworks, which has been traditionally
                   computed as the fraction of removed connections (direct
                   sparsity). This definition, however, fails to recognize
                   unpruned parameters that detached from input or output
                   layers of underlying subnetworks, potentially
                   underestimating actual effective sparsity: the fraction of
                   inactivated connections. While this effect might be
                   negligible for moderately pruned networks (up to 10-100
                   compression rates), we find that it plays an increasing role
                   for thinner subnetworks, greatly distorting comparison
                   between different pruning algorithms. For example, we show
                   that effective compression of a randomly pruned
                   LeNet-300-100 can be orders of magnitude larger than its
                   direct counterpart, while no discrepancy is ever observed
                   when using SynFlow for pruning [Tanaka et al., 2020]. In
                   this work, we adopt the lens of effective sparsity to
                   reevaluate several recent pruning algorithms on common
                   benchmark architectures (e.g., LeNet-300-100, VGG-19,
                   ResNet-18) and discover that their absolute and relative
                   performance changes dramatically in this new and more
                   appropriate framework. To aim for effective, rather than
                   direct, sparsity, we develop a low-cost extension to most
                   pruning algorithms. Further, equipped with effective
                   sparsity as a reference frame, we partially reconfirm that
                   random pruning with appropriate sparsity allocation across
                   layers performs as well or better than more sophisticated
                   algorithms for pruning at initialization [Su et al., 2020].
                   In response to this observation, using a simple analogy of
                   pressure distribution in coupled cylinders from physics, we
                   design novel layerwise sparsity quotas that outperform all
                   existing baselines in the context of random pruning.",
  month         =  jul,
  year          =  2021,
  keywords      = "nlp;pruning-llm",
  archivePrefix = "arXiv",
  eprint        = "2107.02306",
  primaryClass  = "cs.LG",
  arxivid       = "2107.02306"
}

@ARTICLE{Open_Ended_Learning_Team2021-at,
  title         = "{Open-Ended} Learning Leads to Generally Capable Agents",
  author        = "{Open Ended Learning Team} and Stooke, Adam and Mahajan,
                   Anuj and Barros, Catarina and Deck, Charlie and Bauer, Jakob
                   and Sygnowski, Jakub and Trebacz, Maja and Jaderberg, Max
                   and Mathieu, Michael and McAleese, Nat and Bradley-Schmieg,
                   Nathalie and Wong, Nathaniel and Porcel, Nicolas and
                   Raileanu, Roberta and Hughes-Fitt, Steph and Dalibard,
                   Valentin and Czarnecki, Wojciech Marian",
  abstract      = "In this work we create agents that can perform well beyond a
                   single, individual task, that exhibit much wider
                   generalisation of behaviour to a massive, rich space of
                   challenges. We define a universe of tasks within an
                   environment domain and demonstrate the ability to train
                   agents that are generally capable across this vast space and
                   beyond. The environment is natively multi-agent, spanning
                   the continuum of competitive, cooperative, and independent
                   games, which are situated within procedurally generated
                   physical 3D worlds. The resulting space is exceptionally
                   diverse in terms of the challenges posed to agents, and as
                   such, even measuring the learning progress of an agent is an
                   open research problem. We propose an iterative notion of
                   improvement between successive generations of agents, rather
                   than seeking to maximise a singular objective, allowing us
                   to quantify progress despite tasks being incomparable in
                   terms of achievable rewards. We show that through
                   constructing an open-ended learning process, which
                   dynamically changes the training task distributions and
                   training objectives such that the agent never stops
                   learning, we achieve consistent learning of new behaviours.
                   The resulting agent is able to score reward in every one of
                   our humanly solvable evaluation levels, with behaviour
                   generalising to many held-out points in the universe of
                   tasks. Examples of this zero-shot generalisation include
                   good performance on Hide and Seek, Capture the Flag, and
                   Tag. Through analysis and hand-authored probe tasks we
                   characterise the behaviour of our agent, and find
                   interesting emergent heuristic behaviours such as
                   trial-and-error experimentation, simple tool use, option
                   switching, and cooperation. Finally, we demonstrate that the
                   general capabilities of this agent could unlock larger scale
                   transfer of behaviour through cheap finetuning.",
  month         =  jul,
  year          =  2021,
  keywords      = "project 1;RL",
  archivePrefix = "arXiv",
  eprint        = "2107.12808",
  primaryClass  = "cs.LG",
  arxivid       = "2107.12808"
}

@UNPUBLISHED{Scherrer2021-wg,
  title    = "Learning {{Neural Causal Models}} with {{Active Interventions}}",
  author   = "Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and
              Goyal, Anirudh and Schwab, Patrick and Sch{\"o}lkopf, Bernhard
              and Mozer, Michael C and Bengio, Yoshua and Bauer, Stefan and Ke,
              Nan Rosemary",
  abstract = "Discovering causal structures from data is a challenging
              inference problem of fundamental importance in all areas of
              science. The appealing scaling properties of neural networks have
              recently led to a surge of interest in differentiable neural
              network-based methods for learning causal structures from data.
              So far differentiable causal discovery has focused on static
              datasets of observational or interventional origin. In this work,
              we introduce an active intervention-targeting mechanism which
              enables a quick identification of the underlying causal structure
              of the data-generating process. Our method significantly reduces
              the required number of interactions compared with random
              intervention targeting and is applicable for both discrete and
              continuous optimization formulations of learning the underlying
              directed acyclic graph (DAG) from data. We examine the proposed
              method across a wide range of settings and demonstrate superior
              performance on multiple benchmarks from simulated to real-world
              data.",
  month    =  sep,
  year     =  2021,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@ARTICLE{Sharma2021-xf,
  title         = "Skill Induction and Planning with Latent Language",
  author        = "Sharma, Pratyusha and Torralba, Antonio and Andreas, Jacob",
  abstract      = "We present a framework for learning hierarchical policies
                   from demonstrations, using sparse natural language
                   annotations to guide the discovery of reusable skills for
                   autonomous decision-making. We formulate a generative model
                   of action sequences in which goals generate sequences of
                   high-level subtask descriptions, and these descriptions
                   generate sequences of low-level actions. We describe how to
                   train this model using primarily unannotated demonstrations
                   by parsing demonstrations into sequences of named high-level
                   subtasks, using only a small number of seed annotations to
                   ground language in action. In trained models, natural
                   language commands index a combinatorial library of skills;
                   agents can use these skills to plan by generating high-level
                   instruction sequences tailored to novel goals. We evaluate
                   this approach in the ALFRED household simulation
                   environment, providing natural language annotations for only
                   10\% of demonstrations. It achieves task completion rates
                   comparable to state-of-the-art models (outperforming several
                   recent methods with access to ground-truth plans during
                   training and evaluation) while providing structured and
                   human-readable high-level plans.",
  month         =  oct,
  year          =  2021,
  keywords      = "read;comp-cog-sci;ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2110.01517",
  primaryClass  = "cs.LG",
  arxivid       = "2110.01517"
}

@ARTICLE{Jiang2021-vx,
  title         = "{Replay-Guided} Adversarial Environment Design",
  author        = "Jiang, Minqi and Dennis, Michael and Parker-Holder, Jack and
                   Foerster, Jakob and Grefenstette, Edward and
                   Rockt{\"a}schel, Tim",
  abstract      = "Deep reinforcement learning (RL) agents may successfully
                   generalize to new settings if trained on an appropriately
                   diverse set of environment and task configurations.
                   Unsupervised Environment Design (UED) is a promising
                   self-supervised RL paradigm, wherein the free parameters of
                   an underspecified environment are automatically adapted
                   during training to the agent's capabilities, leading to the
                   emergence of diverse training environments. Here, we cast
                   Prioritized Level Replay (PLR), an empirically successful
                   but theoretically unmotivated method that selectively
                   samples randomly-generated training levels, as UED. We argue
                   that by curating completely random levels, PLR, too, can
                   generate novel and complex levels for effective training.
                   This insight reveals a natural class of UED methods we call
                   Dual Curriculum Design (DCD). Crucially, DCD includes both
                   PLR and a popular UED algorithm, PAIRED, as special cases
                   and inherits similar theoretical guarantees. This connection
                   allows us to develop novel theory for PLR, providing a
                   version with a robustness guarantee at Nash equilibria.
                   Furthermore, our theory suggests a highly counterintuitive
                   improvement to PLR: by stopping the agent from updating its
                   policy on uncurated levels (training on less data), we can
                   improve the convergence to Nash equilibria. Indeed, our
                   experiments confirm that our new method, PLR$^\{\perp\}$,
                   obtains better results on a suite of out-of-distribution,
                   zero-shot transfer tasks, in addition to demonstrating that
                   PLR$^\{\perp\}$ improves the performance of PAIRED, from
                   which it inherited its theoretical framework.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  eprint        = "2110.02439",
  primaryClass  = "cs.LG",
  arxivid       = "2110.02439"
}

@ARTICLE{Balestriero2021-hw,
  title         = "Learning in High Dimension Always Amounts to Extrapolation",
  author        = "Balestriero, Randall and Pesenti, Jerome and LeCun, Yann",
  abstract      = "The notion of interpolation and extrapolation is fundamental
                   in various fields from deep learning to function
                   approximation. Interpolation occurs for a sample $x$
                   whenever this sample falls inside or on the boundary of the
                   given dataset's convex hull. Extrapolation occurs when $x$
                   falls outside of that convex hull. One fundamental
                   (mis)conception is that state-of-the-art algorithms work so
                   well because of their ability to correctly interpolate
                   training data. A second (mis)conception is that
                   interpolation happens throughout tasks and datasets, in
                   fact, many intuitions and theories rely on that assumption.
                   We empirically and theoretically argue against those two
                   points and demonstrate that on any high-dimensional ($>$100)
                   dataset, interpolation almost surely never happens. Those
                   results challenge the validity of our current
                   interpolation/extrapolation definition as an indicator of
                   generalization performances.",
  month         =  oct,
  year          =  2021,
  keywords      = "machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2110.09485",
  primaryClass  = "cs.LG",
  arxivid       = "2110.09485"
}

@ARTICLE{Zafrir2021-pm,
  title         = "Prune Once for All: Sparse {Pre-Trained} Language Models",
  author        = "Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen,
                   Haihao and Wasserblat, Moshe",
  abstract      = "Transformer-based language models are applied to a wide
                   range of applications in natural language processing.
                   However, they are inefficient and difficult to deploy. In
                   recent years, many compression algorithms have been proposed
                   to increase the implementation efficiency of large
                   Transformer-based models on target hardware. In this work we
                   present a new method for training sparse pre-trained
                   Transformer language models by integrating weight pruning
                   and model distillation. These sparse pre-trained models can
                   be used to transfer learning for a wide range of tasks while
                   maintaining their sparsity pattern. We demonstrate our
                   method with three known architectures to create sparse
                   pre-trained BERT-Base, BERT-Large and DistilBERT. We show
                   how the compressed sparse pre-trained models we trained
                   transfer their knowledge to five different downstream
                   natural language tasks with minimal accuracy loss. Moreover,
                   we show how to further compress the sparse models' weights
                   to 8bit precision using quantization-aware training. For
                   example, with our sparse pre-trained BERT-Large fine-tuned
                   on SQuADv1.1 and quantized to 8bit we achieve a compression
                   ratio of $40$X for the encoder with less than $1\%$ accuracy
                   loss. To the best of our knowledge, our results show the
                   best compression-to-accuracy ratio for BERT-Base,
                   BERT-Large, and DistilBERT.",
  month         =  nov,
  year          =  2021,
  keywords      = "read;nlp;pruning-llm",
  archivePrefix = "arXiv",
  eprint        = "2111.05754",
  primaryClass  = "cs.CL",
  arxivid       = "2111.05754"
}

@ARTICLE{Nye2021-dw,
  title         = "Show Your Work: Scratchpads for Intermediate Computation
                   with Language Models",
  author        = "Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy
                   and Michalewski, Henryk and Austin, Jacob and Bieber, David
                   and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and
                   Luan, David and Sutton, Charles and Odena, Augustus",
  abstract      = "Large pre-trained language models perform remarkably well on
                   tasks that can be done ``in one pass'', such as generating
                   realistic text or synthesizing computer programs. However,
                   they struggle with tasks that require unbounded multi-step
                   computation, such as adding integers or executing programs.
                   Surprisingly, we find that these same models are able to
                   perform complex multi-step computations -- even in the
                   few-shot regime -- when asked to perform the operation
                   ``step by step'', showing the results of intermediate
                   computations. In particular, we train transformers to
                   perform multi-step computations by asking them to emit
                   intermediate computation steps into a ``scratchpad''. On a
                   series of increasingly complex tasks ranging from long
                   addition to the execution of arbitrary programs, we show
                   that scratchpads dramatically improve the ability of
                   language models to perform multi-step computations.",
  month         =  nov,
  year          =  2021,
  keywords      = "ARC Project;comp-cog-sci;nl-reasoning-workshop",
  archivePrefix = "arXiv",
  eprint        = "2112.00114",
  primaryClass  = "cs.LG",
  arxivid       = "2112.00114"
}

@ARTICLE{Whittington2021-sj,
  title         = "Relating transformers to models and neural representations
                   of the hippocampal formation",
  author        = "Whittington, James C R and Warren, Joseph and Behrens,
                   Timothy E J",
  abstract      = "Many deep neural network architectures loosely based on
                   brain networks have recently been shown to replicate neural
                   firing patterns observed in the brain. One of the most
                   exciting and promising novel architectures, the Transformer
                   neural network, was developed without the brain in mind. In
                   this work, we show that transformers, when equipped with
                   recurrent position encodings, replicate the precisely tuned
                   spatial representations of the hippocampal formation; most
                   notably place and grid cells. Furthermore, we show that this
                   result is no surprise since it is closely related to current
                   hippocampal models from neuroscience. We additionally show
                   the transformer version offers dramatic performance gains
                   over the neuroscience version. This work continues to bind
                   computations of artificial and brain networks, offers a
                   novel understanding of the hippocampal-cortical interaction,
                   and suggests how wider cortical areas may perform complex
                   tasks beyond current neuroscience models such as language
                   comprehension.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  eprint        = "2112.04035",
  primaryClass  = "cs.NE",
  arxivid       = "2112.04035"
}

@ARTICLE{Wei2022-mg,
  title         = "{Chain-of-Thought} Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought
                   prompting, where a few chain of thought demonstrations are
                   provided as exemplars in prompting. Experiments on three
                   large language models show that chain of thought prompting
                   improves performance on a range of arithmetic, commonsense,
                   and symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math
                   word problems, surpassing even finetuned GPT-3 with a
                   verifier.",
  month         =  jan,
  year          =  2022,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2201.11903",
  primaryClass  = "cs.CL",
  arxivid       = "2201.11903"
}

@ARTICLE{Allen2022-as,
  title    = "Physical Design using Differentiable Learned Simulators",
  author   = "Allen, Kelsey R and Lopez-Guevara, Tatiana and Stachenfeld, K and
              Sanchez-Gonzalez, Alvaro and Battaglia, P and Hamrick, Jessica B
              and Pfaff, T",
  abstract = "This work explores a simple, fast, and robust approach to inverse
              design which combines learned forward simulators based on graph
              neural networks with gradient-based design optimization, and
              suggests that despite some remaining challenges, machine
              learning-based simulators are maturing to the point where they
              can support general-purpose design optimization across a variety
              of domains. Designing physical artifacts that serve a purpose---
              such as tools and other functional structures---is central to
              engineering as well as everyday human behavior. Though automating
              design has tremendous promise, general-purpose methods do not yet
              exist. Here we explore a simple, fast, and robust approach to
              inverse design which combines learned forward simulators based on
              graph neural networks with gradient-based design optimization.
              Our approach solves high-dimensional problems with complex
              physical dynamics, including designing surfaces and tools to
              manipulate fluid flows and optimizing the shape of an airfoil to
              minimize drag. This framework produces highquality designs by
              propagating gradients through trajectories of hundreds of steps,
              even when using models that were pre-trained for single-step
              predictions on data substantially different from the design
              tasks. In our fluid manipulation tasks, the resulting designs
              outperformed those found by sampling-based optimization
              techniques. In airfoil design, they matched the quality of those
              obtained with a specialized solver. Our results suggest that
              despite some remaining challenges, machine learning-based
              simulators are maturing to the point where they can support
              general-purpose design optimization across a variety of domains.",
  journal  = "ArXiv",
  year     =  2022,
  keywords = "comp-cog-sci;project 2",
  language = "en",
  arxivid  = "2202.00728"
}

@MISC{Kosoy_undated-ah,
  title    = "Learning Causal Overhypotheses through Exploration in Children
              and Computational Models",
  author   = "Kosoy, Eliza and Liu, Adrian and Collins, Jasmine and Chan, David
              M and Hamrick, Jessica B and Rosemary, Nan and Huang, Sandy Han
              and Kaufmann, Bryanna and Gopnik, Alison and Sch{\"o}lkopf,
              Bernhard and Uhler, Caroline and Zhang, Kun and Ke, N R and
              Canny, J",
  keywords = "comp-cog-sci;development",
  arxivid  = "2202.10430v1"
}

@ARTICLE{Ouyang2022-jj,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray,
                   Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser
                   and Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and
                   Lowe, Ryan",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this
                   paper, we show an avenue for aligning language models with
                   user intent on a wide range of tasks by fine-tuning with
                   human feedback. Starting with a set of labeler-written
                   prompts and prompts submitted through the OpenAI API, we
                   collect a dataset of labeler demonstrations of the desired
                   model behavior, which we use to fine-tune GPT-3 using
                   supervised learning. We then collect a dataset of rankings
                   of model outputs, which we use to further fine-tune this
                   supervised model using reinforcement learning from human
                   feedback. We call the resulting models InstructGPT. In human
                   evaluations on our prompt distribution, outputs from the
                   1.3B parameter InstructGPT model are preferred to outputs
                   from the 175B GPT-3, despite having 100x fewer parameters.
                   Moreover, InstructGPT models show improvements in
                   truthfulness and reductions in toxic output generation while
                   having minimal performance regressions on public NLP
                   datasets. Even though InstructGPT still makes simple
                   mistakes, our results show that fine-tuning with human
                   feedback is a promising direction for aligning language
                   models with human intent.",
  month         =  mar,
  year          =  2022,
  keywords      = "skimmed;RL",
  archivePrefix = "arXiv",
  eprint        = "2203.02155",
  primaryClass  = "cs.CL",
  arxivid       = "2203.02155"
}

@ARTICLE{Weir2022-cv,
  title         = "{One-Shot} Learning from a Demonstration with Hierarchical
                   Latent Language",
  author        = "Weir, Nathaniel and Yuan, Xingdi and C{\^o}t{\'e},
                   Marc-Alexandre and Hausknecht, Matthew and Laroche, Romain
                   and Momennejad, Ida and Van Seijen, Harm and Van Durme,
                   Benjamin",
  abstract      = "Humans have the capability, aided by the expressive
                   compositionality of their language, to learn quickly by
                   demonstration. They are able to describe unseen
                   task-performing procedures and generalize their execution to
                   other contexts. In this work, we introduce DescribeWorld, an
                   environment designed to test this sort of generalization
                   skill in grounded agents, where tasks are linguistically and
                   procedurally composed of elementary concepts. The agent
                   observes a single task demonstration in a Minecraft-like
                   grid world, and is then asked to carry out the same task in
                   a new map. To enable such a level of generalization, we
                   propose a neural agent infused with hierarchical latent
                   language--both at the level of task inference and subtask
                   planning. Our agent first generates a textual description of
                   the demonstrated unseen task, then leverages this
                   description to replicate it. Through multiple evaluation
                   scenarios and a suite of generalization tests, we find that
                   agents that perform text-based inference are better equipped
                   for the challenge under a random split of tasks.",
  month         =  mar,
  year          =  2022,
  keywords      = "ARC Project;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2203.04806",
  primaryClass  = "cs.CL",
  arxivid       = "2203.04806"
}

@ARTICLE{Zelikman2022-xw,
  title         = "{STaR}: Bootstrapping Reasoning With Reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman,
                   Noah D",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with
                   a few rationale examples; if the generated answers are
                   wrong, try again to generate a rationale given the correct
                   answer; fine-tune on all the rationales that ultimately
                   yielded correct answers; repeat. We show that STaR
                   significantly improves performance on multiple datasets
                   compared to a model fine-tuned to directly predict final
                   answers, and performs comparably to fine-tuning a 30$\times$
                   larger state-of-the-art language model on CommensenseQA.
                   Thus, STaR lets a model improve itself by learning from its
                   own generated reasoning.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  eprint        = "2203.14465",
  primaryClass  = "cs.LG",
  arxivid       = "2203.14465"
}

@ARTICLE{Zelikman2022-nb,
  title         = "{STaR}: Bootstrapping Reasoning With Reasoning",
  author        = "Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman,
                   Noah D",
  abstract      = "Generating step-by-step ``chain-of-thought'' rationales
                   improves language model performance on complex reasoning
                   tasks like mathematics or commonsense question-answering.
                   However, inducing language model rationale generation
                   currently requires either constructing massive rationale
                   datasets or sacrificing accuracy by using only few-shot
                   inference. We propose a technique to iteratively leverage a
                   small number of rationale examples and a large dataset
                   without rationales, to bootstrap the ability to perform
                   successively more complex reasoning. This technique, the
                   ``Self-Taught Reasoner'' (STaR), relies on a simple loop:
                   generate rationales to answer many questions, prompted with
                   a few rationale examples; if the generated answers are
                   wrong, try again to generate a rationale given the correct
                   answer; fine-tune on all the rationales that ultimately
                   yielded correct answers; repeat. We show that STaR
                   significantly improves performance on multiple datasets
                   compared to a model fine-tuned to directly predict final
                   answers, and performs comparably to fine-tuning a 30$\times$
                   larger state-of-the-art language model on CommensenseQA.
                   Thus, STaR lets a model improve itself by learning from its
                   own generated reasoning.",
  month         =  mar,
  year          =  2022,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2203.14465",
  primaryClass  = "cs.LG",
  arxivid       = "2203.14465"
}

@ARTICLE{Yang2022-ho,
  title         = "{TextPruner}: A Model Pruning Toolkit for {Pre-Trained}
                   Language Models",
  author        = "Yang, Ziqing and Cui, Yiming and Chen, Zhigang",
  abstract      = "Pre-trained language models have been prevailed in natural
                   language processing and become the backbones of many NLP
                   tasks, but the demands for computational resources have
                   limited their applications. In this paper, we introduce
                   TextPruner, an open-source model pruning toolkit designed
                   for pre-trained language models, targeting fast and easy
                   model compression. TextPruner offers structured
                   post-training pruning methods, including vocabulary pruning
                   and transformer pruning, and can be applied to various
                   models and tasks. We also propose a self-supervised pruning
                   method that can be applied without the labeled data. Our
                   experiments with several NLP tasks demonstrate the ability
                   of TextPruner to reduce the model size without re-training
                   the model.",
  month         =  mar,
  year          =  2022,
  keywords      = "skimmed;nlp;pruning-llm",
  archivePrefix = "arXiv",
  eprint        = "2203.15996",
  primaryClass  = "cs.CL",
  arxivid       = "2203.15996"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chowdhery2022-un,
  title    = "{PaLM}: Scaling Language Modeling with Pathways",
  author   = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and
              Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, P
              and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian
              and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and
              Maynez, Joshua and Rao, Abhishek B and Barnes, Parker and Tay, Yi
              and Shazeer, Noam M and Prabhakaran, Vinodkumar and Reif, Emily
              and Du, Nan and Hutchinson, B and Pope, Reiner and Bradbury,
              James and Austin, Jacob and Isard, M and Gur-Ari, Guy and Yin,
              Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, S and
              Dev, Sunipa and Michalewski, H and Garc{\'\i}a, Xavier and Misra,
              Vedant and Robinson, Kevin and Fedus, L and Zhou, Denny and
              Ippolito, Daphne and Luan, D and Lim, Hyeontaek and Zoph, Barret
              and Spiridonov, A and Sepassi, Ryan and Dohan, David and Agrawal,
              Shivani and Omernick, Mark and Dai, Andrew M and Pillai, T S and
              Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child,
              Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei
              and Wang, Xuezhi and Saeta, Brennan and D{\'\i}az, Mark and
              Firat, Orhan and Catasta, Michele and Wei, Jason and
              Meier-Hellstern, K and Eck, D and Dean, J and Petrov, Slav and
              Fiedel, Noah",
  abstract = "A 540-billion parameter, densely activated, Transformer language
              model, which is called PaLM achieves breakthrough performance,
              outperforming the state-of-the-art on a suite of multi-step
              reasoning tasks, and outperforming average human performance on
              the recently released BIG-bench benchmark. Large language models
              have been shown to achieve remarkable performance across a
              variety of natural language tasks using few-shot learning , which
              drastically reduces the number of task-speciï¬c training examples
              needed to adapt the model to a particular application. To further
              our understanding of the impact of scale on few-shot learning, we
              trained a 540-billion parameter, densely activated, Transformer
              language model, which we call Pathways Language Model (PaLM). We
              trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system
              which enables highly eï¬ƒcient training across multiple TPU Pods.
              We demonstrate continued beneï¬ts of scaling by achieving
              state-of-the-art few-shot learning results on hundreds of
              language understanding and generation benchmarks. On a number of
              these tasks, PaLM 540B achieves breakthrough performance,
              outperforming the ï¬netuned state-of-the-art on a suite of
              multi-step reasoning tasks, and outperforming average human
              performance on the recently released BIG-bench benchmark. A
              signiï¬cant number of BIG-bench tasks showed discontinuous
              improvements from model scale, meaning that performance steeply
              increased as we scaled to our largest model. PaLM also has strong
              capabilities in multilingual tasks and source code generation,
              which we demonstrate on a wide array of benchmarks. We
              additionally provide a comprehensive analysis on bias and
              toxicity, and study the extent of training data memorization with
              respect to model scale. Finally, we discuss the ethical
              considerations related to large language models and discuss
              potential mitigation strategies.",
  journal  = "ArXiv",
  year     =  2022,
  keywords = "nlp",
  language = "en",
  arxivid  = "2204.02311"
}

@ARTICLE{Moradipari2022-uu,
  title         = "Collaborative Multi-agent Stochastic Linear Bandits",
  author        = "Moradipari, Ahmadreza and Ghavamzadeh, Mohammad and
                   Alizadeh, Mahnoosh",
  abstract      = "We study a collaborative multi-agent stochastic linear
                   bandit setting, where $N$ agents that form a network
                   communicate locally to minimize their overall regret. In
                   this setting, each agent has its own linear bandit problem
                   (its own reward parameter) and the goal is to select the
                   best global action w.r.t. the average of their reward
                   parameters. At each round, each agent proposes an action,
                   and one action is randomly selected and played as the
                   network action. All the agents observe the corresponding
                   rewards of the played actions and use an accelerated
                   consensus procedure to compute an estimate of the average of
                   the rewards obtained by all the agents. We propose a
                   distributed upper confidence bound (UCB) algorithm and prove
                   a high probability bound on its $T$-round regret in which we
                   include a linear growth of regret associated with each
                   communication round. Our regret bound is of order
                   $\mathcal\{O\}\Big(\sqrt\{\frac\{T\}\{N
                   \log(1/|\lambda_2|)\}\}\cdot (\log T)^2\Big)$, where
                   $\lambda_2$ is the second largest (in absolute value)
                   eigenvalue of the communication matrix.",
  month         =  may,
  year          =  2022,
  keywords      = "ccm-project",
  archivePrefix = "arXiv",
  eprint        = "2205.06331",
  primaryClass  = "cs.LG",
  arxivid       = "2205.06331"
}

@ARTICLE{Kumar2022-lq,
  title         = "Using Natural Language and Program Abstractions to Instill
                   Human Inductive Biases in Machines",
  author        = "Kumar, Sreejan and Correa, Carlos G and Dasgupta, Ishita and
                   Marjieh, Raja and Hu, Michael Y and Hawkins, Robert D and
                   Daw, Nathaniel D and Cohen, Jonathan D and Narasimhan,
                   Karthik and Griffiths, Thomas L",
  abstract      = "Strong inductive biases give humans the ability to quickly
                   learn to perform a variety of tasks. Although meta-learning
                   is a method to endow neural networks with useful inductive
                   biases, agents trained by meta-learning may sometimes
                   acquire very different strategies from humans. We show that
                   co-training these agents on predicting representations from
                   natural language task descriptions and programs induced to
                   generate such tasks guides them toward more human-like
                   inductive biases. Human-generated language descriptions and
                   program induction models that add new learned primitives
                   both contain abstract concepts that can compress description
                   length. Co-training on these representations result in more
                   human-like behavior in downstream meta-reinforcement
                   learning agents than less abstract controls (synthetic
                   language descriptions, program induction without learned
                   primitives), suggesting that the abstraction supported by
                   these representations is key.",
  month         =  may,
  year          =  2022,
  keywords      = "ARC Project;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2205.11558",
  primaryClass  = "cs.AI",
  arxivid       = "2205.11558"
}

@ARTICLE{Tang2022-vx,
  title         = "From Perception to Programs: Regularize, Overparameterize,
                   and Amortize",
  author        = "Tang, Hao and Ellis, Kevin",
  abstract      = "Toward combining inductive reasoning with perception
                   abilities, we develop techniques for neurosymbolic program
                   synthesis where perceptual input is first parsed by neural
                   nets into a low-dimensional interpretable representation,
                   which is then processed by a synthesized program. We explore
                   several techniques for relaxing the problem and jointly
                   learning all modules end-to-end with gradient descent:
                   multitask learning; amortized inference;
                   overparameterization; and a differentiable strategy for
                   penalizing lengthy programs. Collectedly this toolbox
                   improves the stability of gradient-guided program search,
                   and suggests ways of learning both how to perceive input as
                   discrete abstractions, and how to symbolically process those
                   abstractions as programs.",
  month         =  jun,
  year          =  2022,
  keywords      = "read",
  archivePrefix = "arXiv",
  eprint        = "2206.05922",
  primaryClass  = "cs.AI",
  arxivid       = "2206.05922"
}

@ARTICLE{Xu2022-eq,
  title         = "{EST}: Evaluating Scientific Thinking in Artificial Agents",
  author        = "Xu, Manjie and Jiang, Guangyuan and Zhang, Chi and Zhu,
                   Song-Chun and Zhu, Yixin",
  abstract      = "Theoretical ideas and empirical research have shown us a
                   seemingly surprising result: children, even very young
                   toddlers, demonstrate learning and thinking in a strikingly
                   similar manner to scientific reasoning in formal research.
                   Encountering a novel phenomenon, children make hypotheses
                   against data, conduct causal inference from observation,
                   test their theory via experimentation, and correct the
                   proposition if inconsistency arises. Rounds of such
                   processes continue until the underlying mechanism is found.
                   Towards building machines that can learn and think like
                   people, one natural question for us to ask is: whether the
                   intelligence we achieve today manages to perform such a
                   scientific thinking process, and if any, at what level. In
                   this work, we devise the EST environment for evaluating the
                   scientific thinking ability in artificial agents. Motivated
                   by the stream of research on causal discovery, we build our
                   interactive EST environment based on Blicket detection.
                   Specifically, in each episode of EST, an agent is presented
                   with novel observations and asked to figure out all objects'
                   Blicketness. At each time step, the agent proposes new
                   experiments to validate its hypothesis and updates its
                   current belief. By evaluating Reinforcement Learning (RL)
                   agents on both a symbolic and visual version of this task,
                   we notice clear failure of today's learning methods in
                   reaching a level of intelligence comparable to humans. Such
                   inefficacy of learning in scientific thinking calls for
                   future research in building humanlike intelligence.",
  month         =  jun,
  year          =  2022,
  keywords      = "skimmed;project 1;machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2206.09203",
  primaryClass  = "cs.AI",
  arxivid       = "2206.09203"
}

@ARTICLE{Odouard2022-dv,
  title         = "Evaluating Understanding on Conceptual Abstraction
                   Benchmarks",
  author        = "Odouard, Victor Vikram and Mitchell, Melanie",
  abstract      = "A long-held objective in AI is to build systems that
                   understand concepts in a humanlike way. Setting aside the
                   difficulty of building such a system, even trying to
                   evaluate one is a challenge, due to present-day AI's
                   relative opacity and its proclivity for finding shortcut
                   solutions. This is exacerbated by humans' tendency to
                   anthropomorphize, assuming that a system that can recognize
                   one instance of a concept must also understand other
                   instances, as a human would. In this paper, we argue that
                   understanding a concept requires the ability to use it in
                   varied contexts. Accordingly, we propose systematic
                   evaluations centered around concepts, by probing a system's
                   ability to use a given concept in many different
                   instantiations. We present case studies of such an
                   evaluations on two domains -- RAVEN (inspired by Raven's
                   Progressive Matrices) and the Abstraction and Reasoning
                   Corpus (ARC) -- that have been used to develop and assess
                   abstraction abilities in AI systems. Our concept-based
                   approach to evaluation reveals information about AI systems
                   that conventional test sets would have left hidden.",
  month         =  jun,
  year          =  2022,
  keywords      = "ARC Project;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2206.14187",
  primaryClass  = "cs.AI",
  arxivid       = "2206.14187"
}

@ARTICLE{Huang2022-mi,
  title         = "Inner Monologue: Embodied Reasoning through Planning with
                   Language Models",
  author        = "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris
                   and Liang, Jacky and Florence, Pete and Zeng, Andy and
                   Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen
                   and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and
                   Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter,
                   Brian",
  abstract      = "Recent works have shown how the reasoning capabilities of
                   Large Language Models (LLMs) can be applied to domains
                   beyond natural language processing, such as planning and
                   interaction for robots. These embodied problems require an
                   agent to understand many semantic aspects of the world: the
                   repertoire of skills available, how these skills influence
                   the world, and how changes to the world map back to the
                   language. LLMs planning in embodied environments need to
                   consider not just what skills to do, but also how and when
                   to do them - answers that change over time in response to
                   the agent's own choices. In this work, we investigate to
                   what extent LLMs used in such embodied contexts can reason
                   over sources of feedback provided through natural language,
                   without any additional training. We propose that by
                   leveraging environment feedback, LLMs are able to form an
                   inner monologue that allows them to more richly process and
                   plan in robotic control scenarios. We investigate a variety
                   of sources of feedback, such as success detection, scene
                   description, and human interaction. We find that closed-loop
                   language feedback significantly improves high-level
                   instruction completion on three domains, including simulated
                   and real table top rearrangement tasks and long-horizon
                   mobile manipulation tasks in a kitchen environment in the
                   real world.",
  month         =  jul,
  year          =  2022,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2207.05608",
  primaryClass  = "cs.RO",
  arxivid       = "2207.05608"
}

@ARTICLE{Dasgupta2022-ug,
  title         = "Language models show human-like content effects on reasoning",
  author        = "Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie
                   C Y and Creswell, Antonia and Kumaran, Dharshan and
                   McClelland, James L and Hill, Felix",
  abstract      = "Abstract reasoning is a key ability for an intelligent
                   system. Large language models achieve above-chance
                   performance on abstract reasoning tasks, but exhibit many
                   imperfections. However, human abstract reasoning is also
                   imperfect, and depends on our knowledge and beliefs about
                   the content of the reasoning problem. For example, humans
                   reason much more reliably about logical rules that are
                   grounded in everyday situations than arbitrary rules about
                   abstract attributes. The training experiences of language
                   models similarly endow them with prior expectations that
                   reflect human knowledge and beliefs. We therefore
                   hypothesized that language models would show human-like
                   content effects on abstract reasoning problems. We explored
                   this hypothesis across three logical reasoning tasks:
                   natural language inference, judging the logical validity of
                   syllogisms, and the Wason selection task (Wason, 1968). We
                   find that state of the art large language models (with 7 or
                   70 billion parameters; Hoffman et al., 2022) reflect many of
                   the same patterns observed in humans across these tasks --
                   like humans, models reason more effectively about believable
                   situations than unrealistic or abstract ones. Our findings
                   have implications for understanding both these cognitive
                   effects, and the factors that contribute to language model
                   performance.",
  month         =  jul,
  year          =  2022,
  keywords      = "read;compling-cogsci2023;LLMs;nl-reasoning-workshop",
  archivePrefix = "arXiv",
  eprint        = "2207.07051",
  primaryClass  = "cs.CL",
  arxivid       = "2207.07051"
}

@ARTICLE{Merrill2022-sv,
  title         = "Entailment Semantics Can Be Extracted from an Ideal Language
                   Model",
  author        = "Merrill, William and Warstadt, Alex and Linzen, Tal",
  abstract      = "Language models are often trained on text alone, without
                   additional grounding. There is debate as to how much of
                   natural language semantics can be inferred from such a
                   procedure. We prove that entailment judgments between
                   sentences can be extracted from an ideal language model that
                   has perfectly learned its target distribution, assuming the
                   training sentences are generated by Gricean agents, i.e.,
                   agents who follow fundamental principles of communication
                   from the linguistic theory of pragmatics. We also show
                   entailment judgments can be decoded from the predictions of
                   a language model trained on such Gricean data. Our results
                   reveal a pathway for understanding the semantic information
                   encoded in unlabeled linguistic data and a potential
                   framework for extracting semantics from language models.",
  month         =  sep,
  year          =  2022,
  keywords      = "compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2209.12407",
  primaryClass  = "cs.CL",
  arxivid       = "2209.12407"
}

@ARTICLE{Merullo2022-zx,
  title         = "Linearly Mapping from Image to Text Space",
  author        = "Merullo, Jack and Castricato, Louis and Eickhoff, Carsten
                   and Pavlick, Ellie",
  abstract      = "The extent to which text-only language models (LMs) learn to
                   represent the physical, non-linguistic world is an open
                   question. Prior work has shown that pretrained LMs can be
                   taught to ``understand'' visual inputs when the models'
                   parameters are updated on image captioning tasks. We test a
                   stronger hypothesis: that the conceptual representations
                   learned by text-only models are functionally equivalent (up
                   to a linear transformation) to those learned by models
                   trained on vision tasks. Specifically, we show that the
                   image representations from vision models can be transferred
                   as continuous prompts to frozen LMs by training only a
                   single linear projection. Using these to prompt the LM
                   achieves competitive performance on captioning and visual
                   question answering tasks compared to models that tune both
                   the image encoder and text decoder (such as the MAGMA
                   model). We compare three image encoders with increasing
                   amounts of linguistic supervision seen during pretraining:
                   BEIT (no linguistic information), NF-ResNET (lexical
                   category information), and CLIP (full natural language
                   descriptions). We find that all three encoders perform
                   equally well at transferring visual property information to
                   the language model (e.g., whether an animal is large or
                   small), but that image encoders pretrained with linguistic
                   supervision more saliently encode category information
                   (e.g., distinguishing hippo vs.\textbackslash elephant) and
                   thus perform significantly better on benchmark
                   language-and-vision tasks. Our results indicate that LMs
                   encode conceptual information structurally similarly to
                   vision-based models, even those that are solely trained on
                   images.",
  month         =  sep,
  year          =  2022,
  keywords      = "read;compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2209.15162",
  primaryClass  = "cs.CL",
  arxivid       = "2209.15162"
}

@ARTICLE{Yao2022-vk,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting)
                   and acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the
                   use of LLMs to generate both reasoning traces and
                   task-specific actions in an interleaved manner, allowing for
                   greater synergy between the two: reasoning traces help the
                   model induce, track, and update action plans as well as
                   handle exceptions, while actions allow it to interface with
                   external sources, such as knowledge bases or environments,
                   to gather additional information. We apply our approach,
                   named ReAct, to a diverse set of language and decision
                   making tasks and demonstrate its effectiveness over
                   state-of-the-art baselines, as well as improved human
                   interpretability and trustworthiness over methods without
                   reasoning or acting components. Concretely, on question
                   answering (HotpotQA) and fact verification (Fever), ReAct
                   overcomes issues of hallucination and error propagation
                   prevalent in chain-of-thought reasoning by interacting with
                   a simple Wikipedia API, and generates human-like
                   task-solving trajectories that are more interpretable than
                   baselines without reasoning traces. On two interactive
                   decision making benchmarks (ALFWorld and WebShop), ReAct
                   outperforms imitation and reinforcement learning methods by
                   an absolute success rate of 34\% and 10\% respectively,
                   while being prompted with only one or two in-context
                   examples. Project site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  keywords      = "read;ARC Project;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2210.03629",
  primaryClass  = "cs.CL",
  arxivid       = "2210.03629"
}

@ARTICLE{Lynch2022-su,
  title         = "Interactive Language: Talking to Robots in Real Time",
  author        = "Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and
                   Ding, Tianli and Betker, James and Baruch, Robert and
                   Armstrong, Travis and Florence, Pete",
  abstract      = "We present a framework for building interactive, real-time,
                   natural language-instructable robots in the real world, and
                   we open source related assets (dataset, environment,
                   benchmark, and policies). Trained with behavioral cloning on
                   a dataset of hundreds of thousands of language-annotated
                   trajectories, a produced policy can proficiently execute an
                   order of magnitude more commands than previous works:
                   specifically we estimate a 93.5\% success rate on a set of
                   87,000 unique natural language strings specifying raw
                   end-to-end visuo-linguo-motor skills in the real world. We
                   find that the same policy is capable of being guided by a
                   human via real-time language to address a wide range of
                   precise long-horizon rearrangement goals, e.g. ``make a
                   smiley face out of blocks''. The dataset we release
                   comprises nearly 600,000 language-labeled trajectories, an
                   order of magnitude larger than prior available datasets. We
                   hope the demonstrated results and associated assets enable
                   further advancement of helpful, capable,
                   natural-language-interactable robots. See videos at
                   https://interactive-language.github.io.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  eprint        = "2210.06407",
  primaryClass  = "cs.RO",
  arxivid       = "2210.06407"
}

@ARTICLE{Arehalli2022-xn,
  title         = "Syntactic Surprisal From Neural Models Predicts, But
                   Underestimates, Human Processing Difficulty From Syntactic
                   Ambiguities",
  author        = "Arehalli, Suhas and Dillon, Brian and Linzen, Tal",
  abstract      = "Humans exhibit garden path effects: When reading sentences
                   that are temporarily structurally ambiguous, they slow down
                   when the structure is disambiguated in favor of the less
                   preferred alternative. Surprisal theory (Hale, 2001; Levy,
                   2008), a prominent explanation of this finding, proposes
                   that these slowdowns are due to the unpredictability of each
                   of the words that occur in these sentences. Challenging this
                   hypothesis, van Schijndel \& Linzen (2021) find that
                   estimates of the cost of word predictability derived from
                   language models severely underestimate the magnitude of
                   human garden path effects. In this work, we consider whether
                   this underestimation is due to the fact that humans weight
                   syntactic factors in their predictions more highly than
                   language models do. We propose a method for estimating
                   syntactic predictability from a language model, allowing us
                   to weigh the cost of lexical and syntactic predictability
                   independently. We find that treating syntactic
                   predictability independently from lexical predictability
                   indeed results in larger estimates of garden path. At the
                   same time, even when syntactic predictability is
                   independently weighted, surprisal still greatly
                   underestimate the magnitude of human garden path effects.
                   Our results support the hypothesis that predictability is
                   not the only factor responsible for the processing cost
                   associated with garden path sentences.",
  month         =  oct,
  year          =  2022,
  keywords      = "compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2210.12187",
  primaryClass  = "cs.CL",
  arxivid       = "2210.12187"
}

@ARTICLE{Lampinen2022-oy,
  title         = "Can language models handle recursively nested grammatical
                   structures? A case study on comparing models and humans",
  author        = "Lampinen, Andrew Kyle",
  abstract      = "How should we compare the capabilities of language models
                   and humans? Here, I consider a case study: processing of
                   recursively nested grammatical structures. Prior work has
                   suggested that language models cannot handle these
                   structures as reliably as humans can. However, the humans
                   were provided with instructions and training before being
                   evaluated, while the language models were evaluated
                   zero-shot. I therefore attempt to more closely match the
                   evaluation paradigms by providing language models with
                   few-shot prompts. A simple prompt, which contains
                   substantially less content than the human training, allows
                   large language models to consistently outperform the human
                   results. The same prompt even allows extrapolation to more
                   deeply nested conditions than have been tested in humans.
                   Further, a reanalysis of the prior human experiments
                   suggests that the humans may not perform above chance at the
                   difficult structures initially. These results suggest that
                   large language models can in fact process recursively nested
                   grammatical structures comparably to humans. This case study
                   highlights how discrepancies in the quantity of
                   experiment-specific context can confound comparisons of
                   language models and humans. I use this case study to reflect
                   on the broader challenge of comparing human and model
                   capabilities, and to suggest that there is an important
                   difference between evaluating cognitive models of a specific
                   phenomenon and evaluating broadly-trained models.",
  month         =  oct,
  year          =  2022,
  keywords      = "read;compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2210.15303",
  primaryClass  = "cs.CL",
  arxivid       = "2210.15303"
}

@ARTICLE{Webb2022-ao,
  title         = "Emergent Analogical Reasoning in Large Language Models",
  author        = "Webb, Taylor and Holyoak, Keith J and Lu, Hongjing",
  abstract      = "The recent advent of large language models has reinvigorated
                   debate over whether human cognitive capacities might emerge
                   in such generic models given sufficient training data. Of
                   particular interest is the ability of these models to reason
                   about novel problems zero-shot, without any direct training.
                   In human cognition, this capacity is closely tied to an
                   ability to reason by analogy. Here, we performed a direct
                   comparison between human reasoners and a large language
                   model (the text-davinci-003 variant of GPT-3) on a range of
                   analogical tasks, including a novel text-based matrix
                   reasoning task closely modeled on Raven's Progressive
                   Matrices. We found that GPT-3 displayed a surprisingly
                   strong capacity for abstract pattern induction, matching or
                   even surpassing human capabilities in most settings. Our
                   results indicate that large language models such as GPT-3
                   have acquired an emergent ability to find zero-shot
                   solutions to a broad range of analogy problems.",
  month         =  dec,
  year          =  2022,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2212.09196",
  primaryClass  = "cs.AI",
  arxivid       = "2212.09196"
}

@ARTICLE{Lewis2022-py,
  title         = "Does {CLIP} Bind Concepts? Probing Compositionality in Large
                   Image Models",
  author        = "Lewis, Martha and Yu, Qinan and Merullo, Jack and Pavlick,
                   Ellie",
  abstract      = "Large-scale models combining text and images have made
                   incredible progress in recent years. However, they can still
                   fail at tasks requiring compositional knowledge, such as
                   correctly picking out a red cube from a picture of multiple
                   shapes. We examine the ability of CLIP (Radford et al.,
                   2021), to caption images requiring compositional knowledge.
                   We implement five compositional language models to probe the
                   kinds of structure that CLIP may be using, and develop a
                   novel training algorithm, Compositional Skipgram for Images
                   (CoSI), to train these models. We look at performance in
                   attribute-based tasks, requiring the identification of a
                   particular combination of attribute and object (such as
                   ``red cube''), and in relational settings, where the spatial
                   relation between two shapes (such as ``cube behind sphere'')
                   must be identified. We find that in some conditions, CLIP is
                   able to learn attribute-object labellings, and to generalize
                   to unseen attribute-object combinations. However, we also
                   see evidence that CLIP is not able to bind features together
                   reliably. Moreover, CLIP is not able to reliably learn
                   relations between objects, whereas some compositional models
                   are able to learn these perfectly. Of the five models we
                   developed, none were able to generalize to unseen relations.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  eprint        = "2212.10537",
  primaryClass  = "cs.CV",
  arxivid       = "2212.10537"
}

@ARTICLE{Hinton2022-lf,
  title         = "The {Forward-Forward} Algorithm: Some Preliminary
                   Investigations",
  author        = "Hinton, Geoffrey",
  abstract      = "The aim of this paper is to introduce a new learning
                   procedure for neural networks and to demonstrate that it
                   works well enough on a few small problems to be worth
                   further investigation. The Forward-Forward algorithm
                   replaces the forward and backward passes of backpropagation
                   by two forward passes, one with positive (i.e. real) data
                   and the other with negative data which could be generated by
                   the network itself. Each layer has its own objective
                   function which is simply to have high goodness for positive
                   data and low goodness for negative data. The sum of the
                   squared activities in a layer can be used as the goodness
                   but there are many other possibilities, including minus the
                   sum of the squared activities. If the positive and negative
                   passes could be separated in time, the negative passes could
                   be done offline, which would make the learning much simpler
                   in the positive pass and allow video to be pipelined through
                   the network without ever storing activities or stopping to
                   propagate derivatives.",
  month         =  dec,
  year          =  2022,
  keywords      = "machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2212.13345",
  primaryClass  = "cs.LG",
  arxivid       = "2212.13345"
}

@ARTICLE{Adaptive_Agent_Team2023-vt,
  title         = "{Human-Timescale} Adaptation in an {Open-Ended} Task Space",
  author        = "{Adaptive Agent Team} and Bauer, Jakob and Baumli, Kate and
                   Baveja, Satinder and Behbahani, Feryal and Bhoopchand,
                   Avishkar and Bradley-Schmieg, Nathalie and Chang, Michael
                   and Clay, Natalie and Collister, Adrian and Dasagi,
                   Vibhavari and Gonzalez, Lucy and Gregor, Karol and Hughes,
                   Edward and Kashem, Sheleem and Loks-Thompson, Maria and
                   Openshaw, Hannah and Parker-Holder, Jack and Pathak, Shreya
                   and Perez-Nieves, Nicolas and Rakicevic, Nemanja and
                   Rockt{\"a}schel, Tim and Schroecker, Yannick and Sygnowski,
                   Jakub and Tuyls, Karl and York, Sarah and Zacherl, Alexander
                   and Zhang, Lei",
  abstract      = "Foundation models have shown impressive adaptation and
                   scalability in supervised and self-supervised learning
                   problems, but so far these successes have not fully
                   translated to reinforcement learning (RL). In this work, we
                   demonstrate that training an RL agent at scale leads to a
                   general in-context learning algorithm that can adapt to
                   open-ended novel embodied 3D problems as quickly as humans.
                   In a vast space of held-out environment dynamics, our
                   adaptive agent (AdA) displays on-the-fly hypothesis-driven
                   exploration, efficient exploitation of acquired knowledge,
                   and can successfully be prompted with first-person
                   demonstrations. Adaptation emerges from three ingredients:
                   (1) meta-reinforcement learning across a vast, smooth and
                   diverse task distribution, (2) a policy parameterised as a
                   large-scale attention-based memory architecture, and (3) an
                   effective automated curriculum that prioritises tasks at the
                   frontier of an agent's capabilities. We demonstrate
                   characteristic scaling laws with respect to network size,
                   memory length, and richness of the training task
                   distribution. We believe our results lay the foundation for
                   increasingly general and adaptive RL agents that perform
                   well across ever-larger open-ended domains.",
  month         =  jan,
  year          =  2023,
  keywords      = "skimmed;comp-cog-sci;RL;project 1",
  archivePrefix = "arXiv",
  eprint        = "2301.07608",
  primaryClass  = "cs.LG",
  arxivid       = "2301.07608"
}

@ARTICLE{Koh2023-oc,
  title         = "Grounding Language Models to Images for Multimodal
                   Generation",
  author        = "Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel",
  abstract      = "We propose an efficient method to ground pretrained
                   text-only language models to the visual domain, enabling
                   them to process and generate arbitrarily interleaved
                   image-and-text data. Our method leverages the abilities of
                   language models learnt from large scale text-only
                   pretraining, such as in-context learning and free-form text
                   generation. We keep the language model frozen, and finetune
                   input and output linear layers to enable cross-modality
                   interactions. This allows our model to process arbitrarily
                   interleaved image-and-text inputs, and generate free-form
                   text interleaved with retrieved images. We achieve strong
                   zero-shot performance on grounded tasks such as contextual
                   image retrieval and multimodal dialogue, and showcase
                   compelling interactive abilities. Our approach works with
                   any off-the-shelf language model and paves the way towards
                   an effective, general solution for leveraging pretrained
                   language models in visually grounded settings.",
  month         =  jan,
  year          =  2023,
  keywords      = "skimmed;compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2301.13823",
  primaryClass  = "cs.CL",
  arxivid       = "2301.13823"
}

@ARTICLE{Marjieh2023-jy,
  title         = "What Language Reveals about Perception: Distilling
                   Psychophysical Knowledge from Large Language Models",
  author        = "Marjieh, Raja and Sucholutsky, Ilia and van Rijn, Pol and
                   Jacoby, Nori and Griffiths, Thomas L",
  abstract      = "Understanding the extent to which the perceptual world can
                   be recovered from language is a fundamental problem in
                   cognitive science. We reformulate this problem as that of
                   distilling psychophysical information from text and show how
                   this can be done by combining large language models (LLMs)
                   with a classic psychophysical method based on similarity
                   judgments. Specifically, we use the prompt auto-completion
                   functionality of GPT3, a state-of-the-art LLM, to elicit
                   similarity scores between stimuli and then apply
                   multidimensional scaling to uncover their underlying
                   psychological space. We test our approach on six perceptual
                   domains and show that the elicited judgments strongly
                   correlate with human data and successfully recover
                   well-known psychophysical structures such as the color wheel
                   and pitch spiral. We also explore meaningful divergences
                   between LLM and human representations. Our work showcases
                   how combining state-of-the-art machine models with
                   well-known cognitive paradigms can shed new light on
                   fundamental questions in perception and language research.",
  month         =  feb,
  year          =  2023,
  keywords      = "read",
  archivePrefix = "arXiv",
  eprint        = "2302.01308",
  primaryClass  = "cs.CL",
  arxivid       = "2302.01308"
}

@ARTICLE{Carta2023-ng,
  title         = "Grounding Large Language Models in Interactive Environments
                   with Online Reinforcement Learning",
  author        = "Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and
                   Lamprier, Sylvain and Sigaud, Olivier and Oudeyer,
                   Pierre-Yves",
  abstract      = "Recent works successfully leveraged Large Language Models'
                   (LLM) abilities to capture abstract knowledge about world's
                   physics to solve decision-making problems. Yet, the
                   alignment between LLMs' knowledge and the environment can be
                   wrong and limit functional competence due to lack of
                   grounding. In this paper, we study an approach to achieve
                   this alignment through functional grounding: we consider an
                   agent using an LLM as a policy that is progressively updated
                   as the agent interacts with the environment, leveraging
                   online Reinforcement Learning to improve its performance to
                   solve goals. Using an interactive textual environment
                   designed to study higher-level forms of functional
                   grounding, and a set of spatial and navigation tasks, we
                   study several scientific questions: 1) Can LLMs boost sample
                   efficiency for online learning of various RL tasks? 2) How
                   can it boost different forms of generalization? 3) What is
                   the impact of online learning? We study these questions by
                   functionally grounding several variants (size, architecture)
                   of FLAN-T5.",
  month         =  feb,
  year          =  2023,
  keywords      = "read;ARC Project;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2302.02662",
  primaryClass  = "cs.LG",
  arxivid       = "2302.02662"
}

@ARTICLE{Mialon2023-jz,
  title         = "Augmented Language Models: a Survey",
  author        = "Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli,
                   Maria and Nalmpantis, Christoforos and Pasunuru, Ram and
                   Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo
                   and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave,
                   Edouard and LeCun, Yann and Scialom, Thomas",
  abstract      = "This survey reviews works in which language models (LMs) are
                   augmented with reasoning skills and the ability to use
                   tools. The former is defined as decomposing a potentially
                   complex task into simpler subtasks while the latter consists
                   in calling external modules such as a code interpreter. LMs
                   can leverage these augmentations separately or in
                   combination via heuristics, or learn to do so from
                   demonstrations. While adhering to a standard missing tokens
                   prediction objective, such augmented LMs can use various,
                   possibly non-parametric external modules to expand their
                   context processing ability, thus departing from the pure
                   language modeling paradigm. We therefore refer to them as
                   Augmented Language Models (ALMs). The missing token
                   objective allows ALMs to learn to reason, use tools, and
                   even act, while still performing standard natural language
                   tasks and even outperforming most regular LMs on several
                   benchmarks. In this work, after reviewing current advance in
                   ALMs, we conclude that this new research direction has the
                   potential to address common limitations of traditional LMs
                   such as interpretability, consistency, and scalability
                   issues.",
  month         =  feb,
  year          =  2023,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2302.07842",
  primaryClass  = "cs.CL",
  arxivid       = "2302.07842"
}

@ARTICLE{Ainooson2023-ad,
  title         = "An Approach for Solving Tasks on the Abstract Reasoning
                   Corpus",
  author        = "Ainooson, James and Sanyal, Deepayan and Michelson, Joel P
                   and Yang, Yuan and Kunda, Maithilee",
  abstract      = "The Abstract Reasoning Corpus (ARC) is an intelligence tests
                   for measuring fluid intelligence in artificial intelligence
                   systems and humans alike. In this paper we present a system
                   for reasoning about and solving ARC tasks. Our system relies
                   on a program synthesis approach that searches a space of
                   potential programs for ones that can solve tasks from the
                   ARC. Programs are in a domain specific language, and in some
                   instances our search algorithm is guided by insights from a
                   corpus of ground truth programs. In particular: We describe
                   an imperative style domain specific language, called Visual
                   Imagery Reasoning Language (VIMRL), for reasoning about
                   tasks in the ARC. We also demonstrate an innovative approach
                   for how large search spaces can be decomposed using special
                   high level functions that determine their own arguments
                   through local searches on a given task item. Finally, we
                   share our results obtained on the publicly available ARC
                   items as well as our system's strong performance on a
                   private test, recently tying for 4th place on the global
                   ARCathon 2022 challenge.",
  month         =  feb,
  year          =  2023,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2302.09425",
  primaryClass  = "cs.AI",
  arxivid       = "2302.09425"
}

@ARTICLE{Lowe2023-jf,
  title         = "Regularised neural networks mimic human insight",
  author        = "L{\"o}we, Anika T and Touzo, L{\'e}o and Muhle-Karbe, Paul S
                   and Saxe, Andrew M and Summerfield, Christopher and Schuck,
                   Nicolas W",
  abstract      = "Humans sometimes show sudden improvements in task
                   performance that have been linked to moments of insight.
                   Such insight-related performance improvements appear special
                   because they are preceded by an extended period of impasse,
                   are unusually abrupt, and occur only in some, but not all,
                   learners. Here, we ask whether insight-like behaviour also
                   occurs in artificial neural networks trained with gradient
                   descent algorithms. We compared learning dynamics in humans
                   and regularised neural networks in a perceptual decision
                   task that provided a hidden opportunity which allowed to
                   solve the task more efficiently. We show that humans tend to
                   discover this regularity through insight, rather than
                   gradually. Notably, neural networks with regularised gate
                   modulation closely mimicked behavioural characteristics of
                   human insights, exhibiting delay of insight, suddenness and
                   selective occurrence. Analyses of network learning dynamics
                   revealed that insight-like behaviour crucially depended on
                   noise added to gradient updates, and was preceded by
                   ``silent knowledge'' that is initially suppressed by
                   regularised (attentional) gating. This suggests that
                   insights can arise naturally from gradual learning, where
                   they reflect the combined influences of noise, attentional
                   gating and regularisation.",
  month         =  feb,
  year          =  2023,
  keywords      = "project 1;Paper1",
  archivePrefix = "arXiv",
  eprint        = "2302.11351",
  primaryClass  = "cs.AI",
  arxivid       = "2302.11351"
}

@ARTICLE{Camposampiero2023-xn,
  title         = "Visual Abstraction and Reasoning through Language",
  author        = "Camposampiero, Giacomo and Houmard, Loic and Estermann,
                   Benjamin and Mathys, Jo{\"e}l and Wattenhofer, Roger",
  abstract      = "While Artificial Intelligence (AI) models have achieved
                   human or even superhuman performance in narrowly defined
                   applications, they still struggle to show signs of broader
                   and more flexible intelligence. The Abstraction and
                   Reasoning Corpus (ARC), introduced by
                   Fran\textbackslashc\{c\}ois Chollet, aims to assess how
                   close AI systems are to human-like cognitive abilities. Most
                   current approaches rely on carefully handcrafted
                   domain-specific languages (DSLs), which are used to
                   brute-force solutions to the tasks present in ARC. In this
                   work, we propose a general framework for solving ARC based
                   on natural language descriptions of the tasks. While not yet
                   beating state-of-the-art DSL models on ARC, we demonstrate
                   the immense potential of our approach hinted at by the
                   ability to solve previously unsolved tasks.",
  month         =  mar,
  year          =  2023,
  keywords      = "read;ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2303.04091",
  primaryClass  = "cs.AI",
  arxivid       = "2303.04091"
}

@ARTICLE{Suris2023-ah,
  title         = "{ViperGPT}: Visual Inference via Python Execution for
                   Reasoning",
  author        = "Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl",
  abstract      = "Answering visual queries is a complex task that requires
                   both visual processing and reasoning. End-to-end models, the
                   dominant approach for this task, do not explicitly
                   differentiate between the two, limiting interpretability and
                   generalization. Learning modular programs presents a
                   promising alternative, but has proven challenging due to the
                   difficulty of learning both the programs and modules
                   simultaneously. We introduce ViperGPT, a framework that
                   leverages code-generation models to compose
                   vision-and-language models into subroutines to produce a
                   result for any query. ViperGPT utilizes a provided API to
                   access the available modules, and composes them by
                   generating Python code that is later executed. This simple
                   approach requires no further training, and achieves
                   state-of-the-art results across various complex visual
                   tasks.",
  month         =  mar,
  year          =  2023,
  keywords      = "read;ARC Project;project 2;machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2303.08128",
  primaryClass  = "cs.CV",
  arxivid       = "2303.08128"
}

@ARTICLE{Park2023-ud,
  title         = "Generative Agents: Interactive Simulacra of Human Behavior",
  author        = "Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and
                   Morris, Meredith Ringel and Liang, Percy and Bernstein,
                   Michael S",
  abstract      = "Believable proxies of human behavior can empower interactive
                   applications ranging from immersive environments to
                   rehearsal spaces for interpersonal communication to
                   prototyping tools. In this paper, we introduce generative
                   agents--computational software agents that simulate
                   believable human behavior. Generative agents wake up, cook
                   breakfast, and head to work; artists paint, while authors
                   write; they form opinions, notice each other, and initiate
                   conversations; they remember and reflect on days past as
                   they plan the next day. To enable generative agents, we
                   describe an architecture that extends a large language model
                   to store a complete record of the agent's experiences using
                   natural language, synthesize those memories over time into
                   higher-level reflections, and retrieve them dynamically to
                   plan behavior. We instantiate generative agents to populate
                   an interactive sandbox environment inspired by The Sims,
                   where end users can interact with a small town of twenty
                   five agents using natural language. In an evaluation, these
                   generative agents produce believable individual and emergent
                   social behaviors: for example, starting with only a single
                   user-specified notion that one agent wants to throw a
                   Valentine's Day party, the agents autonomously spread
                   invitations to the party over the next two days, make new
                   acquaintances, ask each other out on dates to the party, and
                   coordinate to show up for the party together at the right
                   time. We demonstrate through ablation that the components of
                   our agent architecture--observation, planning, and
                   reflection--each contribute critically to the believability
                   of agent behavior. By fusing large language models with
                   computational, interactive agents, this work introduces
                   architectural and interaction patterns for enabling
                   believable simulations of human behavior.",
  month         =  apr,
  year          =  2023,
  keywords      = "LLMs",
  archivePrefix = "arXiv",
  eprint        = "2304.03442",
  primaryClass  = "cs.HC",
  arxivid       = "2304.03442"
}

@ARTICLE{Prystawski2023-hp,
  title         = "Why think step-by-step? Reasoning emerges from the locality
                   of experience",
  author        = "Prystawski, Ben and Goodman, Noah D",
  abstract      = "Humans have a powerful and mysterious capacity to reason. By
                   working through a series of purely mental steps, we can make
                   inferences we would not be capable of making directly --
                   despite that fact that we get no additional data from the
                   world. Similarly, large language models can perform better
                   at complex tasks through chain-of-thought reasoning, where
                   they generate intermediate steps before answering a
                   question. We use language models to investigate the
                   questions of when and why reasoning is helpful, testing the
                   hypothesis that reasoning is effective when training data
                   consisting of local clusters of variables that influence
                   each other strongly. These training conditions enable the
                   chaining of accurate local inferences in order to estimate
                   relationships between variables that were not seen together
                   in training. We train an autoregressive transformer on
                   samples from joint distributions defined by Bayes nets, but
                   only include a subset of all the variables in each sample.
                   We compare language models' ability to match conditional
                   probabilities both with and without intermediate reasoning
                   steps, finding that intermediate steps help only when the
                   training data is locally structured with respect to
                   dependencies between variables. Furthermore, intermediate
                   variables need to be relevant to the relationship between
                   observed information and target inferences. Our results
                   illustrate how the statistical structure of training data
                   drives the effectiveness of reasoning step by step.",
  month         =  apr,
  year          =  2023,
  keywords      = "read;LLMs",
  archivePrefix = "arXiv",
  eprint        = "2304.03843",
  primaryClass  = "cs.AI",
  arxivid       = "2304.03843"
}

@ARTICLE{Chen2023-lu,
  title         = "Teaching Large Language Models to {Self-Debug}",
  author        = "Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and
                   Zhou, Denny",
  abstract      = "Large language models (LLMs) have achieved impressive
                   performance on code generation. However, for complex
                   programming tasks, generating the correct solution in one go
                   becomes challenging, thus some prior works have designed
                   program repair approaches to improve code generation
                   performance. In this work, we propose Self-Debugging, which
                   teaches a large language model to debug its predicted
                   program via few-shot demonstrations. In particular, we
                   demonstrate that Self-Debugging can teach the large language
                   model to perform rubber duck debugging; i.e., without any
                   feedback on the code correctness or error messages, the
                   model is able to identify its mistakes by explaining the
                   generated code in natural language. Self-Debugging achieves
                   the state-of-the-art performance on several code generation
                   benchmarks, including the Spider dataset for text-to-SQL
                   generation, TransCoder for C++-to-Python translation, and
                   MBPP for text-to-Python generation. On the Spider benchmark
                   where there are no unit tests to verify the correctness of
                   predictions, Self-Debugging with code explanation
                   consistently improves the baseline by 2-3\%, and improves
                   the prediction accuracy on problems of the hardest label by
                   9\%. On TransCoder and MBPP where unit tests are available,
                   Self-Debugging improves the baseline accuracy by up to 12\%.
                   Meanwhile, by leveraging feedback messages and reusing
                   failed predictions, Self-Debugging notably improves sample
                   efficiency, and can match or outperform baseline models that
                   generate more than 10x candidate programs.",
  month         =  apr,
  year          =  2023,
  keywords      = "skimmed;ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2304.05128",
  primaryClass  = "cs.CL",
  arxivid       = "2304.05128"
}

@ARTICLE{Balestriero2023-gs,
  title         = "A Cookbook of {Self-Supervised} Learning",
  author        = "Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and
                   Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and
                   Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and
                   Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew
                   Gordon and Geiping, Jonas and Garrido, Quentin and
                   Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and
                   LeCun, Yann and Goldblum, Micah",
  abstract      = "Self-supervised learning, dubbed the dark matter of
                   intelligence, is a promising path to advance machine
                   learning. Yet, much like cooking, training SSL methods is a
                   delicate art with a high barrier to entry. While many
                   components are familiar, successfully training a SSL method
                   involves a dizzying set of choices from the pretext tasks to
                   training hyper-parameters. Our goal is to lower the barrier
                   to entry into SSL research by laying the foundations and
                   latest SSL recipes in the style of a cookbook. We hope to
                   empower the curious researcher to navigate the terrain of
                   methods, understand the role of the various knobs, and gain
                   the know-how required to explore how delicious SSL can be.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  eprint        = "2304.12210",
  primaryClass  = "cs.LG",
  arxivid       = "2304.12210"
}

@ARTICLE{Moskvichev2023-ka,
  title         = "The {ConceptARC} Benchmark: Evaluating Understanding and
                   Generalization in the {ARC} Domain",
  author        = "Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell,
                   Melanie",
  abstract      = "The abilities to form and abstract concepts is key to human
                   intelligence, but such abilities remain lacking in
                   state-of-the-art AI systems. There has been substantial
                   research on conceptual abstraction in AI, particularly using
                   idealized domains such as Raven's Progressive Matrices and
                   Bongard problems, but even when AI systems succeed on such
                   problems, the systems are rarely evaluated in depth to see
                   if they have actually grasped the concepts they are meant to
                   capture. In this paper we describe an in-depth evaluation
                   benchmark for the Abstraction and Reasoning Corpus (ARC), a
                   collection of few-shot abstraction and analogy problems
                   developed by Chollet [2019]. In particular, we describe
                   ConceptARC, a new, publicly available benchmark in the ARC
                   domain that systematically assesses abstraction and
                   generalization abilities on a number of basic spatial and
                   semantic concepts. ConceptARC differs from the original ARC
                   dataset in that it is specifically organized around
                   ``concept groups'' -- sets of problems that focus on
                   specific concepts and that are vary in complexity and level
                   of abstraction. We report results on testing humans on this
                   benchmark as well as three machine solvers: the top two
                   programs from a 2021 ARC competition and OpenAI's GPT-4. Our
                   results show that humans substantially outperform the
                   machine solvers on this benchmark, showing abilities to
                   abstract and generalize concepts that are not yet captured
                   by AI systems. We believe that this benchmark will spur
                   improvements in the development of AI systems for conceptual
                   abstraction and in the effective evaluation of such systems.",
  month         =  may,
  year          =  2023,
  keywords      = "ARC Project",
  archivePrefix = "arXiv",
  eprint        = "2305.07141",
  primaryClass  = "cs.LG",
  arxivid       = "2305.07141"
}

@INCOLLECTION{Lupyan2021-sj,
  title     = "Does {{Vocabulary Help Structure}} the {{Mind}}?",
  booktitle = "Minnesota {{Symposia}} on {{Child Psychology}}",
  author    = "Lupyan, Gary and Zettersten, Martin",
  abstract  = "The idea that language shapes thinking seemed plausible when
               scientists were in the dark about how thinking works. This
               chapter describes several mechanisms by which the words of a
               language can help structure knowledge and navigate cognitive
               problems. It is because thought and language seem so closely
               linked that language is so often used as a window to thought.
               The cognitive priority view faces two serious problems. The
               first is accounting for the cross-linguistic diversity of
               vocabularies. The second problem is the problem of origin. The
               chapter discusses new data aimed at both collecting verbal
               complexity measures independently from the original Bongard
               problems themselves, and collecting more objective measures of
               solution accuracy. It provides further evidence for the idea
               that easier-to-name visual features are more likely to be used
               by people when judging visual similarity.",
  publisher = "John Wiley \& Sons, Ltd",
  pages     = "160--199",
  year      =  2021,
  keywords  = "Bongard problems,cognitive priority,cross-linguistic
               diversity,name visual features,verbal complexity,visual
               similarity,vocabularies;comp-cog-sci",
  isbn      = "9781119684527",
  doi       = "10.1002/9781119684527.ch6"
}

@ARTICLE{Hill2019-uj,
  title    = "{{{BDNF}}}, Endurance Activity, and Mechanisms Underlying the
              Evolution of Hominin Brains",
  author   = "Hill, Tyler and Polk, John D",
  abstract = "Objectives As a complex, polygenic trait, brain size has likely
              been influenced by a range of direct and indirect selection
              pressures for both cognitive and non-cognitive functions and
              capabilities. It has been hypothesized that hominin brain
              expansion was, in part, a correlated response to selection acting
              on aerobic capacity (Raichlen \& Polk, 2013). According to this
              hypothesis, selection for aerobic capacity increased the activity
              of various signaling molecules, including those involved in brain
              growth. One key molecule is brain-derived neurotrophic factor
              (BDNF), a protein that regulates neuronal development, survival,
              and plasticity in mammals. This review updates, partially tests,
              and expands Raichlen and Polk's (2013) hypothesis by evaluating
              evidence for BDNF as a mediator of brain size. Discussion We
              contend that selection for endurance capabilities in a hot
              climate favored changes to muscle composition, mitochondrial
              dynamics and increased energy budget through pathways involving
              regulation of PGC-1$\alpha$ and MEF2 genes, both of which promote
              BDNF activity. In addition, the evolution of hairlessness and the
              skin's thermoregulatory response provide other molecular pathways
              that promote both BDNF activity and neurotransmitter synthesis.
              We discuss how these pathways contributed to the evolution of
              brain size and function in human evolution and propose avenues
              for future research. Our results support Raichlen and Polk's
              contention that selection for non-cognitive functions has direct
              mechanistic linkages to the evolution of brain size in hominins.",
  journal  = "Am. J. Phys. Anthropol.",
  volume   =  168,
  number   = "S67",
  pages    = "47--62",
  year     =  2019,
  keywords = "BDNF,brain
              growth,exercise,MEF2,neurotrophins,PGC-1$\alpha$,thermoregulation;exercise-brain",
  issn     = "0002-9483, 1096-8644",
  doi      = "10.1002/ajpa.23762"
}

@ARTICLE{Yonelinas2010-bt,
  title    = "Recollection and familiarity: examining controversial assumptions
              and new directions",
  author   = "Yonelinas, Andrew P and Aly, Mariam and Wang, Wei-Chun and Koen,
              Joshua D",
  abstract = "It is well accepted that recognition memory reflects the
              contribution of two separable memory retrieval processes, namely
              recollection and familiarity. However, fundamental questions
              remain regarding the functional nature and neural substrates of
              these processes. In this article, we describe a simple
              quantitative model of recognition memory (i.e., the dual-process
              signal detection model) that has been useful in integrating
              findings from a broad range of cognitive studies, and that is now
              being applied in a growing number of neuroscientific
              investigations of memory. The model makes several strong
              assumptions about the behavioral nature and neural substrates of
              recollection and familiarity. A review of the literature
              indicates that these assumptions are generally well supported,
              but that there are clear boundary conditions in which these
              assumptions break down. We argue that these findings provide
              important insights into the operation of the processes underlying
              recognition. Finally, we consider how the dual-process approach
              relates to recent neuroanatomical and computational models and
              how it might be integrated with recent findings concerning the
              role of medial temporal lobe regions in other cognitive functions
              such as novelty detection, perception, implicit memory and
              short-term memory.",
  journal  = "Hippocampus",
  volume   =  20,
  number   =  11,
  pages    = "1178--1194",
  month    =  nov,
  year     =  2010,
  keywords = "l\&m final",
  language = "en",
  issn     = "1050-9631, 1098-1063",
  pmid     = "20848606",
  doi      = "10.1002/hipo.20864",
  pmc      = "PMC4251874"
}

@ARTICLE{Newcombe2013-vt,
  title    = "Cognitive development: changing views of cognitive change",
  author   = "Newcombe, Nora S",
  abstract = "UNLABELLED: The aim of research in cognitive development is to
              understand the origins of human knowledge and to provide an
              account of cognitive change. Theorizing regarding these issues is
              rooted in the nativist-empiricist debate. This article traces
              changing views in that debate, from the beginnings of psychology,
              through the cognitive revolution, Piaget, and alternatives to
              Piaget, including nativism, Vygotskyan theory, and
              information-processing work. The last section presents current
              theorizing and outlines various modern versions of nativism,
              constructivism, and empiricism. WIREs Cogn Sci 2013, 4:479-491.
              doi: 10.1002/wcs.1245 For further resources related to this
              article, please visit the WIREs website. CONFLICT OF INTEREST:
              The author has declared no conflicts of interest for this
              article.",
  journal  = "Wiley Interdiscip. Rev. Cogn. Sci.",
  volume   =  4,
  number   =  5,
  pages    = "479--491",
  month    =  sep,
  year     =  2013,
  keywords = "dev-cog-neuro;development",
  language = "en",
  issn     = "1939-5078, 1939-5086",
  pmid     = "26304241",
  doi      = "10.1002/wcs.1245"
}

@ARTICLE{Glenberg2000-dv,
  title    = "Symbol Grounding and Meaning: A Comparison of {High-Dimensional}
              and Embodied Theories of Meaning",
  author   = "Glenberg, Arthur M and Robertson, David A",
  abstract = "Latent Semantic Analysis (Landauer \& Dumais, 1997) and
              Hyperspace Analogue to Language (Burgess \& Lund, 1997) model
              meaning as the relations among abstract symbols that are
              arbitrarily related to what they signify. These symbols are
              ungrounded in that they are not tied to perceptual experience or
              action. Because the symbols are ungrounded, they cannot, in
              principle, capture the meaning of novel situations. In contrast,
              participants in three experiments found it trivially easy to
              discriminate between descriptions of sensible novel situations
              (e.g., using a newspaper to protect one's face from the wind) and
              nonsense novel situations (e.g., using a matchbook to protect
              one's face from the wind). These results support the Indexical
              Hypothesis that the meaning of a sentence is constructed by (a)
              indexing words and phrases to real objects or perceptual, analog
              symbols; (b) deriving affordances from the objects and symbols;
              and (c) meshing the affordances under the guidance of syntax.",
  journal  = "J. Mem. Lang.",
  volume   =  43,
  number   =  3,
  pages    = "379--401",
  month    =  oct,
  year     =  2000,
  keywords = "meaning; language; embodiment; computational models; Latent
              Semantic Analysis; Hyperspace Analogue to
              Language;compling-cogsci2023",
  issn     = "0749-596X",
  doi      = "10.1006/jmla.2000.2714"
}

@ARTICLE{Howard2002-ir,
  title    = "A Distributed Representation of Temporal Context",
  author   = "Howard, Marc W and Kahana, Michael J",
  abstract = "The principles of recency and contiguity are two cornerstones of
              the theoretical and empirical analysis of human memory. Recency
              has been alternatively explained by mechanisms of decay,
              displacement, and retroactive interference. Another account of
              recency is based on the idea of variable context (Estes, 1955;
              Mensink \& Raaijmakers, 1989). Such notions are typically cast in
              terms of a randomly fluctuating population of elements reflective
              of subtle changes in the environment or in the subjects' mental
              state. This random context view has recently been incorporated
              into distributed and neural network memory models (Murdock, 1997;
              Murdock, Smith, \& Bai, 2001). Here we propose an alternative
              model. Rather than being driven by random fluctuations, this
              formulation, the temporal context model (TCM), uses retrieval of
              prior contextual states to drive contextual drift. In TCM,
              retrieved context is an inherently asymmetric retrieval cue. This
              allows the model to provide a principled explanation of the
              widespread advantage for forward recalls in free and serial
              recall. Modeling data from single-trial free recall, we
              demonstrate that TCM can simultaneously explain recency and
              contiguity effects across time scales.",
  journal  = "J. Math. Psychol.",
  volume   =  46,
  number   =  3,
  pages    = "269--299",
  month    =  jun,
  year     =  2002,
  keywords = "free recall; temporal context; episodic
              memory;skimmed;learning\&memory2023",
  issn     = "0022-2496",
  doi      = "10.1006/jmps.2001.1388"
}

@BOOK{Epstein2009-te,
  title     = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  publisher = "Springer Netherlands",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "comp-cog-sci",
  isbn      = "9781402096242, 9781402067105",
  doi       = "10.1007/978-1-4020-6710-5"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Turing2009-sc,
  title     = "Computing {{Machinery}} and {{Intelligence}}",
  booktitle = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  author    = "Turing, Alan M",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  abstract  = "I propose to consider the question, ``Can machines think?''â™£
               This should begin with definitions of the meaning of the terms
               ``machine'' and ``think''. The definitions might be framed so as
               to reflect so far as possible the normal use of the words, but
               this attitude is dangerous. If the meaning of the words
               ``machine'' and ``think'' are to be found by examining how they
               are commonly used it is difficult to escape the conclusion that
               the meaning and the answer to the question, ``Can machines
               think?'' is to be sought in a statistical survey such as a
               Gallup poll.",
  publisher = "Springer Netherlands",
  pages     = "23--65",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "Computing Machinery,Digital Computer,Performance Capacity,Real
               Robot,Turing Machine;comp-cog-sci",
  isbn      = "9781402067105",
  doi       = "10.1007/978-1-4020-6710-5\_3"
}

@INCOLLECTION{Liebana2020-zp,
  title     = "{VGDL} and the {GVGAI} Framework",
  booktitle = "General Video Game Artificial Intelligence",
  author    = "Li{\'e}bana, Diego P{\'e}rez",
  editor    = "Li{\'e}bana, Diego P{\'e}rez and Lucas, Simon M and Gaina,
               Raluca D and Togelius, Julian and Khalifa, Ahmed and Liu, Jialin",
  publisher = "Springer International Publishing",
  pages     = "13--30",
  year      =  2020,
  address   = "Cham",
  isbn      = "9783031021220",
  doi       = "10.1007/978-3-031-02122-0\_2"
}

@INPROCEEDINGS{Anderson2018-ox,
  title     = "Deceptive Games",
  booktitle = "Applications of Evolutionary Computation",
  author    = "Anderson, Damien and Stephenson, Matthew and Togelius, Julian
               and Salge, Christoph and Levine, John and Renz, Jochen",
  abstract  = "Deceptive games are games where the reward structure or other
               aspects of the game are designed to lead the agent away from a
               globally optimal policy. While many games are already deceptive
               to some extent, we designed a series of games in the Video Game
               Description Language (VGDL) implementing specific types of
               deception, classified by the cognitive biases they exploit. VGDL
               games can be run in the General Video Game Artificial
               Intelligence (GVGAI) Framework, making it possible to test a
               variety of existing AI agents that have been submitted to the
               GVGAI Competition on these deceptive games. Our results show
               that all tested agents are vulnerable to several kinds of
               deception, but that different agents have different weaknesses.
               This suggests that we can use deception to understand the
               capabilities of a game-playing algorithm, and game-playing
               algorithms to characterize the deception displayed by a game.",
  publisher = "Springer International Publishing",
  pages     = "376--391",
  year      =  2018,
  keywords  = "read;project 1;RL;Paper1",
  doi       = "10.1007/978-3-319-77538-8\_26"
}

@ARTICLE{Sutton1988-ad,
  title     = "Learning to predict by the methods of temporal differences",
  author    = "Sutton, Richard S",
  abstract  = "This article introduces a class of incremental learning
               procedures specialized for prediction-that is, for using past
               experience with an incompletely known system to predict its
               future behavior. Whereas conventional prediction-learning
               methods assign credit by means of the difference between
               predicted and actual outcomes, the new methods assign credit by
               means of the difference between temporally successive
               predictions. Although such temporal-difference methods have been
               used in Samuel's checker player, Holland's bucket brigade, and
               the author's Adaptive Heuristic Critic, they have remained
               poorly understood. Here we prove their convergence and
               optimality for special cases and relate them to
               supervised-learning methods. For most real-world prediction
               problems, temporal-difference methods require less memory and
               less peak computation than conventional methods and they produce
               more accurate predictions. We argue that most problems to which
               supervised learning is currently applied are really prediction
               problems of the sort to which temporal-difference methods can be
               applied to advantage.",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  3,
  number    =  1,
  pages     = "9--44",
  month     =  aug,
  year      =  1988,
  keywords  = "RL;ccm2023",
  issn      = "0885-6125, 1573-0565",
  doi       = "10.1007/BF00115009"
}

@ARTICLE{Bourlard1988-if,
  title    = "{Auto-Association} by Multilayer Perceptrons and Singular Value
              Decomposition",
  author   = "Bourlard, H and Kamp, Y",
  abstract = "The multilayer perceptron, when working in auto-association mode,
              is sometimes considered as an interesting candidate to perform
              data compression or dimensionality reduction of the feature space
              in information processing applications. The present paper shows
              that, for auto-association, the nonlinearities of the hidden
              units are useless and that the optimal parameter values can be
              derived directly by purely linear techniques relying on singular
              value decomposition and low rank matrix approximation, similar in
              spirit to the well-known Karhunen-Lo6ve transform. This approach
              appears thus as an efficient alternative to the general error
              back-propagation algorithm commonly used for training multilayer
              perceptrons. Moreover, it also gives a clear interpretation of
              the r61e of the different parameters.",
  journal  = "Biol. Cybern.",
  volume   =  59,
  number   = "4-5",
  pages    = "291--294",
  month    =  sep,
  year     =  1988,
  keywords = "comp-cog-sci",
  issn     = "0340-1200, 1432-0770",
  doi      = "10.1007/BF00332918"
}

@ARTICLE{Ollinger2014-qq,
  title    = "The dynamics of search, impasse, and representational change
              provide a coherent explanation of difficulty in the nine-dot
              problem",
  author   = "{\"O}llinger, Michael and Jones, Gary and Knoblich, G{\"u}nther",
  abstract = "The nine-dot problem is often used to demonstrate and explain
              mental impasse, creativity, and out of the box thinking. The
              present study investigated the interplay of a restricted initial
              search space, the likelihood of invoking a representational
              change, and the subsequent constraining of an unrestricted search
              space. In three experimental conditions, participants worked on
              different versions of the nine-dot problem that hinted at
              removing particular sources of difficulty from the standard
              problem. The hints were incremental such that the first suggested
              a possible route for a solution attempt; the second additionally
              indicated the dot at which lines meet on the solution path; and
              the final condition also provided non-dot locations that appear
              in the solution path. The results showed that in the experimental
              conditions, representational change is encountered more quickly
              and problems are solved more often than for the control group. We
              propose a cognitive model that focuses on general problem-solving
              heuristics and representational change to explain problem
              difficulty.",
  journal  = "Psychol. Res.",
  volume   =  78,
  number   =  2,
  pages    = "266--275",
  month    =  mar,
  year     =  2014,
  keywords = "read;project 1;Paper1",
  language = "en",
  issn     = "0340-0727, 1430-2772",
  pmid     = "23708954",
  doi      = "10.1007/s00426-013-0494-8"
}

@ARTICLE{Piantadosi2021-ie,
  title    = "The {{Computational Origin}} of {{Representation}}",
  author   = "Piantadosi, Steven T",
  abstract = "Each of our theories of mental representation provides some
              insight into how the mind works. However, these insights often
              seem incompatible, as the debates between symbolic, dynamical,
              emergentist, sub-symbolic, and grounded approaches to cognition
              attest. Mental representations---whatever they are---must share
              many features with each of our theories of representation, and
              yet there are few hypotheses about how a synthesis could be
              possible. Here, I develop a theory of the underpinnings of
              symbolic cognition that shows how sub-symbolic dynamics may give
              rise to higher-level cognitive representations of structures,
              systems of knowledge, and algorithmic processes. This theory
              implements a version of conceptual role semantics by positing an
              internal universal representation language in which learners may
              create mental models to capture dynamics they observe in the
              world. The theory formalizes one account of how truly novel
              conceptual content may arise, allowing us to explain how even
              elementary logical and computational operations may be learned
              from a more primitive basis. I provide an implementation that
              learns to represent a variety of structures, including logic,
              number, kinship trees, regular languages, context-free languages,
              domains of theories like magnetism, dominance hierarchies, list
              structures, quantification, and computational primitives like
              repetition, reversal, and recursion. This account is based on
              simple discrete dynamical processes that could be implemented in
              a variety of different physical or biological systems. In
              particular, I describe how the required dynamics can be directly
              implemented in a connectionist framework. The resulting theory
              provides an ``assembly language'' for cognition, where high-level
              theories of symbolic computation can be implemented in simple
              dynamics that themselves could be encoded in biologically
              plausible systems.",
  journal  = "Minds Mach.",
  volume   =  31,
  number   =  1,
  pages    = "1--58",
  month    =  mar,
  year     =  2021,
  keywords = "comp-cog-sci",
  issn     = "0924-6495, 1572-8641",
  doi      = "10.1007/s11023-020-09540-9"
}

@ARTICLE{Lake2020-wr,
  title    = "People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few
              Examples}}",
  author   = "Lake, Brenden M and Piantadosi, Steven T",
  abstract = "Machine learning has made major advances in categorizing objects
              in images, yet the best algorithms miss important aspects of how
              people learn and think about categories. People can learn richer
              concepts from fewer examples, including causal models that
              explain how members of a category are formed. Here, we explore
              the limits of this human ability to infer causal
              ``programs''---latent generating processes with nontrivial
              algorithmic properties---from one, two, or three visual examples.
              People were asked to extrapolate the programs in several ways,
              for both classifying and generating new examples. As a theory of
              these inductive abilities, we present a Bayesian program learning
              model that searches the space of programs for the best
              explanation of the observations. Although variable, people's
              judgments are broadly consistent with the model and inconsistent
              with several alternatives, including a pretrained deep neural
              network for object recognition, indicating that people can learn
              and reason with rich algorithmic abstractions from sparse input
              data.",
  journal  = "Computational Brain \& Behavior",
  volume   =  3,
  number   =  1,
  pages    = "54--65",
  month    =  mar,
  year     =  2020,
  keywords = "comp-cog-sci",
  issn     = "2522-0861, 2522-087X",
  doi      = "10.1007/s42113-019-00053-y"
}

@ARTICLE{Nosofsky2022-fh,
  title    = "Generalization in Distant Regions of a {Rule-Described} Category
              Space: a Mixed Exemplar and {Logical-Rule-Based} Account",
  author   = "Nosofsky, Robert M and Hu, Mingjia",
  abstract = "An important question in the cognitive-psychology of category
              learning concerns the manner in which observers generalize their
              trained category knowledge at time of transfer. In recent work,
              Conaway and Kurtz (Conaway and Kurtz, Psychonomic Bulletin \&
              Review 24:1312--1323, 2017) reported results from a novel
              paradigm in which participants learned rule-described categories
              defined over two dimensions and then classified test items in
              distant transfer regions of the category space. The paradigm
              yielded results that challenged the predictions from both
              exemplar-based and logical-rule-based models of categorization
              but that the authors suggested were as predicted by a divergent
              auto-encoder (DIVA) model (Kurtz, Psychonomic Bulletin \& Review
              14:560--576, 2007, Kurtz, Psychology of learning and motivation,
              Academic Press, New York, 2015). In this article, we pursue these
              challenges by conducting replications and extensions of the
              original experiment and fitting a variety of computational models
              to the resulting data. We find that even an extended version of
              the exemplar model that makes allowance for learning-during-test
              (LDT) processes fails to account for the results. In addition,
              models that presume a mixture of salient logical rules also fail
              to account for the findings. However, as a proof of concept, we
              illustrate that a model that assumes a mixture of strategies
              across subjects---some relying on exemplar-based memories with
              LDT, and others on salient logical rules---provides an
              outstanding account of the data. By comparison, DIVA performs
              considerably worse than does this LDT-exemplar-rule mixture
              account. These results converge with past ones reported in the
              literature that point to multiple forms of category
              representation as well as to the role of LDT processes in
              influencing how observers generalize their category knowledge.",
  journal  = "Computational Brain \& Behavior",
  volume   =  5,
  number   =  4,
  pages    = "435--466",
  month    =  dec,
  year     =  2022,
  keywords = "comp-cog-sci",
  issn     = "2522-087X",
  doi      = "10.1007/s42113-022-00151-4"
}

@ARTICLE{Brooks1991-sg,
  title    = "Intelligence without representation",
  author   = "Brooks, Rodney A",
  abstract = "Artificial intelligence research has foundered on the issue of
              representation. When intelligence is approached in an incremental
              manner, with strict reliance on interfacing to the real world
              through perception and action, reliance on representation
              disappears. In this paper we outline our approach to
              incrementally building complete intelligent Creatures. The
              fundamental decomposition of the intelligent system is not into
              independent information processing units which must interface
              with each other via representations. Instead, the intelligent
              system is decomposed into independent and parallel activity
              producers which all interface directly to the world through
              perception and action, rather than interface to each other
              particularly much. The notions of central and peripheral systems
              evaporate---everything is both central and peripheral. Based on
              these principles we have built a very successful series of mobile
              robots which operate without supervision as Creatures in standard
              office environments.",
  journal  = "Artif. Intell.",
  volume   =  47,
  number   =  1,
  pages    = "139--159",
  month    =  jan,
  year     =  1991,
  keywords = "project 2",
  issn     = "0004-3702",
  doi      = "10.1016/0004-3702(91)90053-M"
}

@ARTICLE{Fodor1988-gq,
  title    = "Connectionism and Cognitive Architecture: {{A}} Critical Analysis",
  author   = "Fodor, Jerry A and Pylyshyn, Zenon W",
  abstract = "This paper explores differences between Connectionist proposals
              for cognitive architecture and the sorts of models that have
              traditionally been assumed in cognitive science. We claim that
              the major distinction is that, while both Connectionist and
              Classical architectures postulate representational mental states,
              the latter but not the former are committed to a symbol-level of
              representation, or to a `language of thought': i.e., to
              representational states that have combinatorial syntactic and
              semantic structure. Several arguments for combinatorial structure
              in mental representations are then reviewed. These include
              arguments based on the `systematicity' of mental representation:
              i.e., on the fact that cognitive capacities always exhibit
              certain symmetries, so that the ability to entertain a given
              thought implies the ability to entertain thoughts with
              semantically related contents. We claim that such arguments make
              a powerful case that mind/brain architecture is not Connectionist
              at the cognitive level. We then consider the possibility that
              Connectionism may provide an account of the neural (or `abstract
              neurological') structures in which Classical cognitive
              architecture is implemented. We survey a number of the standard
              arguments that have been offered in favor of Connectionism, and
              conclude that they are coherent only on this interpretation.
              R{\'e}sum{\'e} Cet article{\'e}tudie les diff{\'e}rences entre
              mod{\`e}les connectionistes et mod{\`e}les classiques de la
              structure cognitive. Nous pensons que, bien que les deux types de
              mod{\`e}les stipulent l'existence d'{\'e}tats mentaux
              repr{\'e}sentationnels, la diff{\'e}rence essentielle est que
              seuls les mod{\`e}les classiques requi{\`e}rent l'existence d'un
              niveau de repr{\'e}sentation symbolique---un ``langage de la
              pens{\'e}e''---, c'est-{\`a}-dire d'{\'e}tats
              repr{\'e}sentationnels poss{\'e}dant une structure syntaxique et
              s{\'e}mantique. Nous examinons ensuite diff{\'e}rents arguments
              qui militent en faveur de l'existence de repr{\'e}sentations
              mentales ayant ces propri{\'e}t{\'e}s. Certains de ces arguments
              reposent sur la ``syst{\'e}maticit{\'e}'' des repr{\'e}sentations
              mentales, c'est-{\`a}-dire sur le fait que les capacit{\'e}s
              cognitives exhibent toujours certaines sym{\'e}tries, de sorte
              que la capacit{\'e}d'entretenir certaines pens{\'e}es implique la
              capacit{\'e}d'entretenir d'autres pens{\'e}es apparent{\'e}es par
              leur contenu s{\'e}mantique. Nous pensons que ces arguments
              montrent de mani{\`e}re convainquante que l'architecture de
              l'esprit/du cerveau n'est pas connectioniste au niveau cognitif.
              Nous nous demandons ensuite s'il est possible d'interpr{\'e}ter
              le connectionisme comme une analyse des structures neuronales (ou
              des structures neurologiques ``abstraites'') dans lesquelles est
              r{\'e}alis{\'e}e l'architecture cognitive classique. Nous
              examinons plusieurs des arguments avanc{\'e}s habituellement en
              d{\'e}fense du connectionisme, et en concluons que ceux-ci n'ont
              de sens que dans cette interpr{\'e}tation.",
  journal  = "Cognition",
  volume   =  28,
  number   =  1,
  pages    = "3--71",
  year     =  1988,
  keywords = "comp-cog-sci",
  issn     = "0010-0277",
  doi      = "10.1016/0010-0277(88)90031-5"
}

@ARTICLE{Kaplan1990-zd,
  title    = "In search of insight",
  author   = "Kaplan, Craig A and Simon, Herbert A",
  abstract = "This paper describes the process of attaining the insight
              required to solve a particular problem---the Mutilated
              Checkerboard (MC) problem. It shows that attaining insight
              requires discovering an effective problem representation, and
              that performance on insight problems can be predicted from the
              availability of generators and constraints in the search for such
              a representation. To test these claims we varied the salience of
              features leading to the critical concept of parity in the MC
              problem. Using chronometric measures, verbal protocols, and
              computer simulations, we explored first why it is difficult to
              find a representation for the Checkerboard problem, and then
              tested four potential sources of search constraint for reducing
              the difficulty: cue salience manipulations, prior knowledge,
              hints, and heuristics. While subjects used each of these four
              sources of constraint, a particular heuristic---noticing
              properties of the situation that remained invariant during
              solution attempts (the Notice Invariants heuristic)---proved to
              be a particularly powerful means for focusing search. In
              conjunction with hints and independently, it played a major part
              in producing the insight that yielded an effective problem
              representation and solution.",
  journal  = "Cogn. Psychol.",
  volume   =  22,
  number   =  3,
  pages    = "374--419",
  month    =  jul,
  year     =  1990,
  keywords = "comp-cog-sci;project 1;Paper1",
  issn     = "0010-0285",
  doi      = "10.1016/0010-0285(90)90008-R"
}

@ARTICLE{Landau1988-oy,
  title    = "The Importance of Shape in Early Lexical Learning",
  author   = "Landau, Barbara and Smith, Linda B and Jones, Susan S",
  journal  = "Cogn. Dev.",
  volume   =  3,
  number   =  3,
  pages    = "299--321",
  month    =  jul,
  year     =  1988,
  keywords = "development",
  issn     = "0885-2014",
  doi      = "10.1016/0885-2014(88)90014-7"
}

@INCOLLECTION{Daw2014-xf,
  title     = "Chapter 16 - Advanced Reinforcement Learning",
  booktitle = "Neuroeconomics (Second Edition)",
  author    = "Daw, Nathaniel D",
  editor    = "Glimcher, Paul W and Fehr, Ernst",
  abstract  = "This chapter reviews issues of current research in reinforcement
               learning theories and their neural substrates. We consider how
               the formal constructs of states, actions, and rewards that these
               theories describe can be understood to map onto counterparts
               experienced by biological organisms learning in the real world.
               In each case, this correspondence involves significant
               difficulties. However, elaborated theoretical accounts from
               computer science clarify, in each case, how to extend these
               theories to more realistic circumstances while still preserving
               the core prediction error-driven learning mechanism that has
               been prominent in neuroeconomic accounts.",
  publisher = "Academic Press",
  pages     = "299--320",
  month     =  jan,
  year      =  2014,
  address   = "San Diego",
  keywords  = "Dopamine; Hierarchical reinforcement learning; Reinforcement
               learning; Uncertainty;skimmed;RL;ccm2023",
  isbn      = "9780124160088",
  doi       = "10.1016/B978-0-12-416008-8.00016-4"
}

@INCOLLECTION{Forbus1988-zh,
  title     = "Chapter 7 - Qualitative Physics: Past, Present, and Future",
  booktitle = "Exploring Artificial Intelligence",
  author    = "Forbus, Kenneth D",
  editor    = "Shrobe, Howard E and {the American Association for Artificial
               Intelligence}",
  abstract  = "Publisher Summary Qualitative physics is concerned with
               representing and reasoning about the physical world. The goal of
               qualitative physics is to capture both the commonsense knowledge
               of the person on the street and the tacit knowledge underlying
               the quantitative knowledge used by engineers and scientists. The
               key to qualitative physics is to find ways to represent
               continuous properties of the world by discrete systems of
               symbols. One can always quantize something continuous, but not
               all quantizations are equally useful. One way to state the idea
               is the relevance principle: The distinctions made by a
               quantization must be relevant to the kind of reasoning
               performed. This chapter describes what qualitative physics is,
               why one should be doing it, and where it came from. It discusses
               some open problems in qualitative physics.",
  publisher = "Morgan Kaufmann",
  pages     = "239--296",
  month     =  jan,
  year      =  1988,
  keywords  = "skimmed;project 2",
  isbn      = "9780934613675",
  doi       = "10.1016/B978-0-934613-67-5.50011-3"
}

@ARTICLE{Craik1972-vo,
  title    = "Levels of processing: A framework for memory research",
  author   = "Craik, Fergus I M and Lockhart, Robert S",
  abstract = "This paper briefly reviews the evidence for multistore theories
              of memory and points out some difficulties with the approach. An
              alternative framework for human memory research is then outlined
              in terms of depth or levels of processing. Some current data and
              arguments are reexamined in the light of this alternative
              framework and implications for further research considered.",
  journal  = "Journal of Verbal Learning and Verbal Behavior",
  volume   =  11,
  number   =  6,
  pages    = "671--684",
  month    =  dec,
  year     =  1972,
  keywords = "l\&m final",
  issn     = "0022-5371",
  doi      = "10.1016/S0022-5371(72)80001-X"
}

@ARTICLE{Thevenot2008-ob,
  title    = "A generalization of the representational change theory from
              insight to non-insight problems: the case of arithmetic word
              problems",
  author   = "Thevenot, Catherine and Oakhill, Jane",
  abstract = "This paper provides evidence for a possible generalization of
              Knoblich and colleagues' representational change theory
              [Knoblich, G., Ohlsson, S., Haider, H., \& Rhenius, D. (1999).
              Constraint relaxation and chunk decomposition in insight problem
              solving. Journal of Experimental Psychology: Learning, Memory,
              and Cognition, 25, 1534-1555; Knoblich, G., Ohlsson, S., \&
              Raney, G. E. (2001). An eye movement study of insight problem.
              Memory and Cognition, 29, 1000-1009] outside its original scope
              of application. While this theory has been proposed to explain
              insight problem solving, we demonstrate here that its main
              concepts, namely, constraint relaxation and chunk decomposition,
              are applicable to incremental problem solving. In a first
              experiment, we confirm, as already shown by problem solving and
              reasoning researchers, that individuals avoid the construction of
              alternative representations of the problems when possible. In the
              second and third experiments, we show that alternative
              representations of arithmetic problems are easier to construct
              and maintain when they violate constraints of narrow rather than
              wide scope. The specificity of insight problem solving is
              discussed in the light of these new findings.",
  journal  = "Acta Psychol.",
  volume   =  129,
  number   =  3,
  pages    = "315--324",
  month    =  nov,
  year     =  2008,
  keywords = "project 1;Paper1",
  language = "en",
  issn     = "0001-6918, 1873-6297",
  pmid     = "18834964",
  doi      = "10.1016/j.actpsy.2008.08.008"
}

@ARTICLE{Silver2021-ss,
  title    = "Reward is enough",
  author   = "Silver, David and Singh, Satinder and Precup, Doina and Sutton,
              Richard S",
  abstract = "In this article we hypothesise that intelligence, and its
              associated abilities, can be understood as subserving the
              maximisation of reward. Accordingly, reward is enough to drive
              behaviour that exhibits abilities studied in natural and
              artificial intelligence, including knowledge, learning,
              perception, social intelligence, language, generalisation and
              imitation. This is in contrast to the view that specialised
              problem formulations are needed for each ability, based on other
              signals or objectives. Furthermore, we suggest that agents that
              learn through trial and error experience to maximise reward could
              learn behaviour that exhibits most if not all of these abilities,
              and therefore that powerful reinforcement learning agents could
              constitute a solution to artificial general intelligence.",
  journal  = "Artif. Intell.",
  volume   =  299,
  pages    = "103535",
  month    =  oct,
  year     =  2021,
  keywords = "Artificial intelligence; Artificial general intelligence;
              Reinforcement learning; Reward;read;learning\&memory2023;RL",
  issn     = "0004-3702",
  doi      = "10.1016/j.artint.2021.103535"
}

@ARTICLE{Wise2023-jo,
  title    = "Interactive cognitive maps support flexible behavior under threat",
  author   = "Wise, Toby and Charpentier, Caroline J and Dayan, Peter and
              Mobbs, Dean",
  abstract = "In social environments, survival can depend upon inferring and
              adapting to other agents' goal-directed behavior. However, it
              remains unclear how humans achieve this, despite the fact that
              many decisions must account for complex, dynamic agents acting
              according to their own goals. Here, we use a predator-prey task
              (total n = 510) to demonstrate that humans exploit an interactive
              cognitive map of the social environment to infer other agents'
              preferences and simulate their future behavior, providing for
              flexible, generalizable responses. A model-based inverse
              reinforcement learning model explained participants' inferences
              about threatening agents' preferences, with participants using
              this inferred knowledge to enact generalizable, model-based
              behavioral responses. Using tree-search planning models, we then
              found that behavior was best explained by a planning algorithm
              that incorporated simulations of the threat's goal-directed
              behavior. Our results indicate that humans use a cognitive map to
              determine other agents' preferences, facilitating generalized
              predictions of their behavior and effective responses.",
  journal  = "Cell Rep.",
  volume   =  42,
  number   =  8,
  pages    = "113008",
  month    =  aug,
  year     =  2023,
  keywords = "CP: Neuroscience; avoidance; cognitive maps; decision-making;
              learning; planning; social inference;skimmed;comp-cog-sci",
  language = "en",
  issn     = "2211-1247",
  pmid     = "37610871",
  doi      = "10.1016/j.celrep.2023.113008"
}

@ARTICLE{Levine2014-bg,
  title    = "Low {{Protein Intake Is Associated}} with a {{Major Reduction}}
              in {{{IGF-1}}}, {{Cancer}}, and {{Overall Mortality}} in the 65
              and {{Younger}} but {{Not Older Population}}",
  author   = "Levine, Morgan E and Suarez, Jorge A and Brandhorst, Sebastian
              and Balasubramanian, Priya and Cheng, Chia-Wei and Madia,
              Federica and Fontana, Luigi and Mirisola, Mario G and
              Guevara-Aguirre, Jaime and Wan, Junxiang and Passarino, Giuseppe
              and Kennedy, Brian K and Wei, Min and Cohen, Pinchas and
              Crimmins, Eileen M and Longo, Valter D",
  abstract = "Mice and humans with growth hormone receptor/ IGF-1 deficiencies
              display major reductions in agerelated diseases. Because protein
              restriction reduces GHR-IGF-1 activity, we examined links between
              protein intake and mortality. Respondents aged 50--65 reporting
              high protein intake had a 75\% increase in overall mortality and
              a 4-fold increase in cancer death risk during the following 18
              years. These associations were either abolished or attenuated if
              the proteins were plant derived. Conversely, high protein intake
              was associated with reduced cancer and overall mortality in
              respondents over 65, but a 5-fold increase in diabetes mortality
              across all ages. Mouse studies confirmed the effect of high
              protein intake and GHR-IGF-1 signaling on the incidence and
              progression of breast and melanoma tumors, but also the
              detrimental effects of a low protein diet in the very old. These
              results suggest that low protein intake during middle age
              followed by moderate to high protein consumption in old adults
              may optimize healthspan and longevity.",
  journal  = "Cell Metab.",
  volume   =  19,
  number   =  3,
  pages    = "407--417",
  month    =  mar,
  year     =  2014,
  keywords = "longevity",
  issn     = "1550-4131",
  doi      = "10.1016/j.cmet.2014.02.006"
}

@ARTICLE{Griffiths2019-hj,
  title    = "Doing more with less: Meta-reasoning and meta-learning in humans
              and machines",
  author   = "Griffiths, Thomas L and Callaway, Frederick and Chang, Michael B
              and Grant, Erin and Krueger, Paul M and Lieder, Falk",
  abstract = "Artificial intelligence systems use an increasing amount of
              computation and data to solve very specific problems. By
              contrast, human minds solve a wide range of problems using a
              fixed amount of computation and limited experience. We identify
              two abilities that we see as crucial to this kind of general
              intelligence: meta-reasoning (deciding how to allocate
              computational resources) and meta-learning (modeling the learning
              environment to make better use of limited data). We summarize the
              relevant AI literature and relate the resulting ideas to recent
              work in psychology. (PsycINFO Database Record (c) 2019 APA, all
              rights reserved)",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "24--30",
  month    =  oct,
  year     =  2019,
  issn     = "2352-1546, 2352-1554",
  doi      = "10.1016/j.cobeha.2019.01.005"
}

@ARTICLE{Lake2019-zq,
  title    = "The {{Omniglot}} Challenge: A 3-Year Progress Report",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "97--104",
  month    =  oct,
  year     =  2019,
  keywords = "comp-cog-sci",
  issn     = "2352-1546",
  doi      = "10.1016/j.cobeha.2019.04.007"
}

@ARTICLE{Bonawitz2011-pi,
  title    = "The double-edged sword of pedagogy: Instruction limits
              spontaneous exploration and discovery",
  author   = "Bonawitz, Elizabeth and Shafto, Patrick and Gweon, Hyowon and
              Goodman, Noah D and Spelke, Elizabeth and Schulz, Laura",
  abstract = "Motivated by computational analyses, we look at how teaching
              affects exploration and discovery. In Experiment 1, we
              investigated children's exploratory play after an adult
              pedagogically demonstrated a function of a toy, after an
              interrupted pedagogical demonstration, after a na{\"\i}ve adult
              demonstrated the function, and at baseline. Preschoolers in the
              pedagogical condition focused almost exclusively on the target
              function; by contrast, children in the other conditions explored
              broadly. In Experiment 2, we show that children restrict their
              exploration both after direct instruction to themselves and after
              overhearing direct instruction given to another child; they do
              not show this constraint after observing direct instruction given
              to an adult or after observing a non-pedagogical intentional
              action. We discuss these findings as the result of rational
              inductive biases. In pedagogical contexts, a teacher's failure to
              provide evidence for additional functions provides evidence for
              their absence; such contexts generalize from child to child
              (because children are likely to have comparable states of
              knowledge) but not from adult to child. Thus, pedagogy promotes
              efficient learning but at a cost: children are less likely to
              perform potentially irrelevant actions but also less likely to
              discover novel information.",
  journal  = "Cognition",
  volume   =  120,
  number   =  3,
  pages    = "322--330",
  month    =  sep,
  year     =  2011,
  keywords = "development",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "21216395",
  doi      = "10.1016/j.cognition.2010.10.001",
  pmc      = "PMC3369499"
}

@ARTICLE{Gallistel2021-ft,
  title    = "The physical basis of memory",
  author   = "Gallistel, C R",
  abstract = "Neuroscientists are searching for the engram within the
              conceptual framework established by John Locke's theory of mind.
              This framework was elaborated before the development of
              information theory, before the development of information
              processing machines and the science of computation, before the
              discovery that molecules carry hereditary information, before the
              discovery of the codon code and the molecular machinery for
              editing the messages written in this code and translating it into
              transcription factors that mark abstract features of organic
              structure such as anterior and distal. The search for the engram
              needs to abandon Locke's conceptual framework and work within a
              framework informed by these developments. The engram is the
              medium by which information extracted from past experience is
              transmitted to the computations that inform future behavior. The
              information-conveying symbols in the engram are rapidly generated
              in the course of computations, which implies that they are
              molecules.",
  journal  = "Cognition",
  volume   =  213,
  pages    = "104533",
  month    =  aug,
  year     =  2021,
  keywords = "Communication channel; Engram; Molecules; Plastic
              synapse;comp-cog-sci;learning\&memory2023",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "33375954",
  doi      = "10.1016/j.cognition.2020.104533"
}

@ARTICLE{Lakretz2021-go,
  title    = "Mechanisms for handling nested dependencies in neural-network
              language models and humans",
  author   = "Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and
              Marelli, Marco and Baroni, Marco and Dehaene, Stanislas",
  abstract = "Recursive processing in sentence comprehension is considered a
              hallmark of human linguistic abilities. However, its underlying
              neural mechanisms remain largely unknown. We studied whether a
              modern artificial neural network trained with ``deep learning''
              methods mimics a central aspect of human sentence processing,
              namely the storing of grammatical number and gender information
              in working memory and its use in long-distance agreement (e.g.,
              capturing the correct number agreement between subject and verb
              when they are separated by other phrases). Although the network,
              a recurrent architecture with Long Short-Term Memory units, was
              solely trained to predict the next word in a large corpus,
              analysis showed the emergence of a very sparse set of specialized
              units that successfully handled local and long-distance syntactic
              agreement for grammatical number. However, the simulations also
              showed that this mechanism does not support full recursion and
              fails with some long-range embedded dependencies. We tested the
              model's predictions in a behavioral experiment where humans
              detected violations in number agreement in sentences with
              systematic variations in the singular/plural status of multiple
              nouns, with or without embedding. Human and model error patterns
              were remarkably similar, showing that the model echoes various
              effects observed in human data. However, a key difference was
              that, with embedded long-range dependencies, humans remained
              above chance level, while the model's systematic errors brought
              it below chance. Overall, our study shows that exploring the ways
              in which modern artificial neural networks process sentences
              leads to precise and testable hypotheses about human linguistic
              performance.",
  journal  = "Cognition",
  volume   =  213,
  pages    = "104699",
  month    =  aug,
  year     =  2021,
  keywords = "Grammatical agreement; Language models; Long-range dependencies;
              Recurrent neural networks; Recursion; Relative clauses; Syntactic
              processing;read;compling-cogsci2023",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "33941375",
  doi      = "10.1016/j.cognition.2021.104699"
}

@ARTICLE{Sumers2023-kl,
  title    = "Show or tell? Exploring when (and why) teaching with language
              outperforms demonstration",
  author   = "Sumers, Theodore R and Ho, Mark K and Hawkins, Robert D and
              Griffiths, Thomas L",
  abstract = "People use a wide range of communicative acts across different
              modalities, from concrete demonstrations to abstract language.
              While these modalities are typically studied independently, we
              take a comparative approach and ask when and why one modality
              might outperform another. We present a series of real-time,
              multi-player experiments asking participants to teach concepts
              using either demonstrations or language. Our first experiment
              (N=416) asks when language might outperform demonstration. We
              manipulate the complexity of the concept being taught and find
              that language communicates complex concepts more effectively than
              demonstration. We then ask why language succeeds in this setting.
              We hypothesized that language allowed teachers to reference
              abstract object features (e.g., shapes and colors), while
              demonstration teachers could only provide concrete examples
              (specific positive or negative objects). To test this hypothesis,
              our second experiment (N=568) ablated object features from the
              teacher's interface. This manipulation severely impaired
              linguistic (but not demonstrative) teaching. Our findings suggest
              that language communicates complex concepts by directly
              transmitting abstract rules. In contrast, demonstrations transmit
              examples, requiring the learner to infer the rules.",
  journal  = "Cognition",
  volume   =  232,
  pages    = "105326",
  month    =  mar,
  year     =  2023,
  keywords = "Abstraction; Communication; Demonstration; Language;
              Pedagogy;comp-cog-sci",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "36473238",
  doi      = "10.1016/j.cognition.2022.105326"
}

@ARTICLE{Coenen2015-wp,
  title    = "Strategies to intervene on causal systems are adaptively selected",
  author   = "Coenen, Anna and Rehder, Bob and Gureckis, Todd M",
  abstract = "How do people choose interventions to learn about causal systems?
              Here, we considered two possibilities. First, we test an
              information sampling model, information gain, which values
              interventions that can discriminate between a learner's
              hypotheses (i.e. possible causal structures). We compare this
              discriminatory model to a positive testing strategy that instead
              aims to confirm individual hypotheses. Experiment 1 shows that
              individual behavior is described best by a mixture of these two
              alternatives. In Experiment 2 we find that people are able to
              adaptively alter their behavior and adopt the discriminatory
              model more often after experiencing that the confirmatory
              strategy leads to a subjective performance decrement. In
              Experiment 3, time pressure leads to the opposite effect of
              inducing a change towards the simpler positive testing strategy.
              These findings suggest that there is no single strategy that
              describes how intervention decisions are made. Instead, people
              select strategies in an adaptive fashion that trades off their
              expected performance and cognitive effort.",
  journal  = "Cogn. Psychol.",
  volume   =  79,
  pages    = "102--133",
  month    =  jun,
  year     =  2015,
  keywords = "Causal learning; Hypothesis testing; Information gain;
              Interventions; Self-directed learning;project 1",
  language = "en",
  issn     = "0010-0285, 1095-5623",
  pmid     = "25935867",
  doi      = "10.1016/j.cogpsych.2015.02.004"
}

@ARTICLE{Bramley2018-zb,
  title    = "Intuitive experimentation in the physical world",
  author   = "Bramley, Neil R and Gerstenberg, Tobias and Tenenbaum, Joshua B
              and Gureckis, Todd M",
  abstract = "Many aspects of our physical environment are hidden. For example,
              it is hard to estimate how heavy an object is from visual
              observation alone. In this paper we examine how people actively
              ``experiment'' within the physical world to discover such latent
              properties. In the first part of the paper, we develop a novel
              framework for the quantitative analysis of the information
              produced by physical interactions. We then describe two
              experiments that present participants with moving objects in
              ``microworlds'' that operate according to continuous
              spatiotemporal dynamics similar to everyday physics (i.e., forces
              of gravity, friction, etc.). Participants were asked to interact
              with objects in the microworlds in order to identify their
              masses, or the forces of attraction/repulsion that governed their
              movement. Using our modeling framework, we find that learners who
              freely interacted with the physical system selectively produced
              evidence that revealed the physical property consistent with
              their inquiry goal. As a result, their inferences were more
              accurate than for passive observers and, in some contexts, for
              yoked participants who watched video replays of an active
              learner's interactions. We characterize active learners' actions
              into a range of micro-experiment strategies and discuss how these
              might be learned or generalized from past experience. The
              technical contribution of this work is the development of a novel
              analytic framework and methodology for the study of interactively
              learning about the physical world. Its empirical contribution is
              the demonstration of sophisticated goal directed human active
              learning in a naturalistic context.",
  journal  = "Cogn. Psychol.",
  volume   =  105,
  pages    = "9--38",
  month    =  sep,
  year     =  2018,
  keywords = "Active learning; Experimental design; Mental simulation; Physical
              understanding;read;project 1;comp-cog-sci;Paper1",
  language = "en",
  issn     = "0010-0285, 1095-5623",
  pmid     = "29885534",
  doi      = "10.1016/j.cogpsych.2018.05.001"
}

@ARTICLE{Wang2010-xb,
  title    = "On the cognitive process of human problem solving",
  author   = "Wang, Yingxu and Chiew, Vincent",
  abstract = "One of the fundamental human cognitive processes is problem
              solving. As a higher-layer cognitive process, problem solving
              interacts with many other cognitive processes such as
              abstraction, searching, learning, decision making, inference,
              analysis, and synthesis on the basis of internal knowledge
              representation by the object--attribute-relation (OAR) model.
              Problem solving is a cognitive process of the brain that searches
              a solution for a given problem or finds a path to reach a given
              goal. When a problem object is identified, problem solving can be
              perceived as a search process in the memory space for finding a
              relationship between a set of solution goals and a set of
              alternative paths. This paper presents both a cognitive model and
              a mathematical model of the problem solving process. The
              cognitive structures of the brain and the mechanisms of internal
              knowledge representation behind the cognitive process of problem
              solving are explained. The cognitive process is formally
              described using real-time process algebra (RTPA) and concept
              algebra. This work is a part of the cognitive computing project
              that designed to reveal and simulate the fundamental mechanisms
              and processes of the brain according to Wang's layered reference
              model of the brain (LRMB), which is expected to lead to the
              development of future generation methodologies for cognitive
              computing and novel cognitive computers that are capable of
              think, learn, and perceive.",
  journal  = "Cogn. Syst. Res.",
  volume   =  11,
  number   =  1,
  pages    = "81--92",
  month    =  mar,
  year     =  2010,
  keywords = "Cognitive informatics; Cognitive computing; Brain informatics;
              Computational intelligence; Reference model of the brain;
              Cognitive processes; Problem solving; Mathematical model; Concept
              algebra; RTPA;project 1",
  issn     = "1389-0417",
  doi      = "10.1016/j.cogsys.2008.08.003"
}

@ARTICLE{Lohse2022-nm,
  title    = "Hypotheses in adult-child interactions stimulate children's
              reasoning and verbalizations",
  author   = "Lohse, Karoline and Hildebrandt, Andrea and Hildebrandt, Frauke",
  abstract = "Adult-child interactions can support children's development and
              are established as predictors of program quality in early
              childhood settings. However, the linguistic components that
              constitute positive interactions have not yet been studied in
              detail. This study investigates the effects of hypotheses
              proposed by adults on children's responses in a dyadic
              picture-book viewing situation. In 2 experiments, adults' use of
              hypotheses (e.g., ``Maybe this is a dwarf's door'') was tested
              against the use of instructive statements (``This is a dwarf's
              door'') and in combination with open questions (``What do you
              think, why is the door so small?''). In Experiment 1, hypotheses
              differed from instructions only by the modal marker ``maybe''.
              Children's responses to hypotheses were longer and contained more
              self-generated explanations as compared to responses to
              instructions. The use of hypotheses also seemed to encourage
              children to attach more importance to their own explanations. In
              Experiment 2, combining hypotheses with open-ended why questions
              elicited longer responses but no more self-generated explanations
              in children than open-ended questions alone. Results indicate
              that subtle differences in adults' utterances can directly
              influence children's reasoning and children's contributions to
              dialogues.",
  journal  = "Early Child. Res. Q.",
  volume   =  58,
  pages    = "254--263",
  month    =  jan,
  year     =  2022,
  keywords = "Adult-child interactions; sustained shared thinking; hypotheses;
              open questions;development",
  issn     = "0885-2006",
  doi      = "10.1016/j.ecresq.2021.09.014"
}

@ARTICLE{Baars2007-hu,
  title    = "An Architectural Model of Conscious and Unconscious Brain
              Functions: {{Global Workspace Theory}} and {{IDA}}",
  author   = "Baars, Bernard J and Franklin, Stan",
  abstract = "While neural net models have been developed to a high degree of
              sophistication, they have some drawbacks at a more integrative,
              ``architectural'' level of analysis. We describe a ``hybrid''
              cognitive architecture that is implementable in neuronal nets,
              and which has uniform brainlike features, including
              activation-passing and highly distributed ``codelets,''
              implementable as small-scale neural nets. Empirically, this
              cognitive architecture accounts qualitatively for the data
              described by Baars' Global Workspace Theory (GWT), and Franklin's
              LIDA architecture, including state-of-the-art models of conscious
              contents in action-planning, Baddeley-style Working Memory, and
              working models of episodic and semantic longterm memory. These
              terms are defined both conceptually and empirically for the
              current theoretical domain. The resulting architecture meets four
              desirable goals for a unified theory of cognition: practical
              workability, autonomous agency, a plausible role for conscious
              cognition, and translatability into plausible neural terms. It
              also generates testable predictions, both empirical and
              computational.",
  journal  = "Neural Netw.",
  volume   =  20,
  number   =  9,
  pages    = "955--961",
  series   = "Brain and Consciousness",
  month    =  nov,
  year     =  2007,
  keywords = "Cognitive architecture,Conscious cognition,Global workspace
              theory,LIDA architecture;comp-cog-sci",
  issn     = "0893-6080",
  doi      = "10.1016/j.neunet.2007.09.013"
}

@ARTICLE{Dehaene2007-ya,
  title    = "Cultural recycling of cortical maps",
  author   = "Dehaene, Stanislas and Cohen, Laurent",
  abstract = "Part of human cortex is specialized for cultural domains such as
              reading and arithmetic, whose invention is too recent to have
              influenced the evolution of our species. Representations of
              letter strings and of numbers occupy reproducible locations
              within large-scale macromaps, respectively in the left
              occipito-temporal and bilateral intraparietal cortex.
              Furthermore, recent fMRI studies reveal a systematic architecture
              within these areas. To explain this paradoxical cerebral
              invariance of cultural maps, we propose a neuronal recycling
              hypothesis, according to which cultural inventions invade
              evolutionarily older brain circuits and inherit many of their
              structural constraints.",
  journal  = "Neuron",
  volume   =  56,
  number   =  2,
  pages    = "384--398",
  month    =  oct,
  year     =  2007,
  keywords = "read;dev-cog-neuro;development",
  language = "en",
  issn     = "0896-6273",
  pmid     = "17964253",
  doi      = "10.1016/j.neuron.2007.10.004"
}

@ARTICLE{Daw2011-kf,
  title    = "Model-based influences on humans' choices and striatal prediction
              errors",
  author   = "Daw, Nathaniel D and Gershman, Samuel J and Seymour, Ben and
              Dayan, Peter and Dolan, Raymond J",
  abstract = "The mesostriatal dopamine system is prominently implicated in
              model-free reinforcement learning, with fMRI BOLD signals in
              ventral striatum notably covarying with model-free prediction
              errors. However, latent learning and devaluation studies show
              that behavior also shows hallmarks of model-based planning, and
              the interaction between model-based and model-free values,
              prediction errors, and preferences is underexplored. We designed
              a multistep decision task in which model-based and model-free
              influences on human choice behavior could be distinguished. By
              showing that choices reflected both influences we could then test
              the purity of the ventral striatal BOLD signal as a model-free
              report. Contrary to expectations, the signal reflected both
              model-free and model-based predictions in proportions matching
              those that best explained choice behavior. These results
              challenge the notion of a separate model-free learner and suggest
              a more integrated computational architecture for high-level human
              decision-making.",
  journal  = "Neuron",
  volume   =  69,
  number   =  6,
  pages    = "1204--1215",
  month    =  mar,
  year     =  2011,
  keywords = "skimmed;learning\&memory2023;RL",
  language = "en",
  issn     = "0896-6273, 1097-4199",
  pmid     = "21435563",
  doi      = "10.1016/j.neuron.2011.02.027",
  pmc      = "PMC3077926"
}

@ARTICLE{Courville2006-en,
  title    = "Bayesian theories of conditioning in a changing world",
  author   = "Courville, Aaron C and Daw, Nathaniel D and Touretzky, David S",
  abstract = "The recent flowering of Bayesian approaches invites the
              re-examination of classic issues in behavior, even in areas as
              venerable as Pavlovian conditioning. A statistical account can
              offer a new, principled interpretation of behavior, and previous
              experiments and theories can inform many unexplored aspects of
              the Bayesian enterprise. Here we consider one such issue: the
              finding that surprising events provoke animals to learn faster.
              We suggest that, in a statistical account of conditioning,
              surprise signals change and therefore uncertainty and the need
              for new learning. We discuss inference in a world that changes
              and show how experimental results involving surprise can be
              interpreted from this perspective, and also how, thus understood,
              these phenomena help constrain statistical theories of animal and
              human learning.",
  journal  = "Trends Cogn. Sci.",
  volume   =  10,
  number   =  7,
  pages    = "294--300",
  month    =  jul,
  year     =  2006,
  keywords = "skimmed;learning\&memory2023;comp-cog-sci",
  language = "en",
  issn     = "1364-6613",
  pmid     = "16793323",
  doi      = "10.1016/j.tics.2006.05.004"
}

@ARTICLE{Kurby2008-dy,
  title    = "Segmentation in the perception and memory of events",
  author   = "Kurby, Christopher A and Zacks, Jeffrey M",
  abstract = "People make sense of continuous streams of observed behavior in
              part by segmenting them into events. Event segmentation seems to
              be an ongoing component of everyday perception. Events are
              segmented simultaneously at multiple timescales, and are grouped
              hierarchically. Activity in brain regions including the posterior
              temporal and parietal cortex and lateral frontal cortex increases
              transiently at event boundaries. The parsing of ongoing activity
              into events is related to the updating of working memory, to the
              contents of long-term memory, and to the learning of new
              procedures. Event segmentation might arise as a side effect of an
              adaptive mechanism that integrates information over the recent
              past to improve predictions about the near future.",
  journal  = "Trends Cogn. Sci.",
  volume   =  12,
  number   =  2,
  pages    = "72--79",
  month    =  feb,
  year     =  2008,
  keywords = "skimmed;learning\&memory2023",
  language = "en",
  issn     = "1364-6613",
  pmid     = "18178125",
  doi      = "10.1016/j.tics.2007.11.004",
  pmc      = "PMC2263140"
}

@ARTICLE{Niv2008-ki,
  title    = "Dialogues on prediction errors",
  author   = "Niv, Yael and Schoenbaum, Geoffrey",
  abstract = "The recognition that computational ideas from reinforcement
              learning are relevant to the study of neural circuits has taken
              the cognitive neuroscience community by storm. A central tenet of
              these models is that discrepancies between actual and expected
              outcomes can be used for learning. Neural correlates of such
              prediction-error signals have been observed now in midbrain
              dopaminergic neurons, striatum, amygdala and even prefrontal
              cortex, and models incorporating prediction errors have been
              invoked to explain complex phenomena such as the transition from
              goal-directed to habitual behavior. Yet, like any revolution, the
              fast-paced progress has left an uneven understanding in its wake.
              Here, we provide answers to ten simple questions about prediction
              errors, with the aim of exposing both the strengths and the
              limitations of this active area of neuroscience research.",
  journal  = "Trends Cogn. Sci.",
  volume   =  12,
  number   =  7,
  pages    = "265--272",
  month    =  jul,
  year     =  2008,
  keywords = "read;RL;learning\&memory2023",
  language = "en",
  issn     = "1364-6613",
  pmid     = "18567531",
  doi      = "10.1016/j.tics.2008.03.006"
}

@ARTICLE{Griffiths2010-wc,
  title    = "Probabilistic Models of Cognition: Exploring Representations and
              Inductive Biases",
  author   = "Griffiths, Thomas L and Chater, Nick and Kemp, Charles and
              Perfors, Amy and Tenenbaum, Joshua B",
  journal  = "Trends Cogn. Sci.",
  volume   =  14,
  number   =  8,
  pages    = "357--364",
  month    =  aug,
  year     =  2010,
  keywords = "comp-cog-sci",
  issn     = "1364-6613",
  doi      = "10.1016/j.tics.2010.05.004"
}

@ARTICLE{Dunsmoor2015-gm,
  title    = "Categories, concepts, and conditioning: how humans generalize
              fear",
  author   = "Dunsmoor, Joseph E and Murphy, Gregory L",
  abstract = "During the past century, Pavlovian conditioning has served as the
              predominant experimental paradigm and theoretical framework to
              understand how humans learn to fear and avoid real or perceived
              dangers. Animal models for translational research offer insight
              into basic behavioral and neurophysiological factors mediating
              the acquisition, expression, inhibition, and generalization of
              fear. However, it is important to consider the limits of
              traditional animal models when applied to humans. Here, we focus
              on the question of how humans generalize fear. We propose that to
              understand fear generalization in humans requires taking into
              account research on higher-level cognition such as category-based
              induction, inferential reasoning, and representation of
              conceptual knowledge. Doing so will open the door for productive
              avenues of new research.",
  journal  = "Trends Cogn. Sci.",
  volume   =  19,
  number   =  2,
  pages    = "73--77",
  month    =  feb,
  year     =  2015,
  keywords = "skimmed;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "1364-6613, 1879-307X",
  pmid     = "25577706",
  doi      = "10.1016/j.tics.2014.12.003",
  pmc      = "PMC4318701"
}

@ARTICLE{Ullman2017-fo,
  title    = "Mind Games: Game Engines as an Architecture for Intuitive Physics",
  author   = "Ullman, Tomer D and Spelke, Elizabeth and Battaglia, Peter and
              Tenenbaum, Joshua B",
  abstract = "We explore the hypothesis that many intuitive physical inferences
              are based on a mental physics engine that is analogous in many
              ways to the machine physics engines used in building interactive
              video games. We describe the key features of game physics engines
              and their parallels in human mental representation, focusing
              especially on the intuitive physics of young infants where the
              hypothesis helps to unify many classic and otherwise puzzling
              phenomena, and may provide the basis for a computational account
              of how the physical knowledge of infants develops. This
              hypothesis also explains several 'physics illusions', and helps
              to inform the development of artificial intelligence (AI) systems
              with more human-like common sense.",
  journal  = "Trends Cogn. Sci.",
  volume   =  21,
  number   =  9,
  pages    = "649--665",
  month    =  sep,
  year     =  2017,
  keywords = "project 2",
  language = "en",
  issn     = "1364-6613, 1879-307X",
  pmid     = "28655498",
  doi      = "10.1016/j.tics.2017.05.012"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bedny2017-as,
  title    = "Evidence from Blindness for a Cognitively Pluripotent Cortex",
  author   = "Bedny, Marina",
  abstract = "Cognitive neuroscience seeks to discover how cognitive functions
              are implemented in neural circuits. Studies of plasticity in
              blindness suggest that this mind-brain mapping is highly flexible
              during development. In blindness, 'visual' cortices take on
              higher-cognitive functions, including language and mathematics,
              becoming sensitive to the grammatical structure of spoken
              sentences and the difficulty of math equations. Visual cortex
              activity at rest becomes synchronized with higher-cognitive
              networks. Such repurposing is striking in light of the cognitive
              and evolutionary differences between vision, language, and
              mathematics. We propose that human cortices are cognitively
              pluripotent, that is, capable of assuming a wide range of
              cognitive functions. Specialization is driven by input during
              development, which is itself constrained by connectivity and
              experience. 'The child who methodically adds two numbers from
              right to left, carrying a digit when necessary, may be using the
              same algorithm that is implemented by the wires and transistors
              of the cash register in the neighborhood supermarketâ€¦' â–“â–“Vision,
              1982, David Marr.",
  journal  = "Trends Cogn. Sci.",
  volume   =  21,
  number   =  9,
  pages    = "637--648",
  month    =  sep,
  year     =  2017,
  keywords = "blindness; development; language; plasticity; visual
              cortex;read;dev-cog-neuro;development",
  language = "en",
  issn     = "1364-6613, 1879-307X",
  pmid     = "28821345",
  doi      = "10.1016/j.tics.2017.06.003"
}

@ARTICLE{Rule2020-db,
  title    = "The {{Child}} as {{Hacker}}",
  author   = "Rule, Joshua S and Tenenbaum, Joshua B and Piantadosi, Steven T",
  journal  = "Trends Cogn. Sci.",
  volume   =  24,
  number   =  11,
  pages    = "900--915",
  month    =  nov,
  year     =  2020,
  keywords = "skimmed;development",
  issn     = "1364-6613",
  doi      = "10.1016/j.tics.2020.07.005"
}

@ARTICLE{Koutstaal2001-rt,
  title    = "Perceptual specificity in visual object priming: functional
              magnetic resonance imaging evidence for a laterality difference
              in fusiform cortex",
  author   = "Koutstaal, W and Wagner, A D and Rotte, M and Maril, A and
              Buckner, R L and Schacter, D L",
  abstract = "Seeing an object on one occasion may facilitate or prime
              processing of the same object if it is later again encountered.
              Such priming may also be found -- but at a reduced level -- for
              different but perceptually similar objects that are alternative
              exemplars or 'tokens' of the initially presented object. We
              explored the neural correlates of this perceptual specificity
              using event-related functional magnetic resonance imaging (fMRI)
              procedures, contrasting neural activity when participants made
              object classification decisions (size judgments) regarding
              previously presented objects (repeated same), alternative
              exemplars of previously presented objects (repeated different),
              or entirely new objects (novel). Many frontal regions (including
              bilateral frontal operculum, bilateral posterior inferior
              frontal/precentral, left anterior inferior frontal, and superior
              frontal cortices) and multiple late visual and posterior regions
              (including middle occipital, fusiform, fusiform-parahippocampal,
              precuneus, and posterior cingulate, all bilaterally),
              demonstrated reduced neural activity for repeated compared to
              novel objects. Greater repetition-induced reductions were
              observed for same than for different exemplars in several of
              these regions (bilateral posterior inferior frontal, right
              precuneus, bilateral middle occipital, bilateral fusiform,
              bilateral parahippocampal and bilateral superior parietal).
              Additionally, right fusiform (occipitotemporal) cortex showed
              significantly less priming for different versus same exemplars
              than did left fusiform. These findings converge with behavioral
              evidence from divided visual field studies and with
              neuropsychological evidence underscoring the key role of right
              occipitotemporal cortex in processing specific visual form
              information; possible differences in the
              representational-functional role of left fusiform are discussed.",
  journal  = "Neuropsychologia",
  volume   =  39,
  number   =  2,
  pages    = "184--199",
  year     =  2001,
  keywords = "learning\&memory2023",
  language = "en",
  issn     = "0028-3932",
  pmid     = "11163375",
  doi      = "10.1016/s0028-3932(00)00087-7"
}

@INCOLLECTION{Weisberg2020-zy,
  title     = "Insight in {Problem-Solving} and Creative Thinking",
  booktitle = "Rethinking Creativity: {Inside-the-Box} Thinking as the Basis
               for Innovation",
  author    = "Weisberg, Robert W",
  abstract  = "Rethinking Creativity - September 2020",
  publisher = "Cambridge University Press",
  pages     = "215--248",
  month     =  sep,
  year      =  2020,
  keywords  = "Insight in problem-solving; insight in creativity; Aha!
               experiences; analytic thinking in insight;project 1",
  doi       = "10.1017/9781108785259.007"
}

@INCOLLECTION{Ashby2011-au,
  title     = "{{COVIS}}",
  booktitle = "Formal {{Approaches}} in {{Categorization}}",
  author    = "Ashby, F Gregory and Paul, Erick J and Maddox, W Todd",
  editor    = "Pothos, Emmanuel M and Wills, Andy J",
  abstract  = "The COVIS model of category learning assumes separate rule-based
               and procedural-learning categorization systems that compete for
               access to response production. The rule-based system selects and
               tests simple verbalizable hypotheses about category membership.
               The procedurallearning system gradually associates
               categorization responses with regions of perceptual space via
               reinforcement learning.",
  publisher = "Cambridge University Press",
  pages     = "65--87",
  year      =  2011,
  address   = "Cambridge",
  keywords  = "comp-cog-sci",
  isbn      = "9780511921322",
  doi       = "10.1017/CBO9780511921322.004"
}

@ARTICLE{Botvinick2017-xr,
  title    = "Building Machines That Learn and Think for Themselves",
  author   = "Botvinick, Matthew and Barrett, David G T and Battaglia, Peter
              and de Freitas, Nando and Kumaran, Darshan and Leibo, Joel Z and
              Lillicrap, Timothy and Modayil, Joseph and Mohamed, Shakir and
              Rabinowitz, Neil C and Rezende, Danilo J and Santoro, Adam and
              Schaul, Tom and Summerfield, Christopher and Wayne, Greg and
              Weber, Theophane and Wierstra, Daan and Legg, Shane and Hassabis,
              Demis",
  abstract = "We agree with Lake and colleagues on their list of `key
              ingredients' for building humanlike intelligence, including the
              idea that model-based reasoning is essential. However, we favor
              an approach that centers on one additional ingredient: autonomy.
              In particular, we aim toward agents that can both build and
              exploit their own internal models, with minimal human
              hand-engineering. We believe an approach centered on autonomous
              learning has the greatest chance of success as we scale toward
              real-world complexity, tackling domains for which ready-made
              formal models are not available. Here we survey several important
              examples of the progress that has been made toward building
              autonomous agents with humanlike abilities, and highlight some
              outstanding challenges.",
  journal  = "Behav. Brain Sci.",
  volume   =  40,
  pages    = "e255",
  year     =  2017,
  keywords = "comp-cog-sci",
  issn     = "0140-525X, 1469-1825",
  doi      = "10.1017/S0140525X17000048"
}

@ARTICLE{Almaatouq2022-xz,
  title    = "Beyond Playing 20 Questions with Nature: Integrative Experiment
              Design in the Social and Behavioral Sciences",
  author   = "Almaatouq, Abdullah and Griffiths, Thomas L and Suchow, Jordan W
              and Whiting, Mark E and Evans, James and Watts, Duncan J",
  abstract = "The dominant paradigm of experiments in the social and behavioral
              sciences views an experiment as a test of a theory, where the
              theory is assumed to generalize beyond the experiment's specific
              conditions. According to this view, which Alan Newell once
              characterized as ``playing twenty questions with nature,'' theory
              is advanced one experiment at a time, and the integration of
              disparate findings is assumed to happen via the scientific
              publishing process. In this article, we argue that the process of
              integration is at best inefficient, and at worst it does not, in
              fact, occur. We further show that the challenge of integration
              cannot be adequately addressed by recently proposed reforms that
              focus on the reliability and replicability of individual
              findings, nor simply by conducting more or larger experiments.
              Rather, the problem arises from the imprecise nature of social
              and behavioral theories and, consequently, a lack of
              commensurability across experiments conducted under different
              conditions. Therefore, researchers must fundamentally rethink how
              they design experiments and how the experiments relate to theory.
              We specifically describe an alternative framework, integrative
              experiment design, which intrinsically promotes commensurability
              and continuous integration of knowledge. In this paradigm,
              researchers explicitly map the design space of possible
              experiments associated with a given research question, embracing
              many potentially relevant theories rather than focusing on just
              one. The researchers then iteratively generate theories and test
              them with experiments explicitly sampled from the design space,
              allowing results to be integrated across experiments. Given
              recent methodological and technological developments, we conclude
              that this approach is feasible and would generate more-reliable,
              more-cumulative empirical and theoretical knowledge than the
              current paradigm-and with far greater efficiency.",
  journal  = "Behav. Brain Sci.",
  pages    = "1--55",
  month    =  dec,
  year     =  2022,
  keywords = "(in)commensurability; cumulative knowledge; experiments;
              generalizability",
  language = "en",
  issn     = "0140-525X, 1469-1825",
  pmid     = "36539303",
  doi      = "10.1017/S0140525X22002874"
}

@ARTICLE{Ollinger2008-fw,
  title    = "Investigating the effect of mental set on insight problem solving",
  author   = "Ollinger, Michael and Jones, Gary and Knoblich, G{\"u}nther",
  abstract = "Mental set is the tendency to solve certain problems in a fixed
              way based on previous solutions to similar problems. The moment
              of insight occurs when a problem cannot be solved using solution
              methods suggested by prior experience and the problem solver
              suddenly realizes that the solution requires different solution
              methods. Mental set and insight have often been linked together
              and yet no attempt thus far has systematically examined the
              interplay between the two. Three experiments are presented that
              examine the extent to which sets of noninsight and insight
              problems affect the subsequent solutions of insight test
              problems. The results indicate a subtle interplay between mental
              set and insight: when the set involves noninsight problems, no
              mental set effects are shown for the insight test problems, yet
              when the set involves insight problems, both facilitation and
              inhibition can be seen depending on the type of insight problem
              presented in the set. A two process model is detailed to explain
              these findings that combines the representational change
              mechanism with that of proceduralization.",
  journal  = "Exp. Psychol.",
  volume   =  55,
  number   =  4,
  pages    = "269--282",
  year     =  2008,
  keywords = "project 1",
  language = "en",
  issn     = "1618-3169",
  pmid     = "18683624",
  doi      = "10.1027/1618-3169.55.4.269"
}

@ARTICLE{Schacter1999-yd,
  title    = "The seven sins of memory. Insights from psychology and cognitive
              neuroscience",
  author   = "Schacter, D L",
  abstract = "Though often reliable, human memory is also fallible. This
              article examines how and why memory can get us into trouble. It
              is suggested that memory's misdeeds can be classified into 7
              basic ``sins'': transience, absentmindedness, blocking,
              misattribution, suggestibility, bias, and persistence. The first
              three sins involve different types of forgetting, the next three
              refer to different types of distortions, and the final sin
              concerns intrusive recollections that are difficult to forget.
              Evidence is reviewed concerning each of the 7 sins from relevant
              sectors of psychology (cognitive, social, and clinical) and from
              cognitive neuroscience studies that include patients with focal
              brain damage or make use of recently developed neuroimaging
              techniques. Although the 7 sins may appear to reflect flaws in
              system design, it is argued instead that they are by-products of
              otherwise adaptive features of memory.",
  journal  = "Am. Psychol.",
  volume   =  54,
  number   =  3,
  pages    = "182--203",
  month    =  mar,
  year     =  1999,
  keywords = "read;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "0003-066X",
  pmid     = "10199218",
  doi      = "10.1037//0003-066x.54.3.182"
}

@ARTICLE{Ormerod2002-tb,
  title    = "Dynamics and constraints in insight problem solving",
  author   = "Ormerod, Thomas C and MacGregor, James N and Chronicle, Edward P",
  abstract = "This article reports 2 experiments that investigated performance
              on a novel insight problem, the 8-coin problem. The authors
              hypothesized that participants would make certain initial moves
              (strategic moves) that seemed to make progress according to the
              problem instructions but that nonetheless would guarantee failure
              to solve the problem. Experiment 1 manipulated the starting state
              of the problem and showed that overall solution rates were lower
              when such strategic moves were available. Experiment 2 showed
              that failure to capitalize on visual hints about the correct
              first move was also associated with the availability of strategic
              moves. The results are interpreted in terms of an
              information-processing framework previously applied to the 9-dot
              problem. The authors argue that in addition to the operation of
              inappropriate constraints, a full account of insight problem
              solving must incorporate a dynamic that steers solution-seeking
              activity toward the constraints.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  28,
  number   =  4,
  pages    = "791--799",
  month    =  jul,
  year     =  2002,
  keywords = "cog-sci;project 1",
  language = "en",
  issn     = "0278-7393",
  pmid     = "12109769",
  doi      = "10.1037//0278-7393.28.4.791"
}

@ARTICLE{Koutstaal1998-ta,
  title    = "Post-event review in older and younger adults: improving memory
              accessibility of complex everyday events",
  author   = "Koutstaal, W and Schacter, D L and Johnson, M K and Angell, K E
              and Gross, M S",
  abstract = "Recalling an event at 1 time often increases the likelihood that
              it will be remembered at a still later time. The authors examined
              the degree to which older and younger adults' memory for everyday
              events that they watched on a videotape was improved by later
              seeing photographs or reading brief verbal descriptions of those
              events. Both older and younger adults recalled more events, in
              greater detail, with than without review. Verbal descriptions
              enhanced later recall to the same degree as reviewing
              photographs. Younger adults generally gained more from review
              than older adults on measures of the absolute number of details
              recalled and when facilitation was assessed relative to a
              no-review control condition, but not when memory for reviewed
              events was expressed as a proportion of each individual's total
              recall. Post-event review has clear potential practical benefits
              for improving memory of older adults.",
  journal  = "Psychol. Aging",
  volume   =  13,
  number   =  2,
  pages    = "277--296",
  month    =  jun,
  year     =  1998,
  keywords = "l\&m final",
  language = "en",
  issn     = "0882-7974",
  pmid     = "9640588",
  doi      = "10.1037//0882-7974.13.2.277"
}

@ARTICLE{Rescorla1988-hl,
  title    = "Pavlovian conditioning: It's not what you think it is",
  author   = "Rescorla, Robert A",
  abstract = "Current thinking about Pavlovian conditioning differs
              substantially from that of 20 years ago. Yet the changes that
              have taken place remain poorly appreciated by psychologists
              generally. Traditional descriptions of conditioning as the
              acquired ability of one stimulus to evoke the original response
              to another because of their pairing are shown to be inadequate.
              They fail to characterize adequately the circumstances producing
              learning, the content of that learning, or the manner in which
              that learning influences performance. Instead, conditioning is
              now described as the learning of relations among events so as to
              allow the organism to represent its environment. Within this
              framework, the study of Pavlovian conditioning continues to be an
              intellectually active area, full of new discoveries and
              information relevant to other areas of psychology. (PsycINFO
              Database Record (c) 2016 APA, all rights reserved)",
  journal  = "Am. Psychol.",
  volume   =  43,
  number   =  3,
  pages    = "151--160",
  month    =  mar,
  year     =  1988,
  keywords = "skimmed;learning\&memory2023",
  issn     = "0003-066X",
  doi      = "10.1037/0003-066X.43.3.151"
}

@ARTICLE{McClelland1995-nl,
  title    = "Why there are complementary learning systems in the hippocampus
              and neocortex: insights from the successes and failures of
              connectionist models of learning and memory",
  author   = "McClelland, James L and McNaughton, Bruce L and O'Reilly, Randall
              C",
  abstract = "Damage to the hippocampal system disrupts recent memory but
              leaves remote memory intact. The account presented here suggests
              that memories are first stored via synaptic changes in the
              hippocampal system, that these changes support reinstatement of
              recent memories in the neocortex, that neocortical synapses
              change a little on each reinstatement, and that remote memory is
              based on accumulated neocortical changes. Models that learn via
              changes to connections help explain this organization. These
              models discover the structure in ensembles of items if learning
              of each item is gradual and interleaved with learning about other
              items. This suggests that the neocortex learns slowly to discover
              the structure in ensembles of experiences. The hippocampal system
              permits rapid learning of new items without disrupting this
              structure, and reinstatement of new memories interleaves them
              with others to integrate them into structured neocortical memory
              systems.",
  journal  = "Psychol. Rev.",
  volume   =  102,
  number   =  3,
  pages    = "419--457",
  month    =  jul,
  year     =  1995,
  keywords = "skimmed;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "0033-295X",
  pmid     = "7624455",
  doi      = "10.1037/0033-295X.102.3.419"
}

@ARTICLE{Nisbett1977-oz,
  title    = "Telling more than we can know: Verbal reports on mental processes",
  author   = "Nisbett, Richard E and Wilson, Timothy D",
  abstract = "Reviews evidence which suggests that there may be little or no
              direct introspective access to higher order cognitive processes.
              Ss are sometimes (a) unaware of the existence of a stimulus that
              importantly influenced a response, (b) unaware of the existence
              of the response, and (c) unaware that the stimulus has affected
              the response. It is proposed that when people attempt to report
              on their cognitive processes, that is, on the processes mediating
              the effects of a stimulus on a response, they do not do so on the
              basis of any true introspection. Instead, their reports are based
              on a priori, implicit causal theories, or judgments about the
              extent to which a particular stimulus is a plausible cause of a
              given response. This suggests that though people may not be able
              to observe directly their cognitive processes, they will
              sometimes be able to report accurately about them. Accurate
              reports will occur when influential stimuli are salient and are
              plausible causes of the responses they produce, and will not
              occur when stimuli are not salient or are not plausible causes.
              (86 ref) (PsycINFO Database Record (c) 2018 APA, all rights
              reserved)",
  journal  = "Psychol. Rev.",
  volume   =  84,
  number   =  3,
  pages    = "231--259",
  month    =  may,
  year     =  1977,
  keywords = "project 1",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/0033-295X.84.3.231"
}

@ARTICLE{Knoblich1999-jd,
  title    = "Constraint relaxation and chunk decomposition in insight problem
              solving",
  author   = "Knoblich, G{\"u}nther and Ohlsson, Stellan and Haider, Hilde and
              Rhenius, Detlef",
  abstract = "Insight problem solving is characterized by impasses, states of
              mind in which the thinker does not know what to do next. The
              authors hypothesized that impasses are broken by changing the
              problem representation, and 2 hypothetical mechanisms for
              representational change are described: the relaxation of
              constraints on the solution and the decomposition of perceptual
              chunks. These 2 mechanisms generate specific predictions about
              the relative difficulty of individual problems and about
              differential transfer effects. The predictions were tested in 4
              experiments using matchstick arithmetic problems. The results
              were consistent with the predictions. Representational change is
              a more powerful explanation for insight than alternative
              hypotheses, if the hypothesized change processes are specified in
              detail. Overcoming impasses in insight is a special case of the
              general need to override the imperatives of past experience in
              the face of novel conditions. (PsycINFO Database Record (c) 2016
              APA, all rights reserved)",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  25,
  number   =  6,
  pages    = "1534--1555",
  month    =  nov,
  year     =  1999,
  issn     = "0278-7393, 1939-1285",
  doi      = "10.1037/0278-7393.25.6.1534"
}

@ARTICLE{Jones2003-sh,
  title    = "Testing two cognitive theories of insight",
  author   = "Jones, Gary",
  abstract = "Insight in problem solving occurs when the problem solver fails
              to see how to solve a problem and then--``aha!''--there is a
              sudden realization how to solve it. Two contemporary theories
              have been proposed to explain insight. The representational
              change theory (e.g., G. Knoblich, S. Ohlsson, \& G. E. Rainey,
              2001) proposes that insight occurs through relaxing self-imposed
              constraints on a problem and by decomposing chunked items in the
              problem. The progress monitoring theory (e.g., J. N. MacGregor,
              T. C. Ormerod, \& E. P. Chronicle, 2001) proposes that insight is
              only sought once it becomes apparent that the distance to the
              goal is unachievable in the moves remaining. These 2 theories are
              tested in an unlimited move problem, to which neither theory has
              previously been applied. The results lend support to both, but
              experimental manipulations to the problem suggest that the
              representational change theory is the better indicator of
              performance. The findings suggest that testable opposing
              predictions can be made to examine theories of insight and that
              the use of eye movement data is a fruitful method of both
              examining insight and testing theories of insight.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  29,
  number   =  5,
  pages    = "1017--1027",
  month    =  sep,
  year     =  2003,
  keywords = "project 1;cog-sci",
  language = "en",
  issn     = "0278-7393",
  pmid     = "14516232",
  doi      = "10.1037/0278-7393.29.5.1017"
}

@ARTICLE{Chronicle2004-bv,
  title    = "What makes an insight problem? The roles of heuristics, goal
              conception, and solution recoding in knowledge-lean problems",
  author   = "Chronicle, Edward P and MacGregor, James N and Ormerod, Thomas C",
  abstract = "Four experiments investigated transformation problems with
              insight characteristics. In Experiment 1, performance on a
              version of the 6-coin problem that had a concrete and
              visualizable solution followed a hill-climbing heuristic.
              Experiment 2 demonstrated that the difficulty of a version of the
              problem that potentially required insight for solution stems from
              the same hill-climbing heuristic, which creates an implicit
              conceptual block. Experiment 3 confirmed that the difficulty of
              the potential insight solution is conceptual, not procedural.
              Experiment 4 demonstrated the same principles of move selection
              on the 6-coin problem and the 10-coin (triangle) problem. It is
              argued that hill-climbing heuristics provide a common framework
              for understanding transformation and insight problem solving.
              Postsolution receding may account for part of the phenomenology
              of insight.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  30,
  number   =  1,
  pages    = "14--27",
  month    =  jan,
  year     =  2004,
  keywords = "cog-sci;project 1",
  language = "en",
  issn     = "0278-7393",
  pmid     = "14736293",
  doi      = "10.1037/0278-7393.30.1.14"
}

@ARTICLE{Nadel2004-xy,
  title    = "The spatial brain",
  author   = "Nadel, Lynn and Hardt, Oliver",
  abstract = "Themes emerging from the collection of articles in the Special
              Section on Long-Term Spatial Memory include the notion of
              multiple spatial systems, the relation between spatial
              representations and episodic memory, the role of context, and the
              neural systems involved in space. The authors conclude that
              distinguishing between egocentric and allocentric spatial systems
              makes sense of both behavioral and neurobiological data. The
              special role of the hippocampal system in allocentric space, and
              as a consequence, in context, suggests how a spatial system might
              end up central to the ability to remember episodes.",
  journal  = "Neuropsychology",
  volume   =  18,
  number   =  3,
  pages    = "473--476",
  month    =  jul,
  year     =  2004,
  keywords = "l\&m final",
  language = "en",
  issn     = "0894-4105",
  pmid     = "15291725",
  doi      = "10.1037/0894-4105.18.3.473"
}

@ARTICLE{Kemp2009-oz,
  title    = "Structured Statistical Models of Inductive Reasoning",
  author   = "Kemp, Charles and Tenenbaum, Joshua B",
  abstract = "Everyday inductive inferences are often guided by rich background
              knowledge. Formal models of induction should aim to incorporate
              this knowledge and should explain how different kinds of
              knowledge lead to the distinctive patterns of reasoning found in
              different inductive contexts. This article presents a Bayesian
              framework that attempts to meet both goals and describe 4
              applications of the framework: a taxonomic model, a spatial
              model, a threshold model, and a causal model. Each model makes
              probabilistic inferences about the extensions of novel
              properties, but the priors for the 4 models are defined over
              different kinds of structures that capture different
              relationships between the categories in a domain. The framework
              therefore shows how statistical inference can operate over
              structured background knowledge, and the authors argue that this
              interaction between structure and statistics is critical for
              explaining the power and flexibility of human reasoning.",
  journal  = "Psychol. Rev.",
  volume   =  116,
  number   =  1,
  pages    = "20--58",
  year     =  2009,
  keywords = "comp-cog-sci",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/a0014282"
}

@ARTICLE{Gershman2010-km,
  title    = "Context, learning, and extinction",
  author   = "Gershman, Samuel J and Blei, David M and Niv, Yael",
  abstract = "A. Redish et al. (2007) proposed a reinforcement learning model
              of context-dependent learning and extinction in conditioning
              experiments, using the idea of ``state classification'' to
              categorize new observations into states. In the current article,
              the authors propose an interpretation of this idea in terms of
              normative statistical inference. They focus on renewal and latent
              inhibition, 2 conditioning paradigms in which contextual
              manipulations have been studied extensively, and show that online
              Bayesian inference within a model that assumes an unbounded
              number of latent causes can characterize a diverse set of
              behavioral results from such manipulations, some of which pose
              problems for the model of Redish et al. Moreover, in both
              paradigms, context dependence is absent in younger animals, or if
              hippocampal lesions are made prior to training. The authors
              suggest an explanation in terms of a restricted capacity to infer
              new causes.",
  journal  = "Psychol. Rev.",
  volume   =  117,
  number   =  1,
  pages    = "197--209",
  month    =  jan,
  year     =  2010,
  keywords = "read;learning\&memory2023",
  language = "en",
  issn     = "0033-295X, 1939-1471",
  pmid     = "20063968",
  doi      = "10.1037/a0017808"
}

@ARTICLE{Ollinger2013-zu,
  title    = "Cognitive mechanisms of insight: the role of heuristics and
              representational change in solving the eight-coin problem",
  author   = "{\"O}llinger, Michael and Jones, Gary and Faber, Amory H and
              Knoblich, G{\"u}nther",
  abstract = "The 8-coin insight problem requires the problem solver to move 2
              coins so that each coin touches exactly 3 others. Ormerod,
              MacGregor, and Chronicle (2002) explained differences in task
              performance across different versions of the 8-coin problem using
              the availability of particular moves in a 2-dimensional search
              space. We explored 2 further explanations by developing 6 new
              versions of the 8-coin problem in order to investigate the
              influence of grouping and self-imposed constraints on solutions.
              The results identified 2 sources of problem difficulty: first,
              the necessity to overcome the constraint that a solution can be
              found in 2-dimensional space and, second, the necessity to
              decompose perceptual groupings. A detailed move analysis
              suggested that the selection of moves was driven by the
              established representation rather than the application of the
              appropriate heuristics. Both results support the assumptions of
              representational change theory (Ohlsson, 1992).",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  39,
  number   =  3,
  pages    = "931--939",
  month    =  may,
  year     =  2013,
  keywords = "project 1;cog-sci;Paper1",
  language = "en",
  issn     = "0278-7393, 1939-1285",
  pmid     = "22799283",
  doi      = "10.1037/a0029194"
}

@ARTICLE{Austerweil2013-uj,
  title    = "A nonparametric Bayesian framework for constructing flexible
              feature representations",
  author   = "Austerweil, Joseph L and Griffiths, Thomas L",
  abstract = "Representations are a key explanatory device used by cognitive
              psychologists to account for human behavior. Understanding the
              effects of context and experience on the representations people
              use is essential, because if two people encode the same stimulus
              using different representations, their response to that stimulus
              may be different. We present a computational framework that can
              be used to define models that flexibly construct feature
              representations (where by a feature we mean a part of the image
              of an object) for a set of observed objects, based on
              nonparametric Bayesian statistics. Austerweil and Griffiths
              (2011) presented an initial model constructed in this framework
              that captures how the distribution of parts affects the features
              people use to represent a set of objects. We build on this work
              in three ways. First, although people use features that can be
              transformed on each observation (e.g., translate on the retinal
              image), many existing feature learning models can only recognize
              features that are not transformed (occur identically each time).
              Consequently, we extend the initial model to infer features that
              are invariant over a set of transformations, and learn different
              structures of dependence between feature transformations. Second,
              we compare two possible methods for capturing the manner that
              categorization affects feature representations. Finally, we
              present a model that learns features incrementally, capturing an
              effect of the order of object presentation on the features people
              learn. We conclude by considering the implications and
              limitations of our empirical and theoretical results.",
  journal  = "Psychol. Rev.",
  volume   =  120,
  number   =  4,
  pages    = "817--851",
  month    =  oct,
  year     =  2013,
  keywords = "project 2",
  language = "en",
  issn     = "0033-295X, 1939-1471",
  pmid     = "24219850",
  doi      = "10.1037/a0034194"
}

@ARTICLE{Piantadosi2016-na,
  title     = "The Logical Primitives of Thought: {{Empirical}} Foundations for
               Compositional Cognitive Models",
  author    = "Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  volume    =  123,
  number    =  4,
  pages     = "392",
  year      =  2016,
  keywords  = "comp-cog-sci",
  issn      = "0033-295X, 1939-1471",
  doi       = "10.1037/a0039980"
}

@ARTICLE{Rosenblatt1958-kl,
  title    = "The Perceptron: {{A}} Probabilistic Model for Information Storage
              and Organization in the Brain",
  author   = "Rosenblatt, F",
  journal  = "Psychol. Rev.",
  volume   =  65,
  number   =  6,
  pages    = "386--408",
  year     =  1958,
  keywords = "read;comp-cog-sci",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/h0042519"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Tolman1946-aq,
  title     = "Studies in spatial learning. I. Orientation and the short-cut",
  author    = "Tolman, E C and Ritchie, B F and Kalish, D",
  abstract  = "The original rough formulation of the expectancy theory is
               difficult to distinguish from the alternative stimulus-response
               doctrines, partly because of the implicit definition of the
               matrix`` x â€¦",
  journal   = "J. Exp. Psychol.",
  publisher = "US: American Psychological Association",
  volume    =  36,
  number    =  1,
  pages     = "13",
  year      =  1946,
  keywords  = "learning\&memory2023",
  issn      = "0022-1015",
  doi       = "10.1037/h0053944"
}

@ARTICLE{Skinner1948-ks,
  title     = "'Superstition' in the pigeon",
  author    = "Skinner, B F",
  journal   = "J. Exp. Psychol.",
  publisher = "American Psychological Association (APA)",
  volume    =  38,
  number    =  2,
  pages     = "168--172",
  year      =  1948,
  keywords  = "skimmed;learning\&memory2023",
  issn      = "0022-1015, 1946-1941",
  doi       = "10.1037/h0055873"
}

@ARTICLE{Maier1930-ck,
  title     = "Reasoning in humans. I. On direction",
  author    = "Maier, N R F",
  abstract  = "A problematical situation was arranged so that it could be
               broken into three parts and presented to the subject as three
               separate experiences. Under these conditions the subjects could
               not find a solution. ``Thus a selected presentation of the
               experience is not enough. The parts of the experience must be
               combined in a certain manner and a 'direction' or way the
               problem is attacked, seems to be a factor which determines the
               nature of the combination. 'Trial and error' may be present in
               the attempts at the solution, but is inadequate to explain the
               sudden appearance of the correct solution, when such solution
               requires productive rather than reproductive thinking.'' These
               results are oriented with respect to the theories of Ach, Selz,
               Wertheimer, etc. The author favors an explanation in terms of
               Gestalt. (PsycINFO Database Record (c) 2016 APA, all rights
               reserved)",
  journal   = "J. Comp. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  10,
  number    =  2,
  pages     = "115--143",
  month     =  apr,
  year      =  1930,
  keywords  = "Paper1",
  issn      = "0093-4127",
  doi       = "10.1037/h0073232"
}

@ARTICLE{Lake2021-tz,
  title     = "Word Meaning in Minds and Machines",
  author    = "Lake, Brenden M and Murphy, Gregory L",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  year      =  2021,
  keywords  = "comp-cog-sci",
  issn      = "0033-295X, 1939-1471",
  doi       = "10.1037/rev0000297"
}

@ARTICLE{Bhatia2022-dp,
  title    = "Transformer networks of human conceptual knowledge",
  author   = "Bhatia, Sudeep and Richie, Russell",
  abstract = "We present a computational model capable of simulating aspects of
              human knowledge for thousands of real-world concepts. Our
              approach involves a pretrained transformer network that is
              further fine-tuned on large data sets of participant-generated
              feature norms. We show that such a model can successfully
              extrapolate from its training data, and predict human knowledge
              for new concepts and features. We apply our model to stimuli from
              25 previous experiments in semantic cognition research and show
              that it reproduces many findings on semantic verification,
              concept typicality, feature distribution, and semantic
              similarity. We also compare our model against several variants,
              and by doing so, establish the model properties that are
              necessary for good prediction. The success of our approach shows
              how a combination of language data and (laboratory-based)
              psychological data can be used to build models with rich world
              knowledge. Such models can be used in the service of new
              psychological applications, such as the modeling of naturalistic
              semantic verification and knowledge retrieval, as well as the
              modeling of real-world categorization, decision-making, and
              reasoning. (PsycInfo Database Record (c) 2022 APA, all rights
              reserved).",
  journal  = "Psychol. Rev.",
  month    =  oct,
  year     =  2022,
  keywords = "read",
  language = "en",
  issn     = "0033-295X, 1939-1471",
  pmid     = "36301272",
  doi      = "10.1037/rev0000319"
}

@ARTICLE{Rich2018-zj,
  title     = "The Limits of Learning: {{Exploration}}, Generalization, and the
               Development of Learning Traps",
  author    = "Rich, Alexander S and Gureckis, Todd M",
  abstract  = "Learning usually improves the accuracy of beliefs through the
               accumulation of experience. But are there limits to learning
               that prevent us from accurately understanding our world? In this
               article we investigate the concept of a ``learning trap''---the
               formation of a stable false belief even with extensive
               experience. Our review highlights how these traps develop
               through the interaction of learning and decision making in
               unknown environments. We further document a particularly
               pernicious learning trap driven by selective attention, a
               mechanism often assumed to facilitate learning in complex
               environments. Using computer simulation, we demonstrate the key
               attributes of the agent and environment that lead to this new
               type of learning trap. Then, in a series of experiments we
               present evidence that people robustly fall into this trap, even
               in the presence of various interventions predicted to meliorate
               it. These results highlight a fundamental limit to learning and
               adaptive behavior that impacts individuals, organizations,
               animals, and machines. (PsycInfo Database Record (c) 2020 APA,
               all rights reserved)",
  journal   = "J. Exp. Psychol. Gen.",
  publisher = "American Psychological Association",
  volume    =  147,
  number    =  11,
  pages     = "1553--1570",
  year      =  2018,
  address   = "US",
  keywords  = "Decision Making,Development,Environment,False
               Beliefs,Generalization (Learning),Learning,Learning
               Environment,Selective Attention;comp-cog-sci",
  issn      = "0096-3445, 1939-2222",
  doi       = "10.1037/xge0000466"
}

@ARTICLE{Rosedahl2022-py,
  title    = "Linear separability, irrelevant variability, and categorization
              difficulty",
  author   = "Rosedahl, Luke A and Ashby, F Gregory",
  abstract = "In rule-based (RB) category-learning tasks, the optimal strategy
              is a simple explicit rule, whereas in information-integration
              (II) tasks, the optimal strategy is impossible to describe
              verbally. This study investigates the effects of two different
              category properties on learning difficulty in category learning
              tasks-namely, linear separability and variability on stimulus
              dimensions that are irrelevant to the categorization decision.
              Previous research had reported that linearly separable II
              categories are easier to learn than nonlinearly separable
              categories, but Experiment 1, which compared performance on
              linearly and nonlinearly separable categories that were equated
              as closely as possible on all other factors that might affect
              difficulty, found that linear separability had no effect on
              learning. Experiments 1 and 2 together also established a novel
              dissociation between RB and II category learning: increasing
              variability on irrelevant stimulus dimensions impaired II
              learning but not RB learning. These results are all predicted by
              the best available measures of difficulty in RB and II tasks.
              (PsycInfo Database Record (c) 2022 APA, all rights reserved).",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  48,
  number   =  2,
  pages    = "159--172",
  month    =  feb,
  year     =  2022,
  language = "en",
  issn     = "0278-7393, 1939-1285",
  pmid     = "33871263",
  doi      = "10.1037/xlm0001000",
  pmc      = "PMC8523591"
}

@ARTICLE{Bontempi1999-og,
  title    = "Time-dependent reorganization of brain circuitry underlying
              long-term memory storage",
  author   = "Bontempi, B and Laurent-Demir, C and Destrade, C and Jaffard, R",
  abstract = "Retrograde amnesia observed following hippocampal lesions in
              humans and animals is typically temporally graded, with recent
              memory being impaired while remote memories remain intact,
              indicating that the hippocampal formation has a time-limited role
              in memory storage. However, this claim remains controversial
              because studies involving hippocampal lesions tell us nothing
              about the contribution of the hippocampus to memory storage if
              this region was present at the time of memory retrieval. We
              therefore used non-invasive functional brain imaging using
              (14C)2-deoxyglucose uptake to examine how the brain circuitry
              underlying long-term memory storage is reorganized over time in
              an intact brain. Regional metabolic activity in the brain was
              mapped in mice tested at different times for retention of a
              spatial discrimination task. Here we report that increasing the
              retention interval from 5 days to 25 days resulted in both
              decreased hippocampal metabolic activity during retention testing
              and a loss of correlation between hippocampal metabolic activity
              and memory performance. Concomitantly, a recruitment of certain
              cortical areas was observed. These results indicate that there is
              a time-dependent reorganization of the neuronal circuitry
              underlying long-term memory storage, in which a transitory
              interaction between the hippocampal formation and the neocortex
              would mediate the establishment of long-lived cortical memory
              representations.",
  journal  = "Nature",
  volume   =  400,
  number   =  6745,
  pages    = "671--675",
  month    =  aug,
  year     =  1999,
  keywords = "l\&m final",
  language = "en",
  issn     = "0028-0836",
  pmid     = "10458162",
  doi      = "10.1038/23270"
}

@ARTICLE{Daw2006-xl,
  title    = "Cortical substrates for exploratory decisions in humans",
  author   = "Daw, Nathaniel D and O'Doherty, John P and Dayan, Peter and
              Seymour, Ben and Dolan, Raymond J",
  abstract = "Decision making in an uncertain environment poses a conflict
              between the opposing demands of gathering and exploiting
              information. In a classic illustration of this
              'exploration-exploitation' dilemma, a gambler choosing between
              multiple slot machines balances the desire to select what seems,
              on the basis of accumulated experience, the richest option,
              against the desire to choose a less familiar option that might
              turn out more advantageous (and thereby provide information for
              improving future decisions). Far from representing idle
              curiosity, such exploration is often critical for organisms to
              discover how best to harvest resources such as food and water. In
              appetitive choice, substantial experimental evidence, underpinned
              by computational reinforcement learning (RL) theory, indicates
              that a dopaminergic, striatal and medial prefrontal network
              mediates learning to exploit. In contrast, although exploration
              has been well studied from both theoretical and ethological
              perspectives, its neural substrates are much less clear. Here we
              show, in a gambling task, that human subjects' choices can be
              characterized by a computationally well-regarded strategy for
              addressing the explore/exploit dilemma. Furthermore, using this
              characterization to classify decisions as exploratory or
              exploitative, we employ functional magnetic resonance imaging to
              show that the frontopolar cortex and intraparietal sulcus are
              preferentially active during exploratory decisions. In contrast,
              regions of striatum and ventromedial prefrontal cortex exhibit
              activity characteristic of an involvement in value-based
              exploitative decision making. The results suggest a model of
              action selection under uncertainty that involves switching
              between exploratory and exploitative behavioural modes, and
              provide a computationally precise characterization of the
              contribution of key decision-related brain systems to each of
              these functions.",
  journal  = "Nature",
  volume   =  441,
  number   =  7095,
  pages    = "876--879",
  month    =  jun,
  year     =  2006,
  keywords = "RL;ccm2023",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "16778890",
  doi      = "10.1038/nature04766",
  pmc      = "PMC2635947"
}

@ARTICLE{Mnih2015-vd,
  title     = "{Human-Level} Control through Deep Reinforcement Learning",
  author    = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
               Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and
               Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and
               Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
               Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
               Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis",
  abstract  = "An artificial agent is developed that learns to play a diverse
               range of classic Atari 2600 computer games directly from sensory
               experience, achieving a performance comparable to that of an
               expert human player; this work paves the way to building
               general-purpose learning algorithms that bridge the divide
               between perception and action.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  518,
  number    =  7540,
  pages     = "529--533",
  month     =  feb,
  year      =  2015,
  keywords  = "read;comp-cog-sci;ccm2023",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14236"
}

@ARTICLE{LeCun2015-hm,
  title     = "Deep Learning",
  author    = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  abstract  = "Deep learning allows computational models that are composed of
               multiple processing layers to learn representations of data with
               multiple levels of abstraction. These methods have dramatically
               improved the state-of-the-art in speech recognition, visual
               object recognition, object detection and many other domains such
               as drug discovery and genomics. Deep learning discovers
               intricate structure in large data sets by using the
               backpropagation algorithm to indicate how a machine should
               change its internal parameters that are used to compute the
               representation in each layer from the representation in the
               previous layer. Deep convolutional nets have brought about
               breakthroughs in processing images, video, speech and audio,
               whereas recurrent nets have shone light on sequential data such
               as text and speech.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "436--444",
  month     =  may,
  year      =  2015,
  keywords  = "read;machine-learning;ccm2023",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14539"
}

@ARTICLE{Ghahramani2015-ko,
  title     = "Probabilistic Machine Learning and Artificial Intelligence",
  author    = "Ghahramani, Zoubin",
  abstract  = "How can a machine learn from experience? Probabilistic modelling
               provides a framework for understanding what learning is, and has
               therefore emerged as one of the principal theoretical and
               practical approaches for designing machines that learn from data
               acquired through experience. The probabilistic framework, which
               describes how to represent and manipulate uncertainty about
               models and predictions, has a central role in scientific data
               analysis, machine learning, robotics, cognitive science and
               artificial intelligence. This Review provides an introduction to
               this framework, and discusses some of the state-of-the-art
               advances in the field, namely, probabilistic programming,
               Bayesian optimization, data compression and automatic model
               discovery.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "452--459",
  month     =  may,
  year      =  2015,
  keywords  = "read;comp-cog-sci;ccm2023",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14541"
}

@ARTICLE{Stachenfeld2017-ra,
  title    = "The hippocampus as a predictive map",
  author   = "Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman,
              Samuel J",
  abstract = "A cognitive map has long been the dominant metaphor for
              hippocampal function, embracing the idea that place cells encode
              a geometric representation of space. However, evidence for
              predictive coding, reward sensitivity and policy dependence in
              place cells suggests that the representation is not purely
              spatial. We approach this puzzle from a reinforcement learning
              perspective: what kind of spatial representation is most useful
              for maximizing future reward? We show that the answer takes the
              form of a predictive representation. This representation captures
              many aspects of place cell responses that fall outside the
              traditional view of a cognitive map. Furthermore, we argue that
              entorhinal grid cells encode a low-dimensionality basis set for
              the predictive representation, useful for suppressing noise in
              predictions and extracting multiscale structure for hierarchical
              planning.",
  journal  = "Nat. Neurosci.",
  volume   =  20,
  number   =  11,
  pages    = "1643--1653",
  month    =  nov,
  year     =  2017,
  keywords = "skimmed;learning\&memory2023",
  language = "en",
  issn     = "1097-6256, 1546-1726",
  pmid     = "28967910",
  doi      = "10.1038/nn.4650"
}

@ARTICLE{Lisman2017-pf,
  title    = "Viewpoints: how the hippocampus contributes to memory, navigation
              and cognition",
  author   = "Lisman, John and Buzs{\'a}ki, Gy{\"o}rgy and Eichenbaum, Howard
              and Nadel, Lynn and Ranganath, Charan and Redish, A David",
  abstract = "The hippocampus serves a critical function in memory, navigation,
              and cognition. Nature Neuroscience asked John Lisman to lead a
              group of researchers in a dialog on shared and distinct
              viewpoints on the hippocampus. There has been a long history of
              studying the hippocampus, but recent work has made it possible to
              study the cellular and network basis of defined
              operations---operations that include cognitive processes that
              have been otherwise difficult to study (see Box 1 for useful
              terminology). These operations deal with the context-dependent
              representation of complex memories, the role of mental
              exploration based on imagined rather than real movements, and the
              use of recalled information for navigation and decision-making.
              The progress that has been made in understanding the hippocampus
              has motivated the study of other brain regions that provide
              hippocampal input or receive hippocampal output; the hippocampus
              is thus serving as a nucleating point for the larger goal of
              understanding the neural codes that allow inter-regional
              communication and more generally, understanding how memory-guided
              behavior is achieved by large scale integration of brain regions.
              In generating a discussion among experts in the study of the
              cognitive processes of the hippocampus, the editors and I have
              posed questions that probe important principles of hippocampal
              function. We hope that the resulting discussion will make clear
              to readers the progress that has been made, while also
              identifying issues where consensus has not yet been achieved and
              that should be pursued in future research. -- John Lisman",
  journal  = "Nat. Neurosci.",
  volume   =  20,
  number   =  11,
  pages    = "1434--1447",
  month    =  oct,
  year     =  2017,
  keywords = "skimmed;learning\&memory2023",
  language = "en",
  issn     = "1097-6256, 1546-1726",
  pmid     = "29073641",
  doi      = "10.1038/nn.4661",
  pmc      = "PMC5943637"
}

@ARTICLE{McClelland2003-bz,
  title    = "The parallel distributed processing approach to semantic
              cognition",
  author   = "McClelland, James L and Rogers, Timothy T",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  4,
  number   =  4,
  pages    = "310--322",
  month    =  apr,
  year     =  2003,
  keywords = "read;comp-cog-sci;ccm2023",
  language = "en",
  issn     = "1471-003X",
  pmid     = "12671647",
  doi      = "10.1038/nrn1076"
}

@ARTICLE{Zador2019-hq,
  title    = "A critique of pure learning and what artificial neural networks
              can learn from animal brains",
  author   = "Zador, Anthony M",
  abstract = "Artificial neural networks (ANNs) have undergone a revolution,
              catalyzed by better supervised learning algorithms. However, in
              stark contrast to young animals (including humans), training such
              networks requires enormous numbers of labeled examples, leading
              to the belief that animals must rely instead mainly on
              unsupervised learning. Here we argue that most animal behavior is
              not the result of clever learning algorithms-supervised or
              unsupervised-but is encoded in the genome. Specifically, animals
              are born with highly structured brain connectivity, which enables
              them to learn very rapidly. Because the wiring diagram is far too
              complex to be specified explicitly in the genome, it must be
              compressed through a ``genomic bottleneck''. The genomic
              bottleneck suggests a path toward ANNs capable of rapid learning.",
  journal  = "Nat. Commun.",
  volume   =  10,
  number   =  1,
  pages    = "3770",
  month    =  aug,
  year     =  2019,
  keywords = "read;cog-sci;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "2041-1723",
  pmid     = "31434893",
  doi      = "10.1038/s41467-019-11786-6",
  pmc      = "PMC6704116"
}

@ARTICLE{Weiss2021-nw,
  title    = "Interacting with volatile environments stabilizes hidden-state
              inference and its brain signatures",
  author   = "Weiss, Aur{\'e}lien and Chambon, Val{\'e}rian and Lee, Junseok K
              and Drugowitsch, Jan and Wyart, Valentin",
  abstract = "Making accurate decisions in uncertain environments requires
              identifying the generative cause of sensory cues, but also the
              expected outcomes of possible actions. Although both cognitive
              processes can be formalized as Bayesian inference, they are
              commonly studied using different experimental frameworks, making
              their formal comparison difficult. Here, by framing a reversal
              learning task either as cue-based or outcome-based inference, we
              found that humans perceive the same volatile environment as more
              stable when inferring its hidden state by interaction with
              uncertain outcomes than by observation of equally uncertain cues.
              Multivariate patterns of magnetoencephalographic (MEG) activity
              reflected this behavioral difference in the neural interaction
              between inferred beliefs and incoming evidence, an effect
              originating from associative regions in the temporal lobe.
              Together, these findings indicate that the degree of control over
              the sampling of volatile environments shapes human learning and
              decision-making under uncertainty.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "2228",
  month    =  apr,
  year     =  2021,
  keywords = "BAMB2023",
  language = "en",
  issn     = "2041-1723",
  pmid     = "33850124",
  doi      = "10.1038/s41467-021-22396-6",
  pmc      = "PMC8044147"
}

@ARTICLE{Bonner2021-zd,
  title    = "Object Representations in the Human Brain Reflect the
              {Co-Occurrence} Statistics of Vision and Language",
  author   = "Bonner, Michael F and Epstein, Russell A",
  abstract = "Abstract A central regularity of visual perception is the
              co-occurrence of objects in the natural environment. Here we use
              machine learning and fMRI to test the hypothesis that object
              co-occurrence statistics are encoded in the human visual system
              and elicited by the perception of individual objects. We
              identified low-dimensional representations that capture the
              latent statistical structure of object co-occurrence in
              real-world scenes, and we mapped these statistical
              representations onto voxel-wise fMRI responses during object
              viewing. We found that cortical responses to single objects were
              predicted by the statistical ensembles in which they typically
              occur, and that this link between objects and their visual
              contexts was made most strongly in parahippocampal cortex,
              overlapping with the anterior portion of scene-selective
              parahippocampal place area. In contrast, a language-based
              statistical model of the co-occurrence of object names in written
              text predicted responses in neighboring regions of
              object-selective visual cortex. Together, these findings show
              that the sensory coding of objects in the human brain reflects
              the latent statistics of object context in visual and linguistic
              experience.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "4081",
  month    =  dec,
  year     =  2021,
  keywords = "comp-cog-sci",
  issn     = "2041-1723",
  doi      = "10.1038/s41467-021-24368-2"
}

@ARTICLE{Ellis2022-je,
  title    = "Synthesizing theories of human language with Bayesian program
              induction",
  author   = "Ellis, Kevin and Albright, Adam and Solar-Lezama, Armando and
              Tenenbaum, Joshua B and O'Donnell, Timothy J",
  abstract = "Automated, data-driven construction and evaluation of scientific
              models and theories is a long-standing challenge in artificial
              intelligence. We present a framework for algorithmically
              synthesizing models of a basic part of human language:
              morpho-phonology, the system that builds word forms from sounds.
              We integrate Bayesian inference with program synthesis and
              representations inspired by linguistic theory and cognitive
              models of learning and discovery. Across 70 datasets from 58
              diverse languages, our system synthesizes human-interpretable
              models for core aspects of each language's morpho-phonology,
              sometimes approaching models posited by human linguists. Joint
              inference across all 70 data sets automatically synthesizes a
              meta-model encoding interpretable cross-language typological
              tendencies. Finally, the same algorithm captures few-shot
              learning dynamics, acquiring new morphophonological rules from
              just one or a few examples. These results suggest routes to more
              powerful machine-enabled discovery of interpretable models in
              linguistics and other scientific domains.",
  journal  = "Nat. Commun.",
  volume   =  13,
  number   =  1,
  pages    = "5024",
  month    =  aug,
  year     =  2022,
  keywords = "skimmed;comp-cog-sci",
  language = "en",
  issn     = "2041-1723",
  pmid     = "36042196",
  doi      = "10.1038/s41467-022-32012-w",
  pmc      = "PMC9427767"
}

@ARTICLE{Muthukrishna2019-wq,
  title    = "A problem in theory",
  author   = "Muthukrishna, Michael and Henrich, Joseph",
  abstract = "The replication crisis facing the psychological sciences is
              widely regarded as rooted in methodological or statistical
              shortcomings. We argue that a large part of the problem is the
              lack of a cumulative theoretical framework or frameworks. Without
              an overarching theoretical framework that generates hypotheses
              across diverse domains, empirical programs spawn and grow from
              personal intuitions and culturally biased folk theories. By
              providing ways to develop clear predictions, including through
              the use of formal modelling, theoretical frameworks set
              expectations that determine whether a new finding is
              confirmatory, nicely integrating with existing lines of research,
              or surprising, and therefore requiring further replication and
              scrutiny. Such frameworks also prioritize certain research foci,
              motivate the use diverse empirical approaches and, often, provide
              a natural means to integrate across the sciences. Thus,
              overarching theoretical frameworks pave the way toward a more
              general theory of human behaviour. We illustrate one such a
              theoretical framework: dual inheritance theory.",
  journal  = "Nat Hum Behav",
  volume   =  3,
  number   =  3,
  pages    = "221--229",
  month    =  mar,
  year     =  2019,
  keywords = "BAMB2023",
  language = "en",
  issn     = "2397-3374",
  pmid     = "30953018",
  doi      = "10.1038/s41562-018-0522-1"
}

@ARTICLE{Gomez2019-ci,
  title    = "Extensive childhood experience with Pok{\'e}mon suggests
              eccentricity drives organization of visual cortex",
  author   = "Gomez, Jesse and Barnett, Michael and Grill-Spector, Kalanit",
  abstract = "The functional organization of human high-level visual cortex,
              such as the face- and place-selective regions, is strikingly
              consistent across individuals. An unanswered question in
              neuroscience concerns which dimensions of visual information
              constrain the development and topography of this shared brain
              organization. To answer this question, we used functional
              magnetic resonance imaging to scan a unique group of adults who,
              as children, had extensive visual experience with Pok{\'e}mon.
              These animal-like, pixelated characters are dissimilar from other
              ecological categories, such as faces and places, along critical
              dimensions (foveal bias, rectilinearity, size, animacy). We show
              not only that adults who have Pok{\'e}mon experience demonstrate
              distinct distributed cortical responses to Pok{\'e}mon, but also
              that the experienced retinal eccentricity during childhood can
              predict the locus of Pok{\'e}mon responses in adulthood. These
              data demonstrate that inherent functional representations in the
              visual cortex-retinal eccentricity-combined with consistent
              viewing behaviour of particular stimuli during childhood result
              in a shared functional topography in adulthood.",
  journal  = "Nat Hum Behav",
  volume   =  3,
  number   =  6,
  pages    = "611--624",
  month    =  jun,
  year     =  2019,
  keywords = "skimmed;dev-cog-neuro;development",
  language = "en",
  issn     = "2397-3374",
  pmid     = "31061489",
  doi      = "10.1038/s41562-019-0592-8",
  pmc      = "PMC7055538"
}

@ARTICLE{Piloto2022-yi,
  title    = "Intuitive Physics Learning in a {Deep-Learning} Model Inspired by
              Developmental Psychology",
  author   = "Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and
              Botvinick, Matthew",
  abstract = "Abstract `Intuitive physics' enables our pragmatic engagement
              with the physical world and forms a key component of `common
              sense' aspects of thought. Current artificial intelligence
              systems pale in their understanding of intuitive physics, in
              comparison to even very young children. Here we address this gap
              between humans and machines by drawing on the field of
              developmental psychology. First, we introduce and open-source a
              machine-learning dataset designed to evaluate conceptual
              understanding of intuitive physics, adopting the
              violation-of-expectation (VoE) paradigm from developmental
              psychology. Second, we build a deep-learning system that learns
              intuitive physics directly from visual data, inspired by studies
              of visual cognition in children. We demonstrate that our model
              can learn a diverse set of physical concepts, which depends
              critically on object-level representations, consistent with
              findings from developmental psychology. We consider the
              implications of these results both for AI and for research on
              human cognition.",
  journal  = "Nature Human Behaviour",
  month    =  jul,
  year     =  2022,
  keywords = "development",
  issn     = "2397-3374",
  doi      = "10.1038/s41562-022-01394-8"
}

@ARTICLE{Ho2022-iy,
  title    = "People construct simplified mental representations to plan",
  author   = "Ho, Mark K and Abel, David and Correa, Carlos G and Littman,
              Michael L and Cohen, Jonathan D and Griffiths, Thomas L",
  abstract = "One of the most striking features of human cognition is the
              ability to plan. Two aspects of human planning stand out-its
              efficiency and flexibility. Efficiency is especially impressive
              because plans must often be made in complex environments, and yet
              people successfully plan solutions to many everyday problems
              despite having limited cognitive resources1-3. Standard accounts
              in psychology, economics and artificial intelligence have
              suggested that human planning succeeds because people have a
              complete representation of a task and then use heuristics to plan
              future actions in that representation4-11. However, this approach
              generally assumes that task representations are fixed. Here we
              propose that task representations can be controlled and that such
              control provides opportunities to quickly simplify problems and
              more easily reason about them. We propose a computational account
              of this simplification process and, in a series of preregistered
              behavioural experiments, show that it is subject to online
              cognitive control12-14 and that people optimally balance the
              complexity of a task representation and its utility for planning
              and acting. These results demonstrate how strategically
              perceiving and conceiving problems facilitates the effective use
              of limited cognitive resources.",
  journal  = "Nature",
  volume   =  606,
  number   =  7912,
  pages    = "129--136",
  month    =  jun,
  year     =  2022,
  keywords = "read;project 1;comp-cog-sci;project 2",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "35589843",
  doi      = "10.1038/s41586-022-04743-9",
  pmc      = "5111694"
}

@ARTICLE{Van_Opheusden2023-ol,
  title    = "Expertise increases planning depth in human gameplay",
  author   = "van Opheusden, Bas and Kuperwajs, Ionatan and Galbiati, Gianni
              and Bnaya, Zahy and Li, Yunqi and Ma, Wei Ji",
  abstract = "A hallmark of human intelligence is the ability to plan multiple
              steps into the future1,2. Despite decades of research3-5, it is
              still debated whether skilled decision-makers plan more steps
              ahead than novices6-8. Traditionally, the study of expertise in
              planning has used board games such as chess, but the complexity
              of these games poses a barrier to quantitative estimates of
              planning depth. Conversely, common planning tasks in cognitive
              science often have a lower complexity9,10 and impose a ceiling
              for the depth to which any player can plan. Here we investigate
              expertise in a complex board game that offers ample opportunity
              for skilled players to plan deeply. We use model fitting methods
              to show that human behaviour can be captured using a
              computational cognitive model based on heuristic search. To
              validate this model, we predict human choices, response times and
              eye movements. We also perform a Turing test and a reconstruction
              experiment. Using the model, we find robust evidence for
              increased planning depth with expertise in both laboratory and
              large-scale mobile data. Experts memorize and reconstruct board
              features more accurately. Using complex tasks combined with
              precise behavioural modelling might expand our understanding of
              human planning and help to bridge the gap with progress in
              artificial intelligence.",
  journal  = "Nature",
  volume   =  618,
  number   =  7967,
  pages    = "1000--1005",
  month    =  jun,
  year     =  2023,
  keywords = "project 1",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "37258667",
  doi      = "10.1038/s41586-023-06124-2",
  pmc      = "3077926"
}

@ARTICLE{Lechner2020-fw,
  title     = "Neural circuit policies enabling auditable autonomy",
  author    = "Lechner, Mathias and Hasani, Ramin and Amini, Alexander and
               Henzinger, Thomas A and Rus, Daniela and Grosu, Radu",
  abstract  = "A central goal of artificial intelligence in high-stakes
               decision-making applications is to design a single algorithm
               that simultaneously expresses generalizability by learning
               coherent representations of their world and interpretable
               explanations of its dynamics. Here, we combine brain-inspired
               neural computation principles and scalable deep learning
               architectures to design compact neural controllers for
               task-specific compartments of a full-stack autonomous vehicle
               control system. We discover that a single algorithm with 19
               control neurons, connecting 32 encapsulated input features to
               outputs by 253 synapses, learns to map high-dimensional inputs
               into steering commands. This system shows superior
               generalizability, interpretability and robustness compared with
               orders-of-magnitude larger black-box learning systems. The
               obtained neural agents enable high-fidelity autonomy for
               task-specific parts of a complex autonomous system. Inspired by
               the brain of the roundworm Caenorhabditis elegans, the authors
               design a highly compact neural network controller directly from
               raw input pixels. Compared with larger networks, this compact
               controller demonstrates improved generalization, robustness and
               interpretability on a lane-keeping task.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  2,
  number    =  10,
  pages     = "642--652",
  month     =  oct,
  year      =  2020,
  language  = "en",
  issn      = "2522-5839, 2522-5839",
  doi       = "10.1038/s42256-020-00237-3"
}

@ARTICLE{Xue2023-uc,
  title     = "{Phy-Q} as a measure for physical reasoning intelligence",
  author    = "Xue, Cheng and Pinto, Vimukthini and Gamage, Chathura and
               Nikonova, Ekaterina and Zhang, Peng and Renz, Jochen",
  abstract  = "Humans are well versed in reasoning about the behaviours of
               physical objects and choosing actions accordingly to accomplish
               tasks, while this remains a major challenge for artificial
               intelligence. To facilitate research addressing this problem, we
               propose a new testbed that requires an agent to reason about
               physical scenarios and take an action appropriately. Inspired by
               the physical knowledge acquired in infancy and the capabilities
               required for robots to operate in real-world environments, we
               identify 15 essential physical scenarios. We create a wide
               variety of distinct task templates, and we ensure that all the
               task templates within the same scenario can be solved by using
               one specific strategic physical rule. By having such a design,
               we evaluate two distinct levels of generalization, namely local
               generalization and broad generalization. We conduct an extensive
               evaluation with human players, learning agents with various
               input types and architectures, and heuristic agents with
               different strategies. Inspired by how the human intelligence
               quotient is calculated, we define the physical reasoning
               quotient (Phy-Q score) that reflects the physical reasoning
               intelligence of an agent using the physical scenarios we
               considered. Our evaluation shows that (1) all the agents are far
               below human performance, and (2) learning agents, even with good
               local generalization ability, struggle to learn the underlying
               physical reasoning rules and fail to generalize broadly. We
               encourage the development of intelligent agents that can reach
               the human-level Phy-Q score. When it comes to reasoning about
               the motion of physical objects, humans have natural intuitive
               physics knowledge. To test how good artificial learning agents
               are in similar predictive abilities, Xue and colleagues present
               a benchmark based on a two-dimensional physics environment in
               which 15 physical reasoning skills are measured.",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  5,
  number    =  1,
  pages     = "83--93",
  month     =  jan,
  year      =  2023,
  keywords  = "read;comp-cog-sci;project 2",
  language  = "en",
  issn      = "2522-5839, 2522-5839",
  doi       = "10.1038/s42256-022-00583-4"
}

@ARTICLE{Davachi2003-ic,
  title    = "Multiple routes to memory: distinct medial temporal lobe
              processes build item and source memories",
  author   = "Davachi, Lila and Mitchell, Jason P and Wagner, Anthony D",
  abstract = "A central function of memory is to permit an organism to
              distinguish between stimuli that have been previously encountered
              and those that are novel. Although the medial temporal lobe
              (which includes the hippocampus and surrounding perirhinal,
              parahippocampal, and entorhinal cortices) is known to be crucial
              for recognition memory, controversy remains regarding how the
              specific subregions within the medial temporal lobe contribute to
              recognition. We used event-related functional MRI to examine the
              relation between activation in distinct medial temporal lobe
              subregions during memory formation and the ability (i) to later
              recognize an item as previously encountered (item recognition)
              and (ii) to later recollect specific contextual details about the
              prior encounter (source recollection). Encoding activation in
              hippocampus and in posterior parahippocampal cortex predicted
              later source recollection, but was uncorrelated with item
              recognition. In contrast, encoding activation in perirhinal
              cortex predicted later item recognition, but not subsequent
              source recollection. These outcomes suggest that the subregions
              within the medial temporal lobe subserve distinct, but
              complementary, learning mechanisms.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  100,
  number   =  4,
  pages    = "2157--2162",
  month    =  feb,
  year     =  2003,
  keywords = "skimmed;learning\&memory2023",
  language = "en",
  issn     = "0027-8424",
  pmid     = "12578977",
  doi      = "10.1073/pnas.0337195100",
  pmc      = "PMC149975"
}

@ARTICLE{Johnson-Laird2010-yq,
  title    = "Mental models and human reasoning",
  author   = "Johnson-Laird, Philip N",
  abstract = "To be rational is to be able to reason. Thirty years ago
              psychologists believed that human reasoning depended on formal
              rules of inference akin to those of a logical calculus. This
              hypothesis ran into difficulties, which led to an alternative
              view: reasoning depends on envisaging the possibilities
              consistent with the starting point---a perception of the world, a
              set of assertions, a memory, or some mixture of them. We
              construct mental models of each distinct possibility and derive a
              conclusion from them. The theory predicts systematic errors in
              our reasoning, and the evidence corroborates this prediction.
              Yet, our ability to use counterexamples to refute invalid
              inferences provides a foundation for rationality. On this
              account, reasoning is a simulation of the world fleshed out with
              our knowledge, not a formal rearrangement of the logical
              skeletons of sentences.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  107,
  number   =  43,
  pages    = "18243--18250",
  year     =  2010,
  keywords = "project 1",
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.1012933107",
  doi      = "10.1073/pnas.1012933107"
}

@ARTICLE{Erickson2011-on,
  title    = "Exercise Training Increases Size of Hippocampus and Improves
              Memory",
  author   = "Erickson, K I and Voss, M W and Prakash, R S and Basak, C and
              Szabo, A and Chaddock, L and Kim, J S and Heo, S and Alves, H and
              White, S M and Wojcicki, T R and Mailey, E and Vieira, V J and
              Martin, S A and Pence, B D and Woods, J A and McAuley, E and
              Kramer, A F",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  108,
  number   =  7,
  pages    = "3017--3022",
  month    =  feb,
  year     =  2011,
  keywords = "exercise-brain",
  issn     = "0027-8424, 1091-6490",
  doi      = "10.1073/pnas.1015950108"
}

@ARTICLE{Yang2012-if,
  title    = "Critical period for acoustic preference in mice",
  author   = "Yang, Eun-Jin and Lin, Eric W and Hensch, Takao K",
  abstract = "Preference behaviors are often established during early life, but
              the underlying neural circuit mechanisms remain unknown. Adapting
              a unique nesting behavior assay, we confirmed a ``critical
              period'' for developing music preference in C57BL/6 mice. Early
              music exposure between postnatal days 15 and 24 reversed their
              innate bias for silent shelter, which typically could not be
              altered in adulthood. Instead, exposing adult mice treated
              acutely with valproic acid or carrying a targeted deletion of the
              Nogo receptor (NgR(-/-)) unmasked a strong plasticity of
              preference consistent with a reopening of the critical period as
              seen in other systems. Imaging of cFos expression revealed a
              prominent neuronal activation in response to the exposed music in
              the prelimbic and infralimbic medial prefrontal cortex only under
              conditions of open plasticity. Neither behavioral changes nor
              selective medial prefrontal cortex activation was observed in
              response to pure tone exposure, indicating a music-specific
              effect. Open-field center crossings were increased concomitant
              with shifts in music preference, suggesting a potential
              anxiolytic effect. Thus, music may offer both a unique window
              into the emotional state of mice and a potentially efficient
              assay for molecular ``brakes'' on critical period plasticity
              common to sensory and higher order brain areas.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   = "109 Suppl 2",
  number   = "Suppl 2",
  pages    = "17213--17220",
  month    =  oct,
  year     =  2012,
  keywords = "skimmed;dev-cog-neuro;development",
  language = "en",
  issn     = "0027-8424, 1091-6490",
  pmid     = "23045690",
  doi      = "10.1073/pnas.1200705109",
  pmc      = "PMC3477391"
}

@ARTICLE{Battaglia2013-eg,
  title    = "Simulation as an engine of physical scene understanding",
  author   = "Battaglia, Peter W and Hamrick, Jessica B and Tenenbaum, Joshua B",
  abstract = "In a glance, we can perceive whether a stack of dishes will
              topple, a branch will support a child's weight, a grocery bag is
              poorly packed and liable to tear or crush its contents, or a tool
              is firmly attached to a table or free to be lifted. Such rapid
              physical inferences are central to how people interact with the
              world and with each other, yet their computational underpinnings
              are poorly understood. We propose a model based on an ``intuitive
              physics engine,'' a cognitive mechanism similar to computer
              engines that simulate rich physics in video games and graphics,
              but that uses approximate, probabilistic simulations to make
              robust and fast inferences in complex natural scenes where
              crucial information is unobserved. This single model fits data
              from five distinct psychophysical tasks, captures several
              illusions and biases, and explains core aspects of human mental
              models and common-sense reasoning that are instrumental to how
              humans understand their everyday world.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  110,
  number   =  45,
  pages    = "18327--18332",
  month    =  nov,
  year     =  2013,
  keywords = "project 2;comp-cog-sci",
  language = "en",
  issn     = "0027-8424, 1091-6490",
  pmid     = "24145417",
  doi      = "10.1073/pnas.1306572110",
  pmc      = "PMC3831455"
}

@ARTICLE{Firestone2020-rp,
  title    = "Performance vs. competence in human-machine comparisons",
  author   = "Firestone, Chaz",
  abstract = "Does the human mind resemble the machines that can behave like
              it? Biologically inspired machine-learning systems approach
              ``human-level'' accuracy in an astounding variety of domains, and
              even predict human brain activity-raising the exciting
              possibility that such systems represent the world like we do.
              However, even seemingly intelligent machines fail in strange and
              ``unhumanlike'' ways, threatening their status as models of our
              minds. How can we know when human-machine behavioral differences
              reflect deep disparities in their underlying capacities, vs. when
              such failures are only superficial or peripheral? This article
              draws on a foundational insight from cognitive science-the
              distinction between performance and competence-to encourage
              ``species-fair'' comparisons between humans and machines. The
              performance/competence distinction urges us to consider whether
              the failure of a system to behave as ideally hypothesized, or the
              failure of one creature to behave like another, arises not
              because the system lacks the relevant knowledge or internal
              capacities (``competence''), but instead because of superficial
              constraints on demonstrating that knowledge (``performance''). I
              argue that this distinction has been neglected by research
              comparing human and machine behavior, and that it should be
              essential to any such comparison. Focusing on the domain of image
              classification, I identify three factors contributing to the
              species-fairness of human-machine comparisons, extracted from
              recent work that equates such constraints. Species-fair
              comparisons level the playing field between natural and
              artificial intelligence, so that we can separate more superficial
              differences from those that may be deep and enduring.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  43,
  pages    = "26562--26571",
  month    =  oct,
  year     =  2020,
  keywords = "artificial intelligence; cognition; deep learning; development;
              perception;read;compling-cogsci2023",
  language = "en",
  issn     = "0027-8424, 1091-6490",
  pmid     = "33051296",
  doi      = "10.1073/pnas.1905334117",
  pmc      = "PMC7604508"
}

@ARTICLE{Allen2020-tf,
  title    = "Rapid trial-and-error learning with simulation supports flexible
              tool use and physical reasoning",
  author   = "Allen, Kelsey R and Smith, Kevin A and Tenenbaum, Joshua B",
  abstract = "Many animals, and an increasing number of artificial agents,
              display sophisticated capabilities to perceive and manipulate
              objects. But human beings remain distinctive in their capacity
              for flexible, creative tool use-using objects in new ways to act
              on the world, achieve a goal, or solve a problem. To study this
              type of general physical problem solving, we introduce the
              Virtual Tools game. In this game, people solve a large range of
              challenging physical puzzles in just a handful of attempts. We
              propose that the flexibility of human physical problem solving
              rests on an ability to imagine the effects of hypothesized
              actions, while the efficiency of human search arises from rich
              action priors which are updated via observations of the world. We
              instantiate these components in the ``sample, simulate, update''
              (SSUP) model and show that it captures human performance across
              30 levels of the Virtual Tools game. More broadly, this model
              provides a mechanism for explaining how people condense general
              physical knowledge into actionable, task-specific plans to
              achieve flexible and efficient physical problem solving.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  47,
  pages    = "29302--29310",
  month    =  nov,
  year     =  2020,
  keywords = "intuitive physics; physical problem solving; tool
              use;read;comp-cog-sci;project 2;Paper1",
  language = "en",
  issn     = "0027-8424, 1091-6490",
  pmid     = "33229515",
  doi      = "10.1073/pnas.1912341117",
  pmc      = "PMC7703630"
}

@ARTICLE{Ambridge2008-jt,
  title    = "Is Structure Dependence an Innate Constraint? New Experimental
              Evidence From Children's {Complex-Question} Production",
  author   = "Ambridge, Ben and Rowland, Caroline F and Pine, Julian M",
  abstract = "According to Crain and Nakayama (1987), when forming complex
              yes/no questions, children do not make errors such as Is the boy
              who smoking is crazy? because they have innate knowledge of
              structure dependence and so will not move the auxiliary from the
              relative clause. However, simple recurrent networks are also able
              to avoid such errors, on the basis of surface distributional
              properties of the input (Lewis \& Elman, 2001; Reali \&
              Christiansen, 2005). Two new elicited production studies revealed
              that (a) children occasionally produce structure-dependence
              errors and (b) the pattern of children's auxiliary-doubling
              errors (Is the boy who is smoking is crazy?) suggests a
              sensitivity to surface co-occurrence patterns in the input. This
              article concludes that current data do not provide any support
              for the claim that structure dependence is an innate constraint,
              and that it is possible that children form a structure-dependent
              grammar on the basis of exposure to input that exhibits this
              property.",
  journal  = "Cogn. Sci.",
  volume   =  32,
  number   =  1,
  pages    = "222--255",
  month    =  jan,
  year     =  2008,
  keywords = "skimmed;compling-cogsci2023",
  language = "en",
  issn     = "0364-0213",
  pmid     = "21635337",
  doi      = "10.1080/03640210701703766"
}

@ARTICLE{Goodman2008-ee,
  title    = "A {{Rational Analysis}} of {{{Rule-Based} Concept Learning}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Feldman, Jacob and
              Griffiths, Thomas L",
  abstract = "This article proposes a new model of human concept learning that
              provides a rational analysis of learning feature-based concepts.
              This model is built upon Bayesian inference for a grammatically
              structured hypothesis space---a concept language of logical
              rules. This article compares the model predictions to human
              generalization judgments in several well-known category learning
              experiments, and finds good agreement for both average and
              individual participant generalizations. This article further
              investigates judgments for a broad set of 7-feature concepts---a
              more natural setting in several ways---and again finds that the
              model explains human performance.",
  journal  = "Cogn. Sci.",
  volume   =  32,
  number   =  1,
  pages    = "108--154",
  month    =  jan,
  year     =  2008,
  keywords = "comp-cog-sci",
  issn     = "0364-0213",
  doi      = "10.1080/03640210701802071"
}

@ARTICLE{Kruschke1993-eg,
  title    = "Human {{Category Learning}}: {{Implications}} for
              {{Backpropagation Models}}",
  author   = "Kruschke, John K",
  abstract = "Backpropagation (Rumelhart et al., 1986a) was proposed as a
              general learning algorithm for multi-layer perceptrons. This a n
              d e demonstrates chat a standard version of backprop fails to
              attend selectively to input dimensions in the same way as humans,
              suffers catastrophic forgetting of previously learned
              associations when novel exemplars are [rained, and can be overly
              sensitive to linear categoy boundaries. Another connecrionist
              model, A L C O V E (Krwchke 1990, 1992), does nor suffer those
              failures. Previous researchers identified these problems; the
              present article repons quantitative fits of the models to new
              human learning data. A L C O V E can be functionally approximated
              by a network that uses linear-sigmoid hidden nodes, like standard
              backprop. Ir is argued that models of human category learning
              should incorporate quasi-local representations and dimensional
              artention learning, as well as error-driuen learning, to address
              simulraneously all three phenomena.",
  journal  = "Conn. Sci.",
  volume   =  5,
  number   =  1,
  pages    = "3--36",
  month    =  jan,
  year     =  1993,
  keywords = "comp-cog-sci",
  issn     = "0954-0091, 1360-0494",
  doi      = "10.1080/09540099308915683"
}

@ARTICLE{Fleck2008-zf,
  title     = "Working memory demands in insight versus analytic problem
               solving",
  author    = "Fleck, Jessica I",
  abstract  = "Working memory is one of the cognitive processes thought to
               differentiate insight and analytic forms of problem solving. The
               present research examined memory involvement in the solution of
               insight versus analytic problems. Participants completed verbal
               and spatial working memory and short-term memory measures and a
               series of analytic and insight problems. Results demonstrated a
               relationship between working-memory capacity and the solution of
               analytic problems and between verbal short-term memory capacity
               and the solution of insight problems. This distinction was
               generally though not universally supported when memory was
               examined in relation to individual problems. Memory involvement
               in insight problem solving was further examined to clarify
               whether restructuring in insight is the end result of active
               memory search or spontaneous processes. The present research
               supports the theory that differences exist in the cognitive
               processes underlying insight versus analytic problem solving,
               and provides support for the spontaneous theory of restructuring
               in insight.",
  journal   = "Eur. J. Cogn. Psychol.",
  publisher = "Routledge",
  volume    =  20,
  number    =  1,
  pages     = "139--176",
  month     =  jan,
  year      =  2008,
  issn      = "0954-1446",
  doi       = "10.1080/09541440601016954"
}

@ARTICLE{Schilling2005-bz,
  title     = "A ``{Small-World}'' Network Model of Cognitive Insight",
  author    = "Schilling, Melissa A",
  abstract  = "Despite many decades of study, scientists still puzzle over the
               process of insight. By what mechanism does a person experience
               that ``Aha!'' moment, when sudden clarity emerges from a tangled
               web of thoughts and ideas? This research integrates
               psychological work on insight with graph theoretic work on
               ``small-world'' phenomenon, to construct a theory that explains
               how insight occurs, how it is similar to and different from more
               typical learning processes, and why it yields an affective
               response in the individual. I propose that cognitive insight
               occurs when an atypical association, forged through random
               recombination or directed search, results in a ``shortcut'' in
               an individual's network of representations. This causes a rapid
               decrease in path length, reorients the individual's
               understanding of the relationships within and among the affected
               representations, and can prompt a cascade of other connections.
               This result is demonstrated by applying graph theoretical
               analysis to network translations of commonly used insight
               problems.",
  journal   = "Creat. Res. J.",
  publisher = "Routledge",
  volume    =  17,
  number    = "2-3",
  pages     = "131--154",
  month     =  jul,
  year      =  2005,
  keywords  = "cog-sci;project 1",
  issn      = "1040-0419",
  doi       = "10.1080/10400419.2005.9651475"
}

@ARTICLE{Gilhooly2005-zf,
  title     = "Differentiating insight from non-insight problems",
  author    = "Gilhooly, K J and Murphy, P",
  abstract  = "This study aimed to investigate whether a range of tasks that
               have been generally classed as requiring insight form an
               empirically separable group of tasks distinct from tasks
               generally classed as non-insight. In this study, 24 insight
               tasks, 10 non-insight tasks, and tests of individual differences
               in cognitive abilities and working memory were administered to
               60 participants. Cluster analysis of the problem-solving tasks
               indicated that the presumed insight problems did tend to cluster
               with other presumed insight problems, and similarly the presumed
               non-insight problems tended to cluster with other presumed
               non-insight tasks. Performance on presumed insight problems was
               particularly linked to measures of ideational flexibility with a
               different pattern of results for the non-insight tasks. Spatial
               insight problems were linked to spatial flexibility and verbal
               insight tasks were linked to vocabulary scores. The results are
               discussed in relation to recent developments of dual process
               theories of thinking.",
  journal   = "Think. Reason.",
  publisher = "Routledge",
  volume    =  11,
  number    =  3,
  pages     = "279--302",
  month     =  aug,
  year      =  2005,
  keywords  = "cog-sci;project 1;Paper1",
  issn      = "1354-6783",
  doi       = "10.1080/13546780442000187"
}

@ARTICLE{Kershaw2013-hb,
  title     = "Multiple paths to transfer and constraint relaxation in insight
               problem solving",
  author    = "Kershaw, Trina C and Flynn, Christopher K and Gordon, Leamarie T",
  abstract  = "In two experiments participants received various training
               methods designed to relax constraints present in the Four-Tree
               problem (deBono, 1967), a difficult insight problem. Geometry
               misconceptions were corrected via direct instruction.
               Participants? difficulty with developing three-dimensional
               representations was addressed via spontaneous analogical
               transfer (Experiment 1) or via cued analogical transfer
               (Experiment 2). We found that, while both training methods were
               effective, alleviating multiple constraints was more effective
               than the alleviation of single constraints via training
               programmes (c.f. Kershaw Nokes \& Ohlsson, 2005) and multiple
               constraints are discussed.",
  journal   = "Think. Reason.",
  publisher = "Routledge",
  volume    =  19,
  number    =  1,
  pages     = "96--136",
  month     =  feb,
  year      =  2013,
  keywords  = "cog-sci;project 1;Paper1",
  issn      = "1354-6783",
  doi       = "10.1080/13546783.2012.742852"
}

@ARTICLE{Berry1983-fx,
  title     = "Metacognitive experience and transfer of logical reasoning",
  author    = "Berry, Dianne C",
  abstract  = "The experiments examine the influence of metacognitive
               experience on the transfer of logical processes in a problem
               solving setting. Subjects were presented with two versions of
               Wason's (1966) selection task. Although they were able to
               perform successfully on the concrete tasks (following a minimal
               explanation of the correct solution on an initial trial), the
               majority were not able to transfer a successful method to the
               abstract tasks. Verbalization during, or following, the concrete
               tasks produced substantial transfer effects however. It is
               suggested that verbalization may lead to an increased awareness
               of past behaviour, particularly of those aspects necessary for
               successful solution. Department of Experimental Psychology,
               University of Oxford, South Parks Road, Oxford.",
  journal   = "The Quarterly Journal of Experimental Psychology Section A",
  publisher = "Routledge",
  volume    =  35,
  number    =  1,
  pages     = "39--49",
  month     =  feb,
  year      =  1983,
  keywords  = "l\&m final",
  issn      = "0272-4987",
  doi       = "10.1080/14640748308402115"
}

@INBOOK{Lombrozo2019-my,
  title     = "``Learning by Thinking'' in Science and in Everyday Life",
  author    = "Lombrozo, Tania",
  abstract  = "AbstractThis chapter introduces ``learning by thinking'' (LbT)
               as a form of learning distinct from familiar forms of learning
               through observation. When learning b",
  publisher = "Oxford University Press",
  month     =  dec,
  year      =  2019,
  keywords  = "project 1",
  language  = "en",
  doi       = "10.1093/oso/9780190212308.003.0010"
}

@INBOOK{Bassok2012-sg,
  title     = "Problem Solving",
  author    = "Bassok, Miriam and Novick, Laura R",
  abstract  = "Abstract. This chapter follows the historical development of
               research on problem solving. It begins with a description of two
               research traditions that addressed",
  publisher = "Oxford University Press",
  month     =  mar,
  year      =  2012,
  language  = "en",
  doi       = "10.1093/oxfordhb/9780199734689.013.0021"
}

@ARTICLE{Jones2015-ss,
  title    = "Models of Semantic Memory",
  author   = "Jones, Michael N and Willits, Jon and Dennis, Simon",
  abstract = "Abstract. Meaning is a fundamental component of nearly all
              aspects of human cognition, but formal models of semantic memory
              have classically lagged behind many",
  month    =  apr,
  year     =  2015,
  keywords = "read",
  doi      = "10.1093/oxfordhb/9780199957996.013.11"
}

@ARTICLE{Ghahramani2013-bu,
  title    = "Bayesian non-parametrics and the probabilistic approach to
              modelling",
  author   = "Ghahramani, Zoubin",
  abstract = "Modelling is fundamental to many fields of science and
              engineering. A model can be thought of as a representation of
              possible data one could predict from a system. The probabilistic
              approach to modelling uses probability theory to express all
              aspects of uncertainty in the model. The probabilistic approach
              is synonymous with Bayesian modelling, which simply uses the
              rules of probability theory in order to make predictions, compare
              alternative models, and learn model parameters and structure from
              data. This simple and elegant framework is most powerful when
              coupled with flexible probabilistic models. Flexibility is
              achieved through the use of Bayesian non-parametrics. This
              article provides an overview of probabilistic modelling and an
              accessible survey of some of the main tools in Bayesian
              non-parametrics. The survey covers the use of Bayesian
              non-parametrics for modelling unknown functions, density
              estimation, clustering, time-series modelling, and representing
              sparsity, hierarchies, and covariance structure. More
              specifically, it gives brief non-technical overviews of Gaussian
              processes, Dirichlet processes, infinite hidden Markov models,
              Indian buffet processes, Kingman's coalescent, Dirichlet
              diffusion trees and Wishart processes.",
  journal  = "Philos. Trans. A Math. Phys. Eng. Sci.",
  volume   =  371,
  number   =  1984,
  pages    = "20110553",
  month    =  feb,
  year     =  2013,
  language = "en",
  issn     = "1364-503X",
  pmid     = "23277609",
  doi      = "10.1098/rsta.2011.0553",
  pmc      = "PMC3538441"
}

@ARTICLE{Dickinson1997-ix,
  title     = "Actions and habits: the development of behavioural autonomy",
  author    = "Dickinson, A and Weiskrantz, Lawrence",
  journal   = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  publisher = "Royal Society",
  volume    =  308,
  number    =  1135,
  pages     = "67--78",
  month     =  jan,
  year      =  1997,
  keywords  = "skimmed;learning\&memory2023",
  issn      = "0962-8436",
  doi       = "10.1098/rstb.1985.0010"
}

@UNPUBLISHED{Schulz2017-ij,
  title    = "Strategic exploration in human adaptive control",
  author   = "Schulz, Eric and Klenske, Edgar D and Bramley, Neil R and
              Speekenbrink, Maarten",
  abstract = "Abstract How do people explore in order to gain rewards in
              uncertain dynamical systems? Within a reinforcement learning
              paradigm, control normally involves trading off between
              exploration (i.e. trying out actions in order to gain more
              knowledge about the system) and exploitation (i.e. using current
              knowledge of the system to maximize reward). We study a novel
              control task in which participants must steer a boat on a grid,
              aiming to follow a path of high reward whilst learning how their
              actions affect the boat's position. We find that participants
              explore strategically yet conservatively, exploring more when
              mistakes are less costly and practicing actions that will be
              required later on.",
  journal  = "bioRxiv",
  pages    = "110486",
  month    =  may,
  year     =  2017,
  keywords = "project 1",
  language = "en",
  doi      = "10.1101/110486"
}

@UNPUBLISHED{Findling2020-rl,
  title    = "Computation noise promotes cognitive resilience to adverse
              conditions during decision-making",
  author   = "Findling, Charles and Wyart, Valentin",
  abstract = "Random noise in information processing systems is widely seen as
              detrimental to function. But despite the large trial-to-trial
              variability of neural activity and behavior, humans and other
              animals show a remarkable adaptability to unexpected adverse
              events occurring during task execution. This cognitive ability,
              described as constitutive of general intelligence, is missing
              from current artificial intelligence (AI) systems which feature
              exact (noise-free) computations. Here we show that implementing
              computation noise in recurrent neural networks boosts their
              cognitive resilience to a variety of adverse conditions entirely
              unseen during training, in a way that resembles human and animal
              cognition. In contrast to artificial agents with exact
              computations, noisy agents exhibit hallmarks of Bayesian
              inference acquired in a `zero-shot' fashion -- without prior
              experience with conditions that require these computations for
              maximizing rewards. We further demonstrate that these cognitive
              benefits result from free-standing regularization of activity
              patterns in noisy neural networks. Together, these findings
              suggest that intelligence may ride on computation noise to
              promote near-optimal decision-making in adverse conditions
              without any engineered cognitive sophistication. \#\#\# Competing
              Interest Statement The authors have declared no competing
              interest.",
  journal  = "bioRxiv",
  pages    = "2020.06.10.145300",
  month    =  jun,
  year     =  2020,
  keywords = "BAMB2023",
  language = "en",
  doi      = "10.1101/2020.06.10.145300"
}

@UNPUBLISHED{Eckstein2023-rg,
  title    = "Predictive and Interpretable: Combining Artificial Neural
              Networks and Classic Cognitive Models to Understand Human
              Learning and Decision Making",
  author   = "Eckstein, Maria K and Summerfield, Christopher and Daw, Nathaniel
              D and Miller, Kevin J",
  abstract = "Quantitative models of behavior are a fundamental tool in
              cognitive science. Typically, models are hand-crafted to
              implement specific cognitive mechanisms. Such ``classic'' models
              are interpretable by design, but may provide poor fit to
              experimental data. Artificial neural networks (ANNs), on the
              contrary, can fit arbitrary datasets at the cost of opaque
              mechanisms. Here, we adopt a hybrid approach, combining the
              predictive power of ANNs with the interpretability of classic
              models. We apply this approach to Reinforcement Learning (RL),
              beginning with classic RL models and replacing their components
              one-by-one with ANNs. We find that hybrid models can provide
              similar fit to fully-general ANNs, while retaining the
              interpretability of classic cognitive models: They reveal
              reward-based learning mechanisms in humans that are strikingly
              similar to classic RL. They also reveal mechanisms not contained
              in classic models, including separate rewardblind mechanisms, and
              the specific memory contents relevant to reward-based and
              reward-blind mechanisms. \#\#\# Competing Interest Statement The
              authors have declared no competing interest.",
  journal  = "bioRxiv",
  pages    = "2023.05.17.541226",
  month    =  may,
  year     =  2023,
  keywords = "BAMB2023",
  language = "en",
  doi      = "10.1101/2023.05.17.541226"
}

@TECHREPORT{Rule2018-zm,
  title       = "Learning List Concepts through Program Induction",
  author      = "Rule, Joshua and Schulz, Eric and Piantadosi, Steven T and
                 Tenenbaum, Joshua B",
  abstract    = "Humans master complex systems of interrelated concepts like
                 mathematics and natural language. Previous work suggests
                 learning these systems relies on iteratively and directly
                 revising a language-like conceptual representation. We
                 introduce and assess a novel concept learning paradigm called
                 Martha's Magical Machines that captures complex relationships
                 between concepts. We model human concept learning in this
                 paradigm as a search in the space of term rewriting systems,
                 previously developed as an abstract model of computation. Our
                 model accurately predicts that participants learn some
                 transformations more easily than others and that they learn
                 harder concepts more easily using a bootstrapping curriculum
                 focused on their compositional parts. Our results suggest that
                 term rewriting systems may be a useful model of human
                 conceptual representations.",
  institution = "Animal Behavior and Cognition",
  month       =  may,
  year        =  2018,
  keywords    = "comp-cog-sci",
  doi         = "10.1101/321505"
}

@INPROCEEDINGS{Kar2011-or,
  title     = "Bandit problems in networks: Asymptotically efficient
               distributed allocation rules",
  booktitle = "2011 50th {IEEE} Conference on Decision and Control and European
               Control Conference",
  author    = "Kar, Soummya and Poor, H Vincent and Cui, Shuguang",
  abstract  = "This paper studies the multi-agent bandit problem in a
               distributed networked setting. The setting considered assumes
               only one bandit (the major bandit) has accessible reward
               information from its samples, whereas the rest (the minor
               bandits) have unobservable rewards. Under the assumption that
               the minor bandits are aware of the sampling pattern of the major
               bandit (but with no direct access to its rewards), a lower bound
               on the expected average network regret is obtained. The lower
               bound resembles the logarithmic optimal regret attained in
               single (classical) bandit problems, but in addition is shown to
               scale down with the number of agents. A collaborative and
               adaptive distributed allocation rule DA is proposed and is shown
               to achieve the lower bound on the expected average regret for a
               connected inter-bandit communication network. In particular, it
               is shown that under the DA allocation rule, the minor bandits
               attain sub-logarithmic expected regrets as opposed to
               logarithmic in the single agent setting.",
  pages     = "1771--1778",
  month     =  dec,
  year      =  2011,
  keywords  = "Resource management;Decision making;Symmetric
               matrices;Vectors;Collaboration;Random variables;Density
               measurement;Networked Bandit Problems;Distributed Allocation
               Rules;Asymptotically Efficient;Partially Observable
               Rewards;ccm-project",
  issn      = "0743-1546",
  doi       = "10.1109/CDC.2011.6160719"
}

@INPROCEEDINGS{He2016-if,
  title      = "Deep {{Residual Learning}} for {{Image Recognition}}",
  booktitle  = "2016 {{{IEEE} Conference}} on {{Computer Vision}} and {{Pattern
                Recognition}} ({{{CVPR}}})",
  author     = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  abstract   = "Deeper neural networks are more difficult to train. We present
                a residual learning framework to ease the training of networks
                that are substantially deeper than those used previously. We
                explicitly reformulate the layers as learning residual
                functions with reference to the layer inputs, instead of
                learning unreferenced functions. We provide comprehensive
                empirical evidence showing that these residual networks are
                easier to optimize, and can gain accuracy from considerably
                increased depth. On the ImageNet dataset we evaluate residual
                nets with a depth of up to 152 layers---8$\times$ deeper than
                VGG nets [40] but still having lower complexity. An ensemble of
                these residual nets achieves 3.57\% error on the ImageNet test
                set. This result won the 1st place on the ILSVRC 2015
                classification task. We also present analysis on CIFAR-10 with
                100 and 1000 layers.",
  publisher  = "IEEE",
  pages      = "770--778",
  month      =  jun,
  year       =  2016,
  address    = "Las Vegas, NV, USA",
  keywords   = "machine-learning",
  conference = "2016 IEEE Conference on Computer Vision and Pattern Recognition
                (CVPR)",
  isbn       = "9781467388511",
  doi        = "10.1109/CVPR.2016.90"
}

@ARTICLE{Newell1956-lm,
  title    = "The Logic Theory {Machine--{{A}}} Complex Information Processing
              System",
  author   = "Newell, A and Simon, H",
  abstract = "In this paper we describe a complex information processing
              system, which we call the logic theory machine, that is capable
              of discovering proofs for theorems in symbolic logic. This
              system, in contrast to the systematic algorithms that are
              ordinarily employed in computation, relies heavily on heuristic
              methods similar to those that have been observed in . human
              problem solving activity. The specification is written in a
              formal language, of the nature of a pseudo-code, that is suitable
              for coding for digital computers. However, the present paper is
              concerned exclusively with specification of the system, and not
              with its realization in a computer. The logic theory machine is
              part of a program of research to understand complex information
              processing systems by specifying and synthesizing a substantial
              variety of such systems for empirical study.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "61--79",
  month    =  sep,
  year     =  1956,
  keywords = "Automatic programming,Formal languages,Heuristic
              algorithms,Humans,Information analysis,Information
              processing,Logic,Pattern recognition,Problem-solving;comp-cog-sci",
  issn     = "2168-2712",
  doi      = "10.1109/TIT.1956.1056797"
}

@ARTICLE{Chomsky1956-aj,
  title    = "Three Models for the Description of Language",
  author   = "Chomsky, N",
  abstract = "We investigate several conceptions of linguistic structure to
              determine whether or not they can provide simple and
              ``revealing'' grammars that generate all of the sentences of
              English and only these. We find that no finite-state Markov
              process that produces symbols with transition from state to state
              can serve as an English grammar. Furthermore, the particular
              subclass of such processes that producen-order statistical
              approximations to English do not come closer, with increasingn,
              to matching the output of an English grammar. We formalize-the
              notions of ``phrase structure'' and show that this gives us a
              method for describing language which is essentially more
              powerful, though still representable as a rather elementary type
              of finite-state process. Nevertheless, it is successful only when
              limited to a small subset of simple sentences. We study the
              formal properties of a set of grammatical transformations that
              carry sentences with phrase structure into new sentences with
              derived phrase structure, showing that transformational grammars
              are processes of the same elementary type as phrase-structure
              grammars; that the grammar of English is materially simplified if
              phrase structure description is limited to a kernel of simple
              sentences from which all other sentences are constructed by
              repeated transformations; and that this view of linguistic
              structure gives a certain insight into the use and understanding
              of language.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "113--124",
  month    =  sep,
  year     =  1956,
  keywords = "Impedance matching,Kernel,Laboratories,Markov processes,Natural
              languages,Research and development,Testing;comp-cog-sci",
  issn     = "2168-2712",
  doi      = "10.1109/TIT.1956.1056813"
}

@ARTICLE{Bengio2013-nf,
  title    = "Representation {{Learning}}: {{A Review}} and {{New
              Perspectives}}",
  author   = "Bengio, Y and Courville, A and Vincent, P",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical
              connections between representation learning, density estimation,
              and manifold learning.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  month    =  aug,
  year     =  2013,
  keywords = "Abstracts,autoencoder,Boltzmann machine,Deep learning,Feature
              extraction,feature learning,Learning systems,Machine
              learning,Manifolds,neural nets,Neural networks,representation
              learning,Speech recognition,unsupervised learning;comp-cog-sci",
  issn     = "0162-8828, 2160-9292",
  doi      = "10.1109/TPAMI.2013.50"
}

@ARTICLE{Grant2003-tf,
  title    = "Eye movements and problem solving: Guiding attention guides
              thought",
  author   = "Grant, Elizabeth R and Spivey, Michael J",
  abstract = "Overt visual attention during diagram-based problem solving, as
              measured by eye movements, has been used in numerous studies to
              reveal critical aspects of the problem-solving process that
              traditional measures like solution time and accuracy cannot
              address. In Experiment 1, we used this methodology to show that
              particular fixation patterns correlate with success in solving
              the tumor-and-lasers radiation problem. Given this correlation
              between attention to a particular diagram feature and
              problem-solving insight, we investigated participants' cognitive
              sensitivity to perceptual changes in that diagram feature. In
              Experiment 2, we found that perceptually highlighting the
              critical diagram component, identified in Experiment 1,
              significantly increased the frequency of correct solutions.
              Taking a situated perspective on cognition, we suggest that
              environmentally controlled perceptual properties can guide
              attention and eye movements in ways that assist in developing
              problem-solving insights that dramatically improve reasoning.
              (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
  journal  = "Psychol. Sci.",
  volume   =  14,
  number   =  5,
  pages    = "462--466",
  month    =  sep,
  year     =  2003,
  issn     = "0956-7976, 1467-9280",
  doi      = "10.1111/1467-9280.02454"
}

@ARTICLE{Chater2013-hv,
  title    = "Programs as causal models: speculations on mental programs and
              mental representation",
  author   = "Chater, Nick and Oaksford, Mike",
  abstract = "Judea Pearl has argued that counterfactuals and causality are
              central to intelligence, whether natural or artificial, and has
              helped create a rich mathematical and computational framework for
              formally analyzing causality. Here, we draw out connections
              between these notions and various current issues in cognitive
              science, including the nature of mental ``programs'' and mental
              representation. We argue that programs (consisting of algorithms
              and data structures) have a causal (counterfactual-supporting)
              structure; these counterfactuals can reveal the nature of mental
              representations. Programs can also provide a causal model of the
              external world. Such models are, we suggest, ubiquitous in
              perception, cognition, and language processing.",
  journal  = "Cogn. Sci.",
  volume   =  37,
  number   =  6,
  pages    = "1171--1191",
  month    =  aug,
  year     =  2013,
  keywords = "comp-cog-sci;project 1;project 2",
  language = "en",
  issn     = "0364-0213, 1551-6709",
  pmid     = "23855554",
  doi      = "10.1111/cogs.12062"
}

@ARTICLE{Lake2018-gk,
  title    = "The {{Emergence}} of {{Organizing Structure}} in {{Conceptual
              Representation}}",
  author   = "Lake, Brenden M and Lawrence, Neil D and Tenenbaum, Joshua B",
  abstract = "Both scientists and children make important structural
              discoveries, yet their computational underpinnings are not well
              understood. Structure discovery has previously been formalized as
              probabilistic inference about the right structural form---where
              form could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum,
              2008). Although this approach can learn intuitive organizations,
              including a tree for animals and a ring for the color circle, it
              assumes a strong inductive bias that considers only these
              particular forms, and each form is explicitly provided as initial
              knowledge. Here we introduce a new computational model of how
              organizing structure can be discovered, utilizing a broad
              hypothesis space with a preference for sparse connectivity. Given
              that the inductive bias is more general, the model's initial
              knowledge shows little qualitative resemblance to some of the
              discoveries it supports. As a consequence, the model can also
              learn complex structures for domains that lack intuitive
              description, as well as predict human property induction
              judgments without explicit structural forms. By allowing form to
              emerge from sparsity, our approach clarifies how both the
              richness and flexibility of human conceptual organization can
              coexist.",
  journal  = "Cogn. Sci.",
  volume   =  42,
  number   = "S3",
  pages    = "809--832",
  year     =  2018,
  keywords = "Bayesian modeling,Sparsity,Structure discovery,Unsupervised
              learning;comp-cog-sci",
  issn     = "0364-0213, 1551-6709",
  doi      = "10.1111/cogs.12580"
}

@ARTICLE{Van_Schijndel2021-qq,
  title    = "{Single-Stage} Prediction Models Do Not Explain the Magnitude of
              Syntactic Disambiguation Difficulty",
  author   = "van Schijndel, Marten and Linzen, Tal",
  abstract = "The disambiguation of a syntactically ambiguous sentence in favor
              of a less preferred parse can lead to slower reading at the
              disambiguation point. This phenomenon, referred to as a
              garden-path effect, has motivated models in which readers
              initially maintain only a subset of the possible parses of the
              sentence, and subsequently require time-consuming reanalysis to
              reconstruct a discarded parse. A more recent proposal argues that
              the garden-path effect can be reduced to surprisal arising in a
              fully parallel parser: words consistent with the initially
              dispreferred but ultimately correct parse are simply less
              predictable than those consistent with the incorrect parse. Since
              predictability has pervasive effects in reading far beyond
              garden-path sentences, this account, which dispenses with
              reanalysis mechanisms, is more parsimonious. Crucially, it
              predicts a linear effect of surprisal: the garden-path effect is
              expected to be proportional to the difference in word surprisal
              between the ultimately correct and ultimately incorrect
              interpretations. To test this prediction, we used recurrent
              neural network language models to estimate word-by-word surprisal
              for three temporarily ambiguous constructions. We then estimated
              the slowdown attributed to each bit of surprisal from human
              self-paced reading times, and used that quantity to predict
              syntactic disambiguation difficulty. Surprisal successfully
              predicted the existence of garden-path effects, but drastically
              underpredicted their magnitude, and failed to predict their
              relative severity across constructions. We conclude that a full
              explanation of syntactic disambiguation difficulty may require
              recovery mechanisms beyond predictability.",
  journal  = "Cogn. Sci.",
  volume   =  45,
  number   =  6,
  pages    = "e12988",
  month    =  jun,
  year     =  2021,
  keywords = "Garden paths; Information theory; Neural networks; Self-paced
              reading; Surprisal;compling-cogsci2023",
  language = "en",
  issn     = "0364-0213, 1551-6709",
  pmid     = "34170031",
  doi      = "10.1111/cogs.12988"
}

@ARTICLE{Pirrone2023-qy,
  title    = "Toward an Atlas of Canonical Cognitive Mechanisms",
  author   = "Pirrone, Angelo and Tsetsos, Konstantinos",
  abstract = "A central goal in Cognitive Science is understanding the
              mechanisms that underlie cognition. Here, we contend that
              Cognitive Science, despite intense multidisciplinary efforts, has
              furnished surprisingly few mechanistic insights. We attribute
              this slow mechanistic progress to the fact that cognitive
              scientists insist on performing underdetermined exercises,
              deriving overparametrized mechanistic theories of complex
              behaviors and seeking validation of these theories to the elusive
              notions of optimality and biological plausibility. We propose
              that mechanistic progress in Cognitive Science will accelerate
              once cognitive scientists start focusing on simpler explananda
              that will enable them to chart an atlas of elementary cognitive
              operations. Looking forward, the next challenge for Cognitive
              Science will be to understand how these elementary cognitive
              processes are pieced together to explain complex behavior.",
  journal  = "Cogn. Sci.",
  volume   =  47,
  number   =  2,
  pages    = "e13243",
  month    =  feb,
  year     =  2023,
  keywords = "Biological plausibility; Cognitive science; Inference;
              Mechanisms; Optimality;read",
  language = "en",
  issn     = "0364-0213, 1551-6709",
  pmid     = "36744746",
  doi      = "10.1111/cogs.13243"
}

@ARTICLE{Dumont2022-am,
  title    = "Transactional longitudinal relations between accuracy and
              reaction time on a measure of cognitive flexibility at 5, 6, and
              7 years of age",
  author   = "Dumont, {\'E}milie and Castellanos-Ryan, Natalie and Parent,
              Sophie and Jacques, Sophie and S{\'e}guin, Jean R and Zelazo,
              Philip David",
  abstract = "Whereas accuracy is used as an indicator of cognitive flexibility
              in preschool-age children, reaction time (RT), or a combination
              of accuracy and RT, provide better indices of performance as
              children transition to school. Theoretical models and
              cross-sectional studies suggest that a speed-accuracy tradeoff
              may be operating across this transition, but the lack of
              longitudinal studies makes this transition difficult to
              understand. The current study explored the longitudinal and
              bidirectional associations between accuracy and RT on the DCCS
              (mixed block) at 5, 6, and 7 years of age using cross-lagged
              panel analyses. The study also examined the roles of working
              memory and language, as potential longitudinal mediators between
              RT at Time X and accuracy at Time X + 1, and explored the role of
              inhibitory control. The sample consisted of 425 children from the
              Quebec Longitudinal Study of Child Development. Results show
              lagged associations from slower RT to greater improvements in
              accuracy between 5 and 6 years and between 6 and 7 years.
              Further, higher accuracy at 6 years predicted faster RT at 7
              years. Only working memory acted as a partial mediator between RT
              at 5 years and accuracy at 6 years. These results provide needed
              longitudinal evidence to support theoretical claims that slower
              RT precedes improved accuracy in the development of cognitive
              flexibility, that working memory may be involved in the early
              stage of this process, and that accuracy and reaction time become
              more efficient in later stages of this process.",
  journal  = "Dev. Sci.",
  volume   =  25,
  number   =  5,
  pages    = "e13254",
  month    =  sep,
  year     =  2022,
  keywords = "DCCS; cognitive flexibility; cross-lagged panel; longitudinal;
              school transition; working memory;development",
  language = "en",
  issn     = "1363-755X, 1467-7687",
  pmid     = "35195319",
  doi      = "10.1111/desc.13254"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chevalier2022-ot,
  title     = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part {II}: Editorial note",
  author    = "Chevalier, Nicolas and Lipina, Sebasti{\'a}n and Scerif, Gaia
               and Segretin, M Soledad",
  abstract  = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part II: Editorial note ---
               University of Edinburgh Research Explorer Skip to main
               navigation Skip to â€¦",
  journal   = "Dev. Sci.",
  publisher = "research.ed.ac.uk",
  volume    =  25,
  number    =  6,
  pages     = "e13326",
  month     =  nov,
  year      =  2022,
  keywords  = "development",
  language  = "en",
  issn      = "1363-755X, 1467-7687",
  pmid      = "36112772",
  doi       = "10.1111/desc.13326"
}

@ARTICLE{Spelke2007-uu,
  title    = "Core Knowledge",
  author   = "Spelke, Elizabeth S and Kinzler, Katherine D",
  abstract = "Human cognition is founded, in part, on four systems for
              representing objects, actions, number, and space. It may be
              based, as well, on a fifth system for representing social
              partners. Each system has deep roots in human phylogeny and
              ontogeny, and it guides and shapes the mental lives of adults.
              Converging research on human infants, non-human primates,
              children and adults in diverse cultures can aid both
              understanding of these systems and attempts to overcome their
              limits.",
  journal  = "Dev. Sci.",
  volume   =  10,
  number   =  1,
  pages    = "89--96",
  year     =  2007,
  keywords = "development",
  issn     = "1363-755X, 1467-7687",
  doi      = "10.1111/j.1467-7687.2007.00569.x"
}

@ARTICLE{Roediger2006-bj,
  title     = "{Test-Enhanced} Learning: Taking Memory Tests Improves
               {Long-Term} Retention",
  author    = "Roediger, Henry L and Karpicke, Jeffrey D",
  abstract  = "Taking a memory test not only assesses what one knows, but also
               enhances later retention, a phenomenon known as the testing
               effect. We studied this effect with educationally relevant
               materials and investigated whether testing facilitates learning
               only because tests offer an opportunity to restudy material. In
               two experiments, students studied prose passages and took one or
               three immediate free-recall tests, without feedback, or
               restudied the material the same number of times as the students
               who received tests. Students then took a final retention test 5
               min, 2 days, or 1 week later. When the final test was given
               after 5 min, repeated studying improved recall relative to
               repeated testing. However, on the delayed tests, prior testing
               produced substantially greater retention than studying, even
               though repeated studying increased students' confidence in their
               ability to remember the material. Testing is a powerful means of
               improving learning, not just assessing it.",
  journal   = "Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  17,
  number    =  3,
  pages     = "249--255",
  month     =  mar,
  year      =  2006,
  keywords  = "l\&m final",
  issn      = "0956-7976",
  doi       = "10.1111/j.1467-9280.2006.01693.x"
}

@ARTICLE{Cepeda2008-xo,
  title     = "Spacing Effects in Learning: A Temporal Ridgeline of Optimal
               Retention",
  author    = "Cepeda, Nicholas J and Vul, Edward and Rohrer, Doug and Wixted,
               John T and Pashler, Harold",
  abstract  = "To achieve enduring retention, people must usually study
               information on multiple occasions. How does the timing of study
               events affect retention? Prior research has examined this issue
               only in a spotty fashion, usually with very short time
               intervals. In a study aimed at characterizing spacing effects
               over significant durations, more than 1,350 individuals were
               taught a set of facts and?after a gap of up to 3.5 months?given
               a review. A final test was administered at a further delay of up
               to 1 year. At any given test delay, an increase in the
               interstudy gap at first increased, and then gradually reduced,
               final test performance. The optimal gap increased as test delay
               increased. However, when measured as a proportion of test delay,
               the optimal gap declined from about 20 to 40\% of a 1-week test
               delay to about 5 to 10\% of a 1-year test delay. The interaction
               of gap and test delay implies that many educational practices
               are highly inefficient.",
  journal   = "Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  19,
  number    =  11,
  pages     = "1095--1102",
  month     =  nov,
  year      =  2008,
  keywords  = "l\&m final",
  issn      = "0956-7976",
  doi       = "10.1111/j.1467-9280.2008.02209.x"
}

@ARTICLE{Ohlsson1984-ni,
  title     = "Restructuring revisited",
  author    = "Ohlsson, S",
  abstract  = "The central concept of the information processing theory of
               problem solving is search. In contrast, the central concept of
               the Gestalt theory of problem solving is restructuring. Both
               concepts express important aspects of human thinking. A theory
               is presented which interprets restructuring and the related
               concept of insight in information processing terms. It is
               hypothesised that restructuring is a change in mental
               representation which affects the applicability of problem
               solving operators. Insight is hypothesized to occur when
               restructuring of the search space brings the goal state within
               the horizon of mental look-ahead.",
  journal   = "Scand. J. Psychol.",
  publisher = "Wiley",
  volume    =  25,
  number    =  2,
  pages     = "117--129",
  month     =  jun,
  year      =  1984,
  keywords  = "project 1",
  language  = "en",
  issn      = "0036-5564, 1467-9450",
  doi       = "10.1111/j.1467-9450.1984.tb01005.x"
}

@ARTICLE{Lupyan2016-mq,
  title     = "The centrality of language in human cognition",
  author    = "Lupyan, Gary",
  abstract  = "The emergence of language---a productive and combinatorial
               system of communication---has been hailed as one of the major
               transitions in evolution. By enabling symbolic culture, language
               allows humans to draw on and expand on the knowledge of their
               ancestors and peers. A common assumption among linguists and
               psychologists is that although language is critical to our
               ability to share our thoughts, it plays a minor, if any, role in
               generating, controlling, and structuring them. I examine some
               assumptions that led to this view of language and discuss an
               alternative according to which normal human cognition is
               language-augmented cognition. I focus on one of the fundamental
               design features of language---the use of words as symbolic
               cues---and argue that language acts as a high-level control
               system for the mind, allowing individuals to sculpt mental
               representations of others as well as their own.",
  journal   = "Lang. Learn.",
  publisher = "Wiley",
  volume    =  66,
  number    =  3,
  pages     = "516--553",
  month     =  sep,
  year      =  2016,
  keywords  = "comp-cog-sci;language",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en",
  issn      = "0023-8333, 1467-9922",
  doi       = "10.1111/lang.12155"
}

@ARTICLE{Fabricius2021-zg,
  title    = "Perceptual Access Reasoning ({PAR}) in Developing a
              Representational Theory of Mind",
  author   = "Fabricius, William V and Gonzales, Christopher R and Pesch,
              Annelise and Weimer, Amy A and Pugliese, John and Carroll,
              Kathleen and Bolnick, Rebecca R and Kupfer, Anne S and Eisenberg,
              Nancy and Spinrad, Tracy L",
  abstract = "An important part of children's social and cognitive development
              is their understanding that people are psychological beings with
              internal, mental states including desire, intention, perception,
              and belief. A full understanding of people as psychological
              beings requires a representational theory of mind (ToM), which is
              an understanding that mental states can faithfully represent
              reality, or misrepresent reality. For the last 35 years,
              researchers have relied on false-belief tasks as the gold
              standard to test children's understanding that beliefs can
              misrepresent reality. In false-belief tasks, children are asked
              to reason about the behavior of agents who have false beliefs
              about situations. Although a large body of evidence indicates
              that most children pass false-belief tasks by the end of the
              preschool years, the evidence we present in this monograph
              suggests that most children do not understand false beliefs or,
              surprisingly, even true beliefs until middle childhood. We argue
              that young children pass false-belief tasks without understanding
              false beliefs by using perceptual access reasoning (PAR). With
              PAR, children understand that seeing leads to knowing in the
              moment, but not that knowing also arises from thinking or
              persists as memory and belief after the situation changes. By the
              same token, PAR leads children to fail true-belief tasks. PAR
              theory can account for performance on other traditional tests of
              representational ToM and related tasks, and can account for the
              factors that have been found to correlate with or affect both
              true- and false-belief performance. The theory provides a new
              laboratory measure which we label the belief understanding scale
              (BUS). This scale can distinguish between a child who is
              operating with PAR versus a child who is understanding beliefs.
              This scale provides a method needed to allow the study of the
              development of representational ToM. In this monograph, we report
              the outcome of the tests that we have conducted of predictions
              generated by PAR theory. The findings demonstrated signature PAR
              limitations in reasoning about the mind during the ages when
              children are hypothesized to be using PAR. In Chapter II,
              secondary analyses of the published true-belief literature
              revealed that children failed several types of true-belief tasks.
              Chapters III through IX describe new empirical data collected
              across multiple studies between 2003 and 2014 from 580 children
              aged 4-7 years, as well as from a small sample of 14 adults.
              Participants were recruited from the Phoenix, Arizona
              metropolitan area. All participants were native English-speakers.
              Children were recruited from university-sponsored and community
              preschools and daycare centers, and from hospital maternity
              wards. Adults were university students who participated to
              partially fulfill course requirements for research participation.
              Sociometric data were collected only in Chapter IX, and are fully
              reported there. In Chapter III, minor alterations in task
              procedures produced wide variations in children's performance in
              3-option false-belief tasks. In Chapter IV, we report findings
              which show that the developmental lag between children's
              understanding ignorance and understanding false belief is longer
              than the lag reported in previous studies. In Chapter V, children
              did not distinguish between agents who have false beliefs versus
              agents who have no beliefs. In Chapter VI, findings showed that
              children found it no easier to reason about true beliefs than to
              reason about false beliefs. In Chapter VII, when children were
              asked to justify their correct answers in false-belief tasks,
              they did not reference agents' false beliefs. Similarly, in
              Chapter VIII, when children were asked to explain agents' actions
              in false-belief tasks, they did not reference agents' false
              beliefs. In Chapter IX, children who were identified as using PAR
              differed from children who understood beliefs along three
              dimensions-in levels of social development, inhibitory control,
              and kindergarten adjustment. Although the findings need
              replication and additional studies of alternative
              interpretations, the collection of results reported in this
              monograph challenges the prevailing view that representational
              ToM is in place by the end of the preschool years. Furthermore,
              the pattern of findings is consistent with the proposal that PAR
              is the developmental precursor of representational ToM. The
              current findings also raise questions about claims that infants
              and toddlers demonstrate ToM-related abilities, and that
              representational ToM is innate.",
  journal  = "Monogr. Soc. Res. Child Dev.",
  volume   =  86,
  number   =  3,
  pages    = "7--154",
  month    =  sep,
  year     =  2021,
  language = "en",
  issn     = "0037-976X, 1540-5834",
  pmid     = "34580875",
  doi      = "10.1111/mono.12432",
  pmc      = "PMC9292623"
}

@ARTICLE{Hubel1970-co,
  title    = "The period of susceptibility to the physiological effects of
              unilateral eye closure in kittens",
  author   = "Hubel, D H and Wiesel, T N",
  abstract = "1. Kittens were visually deprived by suturing the lids of the
              right eye for various periods of time at different ages.
              Recordings were subsequently made from the striate cortex, and
              responses from the two eyes compared. As previously reported,
              monocular eye closure during the first few months of life causes
              a sharp decline in the number of cells that can be influenced by
              the previously closed eye.2. Susceptibility to the effects of eye
              closure begins suddenly near the start of the fourth week,
              remains high until some time between the sixth and eighth weeks,
              and then declines, disappearing finally around the end of the
              third month. Monocular closure for over a year in an adult cat
              produces no detectable effects.3. During the period of high
              susceptibility in the fourth and fifth weeks eye closure for as
              little as 3-4 days leads to a sharp decline in the number of
              cells that can be driven from both eyes, as well as an over-all
              decline in the relative influence of the previously closed eye. A
              6-day closure is enough to give a reduction in the number of
              cells that can be driven by the closed eye to a fraction of the
              normal. The physiological picture is similar to that following a
              3-month monocular deprivation from birth, in which the proportion
              of cells the eye can influence drops from 85 to about 7\%.4.
              Cells of the lateral geniculate receiving input from a deprived
              eye are noticeably smaller and paler to Nissl stain following 3
              or 6 days' deprivation during the fourth week.5. Following 3
              months of monocular deprivation, opening the eye for up to 5 yr
              produces only a very limited recovery in the cortical physiology,
              and no obvious recovery of the geniculate atrophy, even though
              behaviourally there is some return of vision in the deprived eye.
              Closing the normal eye, though necessary for behavioural
              recovery, has no detectable effect on the cortical physiology.
              The amount of possible recovery in the striate cortex is probably
              no greater if the period of eye closure is limited to weeks, but
              after a 5-week closure there is a definite enhancement of the
              recovery, even though it is far from complete.",
  journal  = "J. Physiol.",
  volume   =  206,
  number   =  2,
  pages    = "419--436",
  month    =  feb,
  year     =  1970,
  keywords = "skimmed;dev-cog-neuro;development",
  language = "en",
  issn     = "0022-3751",
  pmid     = "5498493",
  doi      = "10.1113/jphysiol.1970.sp009022",
  pmc      = "PMC1348655"
}

@ARTICLE{Frankland2004-xd,
  title    = "The involvement of the anterior cingulate cortex in remote
              contextual fear memory",
  author   = "Frankland, Paul W and Bontempi, Bruno and Talton, Lynn E and
              Kaczmarek, Leszek and Silva, Alcino J",
  abstract = "Although the molecular, cellular, and systems mechanisms required
              for initial memory processing have been intensively investigated,
              those underlying permanent memory storage remain elusive. We
              present neuroanatomical, pharmacological, and genetic results
              demonstrating that the anterior cingulate cortex plays a critical
              role in remote memory for contextual fear conditioning. Imaging
              of activity-dependent genes shows that the anterior cingulate is
              activated by remote memory and that this activation is impaired
              by a null alpha-CaMKII mutation that blocks remote memory.
              Accordingly, reversible inactivation of this structure in normal
              mice disrupts remote memory without affecting recent memory.",
  journal  = "Science",
  volume   =  304,
  number   =  5672,
  pages    = "881--883",
  month    =  may,
  year     =  2004,
  keywords = "l\&m final",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "15131309",
  doi      = "10.1126/science.1094804"
}

@ARTICLE{Fontana2010-ie,
  title    = "Extending {{Healthy Life {Span--From} Yeast}} to {{Humans}}",
  author   = "Fontana, L and Partridge, L and Longo, V D",
  journal  = "Science",
  volume   =  328,
  number   =  5976,
  pages    = "321--326",
  month    =  apr,
  year     =  2010,
  keywords = "longevity",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.1172539"
}

@ARTICLE{Meltzoff2009-eq,
  title    = "Foundations for a new science of learning",
  author   = "Meltzoff, Andrew N and Kuhl, Patricia K and Movellan, Javier and
              Sejnowski, Terrence J",
  abstract = "Human learning is distinguished by the range and complexity of
              skills that can be learned and the degree of abstraction that can
              be achieved compared with those of other species. Homo sapiens is
              also the only species that has developed formal ways to enhance
              learning: teachers, schools, and curricula. Human infants have an
              intense interest in people and their behavior and possess
              powerful implicit learning mechanisms that are affected by social
              interaction. Neuroscientists are beginning to understand the
              brain mechanisms underlying learning and how shared brain systems
              for perception and action support social learning. Machine
              learning algorithms are being developed that allow robots and
              computers to learn autonomously. New insights from many different
              fields are converging to create a new science of learning that
              may transform educational practices.",
  journal  = "Science",
  volume   =  325,
  number   =  5938,
  pages    = "284--288",
  month    =  jul,
  year     =  2009,
  keywords = "read;learning\&memory2023;cog-sci",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "19608908",
  doi      = "10.1126/science.1175626",
  pmc      = "PMC2776823"
}

@ARTICLE{Tenenbaum2011-ud,
  title    = "How to grow a mind: statistics, structure, and abstraction",
  author   = "Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and
              Goodman, Noah D",
  abstract = "In coming to understand the world-in learning concepts, acquiring
              language, and grasping causal relations-our minds make inferences
              that appear to go far beyond the data available. How do we do it?
              This review describes recent approaches to reverse-engineering
              human learning and cognitive development and, in parallel,
              engineering more humanlike machine learning systems.
              Computational models that perform probabilistic inference over
              hierarchies of flexibly structured representations can address
              some of the deepest questions about the nature and origins of
              human thought: How does abstract knowledge guide learning and
              reasoning from sparse data? What forms does our knowledge take,
              across different domains and tasks? And how is that abstract
              knowledge itself acquired?",
  journal  = "Science",
  volume   =  331,
  number   =  6022,
  pages    = "1279--1285",
  month    =  mar,
  year     =  2011,
  keywords = "read;comp-cog-sci;project 2",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "21393536",
  doi      = "10.1126/science.1192788"
}

@ARTICLE{Karpicke2011-hc,
  title    = "Retrieval practice produces more learning than elaborate studying
              with concept mapping",
  author   = "Karpicke, Jeffrey D and Blunt, Janell R",
  abstract = "Educators rely heavily on learning activities that encourage
              elaborative studying, whereas activities that require students to
              practice retrieving and reconstructing knowledge are used less
              frequently. Here, we show that practicing retrieval produces
              greater gains in meaningful learning than elaborative studying
              with concept mapping. The advantage of retrieval practice
              generalized across texts identical to those commonly found in
              science education. The advantage of retrieval practice was
              observed with test questions that assessed comprehension and
              required students to make inferences. The advantage of retrieval
              practice occurred even when the criterial test involved creating
              concept maps. Our findings support the theory that retrieval
              practice enhances learning by retrieval-specific mechanisms
              rather than by elaborative study processes. Retrieval practice is
              an effective tool to promote conceptual learning about science.
              (PsycINFO Database Record (c) 2018 APA, all rights reserved)",
  journal  = "Science",
  volume   =  331,
  number   =  6018,
  pages    = "772--775",
  month    =  feb,
  year     =  2011,
  keywords = "l\&m final",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.1199327"
}

@ARTICLE{Gopnik2012-iu,
  title    = "Scientific thinking in young children: theoretical advances,
              empirical research, and policy implications",
  author   = "Gopnik, Alison",
  abstract = "New theoretical ideas and empirical research show that very young
              children's learning and thinking are strikingly similar to much
              learning and thinking in science. Preschoolers test hypotheses
              against data and make causal inferences; they learn from
              statistics and informal experimentation, and from watching and
              listening to others. The mathematical framework of probabilistic
              models and Bayesian inference can describe this learning in
              precise ways. These discoveries have implications for early
              childhood education and policy. In particular, they suggest both
              that early childhood experience is extremely important and that
              the trend toward more structured and academic early childhood
              programs is misguided.",
  journal  = "Science",
  volume   =  337,
  number   =  6102,
  pages    = "1623--1627",
  month    =  sep,
  year     =  2012,
  keywords = "project 1;development",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "23019643",
  doi      = "10.1126/science.1223416"
}

@ARTICLE{Schultz1997-ti,
  title    = "A neural substrate of prediction and reward",
  author   = "Schultz, W and Dayan, P and Montague, P R",
  abstract = "The capacity to predict future events permits a creature to
              detect, model, and manipulate the causal structure of its
              interactions with its environment. Behavioral experiments suggest
              that learning is driven by changes in the expectations about
              future salient events such as rewards and punishments.
              Physiological work has recently complemented these studies by
              identifying dopaminergic neurons in the primate whose fluctuating
              output apparently signals changes or errors in the predictions of
              future salient and rewarding events. Taken together, these
              findings can be understood through quantitative theories of
              adaptive optimizing control.",
  journal  = "Science",
  volume   =  275,
  number   =  5306,
  pages    = "1593--1599",
  month    =  mar,
  year     =  1997,
  keywords = "read;learning\&memory2023",
  language = "en",
  issn     = "0036-8075",
  pmid     = "9054347",
  doi      = "10.1126/science.275.5306.1593"
}

@ARTICLE{Knudsen1998-ry,
  title    = "Capacity for plasticity in the adult owl auditory system expanded
              by juvenile experience",
  author   = "Knudsen, E I",
  abstract = "In the process of creating a multimodal map of space,
              auditory-visual neurons in the optic tectum establish
              associations between particular values of auditory spatial cues
              and locations in the visual field. In the barn owl, tectal
              neurons reveal these associations in the match between their
              tuning for interaural time differences (ITDs) and the locations
              of their visual receptive fields (VRFs). In young owls ITD-VRF
              associations can be adjusted by experience over a wide range, but
              the range of adjustment normally becomes quite restricted in
              adults. This normal range of adjustment in adults was greatly
              expanded in owls that had previously learned abnormal ITD-VRF
              associations as juveniles. Thus, the act of learning abnormal
              associations early in life leaves an enduring trace in this
              pathway that enables unusual functional connections to be
              reestablished, as needed, in adulthood, even when the
              associations represented by these connections have not been used
              for an extended period of time.",
  journal  = "Science",
  volume   =  279,
  number   =  5356,
  pages    = "1531--1533",
  month    =  mar,
  year     =  1998,
  keywords = "skimmed;dev-cog-neuro;development",
  language = "en",
  issn     = "0036-8075",
  pmid     = "9488651",
  doi      = "10.1126/science.279.5356.1531"
}

@ARTICLE{Shepard1987-dl,
  title    = "Toward a universal law of generalization for psychological
              science",
  author   = "Shepard, R N",
  abstract = "A psychological space is established for any set of stimuli by
              determining metric distances between the stimuli such that the
              probability that a response learned to any stimulus will
              generalize to any other is an invariant monotonic function of the
              distance between them. To a good approximation, this probability
              of generalization (i) decays exponentially with this distance,
              and (ii) does so in accordance with one of two metrics, depending
              on the relation between the dimensions along which the stimuli
              vary. These empirical regularities are mathematically derivable
              from universal principles of natural kinds and probabilistic
              geometry that may, through evolutionary internalization, tend to
              govern the behaviors of all sentient organisms.",
  journal  = "Science",
  volume   =  237,
  number   =  4820,
  pages    = "1317--1323",
  month    =  sep,
  year     =  1987,
  keywords = "skimmed;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "0036-8075",
  pmid     = "3629243",
  doi      = "10.1126/science.3629243"
}

@ARTICLE{Lake2015-ap,
  title    = "{Human-Level} Concept Learning through Probabilistic Program
              Induction",
  author   = "Lake, B M and Salakhutdinov, R and Tenenbaum, J B",
  journal  = "Science",
  volume   =  350,
  number   =  6266,
  pages    = "1332--1338",
  month    =  dec,
  year     =  2015,
  keywords = "comp-cog-sci",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.aab3050"
}

@ARTICLE{Gershman2015-pa,
  title    = "Computational Rationality: {{A}} Converging Paradigm for
              Intelligence in Brains, Minds, and Machines",
  author   = "Gershman, Samuel J and Horvitz, Eric J and Tenenbaum, Joshua B",
  abstract = "After growing up together, and mostly growing apart in the second
              half of the 20th century, the fields of artificial intelligence
              (AI), cognitive science, and neuroscience are reconverging on a
              shared view of the computational foundations of intelligence that
              promotes valuable cross-disciplinary exchanges on questions,
              methods, and results. We chart advances over the past several
              decades that address challenges of perception and action under
              uncertainty through the lens of computation. Advances include the
              development of representations and inferential procedures for
              large-scale probabilistic inference and machinery for enabling
              reflection and decisions about tradeoffs in effort, precision,
              and timeliness of computations. These tools are deployed toward
              the goal of computational rationality: identifying decisions with
              highest expected utility, while taking into consideration the
              costs of computation in complex real-world problems in which most
              relevant calculations can only be approximated. We highlight key
              concepts with examples that show the potential for interchange
              between computer science, cognitive science, and neuroscience.",
  journal  = "Science",
  volume   =  349,
  number   =  6245,
  pages    = "273--278",
  month    =  jul,
  year     =  2015,
  keywords = "comp-cog-sci",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.aac6076"
}

@ARTICLE{Josselyn2020-xn,
  title    = "Memory engrams: Recalling the past and imagining the future",
  author   = "Josselyn, Sheena A and Tonegawa, Susumu",
  abstract = "In 1904, Richard Semon introduced the term ``engram'' to describe
              the neural substrate for storing memories. An experience, Semon
              proposed, activates a subset of cells that undergo off-line,
              persistent chemical and/or physical changes to become an engram.
              Subsequent reactivation of this engram induces memory retrieval.
              Although Semon's contributions were largely ignored in his
              lifetime, new technologies that allow researchers to image and
              manipulate the brain at the level of individual neurons has
              reinvigorated engram research. We review recent progress in
              studying engrams, including an evaluation of evidence for the
              existence of engrams, the importance of intrinsic excitability
              and synaptic plasticity in engrams, and the lifetime of an
              engram. Together, these findings are beginning to define an
              engram as the basic unit of memory.",
  journal  = "Science",
  volume   =  367,
  number   =  6473,
  month    =  jan,
  year     =  2020,
  keywords = "read;cog-neuro;learning\&memory2023",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "31896692",
  doi      = "10.1126/science.aaw4325",
  pmc      = "PMC7577560"
}

@ARTICLE{Peterson2021-nw,
  title    = "Using large-scale experiments and machine learning to discover
              theories of human decision-making",
  author   = "Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and
              Reichman, Daniel and Griffiths, Thomas L",
  abstract = "Predicting and understanding how people make decisions has been a
              long-standing goal in many fields, with quantitative models of
              human decision-making informing research in both the social
              sciences and engineering. We show how progress toward this goal
              can be accelerated by using large datasets to power
              machine-learning algorithms that are constrained to produce
              interpretable psychological theories. Conducting the largest
              experiment on risky choice to date and analyzing the results
              using gradient-based optimization of differentiable decision
              theories implemented through artificial neural networks, we were
              able to recapitulate historical discoveries, establish that there
              is room to improve on existing theories, and discover a new, more
              accurate model of human decision-making in a form that preserves
              the insights from centuries of research.",
  journal  = "Science",
  volume   =  372,
  number   =  6547,
  pages    = "1209--1214",
  month    =  jun,
  year     =  2021,
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "34112693",
  doi      = "10.1126/science.abe2629"
}

@ARTICLE{Scoville1957-dh,
  title    = "Loss of recent memory after bilateral hippocampal lesions",
  author   = "Scoville, W B and Milner, B",
  journal  = "J. Neurol. Neurosurg. Psychiatry",
  volume   =  20,
  number   =  1,
  pages    = "11--21",
  month    =  feb,
  year     =  1957,
  keywords = "MEMORY; TEMPORAL LOBE/surgery;read;learning\&memory2023;cog-neuro",
  language = "en",
  issn     = "0022-3050",
  pmid     = "13406589",
  doi      = "10.1136/jnnp.20.1.11",
  pmc      = "PMC497229"
}

@ARTICLE{Das2023-ho,
  title     = "Combining Functional and Automata Synthesis to Discover Causal
               Reactive Programs",
  author    = "Das, Ria and Tenenbaum, Joshua B and Solar-Lezama, Armando and
               Tavares, Zenna",
  abstract  = "We present a new algorithm that synthesizes functional reactive
               programs from observation data. The key novelty is to iterate
               between a functional synthesis step, which attempts to generate
               a transition function over observed states, and an automata
               synthesis step, which adds any additional latent state necessary
               to fully account for the observations. We develop a functional
               reactive DSL called Autumn that can express a rich variety of
               causal dynamics in time-varying, Atari-style grid worlds, and
               apply our method to synthesize Autumn programs from data. We
               evaluate our algorithm on a benchmark suite of 30 Autumn
               programs as well as a third-party corpus of grid-world-style
               video games. We find that our algorithm synthesizes 27 out of 30
               programs in our benchmark suite and 21 out of 27 programs from
               the third-party corpus, including several programs describing
               complex latent state transformations, and from input traces
               containing hundreds of observations. We expect that our approach
               will provide a template for how to integrate functional and
               automata synthesis in other induction domains.",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  volume    =  7,
  number    = "POPL",
  pages     = "1628--1658",
  month     =  jan,
  year      =  2023,
  address   = "New York, NY, USA",
  keywords  = "causal, synthesis, automata, reactive;ARC Project;comp-cog-sci",
  doi       = "10.1145/3571249"
}

@ARTICLE{Kounios2014-wq,
  title    = "The cognitive neuroscience of insight",
  author   = "Kounios, John and Beeman, Mark",
  abstract = "Insight occurs when a person suddenly reinterprets a stimulus,
              situation, or event to produce a nonobvious, nondominant
              interpretation. This can take the form of a solution to a problem
              (an ``aha moment''), comprehension of a joke or metaphor, or
              recognition of an ambiguous percept. Insight research began a
              century ago, but neuroimaging and electrophysiological techniques
              have been applied to its study only during the past decade.
              Recent work has revealed insight-related coarse semantic coding
              in the right hemisphere and internally focused attention
              preceding and during problem solving. Individual differences in
              the tendency to solve problems insightfully rather than in a
              deliberate, analytic fashion are associated with different
              patterns of resting-state brain activity. Recent studies have
              begun to apply direct brain stimulation to facilitate insight. In
              sum, the cognitive neuroscience of insight is an exciting new
              area of research with connections to fundamental neurocognitive
              processes.",
  journal  = "Annu. Rev. Psychol.",
  volume   =  65,
  pages    = "71--93",
  year     =  2014,
  keywords = "project 1;Paper1",
  language = "en",
  issn     = "0066-4308, 1545-2085",
  pmid     = "24405359",
  doi      = "10.1146/annurev-psych-010213-115154"
}

@ARTICLE{Goldstone1998-ne,
  title    = "Perceptual learning",
  author   = "Goldstone, R L",
  abstract = "Perceptual learning involves relatively long-lasting changes to
              an organism's perceptual system that improve its ability to
              respond to its environment. Four mechanisms of perceptual
              learning are discussed: attention weighting, imprinting,
              differentiation, and unitization. By attention weighting,
              perception becomes adapted to tasks and environments by
              increasing the attention paid to important dimensions and
              features. By imprinting, receptors are developed that are
              specialized for stimuli or parts of stimuli. By differentiation,
              stimuli that were once indistinguishable become psychologically
              separated. By unitization, tasks that originally required
              detection of several parts are accomplished by detecting a single
              constructed unit representing a complex configuration. Research
              from cognitive psychology, psychophysics, neuroscience,
              expert/novice differences, development, computer science, and
              cross-cultural differences is described that relates to these
              mechanisms. The locus, limits, and applications of perceptual
              learning are also discussed.",
  journal  = "Annu. Rev. Psychol.",
  volume   =  49,
  pages    = "585--612",
  year     =  1998,
  keywords = "read;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "0066-4308",
  pmid     = "9496632",
  doi      = "10.1146/annurev.psych.49.1.585"
}

@ARTICLE{Li1993-jt,
  title    = "The representation of stimulus familiarity in anterior inferior
              temporal cortex",
  author   = "Li, L and Miller, E K and Desimone, R",
  abstract = "1. The inferior temporal (IT) cortex plays an important role in
              both short- and long-term memory for visual patterns. Most
              previous studies of IT neurons have tested their responses in
              recency memory tasks, which require that the memory lasts only
              the length of a single behavioral trial, which may be 150
              presentations of other stimuli, the maximum tested. For some
              cells the maximum decrement in response occurred for those
              stimuli that initially elicited the largest response. There was
              no significant change in response to stimuli that were already
              familiar. 5. The same cells that showed familiarity effects also
              showed reduced responses to the matching stimuli at the end of
              each trial, compared with the responses to the samples.(ABSTRACT
              TRUNCATED AT 400 WORDS)",
  journal  = "J. Neurophysiol.",
  volume   =  69,
  number   =  6,
  pages    = "1918--1929",
  month    =  jun,
  year     =  1993,
  keywords = "learning\&memory2023",
  language = "en",
  issn     = "0022-3077",
  pmid     = "8350131",
  doi      = "10.1152/jn.1993.69.6.1918"
}

@ARTICLE{Carey2004-ya,
  title    = "Bootstrapping \& the Origin of Concepts",
  author   = "Carey, Susan",
  journal  = "Daedalus",
  volume   =  133,
  number   =  1,
  pages    = "59--68",
  month    =  jan,
  year     =  2004,
  keywords = "comp-cog-sci",
  issn     = "0011-5266, 1548-6192",
  doi      = "10.1162/001152604772746701"
}

@ARTICLE{Knudsen2004-ny,
  title    = "Sensitive periods in the development of the brain and behavior",
  author   = "Knudsen, Eric I",
  abstract = "Experience exerts a profound influence on the brain and,
              therefore, on behavior. When the effect of experience on the
              brain is particularly strong during a limited period in
              development, this period is referred to as a sensitive period.
              Such periods allow experience to instruct neural circuits to
              process or represent information in a way that is adaptive for
              the individual. When experience provides information that is
              essential for normal development and alters performance
              permanently, such sensitive periods are referred to as critical
              periods. Although sensitive periods are reflected in behavior,
              they are actually a property of neural circuits. Mechanisms of
              plasticity at the circuit level are discussed that have been
              shown to operate during sensitive periods. A hypothesis is
              proposed that experience during a sensitive period modifies the
              architecture of a circuit in fundamental ways, causing certain
              patterns of connectivity to become highly stable and, therefore,
              energetically preferred. Plasticity that occurs beyond the end of
              a sensitive period, which is substantial in many circuits, alters
              connectivity patterns within the architectural constraints
              established during the sensitive period. Preferences in a circuit
              that result from experience during sensitive periods are
              illustrated graphically as changes in a ''stability landscape,''
              a metaphor that represents the relative contributions of genetic
              and experiential influences in shaping the information processing
              capabilities of a neural circuit. By understanding sensitive
              periods at the circuit level, as well as understanding the
              relationship between circuit properties and behavior, we gain a
              deeper insight into the critical role that experience plays in
              shaping the development of the brain and behavior.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  16,
  number   =  8,
  pages    = "1412--1425",
  month    =  oct,
  year     =  2004,
  keywords = "skimmed;dev-cog-neuro;development",
  language = "en",
  issn     = "0898-929X",
  pmid     = "15509387",
  doi      = "10.1162/0898929042304796"
}

@ARTICLE{Japkowicz2000-wi,
  title    = "Nonlinear {{Autoassociation Is Not Equivalent}} to {{PCA}}",
  author   = "Japkowicz, Nathalie and Hanson, Stephen Jos{\'e} and Gluck, Mark
              A",
  abstract = "A common misperception within the neural network community is
              that even with nonlinearities in their hidden layer,
              autoassociators trained with backpropagation are equivalent to
              linear methods such as principal component analysis (PCA). Our
              purpose is to demonstrate that nonlinear autoassociators actually
              behave differently from linear methods and that they can
              outperform these methods when used for latent extraction,
              projection, and classification. While linear autoassociators
              emulate PCA, and thus exhibit a flat or unimodal reconstruction
              error surface, autoassociators with nonlinearities in their
              hidden layer learn domains by building error reconstruction
              surfaces that, depending on the task, contain multiple local
              valleys. This interpolation bias allows nonlinear autoassociators
              to represent appropriate classifications of nonlinear multimodal
              domains, in contrast to linear autoassociators, which are
              inappropriate for such tasks. In fact, autoassociators with
              hidden unit nonlinearities can be shown to perform nonlinear
              classification and nonlinear recognition.",
  journal  = "Neural Comput.",
  volume   =  12,
  number   =  3,
  pages    = "531--545",
  month    =  mar,
  year     =  2000,
  keywords = "comp-cog-sci",
  issn     = "0899-7667",
  doi      = "10.1162/089976600300015691"
}

@ARTICLE{Hinton2002-on,
  title    = "Training products of experts by minimizing contrastive divergence",
  author   = "Hinton, Geoffrey E",
  abstract = "It is possible to combine multiple latent-variable models of the
              same data by multiplying their probability distributions together
              and then renormalizing. This way of combining individual
              ``expert'' models makes it hard to generate samples from the
              combined model but easy to infer the values of the latent
              variables of each expert, because the combination rule ensures
              that the latent variables of different experts are conditionally
              independent when given the data. A product of experts (PoE) is
              therefore an interesting candidate for a perceptual system in
              which rapid inference is vital and generation is unnecessary.
              Training a PoE by maximizing the likelihood of the data is
              difficult because it is hard even to approximate the derivatives
              of the renormalization term in the combination rule. Fortunately,
              a PoE can be trained using a different objective function called
              ``contrastive divergence'' whose derivatives with regard to the
              parameters can be approximated accurately and efficiently.
              Examples are presented of contrastive divergence learning using
              several types of expert on several types of data.",
  journal  = "Neural Comput.",
  volume   =  14,
  number   =  8,
  pages    = "1771--1800",
  month    =  aug,
  year     =  2002,
  language = "en",
  issn     = "0899-7667",
  pmid     = "12180402",
  doi      = "10.1162/089976602760128018"
}

@ARTICLE{Squire1992-cp,
  title    = "Declarative and nondeclarative memory: multiple brain systems
              supporting learning and memory",
  author   = "Squire, L R",
  abstract = "Abstract The topic of multiple forms of memory is considered from
              a biological point of view. Fact-and-event (declarative,
              explicit) memory is contrasted with a collection of non conscious
              (non-declarative, implicit) memory abilities including skills and
              habits, priming, and simple conditioning. Recent evidence is
              reviewed indicating that declarative and non declarative forms of
              memory have different operating characteristics and depend on
              separate brain systems. A brain-systems framework for
              understanding memory phenomena is developed in light of lesion
              studies involving rats, monkeys, and humans, as well as recent
              studies with normal humans using the divided visual field
              technique, event-related potentials, and positron emission
              tomography (PET).",
  journal  = "J. Cogn. Neurosci.",
  volume   =  4,
  number   =  3,
  pages    = "232--243",
  year     =  1992,
  keywords = "read;cog-neuro;learning\&memory2023",
  language = "en",
  issn     = "0898-929X",
  pmid     = "23964880",
  doi      = "10.1162/jocn.1992.4.3.232"
}

@ARTICLE{Premack1997-aj,
  title    = "Infants {{Attribute {Value}}$\pm$} to the {{{Goal-Directed}
              Actions}} of {{Self-propelled Objects}}",
  author   = "Premack, David and Premack, Ann James",
  abstract = "Motion is a fundamental source of information for basic human
              interpretations; it is basic to the fundamental concept of
              causality and, the present model argues, equally basic to the
              fundamental concept of intentionality.The model is based on two
              main assumptions: When an infant perceives an object (1) moving
              spontaneously and (2) displaying goaldirected action, it will
              interpret the object as intentional and assign to it the unique
              properties of the psychological domain. The key property tested
              was: Do infants attribute value to interactions between
              intentional objects using criteria specified by the model?We
              showed infants (average age 52 weeks) computer-generated
              animations of spontaneously moving ``balls,'' using looking time
              in a standard habituation/dishabituation paradigm. In two
              positive interactions, one ball either ``caressed'' another, or
              ``helped'' it achieve its goal; whereas in two negative
              interactions, one ball either ``hit`` another, or ``prevented''
              it from achieving its goal. In keeping with predictions of the
              model, when transferred to a negative condition, infants who had
              been habituated on a positive condition showed greater
              dishabituation than those habituated on a negative condition. The
              results could not be easily explained by the similarity relations
              among the animations depicting the interactions.The results
              suggest that well before the age when the child can ascribe
              mental states or has a ``theory of mind,'' it recognizes the
              goals of self-propelled objects and attributes value to the
              interactions between them.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  9,
  number   =  6,
  pages    = "848--856",
  month    =  nov,
  year     =  1997,
  keywords = "development",
  issn     = "0898-929X",
  doi      = "10.1162/jocn.1997.9.6.848"
}

@ARTICLE{Barlow1989-pm,
  title     = "Unsupervised learning",
  author    = "Barlow, H B",
  abstract  = "What use can the brain make of the massive flow of sensory
               information that occurs without any associated rewards or
               punishments? This question is reviewed in the light of
               connectionist models of unsupervised learning and some older
               ideas, namely the cognitive maps and working models of Tolman
               and Craik, and the idea that redundancy is important for
               understanding perception (Attneave 1954), the physiology of
               sensory pathways (Barlow 1959), and pattern recognition
               (Watanabe 1960). It is argued that (1) The redundancy of sensory
               messages provides the knowledge incorporated in the maps or
               models. (2) Some of this knowledge can be obtained by
               observations of mean, variance, and covariance of sensory
               messages, and perhaps also by a method called ``minimum entropy
               coding.'' (3) Such knowledge may be incorporated in a model of
               ``what usually happens'' with which incoming messages are
               automatically compared, enabling unexpected discrepancies to be
               immediately identified. (4) Knowledge of the sort incorporated
               into such a filter is a necessary prerequisite of ordinary
               learning, and a representation whose elements are independent
               makes it possible to form associations with logical functions of
               the elements, not just with the elements themselves.",
  journal   = "Neural Comput.",
  publisher = "MIT Press - Journals",
  volume    =  1,
  number    =  3,
  pages     = "295--311",
  month     =  sep,
  year      =  1989,
  keywords  = "read;learning\&memory2023",
  language  = "en",
  issn      = "0899-7667, 1530-888X",
  doi       = "10.1162/neco.1989.1.3.295"
}

@ARTICLE{Wolpert1996-ky,
  title     = "The Lack of A Priori Distinctions Between Learning Algorithms",
  author    = "Wolpert, David H",
  journal   = "Neural Comput.",
  publisher = "MIT Press",
  volume    =  8,
  number    =  7,
  pages     = "1341--1390",
  month     =  oct,
  year      =  1996,
  keywords  = "project 1;machine-learning;l\&m final;Paper1",
  issn      = "0899-7667",
  doi       = "10.1162/neco.1996.8.7.1341"
}

@ARTICLE{Nikolaus2022-qh,
  title     = "Learning English with Peppa Pig",
  author    = "Nikolaus, Mitja and Alishahi, Afra and Chrupa{\l}a, Grzegorz",
  abstract  = "Abstract Recent computational models of the acquisition of
               spoken language via grounding in perception exploit associations
               between spoken and visual modalities and learn to represent
               speech and visual data in a joint vector space. A major
               unresolved issue from the point of ecological validity is the
               training data, typically consisting of images or videos paired
               with spoken descriptions of what is depicted. Such a setup
               guarantees an unrealistically strong correlation between speech
               and the visual data. In the real world the coupling between the
               linguistic and the visual modality is loose, and often
               confounded by correlations with non-semantic aspects of the
               speech signal. Here we address this shortcoming by using a
               dataset based on the children's cartoon Peppa Pig. We train a
               simple bi-modal architecture on the portion of the data
               consisting of dialog between characters, and evaluate on
               segments containing descriptive narrations. Despite the weak and
               confounded signal in this training data, our model succeeds at
               learning aspects of the visual semantics of spoken language.",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press",
  volume    =  10,
  pages     = "922--936",
  month     =  sep,
  year      =  2022,
  keywords  = "skimmed;compling-cogsci2023",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en",
  issn      = "2307-387X",
  doi       = "10.1162/tacl\_a\_00498"
}

@ARTICLE{Dunsmoor2014-np,
  title    = "Stimulus typicality determines how broadly fear is generalized",
  author   = "Dunsmoor, Joseph E and Murphy, Gregory L",
  abstract = "The ability to represent knowledge at the category level promotes
              the transfer of learning. How this ability integrates with basic
              forms of conditioned learning is unknown but could explain why
              conditioned fear is overgeneralized after aversive experiences.
              We examined the impact of stimulus typicality--an important
              determinant of category-based induction--on fear learning and
              generalization. Typicality is known to affect the strength of
              categorical arguments; a premise involving typical exemplars
              (e.g., sparrow) is believed to apply to other members, whereas a
              premise about atypical exemplars (e.g., penguin) generalizes more
              narrowly to similar items. We adopted this framework to human
              fear conditioning and found that fear conditioned to typical
              exemplars generalized more readily to atypical members than vice
              versa, despite equal feature overlap across conditions. These
              findings have implications for understanding why some fearful
              events lead to broad overgeneralization of fear whereas others
              are regarded as isolated episodes.",
  journal  = "Psychol. Sci.",
  volume   =  25,
  number   =  9,
  pages    = "1816--1821",
  month    =  sep,
  year     =  2014,
  keywords = "category-based induction; fear conditioning; generalization;
              reasoning; skin conductance responses;l\&m final",
  language = "en",
  issn     = "0956-7976, 1467-9280",
  pmid     = "25015685",
  doi      = "10.1177/0956797614535401"
}

@ARTICLE{Aslin2012-rt,
  title    = "Statistical learning: From acquiring specific items to forming
              general rules",
  author   = "Aslin, Richard N and Newport, Elissa L",
  abstract = "Statistical learning is a rapid and robust mechanism that enables
              adults and infants to extract patterns of stimulation embedded in
              both language and visual domains. Importantly, statistical
              learning operates implicitly, without instruction, through mere
              exposure to a set of input stimuli. However, much of what
              learners must acquire about a structured domain consists of
              principles or rules that can be applied to novel inputs. Although
              it has been claimed that statistical learning and rule learning
              are separate mechanisms, here we review evidence and provide a
              unifying perspective that argues for a single mechanism of
              statistical learning that accounts for both the learning of the
              input stimuli and the generalization to novel instances. The
              balance between instance-learning and generalization is based on
              two factors: the strength of perceptual biases that highlight
              structural regularities, and the consistency of unique versus
              overlapping contexts in the input.",
  journal  = "Curr. Dir. Psychol. Sci.",
  volume   =  21,
  number   =  3,
  pages    = "170--176",
  month    =  jun,
  year     =  2012,
  keywords = "generalization; infants; rule learning; statistical
              learning;read;learning\&memory2023",
  language = "en",
  issn     = "0963-7214",
  pmid     = "24000273",
  doi      = "10.1177/0963721412436806",
  pmc      = "PMC3758750"
}

@ARTICLE{Karpicke2012-dl,
  title     = "{Retrieval-Based} Learning: Active Retrieval Promotes Meaningful
               Learning",
  author    = "Karpicke, Jeffrey D",
  abstract  = "Retrieval is the key process for understanding learning and for
               promoting learning, yet retrieval is not often granted the
               central role it deserves. Learning is typically identified with
               the encoding or construction of knowledge, and retrieval is
               considered merely the assessment of learning that occurred in a
               prior experience. The retrieval-based learning perspective
               outlined here is grounded in the fact that all expressions of
               knowledge involve retrieval and depend on the retrieval cues
               available in a given context. Further, every time a person
               retrieves knowledge, that knowledge is changed, because
               retrieving knowledge improves one?s ability to retrieve it again
               in the future. Practicing retrieval does not merely produce
               rote, transient learning; it produces meaningful, long-term
               learning. Yet retrieval practice is a tool many students lack
               metacognitive awareness of and do not use as often as they
               should. Active retrieval is an effective but undervalued
               strategy for promoting meaningful learning.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  21,
  number    =  3,
  pages     = "157--163",
  month     =  jun,
  year      =  2012,
  keywords  = "read",
  issn      = "0963-7214",
  doi       = "10.1177/0963721412443552"
}

@ARTICLE{Xu2013-bw,
  title     = "Infants Are Rational Constructivist Learners",
  author    = "Xu, Fei and Kushnir, Tamar",
  abstract  = "What is the nature of human learning, and what insights can be
               gained from understanding early learning in infants and young
               children? This is an important question for understanding the
               human mind, the origins of knowledge, scientific reasoning, and
               how to best structure our educational environment. In this
               article, we argue for a new approach to cognitive development:
               rational constructivism. This view characterizes the child as a
               rational constructive learner, and it sees early learning as
               rational, statistical, and inferential. Empirical evidence for
               this approach has been accumulating rapidly, and a set of
               domain-general statistical and inferential mechanisms have been
               uncovered to explain why infants and young children learn so
               fast and so well.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  22,
  number    =  1,
  pages     = "28--32",
  month     =  feb,
  year      =  2013,
  keywords  = "development",
  issn      = "0963-7214",
  doi       = "10.1177/0963721412469396"
}

@ARTICLE{Piantadosi2016-fe,
  title    = "Four {{Problems Solved}} by the {{Probabilistic Language}} of
              {{Thought}}",
  author   = "Piantadosi, Steven T and Jacobs, Robert A",
  abstract = "We argue for the advantages of the probabilistic language of
              thought (pLOT), a recently emerging approach to modeling human
              cognition. Work using this framework demonstrates how the pLOT
              (a) refines the debate between symbols and statistics in
              cognitive modeling, (b) permits theories that draw on insights
              from both nativist and empiricist approaches, (c) explains the
              origins of novel and complex computational concepts, and (d)
              provides a framework for abstraction that can link sensation and
              conception. In each of these areas, the pLOT provides a
              productive middle ground between historical divides in cognitive
              psychology, pointing to a promising way forward for the field.",
  journal  = "Curr. Dir. Psychol. Sci.",
  volume   =  25,
  number   =  1,
  pages    = "54--59",
  month    =  feb,
  year     =  2016,
  keywords = "comp-cog-sci",
  issn     = "0963-7214, 1467-8721",
  doi      = "10.1177/0963721415609581"
}

@ARTICLE{Phattanasri2007-qt,
  title     = "The Dynamics of Associative Learning in Evolved Model Circuits",
  author    = "Phattanasri, Phattanard and Chiel, Hillel J and Beer, Randall D",
  abstract  = "In this article, we evolve and analyze continuous-time recurrent
               neural networks capable of associating the smells of different
               foods with edibility or inedibility in different environments.
               First, we present an in-depth analysis of this task,
               highlighting the evolutionary challenges it poses and how these
               challenges informed our experimental design. Next, we describe
               the evolution of nonplastic neural circuits that can solve this
               food edibility learning problem. We then show that the dynamics
               of the best evolved nonplastic circuits instantiate finite state
               machines that capture the combinatorial structure of this task.
               Finally, we demonstrate that successful circuits with Hebbian
               synaptic plasticity can also be evolved, but that such circuits
               do not utilize their synaptic plasticity in a traditional way.",
  journal   = "Adapt. Behav.",
  publisher = "SAGE Publications Ltd STM",
  volume    =  15,
  number    =  4,
  pages     = "377--396",
  month     =  dec,
  year      =  2007,
  keywords  = "skimmed;learning\&memory2023",
  issn      = "1059-7123",
  doi       = "10.1177/1059712307084688"
}

@ARTICLE{Dunlosky2013-tu,
  title    = "Improving Students' Learning With Effective Learning Techniques:
              Promising Directions From Cognitive and Educational Psychology",
  author   = "Dunlosky, John and Rawson, Katherine A and Marsh, Elizabeth J and
              Nathan, Mitchell J and Willingham, Daniel T",
  abstract = "Many students are being left behind by an educational system that
              some people believe is in crisis. Improving educational outcomes
              will require efforts on many fronts, but a central premise of
              this monograph is that one part of a solution involves helping
              students to better regulate their learning through the use of
              effective learning techniques. Fortunately, cognitive and
              educational psychologists have been developing and evaluating
              easy-to-use learning techniques that could help students achieve
              their learning goals. In this monograph, we discuss 10 learning
              techniques in detail and offer recommendations about their
              relative utility. We selected techniques that were expected to be
              relatively easy to use and hence could be adopted by many
              students. Also, some techniques (e.g., highlighting and
              rereading) were selected because students report relying heavily
              on them, which makes it especially important to examine how well
              they work. The techniques include elaborative interrogation,
              self-explanation, summarization, highlighting (or underlining),
              the keyword mnemonic, imagery use for text learning, rereading,
              practice testing, distributed practice, and interleaved practice.
              To offer recommendations about the relative utility of these
              techniques, we evaluated whether their benefits generalize across
              four categories of variables: learning conditions, student
              characteristics, materials, and criterion tasks. Learning
              conditions include aspects of the learning environment in which
              the technique is implemented, such as whether a student studies
              alone or with a group. Student characteristics include variables
              such as age, ability, and level of prior knowledge. Materials
              vary from simple concepts to mathematical problems to complicated
              science texts. Criterion tasks include different outcome measures
              that are relevant to student achievement, such as those tapping
              memory, problem solving, and comprehension. We attempted to
              provide thorough reviews for each technique, so this monograph is
              rather lengthy. However, we also wrote the monograph in a modular
              fashion, so it is easy to use. In particular, each review is
              divided into the following sections: General description of the
              technique and why it should work How general are the effects of
              this technique? 2a. Learning conditions 2b. Student
              characteristics 2c. Materials 2d. Criterion tasks Effects in
              representative educational contexts Issues for implementation
              Overall assessment The review for each technique can be read
              independently of the others, and particular variables of interest
              can be easily compared across techniques. To foreshadow our final
              recommendations, the techniques vary widely with respect to their
              generalizability and promise for improving student learning.
              Practice testing and distributed practice received high utility
              assessments because they benefit learners of different ages and
              abilities and have been shown to boost students' performance
              across many criterion tasks and even in educational contexts.
              Elaborative interrogation, self-explanation, and interleaved
              practice received moderate utility assessments. The benefits of
              these techniques do generalize across some variables, yet despite
              their promise, they fell short of a high utility assessment
              because the evidence for their efficacy is limited. For instance,
              elaborative interrogation and self-explanation have not been
              adequately evaluated in educational contexts, and the benefits of
              interleaving have just begun to be systematically explored, so
              the ultimate effectiveness of these techniques is currently
              unknown. Nevertheless, the techniques that received
              moderate-utility ratings show enough promise for us to recommend
              their use in appropriate situations, which we describe in detail
              within the review of each technique. Five techniques received a
              low utility assessment: summarization, highlighting, the keyword
              mnemonic, imagery use for text learning, and rereading. These
              techniques were rated as low utility for numerous reasons.
              Summarization and imagery use for text learning have been shown
              to help some students on some criterion tasks, yet the conditions
              under which these techniques produce benefits are limited, and
              much research is still needed to fully explore their overall
              effectiveness. The keyword mnemonic is difficult to implement in
              some contexts, and it appears to benefit students for a limited
              number of materials and for short retention intervals. Most
              students report rereading and highlighting, yet these techniques
              do not consistently boost students' performance, so other
              techniques should be used in their place (e.g., practice testing
              instead of rereading). Our hope is that this monograph will
              foster improvements in student learning, not only by showcasing
              which learning techniques are likely to have the most
              generalizable effects but also by encouraging researchers to
              continue investigating the most promising techniques.
              Accordingly, in our closing remarks, we discuss some issues for
              how these techniques could be implemented by teachers and
              students, and we highlight directions for future research.",
  journal  = "Psychol. Sci. Public Interest",
  volume   =  14,
  number   =  1,
  pages    = "4--58",
  month    =  jan,
  year     =  2013,
  keywords = "skimmed;learning\&memory2023;l\&m final",
  language = "en",
  issn     = "1529-1006",
  pmid     = "26173288",
  doi      = "10.1177/1529100612453266"
}

@ARTICLE{Gureckis2012-xz,
  title    = "{Self-{{Directed} Learning}}: {{A Cognitive}} and {{Computational
              Perspective}}",
  author   = "Gureckis, Todd M and Markant, Douglas B",
  abstract = "A widely advocated idea in education is that people learn better
              when the flow of experience is under their control (i.e.,
              learning is self-directed). However, the reasons why volitional
              control might result in superior acquisition and the limits to
              such advantages remain poorly understood. In this article, we
              review the issue from both a cognitive and computational
              perspective. On the cognitive side, self-directed learning allows
              individuals to focus effort on useful information they do not yet
              possess, can expose information that is inaccessible via passive
              observation, and may enhance the encoding and retention of
              materials. On the computational side, the development of
              efficient ``active learning'' algorithms that can select their
              own training data is an emerging research topic in machine
              learning. This review argues that recent advances in these
              related fields may offer a fresh theoretical perspective on how
              people gather information to support their own learning.",
  journal  = "Perspect. Psychol. Sci.",
  volume   =  7,
  number   =  5,
  pages    = "464--481",
  month    =  sep,
  year     =  2012,
  keywords = "active learning,intervention-based causal learning,machine
              learning,self-directed learning,self-regulated
              study;comp-cog-sci;project 1",
  issn     = "1745-6916, 1745-6924",
  doi      = "10.1177/1745691612454304"
}

@ARTICLE{Krathwohl2002-bb,
  title     = "A revision of bloom's taxonomy: An overview",
  author    = "Krathwohl, David R",
  abstract  = "From One Dimension to Two Dimensions Objectives that describe
               intended learning outcomes as the result of instruction are
               usually framed in terms of (a) some subject matter content and
               (b) a description of what is to be done with or to that content.
               [...]statements of objectives typically consist of a noun or
               noun phrase-the subject matter content-and a verb or verb
               phrase-the cognitive process(es). [...]any objective could be
               classified in the Taxonomy Table in one or more cells that
               correspond with the intersection of the columns) appropriate for
               categorizing the verbs) and the rows) appropriate for
               categorizing the nouns) or noun phrase(s). Analyze, of course,
               would be 4. Since both categories of cognitive processes are
               likely to be involved (with students being expected to analyze
               before they create), we would place this objective in two cells
               of the Taxonomy Table: B4, Analyze Conceptual Knowledge, and B6,
               Create [based on] Conceptual Knowledge (see Figure 1). In
               addition to showing what was included, the Taxonomy Table also
               suggests what might have been but wasn't. [...]in Figure 2, the
               two blank bottom rows raise questions about whether there might
               have been procedural or metacognitive knowledge objectives that
               could have been included.",
  journal   = "Theory Pract.",
  publisher = "Informa UK Limited",
  volume    =  41,
  number    =  4,
  pages     = "212--218",
  month     =  nov,
  year      =  2002,
  address   = "New York, United States, Columbus",
  keywords  = "project 2",
  language  = "en",
  issn      = "0040-5841, 1543-0421",
  doi       = "10.1207/s15430421tip4104\_2"
}

@ARTICLE{Elman1990-pd,
  title     = "Finding structure in time",
  author    = "Elman, Jeffrey L",
  abstract  = "Time underlies many interesting human behaviors. Thus, the
               question of how to represent time in connectionist models is
               very important. One approach is to represent time implicitly by
               its effects on processing rather than explicitly (as in a
               spatial representation). The current report develops a proposal
               along these lines first described by Jordan (1986) which
               involves the use of recurrent links in order to provide networks
               with a dynamic memory. In this approach, hidden unit patterns
               are fed back to themselves; the internal representations which
               develop thus reflect task demands in the context of prior
               internal states. A set of simulations is reported which range
               from relatively simple problems (temporal version of XOR) to
               discovering syntactic/semantic features for words. The networks
               are able to learn interesting internal representations which
               incorporate task demands with memory demands; indeed, in this
               approach the notion of memory is inextricably bound up with task
               processing. These representations reveal a rich structure, which
               allows them to be highly context-dependent while also expressing
               generalizations across classes of items. These representations
               suggest a method for representing lexical categories and the
               type/token distinction.",
  journal   = "Cogn. Sci.",
  publisher = "Wiley",
  volume    =  14,
  number    =  2,
  pages     = "179--211",
  month     =  mar,
  year      =  1990,
  keywords  = "read;ccm2023;comp-cog-sci",
  language  = "en",
  issn      = "0364-0213, 1551-6709",
  doi       = "10.1207/s15516709cog1402\_1"
}

@ARTICLE{Gilhooly2010-fb,
  title     = "Verbalization and problem solving: insight and spatial factors",
  author    = "Gilhooly, K J and Fioratou, E and Henretty, N",
  abstract  = "Two groups of participants attempted eight examples of each of
               four different problem types formed by combining insight versus
               non-insight and verbal versus spatial factors. The groups were
               given different verbalization instructions viz., Silent (N=40)
               or Direct Concurrent (N=40). There were significant differences
               between insight and non-insight tasks and between spatial and
               verbal tasks in terms of solution rates and latencies.
               Significant interactions between the verbal versus spatial
               factor and verbalization condition on solution rates and
               latencies reflected a greater (negative) effect of verbalizing
               on spatial as against verbal problems. However, no significant
               interactions of the insight versus non-insight factor with
               verbalization condition on solution rates or latencies were
               found. These results favoured the 'business as usual' view of
               insight problem solving as against the 'special process' view
               which predicted larger effects of verbalization for insight
               problems as against non-insight problems.",
  journal   = "Br. J. Psychol.",
  publisher = "Wiley Online Library",
  volume    =  101,
  number    = "Pt 1",
  pages     = "81--93",
  month     =  feb,
  year      =  2010,
  language  = "en",
  issn      = "0007-1269, 2044-8295",
  pmid      = "19309537",
  doi       = "10.1348/000712609X422656"
}

@ARTICLE{Jonas2017-jr,
  title    = "Could a Neuroscientist Understand a Microprocessor?",
  author   = "Jonas, Eric and Kording, Konrad Paul",
  abstract = "There is a popular belief in neuroscience that we are primarily
              data limited, and that producing large, multimodal, and complex
              datasets will, with the help of advanced data analysis
              algorithms, lead to fundamental insights into the way the brain
              processes information. These datasets do not yet exist, and if
              they did we would have no way of evaluating whether or not the
              algorithmically-generated insights were sufficient or even
              correct. To address this, here we take a classical microprocessor
              as a model organism, and use our ability to perform arbitrary
              experiments on it to see if popular data analysis methods from
              neuroscience can elucidate the way it processes information.
              Microprocessors are among those artificial information processing
              systems that are both complex and that we understand at all
              levels, from the overall logical flow, via logical gates, to the
              dynamics of transistors. We show that the approaches reveal
              interesting structure in the data but do not meaningfully
              describe the hierarchy of information processing in the
              microprocessor. This suggests current analytic approaches in
              neuroscience may fall short of producing meaningful understanding
              of neural systems, regardless of the amount of data.
              Additionally, we argue for scientists using complex non-linear
              dynamical systems with known ground truth, such as the
              microprocessor as a validation platform for time-series and
              structure discovery methods.",
  journal  = "PLoS Comput. Biol.",
  volume   =  13,
  number   =  1,
  pages    = "e1005268",
  month    =  jan,
  year     =  2017,
  keywords = "BAMB2023",
  language = "en",
  issn     = "1553-734X, 1553-7358",
  pmid     = "28081141",
  doi      = "10.1371/journal.pcbi.1005268",
  pmc      = "PMC5230747"
}

@ARTICLE{Bonner2018-ii,
  title    = "Computational Mechanisms Underlying Cortical Responses to the
              Affordance Properties of Visual Scenes",
  author   = "Bonner, Michael F and Epstein, Russell A",
  editor   = "Einh{\"a}user, Wolfgang",
  abstract = "Biologically inspired deep convolutional neural networks (CNNs),
              trained for computer vision tasks, have been found to predict
              cortical responses with remarkable accuracy. However, the
              internal operations of these models remain poorly understood, and
              the factors that account for their success are unknown. Here we
              develop a set of techniques for using CNNs to gain insights into
              the computational mechanisms underlying cortical responses. We
              focused on responses in the occipital place area (OPA), a
              scene-selective region of dorsal occipitoparietal cortex. In a
              previous study, we showed that fMRI activation patterns in the
              OPA contain information about the navigational affordances of
              scenes; that is, information about where one can and cannot move
              within the immediate environment. We hypothesized that this
              affordance information could be extracted using a set of purely
              feedforward computations. To test this idea, we examined a deep
              CNN with a feedforward architecture that had been previously
              trained for scene classification. We found that responses in the
              CNN to scene images were highly predictive of fMRI responses in
              the OPA. Moreover the CNN accounted for the portion of OPA
              variance relating to the navigational affordances of scenes. The
              CNN could thus serve as an image-computable candidate model of
              affordance-related responses in the OPA. We then ran a series of
              in silico experiments on this model to gain insights into its
              internal operations. These analyses showed that the computation
              of affordance-related features relied heavily on visual
              information at high-spatial frequencies and cardinal
              orientations, both of which have previously been identified as
              lowlevel stimulus preferences of scene-selective visual cortex.
              These computations also exhibited a strong preference for
              information in the lower visual field, which is consistent with
              known retinotopic biases in the OPA. Visualizations of feature
              selectivity within the CNN suggested that affordance-based
              responses encoded features that define the layout of the spatial
              environment, such as boundary-defining junctions and large
              extended surfaces. Together, these results map the sensory
              functions of the OPA onto a fully quantitative model that
              provides insights into its visual computations. More broadly,
              they advance integrative techniques for understanding visual
              cortex across multiple level of analysis: from the identification
              of cortical sensory functions to the modeling of their underlying
              algorithms.",
  journal  = "PLoS Comput. Biol.",
  volume   =  14,
  number   =  4,
  pages    = "e1006111",
  month    =  apr,
  year     =  2018,
  keywords = "comp-cog-sci",
  issn     = "1553-734X, 1553-7358",
  doi      = "10.1371/journal.pcbi.1006111"
}

@ARTICLE{Dwivedi2021-zk,
  title     = "Unveiling Functions of the Visual Cortex Using {Task-Specific}
               Deep Neural Networks",
  author    = "Dwivedi, Kshitij and Bonner, Michael F and Cichy, Radoslaw
               Martin and Roig, Gemma",
  abstract  = "The human visual cortex enables visual perception through a
               cascade of hierarchical computations in cortical regions with
               distinct functionalities. Here, we introduce an AI-driven
               approach to discover the functional mapping of the visual
               cortex. We related human brain responses to scene images
               measured with functional MRI (fMRI) systematically to a diverse
               set of deep neural networks (DNNs) optimized to perform
               different scene perception tasks. We found a structured mapping
               between DNN tasks and brain regions along the ventral and dorsal
               visual streams. Low-level visual tasks mapped onto early brain
               regions, 3-dimensional scene perception tasks mapped onto the
               dorsal stream, and semantic tasks mapped onto the ventral
               stream. This mapping was of high fidelity, with more than 60\%
               of the explainable variance in nine key regions being explained.
               Together, our results provide a novel functional mapping of the
               human visual cortex and demonstrate the power of the
               computational approach.",
  journal   = "PLoS Comput. Biol.",
  publisher = "Public Library of Science",
  volume    =  17,
  number    =  8,
  pages     = "e1009267",
  month     =  aug,
  year      =  2021,
  keywords  = "Functional magnetic resonance imaging,Linear regression
               analysis,Neural networks,Permutation,Semantics,Sensory
               perception,Vision,Visual cortex;comp-cog-sci",
  issn      = "1553-734X, 1553-7358",
  doi       = "10.1371/journal.pcbi.1009267"
}

@INCOLLECTION{Minsky2019-mb,
  title     = "A {{Framework For Representing Knowledge}}",
  booktitle = "A {{Framework For Representing Knowledge}}",
  author    = "Minsky, M",
  abstract  = "A Framework For Representing Knowledge was published in Frame
               Conceptions and Text Understanding on page 1.",
  publisher = "De Gruyter",
  pages     = "1--25",
  month     =  jul,
  year      =  2019,
  keywords  = "comp-cog-sci",
  isbn      = "9783110858778",
  doi       = "10.1515/9783110858778-003"
}

@INPROCEEDINGS{Mueller2022-pc,
  title     = "Coloring the Blank Slate: Pre-training Imparts a Hierarchical
               Inductive Bias to Sequence-to-sequence Models",
  booktitle = "Findings of the Association for Computational Linguistics: {ACL}
               2022",
  author    = "Mueller, Aaron and Frank, Robert and Linzen, Tal and Wang,
               Luheng and Schuster, Sebastian",
  abstract  = "Relations between words are governed by hierarchical structure
               rather than linear ordering. Sequence-to-sequence (seq2seq)
               models, despite their success in downstream NLP applications,
               often fail to generalize in a hierarchy-sensitive manner when
               performing syntactic transformations---for example, transforming
               declarative sentences into questions. However, syntactic
               evaluations of seq2seq models have only observed models that
               were not pre-trained on natural language data before being
               trained to perform syntactic transformations, in spite of the
               fact that pre-training has been found to induce hierarchical
               linguistic generalizations in language models; in other words,
               the syntactic capabilities of seq2seq models may have been
               greatly understated. We address this gap using the pre-trained
               seq2seq models T5 and BART, as well as their multilingual
               variants mT5 and mBART. We evaluate whether they generalize
               hierarchically on two transformations in two languages: question
               formation and passivization in English and German. We find that
               pre-trained seq2seq models generalize hierarchically when
               performing syntactic transformations, whereas models trained
               from scratch on syntactic transformations do not. This result
               presents evidence for the learnability of hierarchical syntactic
               information from non-annotated natural language text while also
               demonstrating that seq2seq models are capable of syntactic
               generalization, though only after exposure to much more language
               data than human learners receive.",
  publisher = "Association for Computational Linguistics",
  pages     = "1352--1368",
  month     =  may,
  year      =  2022,
  address   = "Dublin, Ireland",
  keywords  = "skimmed;compling-cogsci2023",
  doi       = "10.18653/v1/2022.findings-acl.106"
}

@INPROCEEDINGS{McCoy2019-zo,
  title     = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in
               Natural Language Inference",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "McCoy, Tom and Pavlick, Ellie and Linzen, Tal",
  abstract  = "A machine learning system can score well on a given test set by
               relying on heuristics that are effective for frequent example
               types but break down in more challenging cases. We study this
               issue within natural language inference (NLI), the task of
               determining whether one sentence entails another. We hypothesize
               that statistical NLI models may adopt three fallible syntactic
               heuristics: the lexical overlap heuristic, the subsequence
               heuristic, and the constituent heuristic. To determine whether
               models have adopted these heuristics, we introduce a controlled
               evaluation set called HANS (Heuristic Analysis for NLI Systems),
               which contains many examples where the heuristics fail. We find
               that models trained on MNLI, including BERT, a state-of-the-art
               model, perform very poorly on HANS, suggesting that they have
               indeed adopted these heuristics. We conclude that there is
               substantial room for improvement in NLI systems, and that the
               HANS dataset can motivate and measure progress in this area.",
  publisher = "Association for Computational Linguistics",
  pages     = "3428--3448",
  month     =  jul,
  year      =  2019,
  address   = "Florence, Italy",
  keywords  = "nlp",
  doi       = "10.18653/v1/P19-1334"
}

@ARTICLE{Herrnstein1970-qr,
  title    = "On the law of effect",
  author   = "Herrnstein, R J",
  abstract = "Experiments on single, multiple, and concurrent schedules of
              reinforcement find various correlations between the rate of
              responding and the rate or magnitude of reinforcement. For
              concurrent schedules (i.e., simultaneous choice procedures),
              there is matching between the relative frequencies of responding
              and reinforcement; for multiple schedules (i.e., successive
              discrimination procedures), there are contrast effects between
              responding in each component and reinforcement in the others; and
              for single schedules, there are a host of increasing monotonic
              relations between the rate of responding and the rate of
              reinforcement. All these results, plus several others, can be
              accounted for by a coherent system of equations, the most general
              of which states that the absolute rate of any response is
              proportional to its associated relative reinforcement.",
  journal  = "J. Exp. Anal. Behav.",
  volume   =  13,
  number   =  2,
  pages    = "243--266",
  month    =  mar,
  year     =  1970,
  keywords = "learning\&memory2023",
  language = "en",
  issn     = "0022-5002",
  pmid     = "16811440",
  doi      = "10.1901/jeab.1970.13-243",
  pmc      = "PMC1333768"
}

@ARTICLE{Greenough1987-cn,
  title    = "Experience and brain development",
  author   = "Greenough, W T and Black, J E and Wallace, C S",
  abstract = "This article considers how experience can influence the
              developing and mature brain and proposes a new categorization
              scheme based upon the type of information stored and the brain
              mechanisms that appear to be involved in storing it. In this
              scheme, experience-expectant information storage refers to
              incorporation of environmental information that is ubiquitous in
              the environment and common to all species members, such as the
              basic elements of pattern perception. Experience-expectant
              processes appear to have evolved as a neural preparation for
              incorporating specific information: in many sensory systems,
              synaptic connections between nerve cells are overproduced, and a
              subsequent selection process occurs in which aspects of sensory
              experience determine the pattern of connections that remains.
              Experience-dependent information storage refers to incorporation
              of environmental information that is idiosyncratic, or unique to
              the individual, such as learning about one's specific physical
              environment or vocabulary. The neural basis of
              experience-dependent processes appears to involve active
              formation of new synaptic connections in response to the events
              providing the information to be stored. Although these processes
              probably do not occur entirely independently of one another in
              development, the categories offer a new view more in accord with
              neural mechanisms than were terms like ``critical'' or
              ``sensitive period.''",
  journal  = "Child Dev.",
  volume   =  58,
  number   =  3,
  pages    = "539--559",
  month    =  jun,
  year     =  1987,
  keywords = "read;dev-cog-neuro;development",
  language = "en",
  issn     = "0009-3920",
  pmid     = "3038480",
  doi      = "10.2307/1130197"
}

@INPROCEEDINGS{Madhushani2020-ah,
  title     = "A Dynamic Observation Strategy for Multi-agent Multi-armed
               Bandit Problem",
  booktitle = "2020 European Control Conference ({ECC})",
  author    = "Madhushani, Udari and Leonard, Naomi Ehrich",
  abstract  = "We define and analyze a multi-agent multi-armed bandit problem
               in which decision-making agents can observe the choices and
               rewards of their neighbors under a linear observation cost.
               Neighbors are defined by a network graph that encodes the
               inherent observation constraints of the system. We define a cost
               associated with observations such that at every instance an
               agent makes an observation it receives a constant observation
               regret. We design a sampling algorithm and an observation
               protocol for each agent to maximize its own expected cumulative
               reward through minimizing expected cumulative sampling regret
               and expected cumulative observation regret. For our proposed
               protocol, we prove that total cumulative regret is
               logarithmically bounded. We verify the accuracy of analytical
               bounds using numerical simulations.",
  pages     = "1677--1682",
  month     =  may,
  year      =  2020,
  keywords  = "Random variables;Uncertainty;Protocols;Upper bound;Decision
               making;Numerical simulation;Estimation;ccm-project",
  doi       = "10.23919/ECC51009.2020.9143736"
}

@ARTICLE{De_Jong1998-ci,
  title     = "Scientific Discovery Learning with Computer Simulations of
               Conceptual Domains",
  author    = "De Jong, Ton and Van Joolingen, Wouter R",
  abstract  = "Scientific discovery learning is a highly self-directed and
               constructivistic form of learning. A computer simulation is a
               type of computer-based environment that is well suited for
               discovery learning, the main task of the learner being to infer,
               through experimentation, characteristics of the model underlying
               the simulation. In this article we give a review of the observed
               effectiveness and efficiency of discovery learning in simulation
               environments together with problems that learners may encounter
               in discovery learning, and we discuss how simulations may be
               combined with instructional support in order to overcome these
               problems.",
  journal   = "Rev. Educ. Res.",
  publisher = "American Educational Research Association",
  volume    =  68,
  number    =  2,
  pages     = "179--201",
  month     =  jun,
  year      =  1998,
  keywords  = "project 1",
  issn      = "0034-6543",
  doi       = "10.3102/00346543068002179"
}

@UNPUBLISHED{Wang2022-uc,
  title    = "Finding Structure in One Child's Linguistic Experience",
  author   = "Wang, Wentao and Vong, Wai Keen and Kim, Najoung and Lake,
              Brenden M",
  abstract = "Neural network models have recently made striking progress in
              natural language processing, but they are typically trained on
              orders of magnitude more language input than children receive.
              What can these neural networks, which are primarily
              distributional learners, learn from a naturalistic subset of a
              single child's experience? We examine this question using a
              recent longitudinal dataset collected from a single child,
              consisting of egocentric visual data paired with text
              transcripts. We train both language-only and vision-and-language
              neural networks and analyze the linguistic knowledge they
              acquire. In parallel with findings from Elman's (1990) seminal
              work, the neural networks form emergent clusters of words
              corresponding to syntactic (nouns, transitive and intransitive
              verbs) and semantic categories (e.g., animals and clothing),
              based solely on one child's linguistic input. The networks also
              acquire sensitivity to acceptability contrasts from linguistic
              phenomena such as determiner-noun agreement and argument
              structure. We find that incorporating visual information produces
              an incremental gain in predicting words in context, especially
              for syntactic categories that are comparatively more easily
              grounded such as nouns and verbs, but the underlying linguistic
              representations are not fundamentally altered. Our findings
              demonstrate which kinds of linguistic knowledge are learnable
              from a snapshot of a single child's real developmental
              experience, and which kinds may benefit from stronger inductive
              biases or richer sources of data.",
  month    =  dec,
  year     =  2022,
  keywords = "read;compling-cogsci2023",
  doi      = "10.31234/osf.io/85k3y"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Brandle2021-bo,
  title    = "Exploration beyond bandits",
  author   = "Br{\"a}ndle, Franziska and Binz, Marcel and Schulz, Eric",
  abstract = "We describe novel approaches to study exploration in humans
              beyond simple multi-armed bandit tasks.",
  journal  = "https://psyarxiv.com â€º ...https://psyarxiv.com â€º ...",
  month    =  feb,
  year     =  2021,
  keywords = "Bandits; Exploration; Information search; MDPs;project 1",
  doi      = "10.31234/osf.io/9fnmj"
}

@UNPUBLISHED{Dubey2021-zq,
  title    = "Aha! moments correspond to metacognitive prediction errors",
  author   = "Dubey, Rachit and Ho, Mark K and Mehta, Hermish and Griffiths,
              Tom",
  abstract = "Psychologists have long been fascinated with understanding the
              nature of Aha! moments, moments when we transition from not
              knowing to suddenly realizing the solution to a problem. In this
              work, we present a theoretical framework that explains why we
              experience Aha! moments. Our theory posits that during
              problem-solving, in addition to solving the problem, people also
              maintain a metacognitive model of their ability to solve the
              problem as well as a prediction about the time it would take them
              to solve that problem. Aha! moments arise when we experience a
              positive error in this metacognitive prediction, i.e. when we
              solve a problem much faster than we expected to solve it. We
              posit that this metacognitive error is analogous to a positive
              reward prediction error thereby explaining why we feel so good
              after an Aha! moment. We provide support to our theory across
              three large-scale pre-registered experiments on problem solving,
              demonstrating a link between metacognitive prediction errors and
              Aha! moments. These results highlight the importance of
              metacognitive prediction errors and deepen our understanding of
              human metareasoning.",
  month    =  jun,
  year     =  2021,
  keywords = "Aha! moment; Insight; metacognition; monitoring and control;
              prediction errors; problem solving; reinforcement
              learning;skimmed;project 1",
  doi      = "10.31234/osf.io/c5v42"
}

@UNPUBLISHED{Allen2023-oj,
  title    = "Using Games to Understand the Mind",
  author   = "Allen, Kelsey R and Br{\"a}ndle, Franziska and Botvinick, Matthew
              and Fan, Judith and Gershman, Samuel J and Gopnik, Alison and
              Griffiths, Thomas L and Hartshorne, Joshua K and Hauser, Tobias U
              and Ho, Mark K and al., Et",
  abstract = "Video games are played by over 2 billion people spread across the
              world population, with both children and adults participating.
              Games have gained popularity as an avenue for studying cognition.
              We believe that studying cognition using games can generate
              progress in psychology and in neuroscience similar to the one
              that has occurred in artificial intelligence research over the
              past decades. Using games to understand the mind enables
              researchers to scale up theories of cognition to more complex
              settings, reverse-engineer human inductive biases, create
              experiments that participants want to take part in, and study
              learning over long time horizons. We describe both the advantages
              and drawbacks of using games relative to standard lab-based
              experiments, and lay out a set of recommendations on how to gain
              the most from using games to study cognition. We hope that this
              article will lead to a wider use of games as experimental
              paradigms, elevating the complexity, robustness, and external
              validity of research on the mind.",
  month    =  feb,
  year     =  2023,
  keywords = "cognition; games; psychology;read;project 1;Paper1",
  doi      = "10.31234/osf.io/hbsvj"
}

@UNPUBLISHED{Bigelow2022-sr,
  title    = "{Non-Commitment} in Mental Imagery",
  author   = "Bigelow, Eric J and McCoy, John and Ullman, Tomer D",
  abstract = "We examine non-commitment in the imagination. Across 5 studies (N
              > 1, 800), we find that most people are non-committal about basic
              aspects of their mental images, including features that would be
              readily apparent in real images. While previous work on the
              imagination has discussed the possibility of non-commitment, this
              paper is the first, to our knowledge, to examine this
              systematically and empirically. We find that people do not commit
              to basic properties of specified mental scenes (Studies 1 and 2),
              and that people report non-commitment rather than uncertainty or
              forgetfulness (Study 3). Such non-commitment is present even for
              people with generally vivid imaginations, and those who report
              imagining the specified scene very vividly (Studies 4a, 4b).
              People readily confabulate properties of their mental images when
              non-commitment is not offered as an explicit option (Study 5).
              Taken together, these results establish non-commitment as a
              pervasive component of mental imagery.",
  month    =  oct,
  year     =  2022,
  keywords = "Imagination; mental imagery; non-commitment; vividness;ARC
              Project;comp-cog-sci",
  doi      = "10.31234/osf.io/pn4zd"
}

@ARTICLE{Dekker2022-lm,
  title     = "Determinants of Human Compositional Generalization",
  author    = "Dekker, Ronald Boris and Otto, Fabian and Summerfield,
               Christopher",
  abstract  = "Generalisation (or transfer) is the ability to repurpose
               knowledge in novel settings. It is often asserted that
               generalisation is an important ingredient of human intelligence,
               but its extent, nature and determinants have proved
               controversial. Here, we re-examine this question with a new
               paradigm that formalises the transfer learning problem as one of
               recomposing existing functions to solve unseen problems. We find
               that people can generalise compositionally in ways that are
               elusive for standard neural networks, and that human
               generalisation benefits from training regimes in which items are
               axis-aligned and temporally correlated. We describe a neural
               network model based around a Hebbian gating process which can
               capture how human generalisation benefits from different
               training curricula. We additionally find that adult humans tend
               to learn composable functions asynchronously, exhibiting
               discontinuities in learning that resemble those seen in child
               development.",
  publisher = "PsyArXiv",
  month     =  mar,
  year      =  2022,
  keywords  = "Cognitive Psychology,Computational Neuroscience,Concepts and
               Categories,generalisation,Learning,neural
               network,Neuroscience,Social and Behavioral Sciences;comp-cog-sci",
  doi       = "10.31234/osf.io/qnpw6"
}

@UNPUBLISHED{Ludwin-Peery2021-pr,
  title    = "Limits on Simulation Approaches in Intuitive Physics",
  author   = "Ludwin-Peery, Ethan and Bramley, Neil R and Davis, Ernest and
              Gureckis, Todd M",
  abstract = "A popular explanation of the human ability for physical reasoning
              is that it depends on a sophisticated ability to perform mental
              simulations. According to this perspective, physical reasoning
              problems are approached by repeatedly simulating relevant aspects
              of a scenario, with noise, and making judgments based on
              aggregation over these simulations. In this paper, we describe
              three core tenets of simulation approaches, theoretical
              commitments that must be present in order for a simulation
              approach to be viable. The identification of these tenets
              threatens the plausibility of simulation as a theory of physical
              reasoning, because they appear to be incompatible with what we
              know about cognition more generally. To investigate this apparent
              contradiction, we describe three experiments involving simple
              physical judgments and predictions, and argue their results
              challenge these core predictions of theories of mental
              simulation.",
  month    =  jan,
  year     =  2021,
  keywords = "project 2",
  doi      = "10.31234/osf.io/xhzuc"
}

@ARTICLE{Ruegsegger2017-yh,
  title     = "Running from {{Disease}}: {{Molecular Mechanisms Associating
               Dopamine}} and {{Leptin Signaling}} in the {{Brain}} with
               {{Physical Inactivity}}, {{Obesity}}, and {{Type}} 2
               {{Diabetes}}",
  author    = "Ruegsegger, Gregory N and Booth, Frank W",
  abstract  = "Physical inactivity is a primary contributor to diseases such as
               obesity, cardiovascular disease, and Type 2 diabetes.
               Accelerometry data suggest that a majority of U.S. adults fail
               to perform substantial levels of physical activity needed to
               improve health. Thus, understanding the molecular factors that
               stimulate physical activity, and physical inactivity, is
               imperative for the development of strategies to reduce sedentary
               behavior and in turn prevent chronic disease. Despite many of
               the well-known health benefits of physical activity being
               described, little is known about genetic and biological factors
               that may influence this complex behavior. The mesolimbic
               dopamine system regulates motivating and rewarding behavior as
               well as motor movement. Here, we present data supporting the
               hypothesis that obesity may mechanistically lower voluntary
               physical activity levels via dopamine dysregulation. In doing
               so, we review data that suggests mesolimbic dopamine activity is
               a strong contributor to voluntary physical activity behavior. We
               also summarize findings suggesting that obesity leads to central
               dopaminergic dysfunction, which in turn contributes to
               reductions in physical activity that often accompany obesity.
               Additionally, we highlight examples in which central leptin
               activity influences physical activity levels in a
               dopamine-dependent manner. Future elucidation of these
               mechanisms will help support strategies to increase physical
               activity levels in obese patients and prevent diseases caused by
               physical inactivity.",
  journal   = "Front. Endocrinol.",
  publisher = "Frontiers",
  volume    =  0,
  year      =  2017,
  keywords  = "Dopamine,Leptin,Motivation,Nucleus Accumbens,physical
               activity,physical inactivity;exercise-brain",
  issn      = "1664-2392",
  doi       = "10.3389/fendo.2017.00109"
}

@ARTICLE{Hawkins2019-jn,
  title    = "A {{Framework}} for {{Intelligence}} and {{Cortical Function
              Based}} on {{Grid Cells}} in the {{Neocortex}}",
  author   = "Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy,
              Scott and Ahmad, Subutai",
  journal  = "Front. Neural Circuits",
  volume   =  12,
  pages    = "121",
  month    =  jan,
  year     =  2019,
  keywords = "comp-cog-sci",
  issn     = "1662-5110",
  doi      = "10.3389/fncir.2018.00121"
}

@ARTICLE{Wilson2013-cg,
  title    = "Embodied Cognition is Not What you Think it is",
  author   = "Wilson, Andrew D and Golonka, Sabrina",
  abstract = "The most exciting hypothesis in cognitive science right now is
              the theory that cognition is embodied. Like all good ideas in
              cognitive science, however, embodiment immediately came to mean
              six different things. The most common definitions involve the
              straight-forward claim that ``states of the body modify states of
              the mind.'' However, the implications of embodiment are actually
              much more radical than this. If cognition can span the brain,
              body, and the environment, then the ``states of mind'' of
              disembodied cognitive science won't exist to be modified.
              Cognition will instead be an extended system assembled from a
              broad array of resources. Taking embodiment seriously therefore
              requires both new methods and theory. Here we outline four key
              steps that research programs should follow in order to fully
              engage with the implications of embodiment. The first step is to
              conduct a task analysis, which characterizes from a first person
              perspective the specific task that a perceiving-acting cognitive
              agent is faced with. The second step is to identify the
              task-relevant resources the agent has access to in order to solve
              the task. These resources can span brain, body, and environment.
              The third step is to identify how the agent can assemble these
              resources into a system capable of solving the problem at hand.
              The last step is to test the agent's performance to confirm that
              agent is actually using the solution identified in step 3. We
              explore these steps in more detail with reference to two useful
              examples (the outfielder problem and the A-not-B error), and
              introduce how to apply this analysis to the thorny question of
              language use. Embodied cognition is more than we think it is, and
              we have the tools we need to realize its full potential.",
  journal  = "Front. Psychol.",
  volume   =  4,
  pages    = "58",
  month    =  feb,
  year     =  2013,
  keywords = "A-not-B error; dynamical systems; embodied cognition; language;
              outfielder problem; replacement hypothesis; robotics;project
              1;cog-sci",
  language = "en",
  issn     = "1664-1078",
  pmid     = "23408669",
  doi      = "10.3389/fpsyg.2013.00058",
  pmc      = "PMC3569617"
}

@ARTICLE{Fedor2017-kh,
  title    = "Cognitive Architecture with Evolutionary Dynamics Solves Insight
              Problem",
  author   = "Fedor, Anna and Zachar, Istv{\'a}n and Szil{\'a}gyi, Andr{\'a}s
              and {\"O}llinger, Michael and de Vladar, Harold P and
              Szathm{\'a}ry, E{\"o}rs",
  abstract = "In this paper, we show that a neurally implemented a cognitive
              architecture with evolutionary dynamics can solve the four-tree
              problem. Our model, called Darwinian Neurodynamics, assumes that
              the unconscious mechanism of problem solving during insight tasks
              is a Darwinian process. It is based on the evolution of patterns
              that represent candidate solutions to a problem, and are stored
              and reproduced by a population of attractor networks. In our
              first experiment, we used human data as a benchmark and showed
              that the model behaves comparably to humans: it shows an
              improvement in performance if it is pretrained and primed
              appropriately, just like human participants in Kershaw et al.
              (2013)'s experiment. In the second experiment, we further
              investigated the effects of pretraining and priming in a
              two-by-two design and found a beginner's luck type of effect:
              solution rate was highest in the condition that was primed, but
              not pretrained with patterns relevant for the task. In the third
              experiment, we showed that deficits in computational capacity and
              learning abilities decreased the performance of the model, as
              expected. We conclude that Darwinian Neurodynamics is a promising
              model of human problem solving that deserves further
              investigation.",
  journal  = "Front. Psychol.",
  volume   =  8,
  pages    = "427",
  month    =  mar,
  year     =  2017,
  keywords = "Darwinian Neurodynamics; attractor networks; evolutionary search;
              four-tree problem; insight;project 1",
  language = "en",
  issn     = "1664-1078",
  pmid     = "28405191",
  doi      = "10.3389/fpsyg.2017.00427",
  pmc      = "PMC5370243"
}

@ARTICLE{Hung2018-fi,
  title     = "Effect of {{Acute Exercise Mode}} on {{Serum {Brain-Derived}
               Neurotrophic Factor}} ({{{BDNF}}}) and {{Task Switching
               Performance}}",
  author    = "Hung, Chiao-Ling and Tseng, Jun-Wei and Chao, Hsiao-Han and
               Hung, Tsung-Min and Wang, Ho-Seng",
  abstract  = "Previous studies have consistently reported a positive effect of
               acute exercise on cognition, particularly on executive function.
               However, most studies have focused on aerobic and resistant
               forms of exercise. The purpose of this study was to compare the
               effect of `open-skill' with `closed-skill' exercise (defined in
               terms of the predictability of the performing environment) on
               brain-derived neurotrophic factor (BDNF) production and task
               switching performance. Twenty young adult males participated in
               both closed (running) and open (badminton) skill exercise
               sessions in a counterbalanced order on separate days. The
               exercise sessions consisted of 5 min of warm up exercises
               followed by 30 min of running or badminton. The exercise
               intensity was set at 60\% ($\pm$5\%) of the heart rate reserve
               level (HRR) with HR being monitored by a wireless heart rate
               monitor. Blood samples were taken and participation in a
               task-switching paradigm occurred before and after each exercise
               session. Results showed no differences in serum BDNF or
               task-switching performance at the pre-test stage, however,
               badminton exercise resulted in significantly higher serum BDNF
               levels (a proxy for levels of BDNF in the brain) and near
               significant smaller global switching costs relative to running.
               This study has provided preliminary evidence in support the
               relative benefits of open-skills exercises on BDNF and executive
               function.",
  journal   = "J. Clin. Med. Res.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  10,
  pages     = "301",
  month     =  oct,
  year      =  2018,
  keywords  = "closed-skill,executive function,open-skill,switch
               cost;exercise-brain",
  issn      = "1918-3003",
  doi       = "10.3390/jcm7100301"
}

@ARTICLE{Shiffrin1997-xr,
  title    = "A model for recognition memory: {REM-retrieving} effectively from
              memory",
  author   = "Shiffrin, R M and Steyvers, M",
  abstract = "A new model of recognition memory is reported. This model is
              placed within, and introduces, a more elaborate theory that is
              being developed to predict the phenomena of explicit and
              implicit, and episodic and generic, memory. The recognition model
              is applied to basic findings, including phenomena that pose
              problems for extant models: the list-strength effect (e.g.,
              Ratcliff, Clark, \& Shiffrin, 1990), the mirror effect (e.g.,
              Glanzer \& Adams, 1990), and the normal-ROC slope effect (e.g.,
              Ratcliff, McKoon, \& Tindall, 1994). The model assumes storage of
              separate episodic images for different words, each image
              consisting of a vector of feature values. Each image is an
              incomplete and error prone copy of the studied vector. For the
              simplest case, it is possible to calculate the probability that a
              test item is ``old,'' and it is assumed that a default ``old''
              response is given if this probability is greater than .5. It is
              demonstrated that this model and its more complete and realistic
              versions produce excellent qualitative predictions.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  4,
  number   =  2,
  pages    = "145--166",
  month    =  jun,
  year     =  1997,
  keywords = "learning\&memory2023",
  language = "en",
  issn     = "1069-9384",
  pmid     = "21331823",
  doi      = "10.3758/BF03209391"
}

@ARTICLE{Wixted1997-ll,
  title    = "Genuine power curves in forgetting: A quantitative analysis of
              individual subject forgetting functions",
  author   = "Wixted, John T and Ebbesen, Ebbe B",
  abstract = "Comments on R. B. Anderson and R. D. Tweney's (1997) article
              which argues that the power law of forgetting may be an artifact
              of arithmetically averaging individual subject forgetting
              functions that are truly exponential in form and that geometric
              averaging would avoid this potential problem. The authors agree
              that researchers should always be cognizant of the possibility of
              averaging artifacts, but they also show that their conclusions
              about the form of forgetting remain unchanged (and
              goodness-of-fit statistics are scarcely affected by) whether
              arithmetic or geometric averaging is used. An analysis of
              individual subject forgetting functions shows that they, too, are
              described much better by a power function than by an exponential.
              (PsycINFO Database Record (c) 2016 APA, all rights reserved)",
  journal  = "Mem. Cognit.",
  volume   =  25,
  number   =  5,
  pages    = "731--739",
  month    =  sep,
  year     =  1997,
  keywords = "l\&m final",
  issn     = "0090-502X, 1532-5946",
  doi      = "10.3758/BF03211316"
}

@ARTICLE{Ash2006-bc,
  title    = "The nature of restructuring in insight: an individual-differences
              approach",
  author   = "Ash, Ivan K and Wiley, Jennifer",
  abstract = "The insightful problem-solving process has been proposed to
              involve three main phases: an initial representation phase, in
              which the solver inappropriately represents the problem; an
              initial search through the faulty problem space that may lead to
              impasse; and a postimpasse restructuring phase. Some theories
              propose that the restructuring phase involves controlled search
              processes, whereas other theories propose that restructuring is
              achieved through the automatic redistribution of activation in
              long-term memory. In this study, we used correlations between
              working memory (WM) span measures and problem-solving success to
              test the predictions of these different theories. One group of
              participants received a set of insight problems that allowed for
              a large initial faulty search space, whereas another group
              received a matched set that constrained the initial faulty search
              space in order to isolate the restructuring phase of the
              insightful process. The results suggest that increased ability to
              control attention (as measured by WM span tasks) predicts an
              individual's ability to successfully solve problems that involve
              both the initial search phase and the restructuring phase.
              However, individual differences in ability to control attention
              do not predict success on problems that isolate the restructuring
              phase. These results are interpreted as supporting an
              automatic-redistribution-of-activation account of restructuring.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  13,
  number   =  1,
  pages    = "66--73",
  month    =  feb,
  year     =  2006,
  keywords = "project 1;cog-sci",
  language = "en",
  issn     = "1069-9384",
  pmid     = "16724770",
  doi      = "10.3758/bf03193814"
}

@ARTICLE{Barbosa2023-cx,
  title    = "A practical guide for studying human behavior in the lab",
  author   = "Barbosa, Joao and Stein, Heike and Zorowitz, Sam and Niv, Yael
              and Summerfield, Christopher and Soto-Faraco, Salvador and
              Hyafil, Alexandre",
  abstract = "In the last few decades, the field of neuroscience has witnessed
              major technological advances that have allowed researchers to
              measure and control neural activity with great detail. Yet,
              behavioral experiments in humans remain an essential approach to
              investigate the mysteries of the mind. Their relatively modest
              technological and economic requisites make behavioral research an
              attractive and accessible experimental avenue for neuroscientists
              with very diverse backgrounds. However, like any experimental
              enterprise, it has its own inherent challenges that may pose
              practical hurdles, especially to less experienced behavioral
              researchers. Here, we aim at providing a practical guide for a
              steady walk through the workflow of a typical behavioral
              experiment with human subjects. This primer concerns the design
              of an experimental protocol, research ethics, and subject care,
              as well as best practices for data collection, analysis, and
              sharing. The goal is to provide clear instructions for both
              beginners and experienced researchers from diverse backgrounds in
              planning behavioral experiments.",
  journal  = "Behav. Res. Methods",
  volume   =  55,
  number   =  1,
  pages    = "58--76",
  month    =  jan,
  year     =  2023,
  keywords = "10 rules; Good practices; Human behavioral experiments; Open
              science; Study design",
  language = "en",
  issn     = "1554-351X, 1554-3528",
  pmid     = "35262897",
  doi      = "10.3758/s13428-022-01793-9"
}

@INCOLLECTION{Tolman1948-cr,
  title     = "Cognitive maps in rats and men",
  booktitle = "Image and Environment: Cognitive Mapping and Spatial Behavior",
  author    = "Tolman, E C",
  abstract  = "I shall devote the body of this paper to a description of
               experiments with rats. But I shall also attempt in a few words
               at the close to indicate the significance of these findings on
               rats for the clinical behavior of men. Most of the rat
               investigations, which I shall report, were carried out in the
               Berkeley laboratory. But I shall also include, occasionally,
               accounts of the behavior of non-Berkeley rats who obviously have
               misspent their lives in out-of-State laboratories. Furthermore,
               in reporting our Berkeley experiments I shall have to omit a
               very great many. The ones I shall talk about were carried out by
               graduate students (or underpaid research assistants) who,
               supposedly, got some of their ideas from me. And a few, though a
               very few, were even carried out by me myself. \copyright{} 1973
               by Taylor \& Francis.",
  publisher = "Taylor and Francis",
  pages     = "27--50",
  year      =  1948,
  keywords  = "read;learning\&memory2023",
  isbn      = "9781351513647, 9780202307664",
  doi       = "10.4324/9780203789155-11"
}

@ARTICLE{Rouault2022-cb,
  title    = "Controllability boosts neural and cognitive signatures of
              changes-of-mind in uncertain environments",
  author   = "Rouault, Marion and Weiss, Aur{\'e}lien and Lee, Junseok K and
              Drugowitsch, Jan and Chambon, Valerian and Wyart, Valentin",
  abstract = "In uncertain environments, seeking information about alternative
              choice options is essential for adaptive learning and
              decision-making. However, information seeking is usually
              confounded with changes-of-mind about the reliability of the
              preferred option. Here, we exploited the fact that information
              seeking requires control over which option to sample to isolate
              its behavioral and neurophysiological signatures. We found that
              changes-of-mind occurring with control require more evidence
              against the current option, are associated with reduced
              confidence, but are nevertheless more likely to be confirmed on
              the next decision. Multimodal neurophysiological recordings
              showed that these changes-of-mind are preceded by stronger
              activation of the dorsal attention network in
              magnetoencephalography, and followed by increased pupil-linked
              arousal during the presentation of decision outcomes. Together,
              these findings indicate that information seeking increases the
              saliency of evidence perceived as the direct consequence of one's
              own actions.",
  journal  = "Elife",
  volume   =  11,
  month    =  sep,
  year     =  2022,
  keywords = "confidence; decision-making; exploration; human; inference;
              information seeking; neuroscience;BAMB2023",
  language = "en",
  issn     = "2050-084X",
  pmid     = "36097814",
  doi      = "10.7554/eLife.75038",
  pmc      = "PMC9470160"
}

@ARTICLE{Chu2007-xx,
  title    = "Theory Driven Hints in the Cheap Necklace Problem: A Preliminary
              Investigation",
  author   = "Chu, Yun and Dewald, Andrew D and Chronicle, Edward P",
  abstract = "Three experiments investigated the effects of two hints derived
              from the Criterion for Satisfactory Progress theory (CSP) and
              Representational Change Theory (RCT) on the cheap necklace
              problem (insight problem). In Experiment 1, fewer participants
              given the CSP hint used an incorrect (maximizing) first move than
              participants given the RCT hint or control participants given no
              hint on a single attempt at the problem. Experiment 2 found the
              number of trials to solution was fewer in the CSP condition than
              in the control over ten trials, and there were fewer incorrect
              first moves in the CSP. The results appear to support the CSP
              theory. However, in Experiment 3, the CSP and RCT hints were
              combined yielding a 75\% solution rate over 34.88\% in the
              control. Perhaps aspects from both theories are employed during
              the problem solving process.",
  journal  = "The Journal of Problem Solving",
  volume   =  1,
  number   =  2,
  pages    = "4",
  year     =  2007,
  keywords = "project 1",
  issn     = "1932-6246",
  doi      = "10.7771/1932-6246.1010"
}

@ARTICLE{Batchelder2012-xv,
  title    = "Insight Problem Solving: A Critical Examination of the
              Possibility of Formal Theory",
  author   = "Batchelder, William H and Alexander, Gregory E",
  abstract = "This paper provides a critical examination of the current state
              and future possibility of formal cognitive theory for insight
              problem solving and its associated ``aha!'' experience. Insight
              problems are contrasted with move problems, which have been
              formally defined and studied extensively by cognitive
              psychologists since the pioneering work of Alan Newell and
              Herbert Simon. To facilitate our discussion, a number of
              classical brainteasers are presented along with their solutions
              and some conclusions derived from observing the behavior of many
              students trying to solve them. Some of these problems are
              interesting in their own right, and many of them have not been
              discussed before in the psychological literature. The main
              purpose of presenting the brainteasers is to assist in discussing
              the status of formal cognitive theory for insight problem
              solving, which is argued to be considerably weaker than that
              found in other areas of higher cognition such as human memory,
              decision-making, categorization, and perception. We discuss
              theoretical barriers that have plagued the development of
              successful formal theory for insight problem solving. A few
              suggestions are made that might serve to advance the field.",
  journal  = "The Journal of Problem Solving",
  volume   =  5,
  number   =  1,
  pages    = "6",
  year     =  2012,
  keywords = "project 1",
  issn     = "1932-6246",
  doi      = "10.7771/1932-6246.1143"
}

@ARTICLE{Choromanska_undated-tg,
  title    = "The {{Loss Surfaces}} of {{Multilayer Networks}}",
  author   = "Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and
              Arous, Gerard Ben and LeCun, Yann",
  pages    = "13",
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Ho_undated-eh,
  title       = "Rational simplification in human planning",
  author      = "Ho, Mark K and Cohen, Jonathan D and Griffiths, Thomas L",
  institution = "Princeton University",
  keywords    = "read;project 2"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Alayrac2022-ur,
  title     = "Flamingo: a visual language model for few-shot learning",
  author    = "Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and
               Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel
               and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm
               and {Others}",
  abstract  = "â€¦ Similarly, we demonstrate that the way we train the Flamingo
               models is crucial for â€¦ Flamingo on all nine tasks where
               Flamingo does not achieve SotA with few-shot learning. Flamingo
               â€¦",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "proceedings.neurips.cc",
  volume    =  35,
  pages     = "23716--23736",
  year      =  2022,
  keywords  = "skimmed;ARC Project",
  issn      = "1049-5258"
}

@TECHREPORT{Smolensky2022-ts,
  title       = "Neurocompositional Computing in Human and Machine
                 Intelligence: {{A}} Tutorial",
  author      = "Smolensky, Paul and McCoy, R Thomas and Fernandez, Roland and
                 Goldrick, Matthew and Gao, Jianfeng",
  abstract    = "The past decade has produced a revolution in Artificial
                 Intelligence (AI), after a half-century of AI repeatedly
                 failing to meet expectations. What explains the dramatic
                 change from 20th-century to 21st-century AI, and how can
                 remaining limitations of current AI be overcome? Until now,
                 the widely accepted narrative has attributed the recent
                 progress in AI to technical engineering advances that have
                 yielded massive increases in the quantity of computational
                 resources and training data available to support statistical
                 learning in deep artificial neural networks. Although these
                 quantitative engineering innovations are important, here we
                 show that the latest advances in AI are not solely due to
                 quantitative increases in computing power but also qualitative
                 changes in how that computing power is deployed. These
                 qualitative changes have brought about a new type of computing
                 that we call neurocompositional computing. In
                 neurocompositional computing, neural networks exploit two
                 scientific principles that contemporary theory in cognitive
                 science maintains are simultaneously necessary to enable
                 human-level cognition. The Compositionality Principle asserts
                 that encodings of complex information are structures that are
                 systematically composed from simpler structured encodings. The
                 Continuity Principle states that the encoding and processing
                 of information is formalized with real numbers that vary
                 continuously. These principles have seemed irreconcilable
                 until the recent mathematical discovery that compositionality
                 can be realized not only through the traditional discrete
                 methods of symbolic computing, well developed in 20th-century
                 AI, but also through novel forms of continuous neural
                 computing---neurocompositional computing. The unprecedented
                 progress of 21st-century AI has resulted from the use of
                 limited---first-generation---forms of neurocompositional
                 computing. We show that the new techniques now being deployed
                 in second-generation neurocompositional computing create AI
                 systems that are not only more robust and accurate than
                 current systems, but also more comprehensible---making it
                 possible to diagnose errors in, and exert human control over,
                 artificial neural networks through interpretation of their
                 internal states and direct intervention upon those states.
                 Note: This tutorial is intended for those new to this topic,
                 and does not assume familiarity with cognitive science, AI, or
                 deep learning. Appendices provide more advanced material. Each
                 figure, and the associated box explaining it, provides an
                 exposition, illustration, or further details of a main point
                 of the paper; in order to make these figures relatively
                 self-contained, it has sometimes been necessary to repeat some
                 material from the text. For a brief introduction and
                 additional development of some of this material see
                 ``Neurocompositional computing: From the central paradox of
                 cognition to a new generation of ai systems''
                 (arXiv:2205.01128; to appear, AI Magazine)",
  institution = "Microsoft",
  month       =  may,
  year        =  2022,
  keywords    = "read;comp-cog-sci"
}

@ARTICLE{Wolpert1996-yw,
  title    = "The Existence of A Priori Distinctions Between Learning
              Algorithms",
  author   = "Wolpert', David H",
  journal  = "Neural Coriipiitntioii",
  volume   =  8,
  pages    = "1391--1420",
  year     =  1996,
  keywords = "project 1;machine-learning;l\&m final;Paper1"
}

@UNPUBLISHED{Assouel2022-er,
  title    = "Object-centric Compositional Imagination for Visual Abstract
              Reasoning",
  author   = "Assouel, Rim and Rodriguez, Pau and Taslakian, Perouz and
              Vazquez, David and Bengio, Yoshua",
  abstract = "Like humans devoid of imagination, current machine learning
              systems lack the ability to adapt to new, unexpected situations
              by foreseeing them, which makes them unable to solve new tasks by
              analogical reasoning. In this work, we introduce a new
              compositional imagination framework that improves a model's
              ability to generalize. One of the key components of our framework
              is object-centric inductive biases that enables models to
              perceive the environment as a series of objects, properties, and
              transformations. By composing these key ingredients, it is
              possible to generate new unseen tasks that, when used to train
              the model, improve generalization. Experiments on a simplified
              version of the Abstraction and Reasoning Corpus (ARC) demonstrate
              the effectiveness of our framework.",
  month    =  jun,
  year     =  2022,
  keywords = "ARC Project;comp-cog-sci"
}

@INPROCEEDINGS{Lerer2016-kb,
  title      = "Learning {{Physical Intuition}} of {{Block Towers}} by
                {{Example}}",
  booktitle  = "International {{Conference}} on {{Machine Learning}}",
  author     = "Lerer, Adam and Gross, Sam and Fergus, Rob",
  publisher  = "PMLR",
  pages      = "430--438",
  month      =  jun,
  year       =  2016,
  keywords   = "comp-cog-sci",
  conference = "International Conference on Machine Learning"
}

@MISC{Lampinen_undated-zq,
  title    = "Tell me why! Explanations support learning relational and causal
              structure",
  author   = "Lampinen, Andrew K and Roy, Nicholas A and Dasgupta, Ishita and
              Chan, Stephanie C Y and Tam, Allison C and Mc Clelland, James L
              and Yan, Chen and Santoro, Adam and Rabinowitz, Neil C and Wang,
              Jane X and Hill, Felix",
  keywords = "ARC Project;comp-cog-sci"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Austerweil2015-pf,
  title     = "Structure and flexibility in Bayesian models of cognition",
  author    = "Austerweil, Joseph L and Gershman, Samuel J and Tenenbaum,
               Joshua B and Griffiths, Thomas L",
  abstract  = "â€¦ We discuss nonparametric Bayesian models as a potential answer
               to this â€¦ Bayesian models . We then delve into nonparametric
               Bayesian models for three types of hidden structure : â€¦",
  journal   = "Oxford handbook of computational and mathematical psychology",
  publisher = "Oxford University Press Oxford, UK",
  pages     = "187--208",
  year      =  2015,
  keywords  = "read;project 2"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rescorla1972-fu,
  title     = "A theory of Pavlovian conditioning: Variations in the
               effectiveness of reinforcement and non-reinforcement",
  author    = "Rescorla, Robert A",
  abstract  = "A theory of Pavlovian conditioning : Variations in the
               effectiveness of reinforcement and non - reinforcement | CiNii
               Research â€¦ A theory of Pavlovian conditioning : Variations in
               the effectiveness of reinforcement and non - reinforcement â€¦
               Classical conditioning , Current research and theory 2 64-69,
               1972 â€¦",
  journal   = "Classical conditioning, Current research and theory",
  publisher = "Appleton-Century-Crofts",
  volume    =  2,
  pages     = "64--69",
  year      =  1972,
  keywords  = "skimmed;learning\&memory2023"
}

@UNPUBLISHED{Patel2022-fg,
  title    = "Mapping Language Models to Grounded Conceptual Spaces",
  author   = "Patel, Roma and Pavlick, Ellie",
  abstract = "A fundamental criticism of text-only language models (LMs) is
              their lack of grounding---that is, the ability to tie a word for
              which they have learned a representation, to its actual use in
              the world. However, despite this limitation, large pre-trained
              LMs have been shown to have a remarkable grasp of the conceptual
              structure of language, as demonstrated by their ability to answer
              questions, generate fluent text, or make inferences about
              entities, objects, and properties that they have never physically
              observed. In this work we investigate the extent to which the
              rich conceptual structure that LMs learn indeed reflects the
              conceptual structure of the non-linguistic world---which is
              something that LMs have never observed. We do this by testing
              whether the LMs can learn to map an entire conceptual domain
              (e.g., direction or colour) onto a grounded world representation
              given only a small number of examples. For example, we show a
              model what the word ``left`` means using a textual depiction of a
              grid world, and assess how well it can generalise to related
              concepts, for example, the word ``right'', in a similar grid
              world. We investigate a range of generative language models of
              varying sizes (including GPT-2 and GPT-3), and see that although
              the smaller models struggle to perform this mapping, the largest
              model can not only learn to ground the concepts that it is
              explicitly taught, but appears to generalise to several instances
              of unseen concepts as well. Our results suggest an alternative
              means of building grounded language models: rather than learning
              grounded representations ``from scratch'', it is possible that
              large text-only models learn a sufficiently rich conceptual
              structure that could allow them to be grounded in a
              data-efficient way.",
  month    =  may,
  year     =  2022,
  keywords = "ARC Project;comp-cog-sci"
}

@ARTICLE{Goodman2014-kq,
  title    = "Concepts in a {{Probabilistic Language}} of {{Thought}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, Tobias",
  pages    = "25",
  year     =  2014,
  keywords = "read;comp-cog-sci"
}

@ARTICLE{Vaswani_undated-fb,
  title    = "Attention Is {{All}} You {{Need}}",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz
              and Polosukhin, Illia",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks that include an
              encoder and a decoder. The best performing models also connect
              the encoder and decoder through an attention mechanism. We
              propose a new simple network architecture, the Transformer, based
              solely on attention mechanisms, dispensing with recurrence and
              convolutions entirely. Experiments on two machine translation
              tasks show these models to be superior in quality while being
              more parallelizable and requiring significantly less time to
              train. Our model achieves 28.4 BLEU on the WMT 2014
              Englishto-German translation task, improving over the existing
              best results, including ensembles, by over 2 BLEU. On the WMT
              2014 English-to-French translation task, our model establishes a
              new single-model state-of-the-art BLEU score of 41.0 after
              training for 3.5 days on eight GPUs, a small fraction of the
              training costs of the best models from the literature.",
  pages    = "11",
  keywords = "machine-learning"
}

@INBOOK{2021-fc,
  title     = "Perceptron",
  booktitle = "Wikipedia",
  author    = "{Wikipedia}",
  abstract  = "In machine learning, the perceptron is an algorithm for
               supervised learning of binary classifiers. A binary classifier
               is a function which can decide whether or not an input,
               represented by a vector of numbers, belongs to some specific
               class. It is a type of linear classifier, i.e. a classification
               algorithm that makes its predictions based on a linear predictor
               function combining a set of weights with the feature vector.",
  month     =  aug,
  year      =  2021,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Watkins1992-pk,
  title     = "{Q-Learning}",
  author    = "Watkins, Christopher Jch and Dayan, Peter",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  8,
  number    = "3-4",
  pages     = "279--292",
  year      =  1992,
  keywords  = "machine-learning",
  issn      = "0885-6125"
}

@ARTICLE{Carey1978-gm,
  title    = "Acquiring a {{Single New Word}}",
  author   = "Carey, Susan and Bartlett, Elsa",
  volume   =  15,
  pages    = "14",
  year     =  1978,
  keywords = "comp-cog-sci"
}

@INPROCEEDINGS{Gerstenberg2015-lf,
  title     = "How, whether, why: Causal judgments as counterfactual contrasts",
  booktitle = "{CogSci}",
  author    = "Gerstenberg, Tobias and Goodman, Noah D and Lagnado, David A and
               Tenenbaum, Joshua B",
  year      =  2015,
  keywords  = "comp-cog-sci;project 2"
}

@MISC{Wang_undated-gn,
  title        = "Glue: A multi-task benchmark and analysis platform for
                  natural language {understand-ING}",
  author       = "Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill,
                  Felix and Levy, Omer and Bowman, Samuel R",
  abstract     = "For natural language understanding (NLU) technology to be
                  maximally useful, it must be able to process language in a
                  way that is not exclusive to a single task, genre, or
                  dataset. In pursuit of this objective, we introduce the
                  General Language Understanding Evaluation (GLUE) benchmark, a
                  collection of tools for evaluating the performance of models
                  across a diverse set of existing NLU tasks. By including
                  tasks with limited training data, GLUE is designed to favor
                  and encourage models that share general linguistic knowledge
                  across tasks. GLUE also includes a hand-crafted diagnostic
                  test suite that enables detailed linguistic analysis of
                  models. We evaluate baselines based on current methods for
                  transfer and representation learning and find that multi-task
                  training on all tasks performs better than training a
                  separate model per task. However, the low absolute
                  performance of our best model indicates the need for improved
                  general NLU systems.",
  howpublished = "\url{https://openreview.net/pdf?id=rJ4km2R5t7}",
  note         = "Accessed: 2022-10-21",
  keywords     = "nlp"
}

@ARTICLE{Pitt_undated-sp,
  title    = "Exact {{Number Concepts Are Limited}} to the {{Verbal Count
              Range}}",
  author   = "Pitt, Benjamin and Gibson, Edward and Piantadosi, Steven T",
  abstract = "Previous findings suggest that mentally representing exact
              numbers larger than four depends on a verbal count routine (e.g.,
              ``one, two, three . . .''). However, these findings are
              controversial because they rely on comparisons across radically
              different languages and cultures. We tested the role of language
              in number concepts within a single population---the Tsimane' of
              Bolivia---in which knowledge of number words varies across
              individual adults. We used a novel data-analysis model to
              quantify the point at which participants (N = 30) switched from
              exact to approximate number representations during a simple
              numerical matching task. The results show that these behavioral
              switch points were bounded by participants' verbal count ranges;
              their representations of exact cardinalities were limited to the
              number words they knew. Beyond that range, they resorted to
              numerical approximation. These results resolve competing accounts
              of previous findings and provide unambiguous evidence that large
              exact number concepts are enabled by language.",
  pages    = "11",
  keywords = "comp-cog-sci"
}

@MISC{Wood_undated-zq,
  title        = "Particle filtering for nonparametric Bayesian matrix
                  factorization",
  author       = "Wood, Frank and Griffiths, Thomas L",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2006/file/38ed162a0dbef7b3fe0f628aa08b90e7-Paper.pdf}",
  note         = "Accessed: 2023-3-9"
}

@ARTICLE{Lake_undated-bl,
  title    = "Concept Learning as Motor Program Induction: {{A}} {Large-Scale}
              Empirical Study",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  abstract = "Human concept learning is particularly impressive in two
              respects: the internal structure of concepts can be
              representationally rich, and yet the very same concepts can also
              be learned from just a few examples. Several decades of research
              have dramatically advanced our understanding of these two aspects
              of concepts. While the richness and speed of concept learning are
              most often studied in isolation, the power of human concepts may
              be best explained through their synthesis. This paper presents a
              large-scale empirical study of one-shot concept learning,
              suggesting that rich generative knowledge in the form of a motor
              program can be induced from just a single example of a novel
              concept. Participants were asked to draw novel handwritten
              characters given a reference form, and we recorded the motor data
              used for production. Multiple drawers of the same character not
              only produced visually similar drawings, but they also showed a
              striking correspondence in their strokes, as measured by their
              number, shape, order, and direction. This suggests that
              participants can infer a rich motorbased concept from a single
              example. We also show that the motor programs induced by
              individual subjects provide a powerful basis for one-shot
              classification, yielding far higher accuracy than
              state-of-the-art pattern recognition methods based on just the
              visual form.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Nye2020-np,
  title    = "Learning compositional rules via neural program synthesis",
  author   = "Nye, Maxwell and Solar-Lezama, Armando and Tenenbaum, Josh and
              Lake, Brenden M",
  abstract = "Many aspects of human reasoning, including language, require
              learning rules from very little data. Humans can do this, often
              learning systematic rules from very few examples, and combining
              these rules to form compositional rule-based systems. Current
              neural architectures, on the other hand, often fail to generalize
              in a compositional manner, especially when evaluated in ways that
              vary systematically from training. In this work, we present a
              neuro-symbolic model which learns entire rule systems from a
              small set of examples. Instead of directly predicting outputs
              from inputs, we train our model to induce the explicit system of
              rules governing a set of previously seen examples, drawing upon
              techniques from the neural program synthesis literature. Our
              rule-synthesis approach outperforms neural meta-learning
              techniques in three domains: an artificial instruction-learning
              domain used to evaluate human learning, the SCAN challenge
              datasets, and learning rule-based translations of number words
              into integers for a wide range of human languages.",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  33,
  pages    = "10832--10842",
  year     =  2020,
  keywords = "read;comp-cog-sci",
  issn     = "1049-5258"
}

@ARTICLE{Lloyd2014-ua,
  title    = "Automatic {{Construction}} and {{{Natural-Language} Description}}
              of {{Nonparametric Regression Models}}",
  author   = "Lloyd, James and Duvenaud, David and Grosse, Roger and Tenenbaum,
              Joshua and Ghahramani, Zoubin",
  abstract = "This paper presents the beginnings of an automatic statistician,
              focusing on regression problems. Our system explores an
              open-ended space of statistical models to discover a good
              explanation of a data set, and then produces a detailed report
              with figures and natural-language text. Our approach treats
              unknown regression functions nonparametrically using Gaussian
              processes, which has two important consequences. First, Gaussian
              processes can model functions in terms of high-level properties
              (e.g. smoothness, trends, periodicity, changepoints). Taken
              together with the compositional structure of our language of
              models this allows us to automatically describe functions in
              simple terms. Second, the use of flexible nonparametric models
              and a rich language for composing them in an open-ended manner
              also results in state-of-the-art extrapolation performance
              evaluated over 13 real time series data sets from various
              domains.",
  journal  = "Proc. Conf. AAAI Artif. Intell.",
  volume   =  28,
  number   =  1,
  month    =  jun,
  year     =  2014,
  keywords = "Regression;comp-cog-sci",
  issn     = "2159-5399, 2374-3468"
}

@INCOLLECTION{McClelland1988-qm,
  title     = "The appeal of parallel distributed processing",
  booktitle = "Readings in cognitive science: A perspective from psychology and
               artificial intelligence , (pp",
  author    = "McClelland, James L and Rumelhart, David E and Hinton, G E",
  editor    = "Collins, Allan M",
  abstract  = "multiple simultaneous constraints parallel distributed
               processing [PDP] / examples of PDP models representation and
               learning in PDP models origins of parallel distributed
               processing (PsycInfo Database Record (c) 2022 APA, all rights
               reserved)",
  volume    =  661,
  pages     = "52--72",
  year      =  1988,
  keywords  = "read;ccm2023;comp-cog-sci"
}

@BOOK{Rojas2013-pv,
  title     = "Neural Networks: A Systematic Introduction",
  author    = "Rojas, Ra{\'u}l",
  publisher = "Springer Science \& Business Media",
  year      =  2013,
  keywords  = "machine-learning",
  isbn      = "9783642610684"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Eichenbaum2008-pi,
  title        = "Learning \& memory",
  author       = "Eichenbaum, H",
  abstract     = "â€¦ Next, Eichenbaum incorporates animal and human â€¦ Eichenbaum
                  organizes the text around multiple memory systems, moving
                  from simple to more complex forms of learning and memory â€¦",
  publisher    = "aabmc.org",
  year         =  2008,
  howpublished = "\url{https://aabmc.org/sites/default/files/webform/stories-photos/pdf-learning--memory-howard-eichenbaum-pdf-download-free-book-3e8075a.pdf}",
  note         = "Accessed: 2023-1-30",
  keywords     = "learning\&memory2023;l\&m final"
}

@ARTICLE{Nobandegani_undated-cc,
  title    = "Example {{Generation Under Constraints Using Cascade Correlation
              Neural Nets}}",
  author   = "Nobandegani, Ardavan S and Shultz, Thomas R",
  abstract = "Humans not only can effortlessly imagine a wide range of novel
              instances and scenarios when prompted (e.g., a new shirt), but
              more remarkably, they can adequately generate examples which
              satisfy a given set of constraints (e.g., a new, dotted, pink
              shirt). Recently, Nobandegani and Shultz (2017) proposed a
              framework which permits converting deterministic, discriminative
              neural nets into probabilistic generative models. In this work,
              we formally show that an extension of this framework allows for
              generating examples under a wide range of constraints.
              Furthermore, we show that this framework is consistent with
              developmental findings on children's generative abilities, and
              can account for a developmental shift in infants' probabilistic
              learning and reasoning. We discuss the importance of integrating
              Bayesian and connectionist approaches to computational
              developmental psychology, and how our work contributes to that
              research.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Wellman_undated-aj,
  title    = "Cognitive {{Development}}: {{Foundational Theories}} of {{Core
              Domains}}",
  author   = "Wellman, Henry M and Gelman, Susan A",
  pages    = "39",
  keywords = "development"
}

@INCOLLECTION{Gureckis2015-eh,
  title     = "Reinforcement learning: A computational perspective",
  booktitle = "Oxford handbook of computational and mathematical psychology",
  author    = "Gureckis, Todd and Love, B C",
  publisher = "Oxford University Press",
  year      =  2015,
  keywords  = "read;RL;ccm2023"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Newell1972-xp,
  title    = "Human problem solving",
  author   = "Newell, A and Simon, H A",
  abstract = "â€¦ This research conducted by Allen Newell and Herbert A. â€¦ The
              main concern in this study is given to the integrated activities
              that constitute problem solving and ignores the â€¦",
  year     =  1972,
  keywords = "project 1;Paper1"
}

@MISC{Gershman_undated-tc,
  title        = "Perceptual multistability as Markov chain Monte Carlo
                  inference",
  author       = "Gershman, Samuel J and Vul, Edward and Tenenbaum, Joshua B",
  howpublished = "\url{https://gershmanlab.com/pubs/GershmanVulTenenbaum09.pdf}",
  note         = "Accessed: 2023-1-19",
  keywords     = "comp-cog-sci;project 1"
}

@BOOK{Minsky1988-bi,
  title     = "Perceptrons: Expanded Edition",
  author    = "Minsky, Marvin L and Papert, Seymour A",
  publisher = "MIT press",
  year      =  1988,
  keywords  = "comp-cog-sci",
  isbn      = "9780262631112"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Duncker1945-ed,
  title     = "On problem-solving",
  author    = "Duncker, Karl and Lees, Lynne S",
  abstract  = "â€¦ with the specific problems and with the technical
               possibilities of this field. In 1935 Karl Duncker â€¦ It is
               therefore most fortunate that after Duncker's death one of his
               students decided to preâ€¦",
  journal   = "Psychol. Monogr.",
  publisher = "American Psychological Association",
  volume    =  58,
  number    =  5,
  pages     = "i",
  year      =  1945,
  keywords  = "project 1",
  issn      = "0096-9753"
}

@ARTICLE{Cassimatis_undated-lu,
  title    = "Artificial {{Intelligence}} and {{Cognitive Science Have}} the
              {{Same Problem}}",
  author   = "Cassimatis, Nicholas L",
  abstract = "Cognitive scientists attempting to explain human intelligence
              share a puzzle with artificial intelligence researchers aiming to
              create computers that exhibit humanlevel intelligence: how can a
              system composed of relatively unintelligent parts (such as
              neurons or transistors) behave intelligently? I argue that
              although cognitive science has made significant progress towards
              many of its goals, that solving the puzzle of intelligence
              requires special standards and methods in addition to those
              already employed in cognitive science. To promote such research,
              I suggest creating a subfield within cognitive science called
              intelligence science and propose some guidelines for research
              addressing the intelligence puzzle.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lynch2020-wo,
  title     = "Learning latent plans from play",
  author    = "Lynch, C and Khansari, M and Xiao, T and Kumar, V and {others}",
  abstract  = "Acquiring a diverse repertoire of general-purpose skills remains
               an open challenge for robotics. In this work, we propose
               self-supervising control on top of human teleoperated play â€¦",
  journal   = "on robot learning",
  publisher = "proceedings.mlr.press",
  year      =  2020,
  keywords  = "ARC Project"
}

@UNPUBLISHED{Hernandez2022-ow,
  title    = "Natural Language Descriptions of Deep Visual Features",
  author   = "Hernandez, Evan and Schwettmann, Sarah and Bau, David and
              Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob",
  abstract = "Some neurons in deep networks specialize in recognizing highly
              specific perceptual, structural, or semantic features of inputs.
              In computer vision, techniques exist for identifying neurons that
              respond to individual concept categories like colors, textures,
              and object classes. But these techniques are limited in scope,
              labeling only a small subset of neurons and behaviors in any
              network. Is a richer characterization of neuron-level computation
              possible? We introduce a procedure (called MILAN, for mutual
              information-guided linguistic annotation of neurons) that
              automatically labels neurons with open-ended, compositional,
              natural language descriptions. Given a neuron, MILAN generates a
              description by searching for a natural language string that
              maximizes pointwise mutual information with the image regions in
              which the neuron is active. MILAN produces fine-grained
              descriptions that capture categorical, relational, and logical
              structure in learned features. These descriptions obtain high
              agreement with human-generated feature descriptions across a
              diverse set of model architectures and tasks, and can aid in
              understanding and controlling learned models. We highlight three
              applications of natural language neuron descriptions. First, we
              use MILAN for analysis, characterizing the distribution and
              importance of neurons selective for attribute, category, and
              relational information in vision models. Second, we use MILAN for
              auditing, surfacing neurons sensitive to human faces in datasets
              designed to obscure them. Finally, we use MILAN for editing,
              improving robustness in an image classifier by deleting neurons
              sensitive to text features spuriously correlated with class
              labels.",
  month    =  jan,
  year     =  2022,
  keywords = "ARC Project"
}

@INPROCEEDINGS{Gal2016-il,
  title     = "Dropout as a {{Bayesian Approximation}}: {{Representing Model
               Uncertainty}} in {{Deep Learning}}",
  booktitle = "Proceedings of The 33rd International Conference on Machine
               Learning",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
  abstract  = "Deep learning tools have gained tremendous attention in applied
               machine learning. However such tools for regression and
               classification do not capture model uncertainty. In comparison,
               Bayesian models offer a mathematically grounded framework to
               reason about model uncertainty, but usually come with a
               prohibitive computational cost. In this paper we develop a new
               theoretical framework casting dropout training in deep neural
               networks (NNs) as approximate Bayesian inference in deep
               Gaussian processes. A direct result of this theory gives us
               tools to model uncertainty with dropout NNs -- extracting
               information from existing models that has been thrown away so
               far. This mitigates the problem of representing uncertainty in
               deep learning without sacrificing either computational
               complexity or test accuracy. We perform an extensive study of
               the properties of dropout's uncertainty. Various network
               architectures and non-linearities are assessed on tasks of
               regression and classification, using MNIST as an example. We
               show a considerable improvement in predictive log-likelihood and
               RMSE compared to existing state-of-the-art methods, and finish
               by using dropout's uncertainty in deep reinforcement learning.",
  publisher = "PMLR",
  volume    =  48,
  pages     = "1050--1059",
  series    = "Proceedings of Machine Learning Research",
  year      =  2016,
  address   = "New York, New York, USA",
  keywords  = "machine-learning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Mitchell1980-lf,
  title        = "The need for biases in learning generalizations",
  author       = "Mitchell, Tom M",
  abstract     = "Learning involves the ability to generalize from past
                  experience in order to deal with new situations that are''
                  related to'' this experience. The inductive leaap needed to
                  deal with new situations seems to be possible only under
                  certain biases for choosing one generalization of the
                  situation over another. This paper defines precisely the
                  notion of bias in generalization problems, then shows that
                  biases are necessary for the inductive leap. Classes of
                  justifiable biases are considered, and the relationship
                  between bias and domain-independence is â€¦",
  publisher    = "Citeseer",
  year         =  1980,
  howpublished = "\url{https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6cf35ec34efa592f83e3a1b748aea14957fc784a}",
  note         = "Accessed: 2023-3-28",
  keywords     = "read;learning\&memory2023;l\&m final"
}

@ARTICLE{Hu2019-wh,
  title    = "Hierarchical decision making by generating and following natural
              language instructions",
  author   = "Hu, Hengyuan and Yarats, Denis and Gong, Qucheng and Tian,
              Yuandong and Lewis, Mike",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  32,
  year     =  2019,
  keywords = "ARC Project;comp-cog-sci",
  issn     = "1049-5258"
}

@ARTICLE{Shultz_undated-cv,
  title    = "Probability {{Without Counting}} and {{Dividing}}: {{A Fresh
              Computational Perspective}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments show that preverbal infants can reason
              probabilistically. This raises a deep puzzle because infants lack
              the counting and dividing abilities presumably required to
              compute probabilities. In the standard way of computing
              probabilities, they would have to count or accurately estimate
              large frequencies and divide those values by their total. Here,
              we present a novel neural-network model that learns and uses
              probability distributions without explicit counting or dividing.
              Probability distributions emerge naturally from neural-network
              learning of event sequences, providing a computationally
              sufficient explanation of how infants could succeed at
              probabilistic reasoning. Several alternative explanations are
              discussed and ruled out. Our work bears on several other active
              literatures, and it suggests an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  pages    = "7",
  keywords = "comp-cog-sci;development"
}

@INPROCEEDINGS{Krizhevsky2012-bm,
  title     = "{{ImageNet}} Classification with Deep Convolutional Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J and Bottou, L and Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  volume    =  25,
  year      =  2012,
  keywords  = "machine-learning"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Tenney2022-km,
  title    = "What do you learn from context? Probing for sentence structure in
              contextualized word representations",
  author   = "Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and
              Poliak, Adam and Thomas McCoy, R and Kim, Najoung and Van Durme,
              Benjamin and Bowman, Samuel R and Das, Dipanjan and Pavlick,
              Ellie",
  abstract = "Contextualized representation models such as ELMo (Peters et al.,
              2018a) and BERT (Devlin et al., 2018) have recently achieved
              state-of-the-art results on a diverse array of downstream NLP
              tasks. Building on recent token-level probing work, we introduce
              a novel edge probing task design and construct a broad suite of
              sub-sentence tasks derived from the traditional structured NLP
              pipeline. We probe word-level contextual representations from
              four recent models and investigate how they encode sentence
              structure across a range of syntactic, semantic, local, and
              long-range phenomena. We find that existing models trained on
              language modeling and translation produce strong representations
              for syntactic phenomena, but only offer comparably small
              improvements on semantic tasks over a non-contextual baseline.",
  journal  = "https://openreview.net â€º forum â€º
              id=SJzSgnRcKXhttps://openreview.net â€º forum â€º id=SJzSgnRcKX",
  month    =  feb,
  year     =  2022,
  keywords = "nlp"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Girshick_undated-ee,
  title    = "Rich {{Feature Hierarchies}} for {{Accurate Object Detection}}
              and {{Semantic Segmentation}}",
  author   = "Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik,
              Jitendra",
  abstract = "Object detection performance, as measured on the canonical PASCAL
              VOC dataset, has plateaued in the last few years. The
              best-performing methods are complex ensemble systems that
              typically combine multiple low-level image features with
              high-level context. In this paper, we propose a simple and
              scalable detection algorithm that improves mean average precision
              (mAP) by more than 30\% relative to the previous best result on
              VOC 2012---achieving a mAP of 53.3\%. Our approach combines two
              key insights: (1) one can apply high-capacity convolutional
              neural networks (CNNs) to bottom-up region proposals in order to
              localize and segment objects and (2) when labeled training data
              is scarce, supervised pre-training for an auxiliary task,
              followed by domain-specific fine-tuning, yields a significant
              performance boost. Since we combine region proposals with CNNs,
              we call our method R-CNN: Regions with CNN features. We also
              present experiments that provide insight into what the network
              learns, revealing a rich hierarchy of image features. Source code
              for the complete system is available at
              http://www.cs.berkeley.edu/ Ëœrbg/rcnn.",
  pages    = "8",
  keywords = "comp-cog-sci"
}

@INPROCEEDINGS{Stuhlmuller2010-ka,
  title     = "Learning Structured Generative Concepts",
  author    = "Stuhlmuller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D",
  publisher = "Cognitive Science Society",
  year      =  2010,
  keywords  = "comp-cog-sci",
  isbn      = "9781617388903"
}

@ARTICLE{Blodgett1929-rc,
  title    = "The effect of the introduction of reward upon the maze
              performance of rats",
  author   = "Blodgett, H C",
  abstract = "``The purpose of this investigation was to study the efficiency
              of units of practice when unaccompanied by reward. The method
              devised was that of running two groups of rats through the maze:
              an experimental group which received no reward during the first
              part of learning, but which suddenly had reward introduced in the
              latter part of learning, and a control group which received
              reward throughout the whole of learning. The answer to the
              question as to the efficiency of non-reward units of practice was
              sought in a comparison of the learning curve of the experimental
              group (both before and after the introduction of reward) with
              that of the control group.'' Three mazes were used, two with
              ordinary blinds and one with blinds arranged so that the animal
              could go two ways as well as having as alternatives a long and a
              short path. ``They prevented retracings from one section of the
              maze to another, they were noiseless, and they caused no
              excitement in the animals.'' There were 36 rats in each group.
              The results show that: ``(1) Rats run under a non-reward
              condition learned much more slowly than rats run under a reward
              condition.'' ``(2) Rats previously run under a non-reward
              condition, when suddenly rewarded made a great improvement.''
              ``(3) During the non-reward period, the rats were developing a
              latent learning of the maze which they were able to utilize as
              soon as reward was introduced.'' ``(5) It was demonstrated by the
              use of the two-path maze that the latent learning which was
              developed under non-reward conditions and was made manifest as
              soon as reward was introduced was not the result of any very
              consistently greater frequency of the correct over the incorrect
              path during the non-reward period.'' Bibliography and
              discussions. (PsycINFO Database Record (c) 2016 APA, all rights
              reserved)",
  journal  = "Publ. Psychol.",
  volume   =  4,
  pages    = "113--134",
  year     =  1929,
  keywords = "learning\&memory2023"
}

@MISC{Vong_undated-ni,
  title        = "Few-shot image classification by generating natural language
                  rules",
  author       = "Vong, Wai Keen and Lake, Brenden M",
  howpublished = "\url{https://openreview.net/pdf?id=BxfpZP2sZq}",
  note         = "Accessed: 2023-4-5",
  keywords     = "read;ARC Project"
}

@ARTICLE{Griffiths2011-yk,
  title    = "The Indian Buffet Process: An Introduction and Review",
  author   = "Griffiths, Thomas L and Ghahramani, Zoubin",
  journal  = "J. Mach. Learn. Res.",
  volume   =  12,
  number   =  32,
  pages    = "1185--1224",
  year     =  2011,
  keywords = "skimmed;project 2",
  issn     = "1532-4435, 1533-7928"
}

@BOOK{MacKay2003-nl,
  title     = "Information Theory, Inference and Learning Algorithms",
  author    = "MacKay, David J C and Mac Kay, David J",
  abstract  = "Information theory and inference, often taught separately, are
               here united in one entertaining textbook. These topics lie at
               the heart of many exciting areas of contemporary science and
               engineering - communication, signal processing, data mining,
               machine learning, pattern recognition, computational
               neuroscience, bioinformatics, and cryptography. This textbook
               introduces theory in tandem with applications. Information
               theory is taught alongside practical communication systems, such
               as arithmetic coding for data compression and sparse-graph codes
               for error-correction. A toolbox of inference techniques,
               including message-passing algorithms, Monte Carlo methods, and
               variational approximations, are developed alongside applications
               of these tools to clustering, convolutional codes, independent
               component analysis, and neural networks. The final part of the
               book describes the state of the art in error-correcting codes,
               including low-density parity-check codes, turbo codes, and
               digital fountain codes -- the twenty-first century standards for
               satellite communications, disk drives, and data broadcast.
               Richly illustrated, filled with worked examples and over 400
               exercises, some with detailed solutions, David MacKay's
               groundbreaking book is ideal for self-learning and for
               undergraduate or graduate courses. Interludes on crosswords,
               evolution, and sex provide entertainment along the way. In sum,
               this is a textbook on information, communication, and coding for
               a new generation of students, and an unparalleled entry point
               into these subjects for professionals in areas as diverse as
               computational biology, financial engineering, and machine
               learning.",
  publisher = "Cambridge University Press",
  month     =  sep,
  year      =  2003,
  keywords  = "skimmed;ccm2023;machine-learning",
  language  = "en",
  isbn      = "9780521642989"
}

@MISC{Bramley_undated-rs,
  title        = "Grounding compositional hypothesis generation in specific
                  instances",
  author       = "Bramley, Neil R and Tenenbaum, Joshua B",
  abstract     = "A number of recent computational models treat concept
                  learning as a form of probabilistic rule induction in a space
                  of language-like, compositional concepts. Inference in such
                  models frequently requires repeatedly sampling from a
                  (infinite) distribution over possible concept rules and
                  comparing their relative likelihood in light of current data
                  or evidence. However, we argue that most existing algorithms
                  for top-down sampling are inefficient and cognitively
                  implausible accounts of human hypothesis generation. As a
                  result, we propose an alternative, Instance Driven Generator
                  (IDG), that constructs bottom-up hypotheses directly out of
                  encountered positive instances of a concept. Using a novel
                  rule induction task based on the children's game Zendo, we
                  compare these ``bottomup'' and ``top-down'' approaches to
                  inference. We find that the bottom-up IDG model accounts
                  better for human inferences and results in a computationally
                  more tractable inference mechanism for concept learning
                  models based on a probabilistic language of thought.",
  howpublished = "\url{https://www.bramleylab.ppls.ed.ac.uk/pdfs/bramley2018zendo.pdf}",
  note         = "Accessed: 2022-10-16",
  keywords     = "project 1;comp-cog-sci"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Ebbinghaus1885-na,
  title     = "{\"U}ber das Ged{\"a}chtnis: Untersuchungen zur experimentellen
               Psychologie",
  author    = "Ebbinghaus, Hermann",
  abstract  = "Die Bem{\"u}hungen, f{\"u}r die m{\"a}chtigen Hebel der exakten
               Naturforschung, Experiment und Z{\"a}hlung, auch in der Welt der
               psychischen Vorg{\"a}nge geeignete Angriffspunkte zu gewinnen,
               sind bisher wesentlich auf das groÅ¿se Gebiet der
               Sinnesempfindungen und die psychologische Zeitmessung
               be-schr{\"a}nkt geblieben. Mit den Untersuchungen, deren Methode
               und vorl{\"a}ufige Resultate ich im folgenden mitteile, habe ich
               versucht, einen Schritt weiter in das Innere des psychischen
               Geschehens zu thun und die Erscheinungen des â€¦",
  publisher = "Duncker \& Humblot",
  year      =  1885,
  keywords  = "l\&m final",
  language  = "de"
}

@INCOLLECTION{Hauser2004-go,
  title     = "Evolutionary and Developmental Foundations of Human Knowledge",
  booktitle = "The {{Cognitive Neurosciences Iii}}",
  author    = "Hauser, Marc D and Spelke, Elizabeth",
  editor    = "Gazzaniga, Michael S",
  publisher = "MIT Press",
  year      =  2004,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Mahowald1991-jj,
  title    = "The {{Silicon Retina}}",
  author   = "Mahowald, Misha A and Mead, Carver",
  journal  = "Sci. Am.",
  pages    = "9",
  year     =  1991,
  keywords = "comp-cog-sci",
  issn     = "0036-8733"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Harnad1987-tn,
  title     = "Categorical perception: The groundwork of cognition",
  editor    = "Harnad, Stevan",
  abstract  = "``Categorical Perception'' brings together all the known
               examples of categorical perception, from research on humans and
               animals, infants and adults, in all the sense modalities so far
               investigated: hearing, seeing, and touch. The perceptual
               findings are interpreted in terms of the available cognitive and
               neuroscientific theories of how categorical perception is
               accomplished by the brain: Is it inborn? Is it learned? What is
               it that the mind does to the incoming continuous information to
               sort it into the discrete categories we can see, manipulate,
               name, and describe? Work on elementary perceptual and
               psychophysical categories (colors, sounds) is then compared with
               work on higher order categories: objects (tables, chairs),
               patterns, abstract concepts (goodness, truth). From a focus on
               the most thoroughly investigated case of categorical
               perception---speech perception---the book proceeds to an
               integrative view of categorization in general. (PsycINFO
               Database Record (c) 2016 APA, all rights reserved)",
  journal   = "https://psycnet.apa.org â€º recordhttps://psycnet.apa.org â€º record",
  publisher = "Cambridge University Press Categorical perception",
  volume    =  599,
  year      =  1987,
  address   = "New York, NY, US",
  keywords  = "l\&m final"
}

@BOOK{noauthor_2015-uh,
  title    = "The Conceptual Mind : New Directions in the Study of Concepts",
  abstract = "``The study of concepts has advanced dramatically in recent
              years, with exciting new findings and theoretical developments.
              Core concepts have been investigated in greater depth and new
              lines of inquiry have blossomed, with researchers from an ever
              broader range of disciplines making important contributions. In
              this volume, leading philosophers and cognitive scientists offer
              original essays that present the state-of-the-art in the study of
              concepts. These essays, all commissioned for this book, do not
              merely present the usual surveys and overviews; rather, they
              offer the latest work on concepts by a diverse group of theorists
              as well as discussions of the ideas that should guide research
              over the next decade. The book is an essential companion volume
              to the earlier Concepts: Core Readings, the definitive source for
              classic texts on the nature of concepts. The essays cover
              concepts as they relate to animal cognition, the brain,
              evolution, perception, and language, concepts across cultures,
              concept acquisition and conceptual change, concepts and
              normativity, concepts in context, and conceptual
              individuation''--MIT CogNet.",
  year     =  2015,
  keywords = "Electronic books;comp-cog-sci",
  isbn     = "9780262326872",
  lccn     = "2014034214"
}

@MISC{noauthor_undated-vu,
  title        = "3-second-naive-physics-manifesto.pdf",
  howpublished = "\url{http://www.cs.unibo.it/~nuzzoles/courses/intelligenza-artificiale/exam/3-second-naive-physics-manifesto.pdf}",
  keywords     = "skimmed;comp-cog-sci;project 2"
}

@MISC{noauthor_undated-by,
  title        = "Readings in Cognitive Science : A Perspective from Psychology
                  and Artificial Intelligence | {{{McGill} University Library}}",
  howpublished = "\url{https://mcgill.on.worldcat.org/v2/oclc/555237322}",
  note         = "Accessed: 2021-9-17",
  keywords     = "comp-cog-sci"
}
