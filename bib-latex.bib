@ARTICLE{Peterson2021-nw,
  title    = "Using large-scale experiments and machine learning to discover
              theories of human decision-making",
  author   = "Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and
              Reichman, Daniel and Griffiths, Thomas L",
  abstract = "Predicting and understanding how people make decisions has been a
              long-standing goal in many fields, with quantitative models of
              human decision-making informing research in both the social
              sciences and engineering. We show how progress toward this goal
              can be accelerated by using large datasets to power
              machine-learning algorithms that are constrained to produce
              interpretable psychological theories. Conducting the largest
              experiment on risky choice to date and analyzing the results
              using gradient-based optimization of differentiable decision
              theories implemented through artificial neural networks, we were
              able to recapitulate historical discoveries, establish that there
              is room to improve on existing theories, and discover a new, more
              accurate model of human decision-making in a form that preserves
              the insights from centuries of research.",
  journal  = "Science",
  volume   =  372,
  number   =  6547,
  pages    = "1209--1214",
  month    =  jun,
  year     =  2021,
  language = "en"
}

@BOOK{noauthor_2015-uh,
  title    = "The Conceptual Mind : New Directions in the Study of Concepts",
  abstract = "``The study of concepts has advanced dramatically in recent
              years, with exciting new findings and theoretical developments.
              Core concepts have been investigated in greater depth and new
              lines of inquiry have blossomed, with researchers from an ever
              broader range of disciplines making important contributions. In
              this volume, leading philosophers and cognitive scientists offer
              original essays that present the state-of-the-art in the study of
              concepts. These essays, all commissioned for this book, do not
              merely present the usual surveys and overviews; rather, they
              offer the latest work on concepts by a diverse group of theorists
              as well as discussions of the ideas that should guide research
              over the next decade. The book is an essential companion volume
              to the earlier Concepts: Core Readings, the definitive source for
              classic texts on the nature of concepts. The essays cover
              concepts as they relate to animal cognition, the brain,
              evolution, perception, and language, concepts across cultures,
              concept acquisition and conceptual change, concepts and
              normativity, concepts in context, and conceptual
              individuation''--MIT CogNet.",
  year     =  2015,
  keywords = "Electronic books;comp-cog-sci"
}

@INPROCEEDINGS{Krizhevsky2012-bm,
  title     = "{{ImageNet}} Classification with Deep Convolutional Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J and Bottou, L and Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  volume    =  25,
  year      =  2012,
  keywords  = "machine-learning"
}

@ARTICLE{Thomas_McCoy2020-zy,
  title         = "Universal linguistic inductive biases via meta-learning",
  author        = "Thomas McCoy, R and Grant, Erin and Smolensky, Paul and
                   Griffiths, Thomas L and Linzen, Tal",
  abstract      = "How do learners acquire languages from the limited data
                   available to them? This process must involve some inductive
                   biases - factors that affect how a learner generalizes - but
                   it is unclear which inductive biases can explain observed
                   patterns in language acquisition. To facilitate
                   computational modeling aimed at addressing this question, we
                   introduce a framework for giving particular linguistic
                   inductive biases to a neural network model; such a model can
                   then be used to empirically explore the effects of those
                   inductive biases. This framework disentangles universal
                   inductive biases, which are encoded in the initial values of
                   a neural network's parameters, from non-universal factors,
                   which the neural network must learn from data in a given
                   language. The initial state that encodes the inductive
                   biases is found with meta-learning, a technique through
                   which a model discovers how to acquire new languages more
                   easily via exposure to many possible languages. By
                   controlling the properties of the languages that are used
                   during meta-learning, we can control the inductive biases
                   that meta-learning imparts. We demonstrate this framework
                   with a case study based on syllable structure. First, we
                   specify the inductive biases that we intend to give our
                   model, and then we translate those inductive biases into a
                   space of languages from which a model can meta-learn.
                   Finally, using existing analysis techniques, we verify that
                   our approach has imparted the linguistic inductive biases
                   that it was intended to impart.",
  month         =  jun,
  year          =  2020,
  keywords      = "read;comp-cog-sci",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.16324"
}

@INBOOK{Bassok2012-sg,
  title     = "Problem Solving",
  author    = "Bassok, Miriam and Novick, Laura R",
  abstract  = "Abstract. This chapter follows the historical development of
               research on problem solving. It begins with a description of two
               research traditions that addressed",
  publisher = "Oxford University Press",
  month     =  mar,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Ellis2022-je,
  title    = "Synthesizing theories of human language with Bayesian program
              induction",
  author   = "Ellis, Kevin and Albright, Adam and Solar-Lezama, Armando and
              Tenenbaum, Joshua B and O'Donnell, Timothy J",
  abstract = "Automated, data-driven construction and evaluation of scientific
              models and theories is a long-standing challenge in artificial
              intelligence. We present a framework for algorithmically
              synthesizing models of a basic part of human language:
              morpho-phonology, the system that builds word forms from sounds.
              We integrate Bayesian inference with program synthesis and
              representations inspired by linguistic theory and cognitive
              models of learning and discovery. Across 70 datasets from 58
              diverse languages, our system synthesizes human-interpretable
              models for core aspects of each language's morpho-phonology,
              sometimes approaching models posited by human linguists. Joint
              inference across all 70 data sets automatically synthesizes a
              meta-model encoding interpretable cross-language typological
              tendencies. Finally, the same algorithm captures few-shot
              learning dynamics, acquiring new morphophonological rules from
              just one or a few examples. These results suggest routes to more
              powerful machine-enabled discovery of interpretable models in
              linguistics and other scientific domains.",
  journal  = "Nat. Commun.",
  volume   =  13,
  number   =  1,
  pages    = "5024",
  month    =  aug,
  year     =  2022,
  keywords = "skimmed",
  language = "en"
}

@ARTICLE{Valkov2018-sq,
  title         = "{HOUDINI}: Lifelong Learning as Program Synthesis",
  author        = "Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and
                   Sutton, Charles and Chaudhuri, Swarat",
  abstract      = "We present a neurosymbolic framework for the lifelong
                   learning of algorithmic tasks that mix perception and
                   procedural reasoning. Reusing high-level concepts across
                   domains and learning complex procedures are key challenges
                   in lifelong learning. We show that a program synthesis
                   approach that combines gradient descent with combinatorial
                   search over programs can be a more effective response to
                   these challenges than purely neural methods. Our framework,
                   called HOUDINI, represents neural networks as strongly
                   typed, differentiable functional programs that use symbolic
                   higher-order combinators to compose a library of neural
                   functions. Our learning algorithm consists of: (1) a
                   symbolic program synthesizer that performs a type-directed
                   search over parameterized programs, and decides on the
                   library functions to reuse, and the architectures to combine
                   them, while learning a sequence of tasks; and (2) a neural
                   module that trains these programs using stochastic gradient
                   descent. We evaluate HOUDINI on three benchmarks that
                   combine perception with the algorithmic tasks of counting,
                   summing, and shortest-path computation. Our experiments show
                   that HOUDINI transfers high-level concepts more effectively
                   than traditional transfer learning and progressive neural
                   networks, and that the typed representation of networks
                   significantly accelerates the search.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1804.00218"
}

@ARTICLE{Raffel2019-jw,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   {Text-to-Text} Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of
                   transfer learning techniques for NLP by introducing a
                   unified framework that converts all text-based language
                   problems into a text-to-text format. Our systematic study
                   compares pre-training objectives, architectures, unlabeled
                   data sets, transfer approaches, and other factors on dozens
                   of language understanding tasks. By combining the insights
                   from our exploration with scale and our new ``Colossal Clean
                   Crawled Corpus'', we achieve state-of-the-art results on
                   many benchmarks covering summarization, question answering,
                   text classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683"
}

@MISC{Bramley_undated-rs,
  title        = "Grounding compositional hypothesis generation in specific
                  instances",
  author       = "Bramley, Neil R and Tenenbaum, Joshua B",
  abstract     = "A number of recent computational models treat concept
                  learning as a form of probabilistic rule induction in a space
                  of language-like, compositional concepts. Inference in such
                  models frequently requires repeatedly sampling from a
                  (infinite) distribution over possible concept rules and
                  comparing their relative likelihood in light of current data
                  or evidence. However, we argue that most existing algorithms
                  for top-down sampling are inefficient and cognitively
                  implausible accounts of human hypothesis generation. As a
                  result, we propose an alternative, Instance Driven Generator
                  (IDG), that constructs bottom-up hypotheses directly out of
                  encountered positive instances of a concept. Using a novel
                  rule induction task based on the children's game Zendo, we
                  compare these ``bottomup'' and ``top-down'' approaches to
                  inference. We find that the bottom-up IDG model accounts
                  better for human inferences and results in a computationally
                  more tractable inference mechanism for concept learning
                  models based on a probabilistic language of thought.",
  howpublished = "\url{https://www.bramleylab.ppls.ed.ac.uk/pdfs/bramley2018zendo.pdf}",
  note         = "Accessed: 2022-10-16"
}

@ARTICLE{Nisbett1977-oz,
  title    = "Telling more than we can know: Verbal reports on mental processes",
  author   = "Nisbett, Richard E and Wilson, Timothy D",
  abstract = "Reviews evidence which suggests that there may be little or no
              direct introspective access to higher order cognitive processes.
              Ss are sometimes (a) unaware of the existence of a stimulus that
              importantly influenced a response, (b) unaware of the existence
              of the response, and (c) unaware that the stimulus has affected
              the response. It is proposed that when people attempt to report
              on their cognitive processes, that is, on the processes mediating
              the effects of a stimulus on a response, they do not do so on the
              basis of any true introspection. Instead, their reports are based
              on a priori, implicit causal theories, or judgments about the
              extent to which a particular stimulus is a plausible cause of a
              given response. This suggests that though people may not be able
              to observe directly their cognitive processes, they will
              sometimes be able to report accurately about them. Accurate
              reports will occur when influential stimuli are salient and are
              plausible causes of the responses they produce, and will not
              occur when stimuli are not salient or are not plausible causes.
              (86 ref) (PsycINFO Database Record (c) 2018 APA, all rights
              reserved)",
  journal  = "Psychol. Rev.",
  volume   =  84,
  number   =  3,
  pages    = "231--259",
  month    =  may,
  year     =  1977,
  keywords = "project 1"
}

@ARTICLE{Griffiths2019-hj,
  title    = "Doing more with less: Meta-reasoning and meta-learning in humans
              and machines",
  author   = "Griffiths, Thomas L and Callaway, Frederick and Chang, Michael B
              and Grant, Erin and Krueger, Paul M and Lieder, Falk",
  abstract = "Artificial intelligence systems use an increasing amount of
              computation and data to solve very specific problems. By
              contrast, human minds solve a wide range of problems using a
              fixed amount of computation and limited experience. We identify
              two abilities that we see as crucial to this kind of general
              intelligence: meta-reasoning (deciding how to allocate
              computational resources) and meta-learning (modeling the learning
              environment to make better use of limited data). We summarize the
              relevant AI literature and relate the resulting ideas to recent
              work in psychology. (PsycINFO Database Record (c) 2019 APA, all
              rights reserved)",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "24--30",
  month    =  oct,
  year     =  2019
}

@ARTICLE{De_Jong1998-ci,
  title     = "Scientific Discovery Learning with Computer Simulations of
               Conceptual Domains",
  author    = "De Jong, Ton and Van Joolingen, Wouter R",
  abstract  = "Scientific discovery learning is a highly self-directed and
               constructivistic form of learning. A computer simulation is a
               type of computer-based environment that is well suited for
               discovery learning, the main task of the learner being to infer,
               through experimentation, characteristics of the model underlying
               the simulation. In this article we give a review of the observed
               effectiveness and efficiency of discovery learning in simulation
               environments together with problems that learners may encounter
               in discovery learning, and we discuss how simulations may be
               combined with instructional support in order to overcome these
               problems.",
  journal   = "Rev. Educ. Res.",
  publisher = "American Educational Research Association",
  volume    =  68,
  number    =  2,
  pages     = "179--201",
  month     =  jun,
  year      =  1998,
  keywords  = "project 1"
}

@ARTICLE{Gopnik2012-iu,
  title    = "Scientific thinking in young children: theoretical advances,
              empirical research, and policy implications",
  author   = "Gopnik, Alison",
  abstract = "New theoretical ideas and empirical research show that very young
              children's learning and thinking are strikingly similar to much
              learning and thinking in science. Preschoolers test hypotheses
              against data and make causal inferences; they learn from
              statistics and informal experimentation, and from watching and
              listening to others. The mathematical framework of probabilistic
              models and Bayesian inference can describe this learning in
              precise ways. These discoveries have implications for early
              childhood education and policy. In particular, they suggest both
              that early childhood experience is extremely important and that
              the trend toward more structured and academic early childhood
              programs is misguided.",
  journal  = "Science",
  volume   =  337,
  number   =  6102,
  pages    = "1623--1627",
  month    =  sep,
  year     =  2012,
  keywords = "project 1",
  language = "en"
}

@ARTICLE{Xu2022-eq,
  title         = "{EST}: Evaluating Scientific Thinking in Artificial Agents",
  author        = "Xu, Manjie and Jiang, Guangyuan and Zhang, Chi and Zhu,
                   Song-Chun and Zhu, Yixin",
  abstract      = "Theoretical ideas and empirical research have shown us a
                   seemingly surprising result: children, even very young
                   toddlers, demonstrate learning and thinking in a strikingly
                   similar manner to scientific reasoning in formal research.
                   Encountering a novel phenomenon, children make hypotheses
                   against data, conduct causal inference from observation,
                   test their theory via experimentation, and correct the
                   proposition if inconsistency arises. Rounds of such
                   processes continue until the underlying mechanism is found.
                   Towards building machines that can learn and think like
                   people, one natural question for us to ask is: whether the
                   intelligence we achieve today manages to perform such a
                   scientific thinking process, and if any, at what level. In
                   this work, we devise the EST environment for evaluating the
                   scientific thinking ability in artificial agents. Motivated
                   by the stream of research on causal discovery, we build our
                   interactive EST environment based on Blicket detection.
                   Specifically, in each episode of EST, an agent is presented
                   with novel observations and asked to figure out all objects'
                   Blicketness. At each time step, the agent proposes new
                   experiments to validate its hypothesis and updates its
                   current belief. By evaluating Reinforcement Learning (RL)
                   agents on both a symbolic and visual version of this task,
                   we notice clear failure of today's learning methods in
                   reaching a level of intelligence comparable to humans. Such
                   inefficacy of learning in scientific thinking calls for
                   future research in building humanlike intelligence.",
  month         =  jun,
  year          =  2022,
  keywords      = "skimmed;project 1;machine-learning",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2206.09203"
}

@ARTICLE{Lohse2022-nm,
  title    = "Hypotheses in adult-child interactions stimulate children's
              reasoning and verbalizations",
  author   = "Lohse, Karoline and Hildebrandt, Andrea and Hildebrandt, Frauke",
  abstract = "Adult-child interactions can support children's development and
              are established as predictors of program quality in early
              childhood settings. However, the linguistic components that
              constitute positive interactions have not yet been studied in
              detail. This study investigates the effects of hypotheses
              proposed by adults on children's responses in a dyadic
              picture-book viewing situation. In 2 experiments, adults' use of
              hypotheses (e.g., ``Maybe this is a dwarf's door'') was tested
              against the use of instructive statements (``This is a dwarf's
              door'') and in combination with open questions (``What do you
              think, why is the door so small?''). In Experiment 1, hypotheses
              differed from instructions only by the modal marker ``maybe''.
              Children's responses to hypotheses were longer and contained more
              self-generated explanations as compared to responses to
              instructions. The use of hypotheses also seemed to encourage
              children to attach more importance to their own explanations. In
              Experiment 2, combining hypotheses with open-ended why questions
              elicited longer responses but no more self-generated explanations
              in children than open-ended questions alone. Results indicate
              that subtle differences in adults' utterances can directly
              influence children's reasoning and children's contributions to
              dialogues.",
  journal  = "Early Child. Res. Q.",
  volume   =  58,
  pages    = "254--263",
  month    =  jan,
  year     =  2022,
  keywords = "Adult-child interactions; sustained shared thinking; hypotheses;
              open questions"
}

@ARTICLE{Bramley2018-zb,
  title    = "Intuitive experimentation in the physical world",
  author   = "Bramley, Neil R and Gerstenberg, Tobias and Tenenbaum, Joshua B
              and Gureckis, Todd M",
  abstract = "Many aspects of our physical environment are hidden. For example,
              it is hard to estimate how heavy an object is from visual
              observation alone. In this paper we examine how people actively
              ``experiment'' within the physical world to discover such latent
              properties. In the first part of the paper, we develop a novel
              framework for the quantitative analysis of the information
              produced by physical interactions. We then describe two
              experiments that present participants with moving objects in
              ``microworlds'' that operate according to continuous
              spatiotemporal dynamics similar to everyday physics (i.e., forces
              of gravity, friction, etc.). Participants were asked to interact
              with objects in the microworlds in order to identify their
              masses, or the forces of attraction/repulsion that governed their
              movement. Using our modeling framework, we find that learners who
              freely interacted with the physical system selectively produced
              evidence that revealed the physical property consistent with
              their inquiry goal. As a result, their inferences were more
              accurate than for passive observers and, in some contexts, for
              yoked participants who watched video replays of an active
              learner's interactions. We characterize active learners' actions
              into a range of micro-experiment strategies and discuss how these
              might be learned or generalized from past experience. The
              technical contribution of this work is the development of a novel
              analytic framework and methodology for the study of interactively
              learning about the physical world. Its empirical contribution is
              the demonstration of sophisticated goal directed human active
              learning in a naturalistic context.",
  journal  = "Cogn. Psychol.",
  volume   =  105,
  pages    = "9--38",
  month    =  sep,
  year     =  2018,
  keywords = "Active learning; Experimental design; Mental simulation; Physical
              understanding",
  language = "en"
}

@ARTICLE{Coenen2015-wp,
  title    = "Strategies to intervene on causal systems are adaptively selected",
  author   = "Coenen, Anna and Rehder, Bob and Gureckis, Todd M",
  abstract = "How do people choose interventions to learn about causal systems?
              Here, we considered two possibilities. First, we test an
              information sampling model, information gain, which values
              interventions that can discriminate between a learner's
              hypotheses (i.e. possible causal structures). We compare this
              discriminatory model to a positive testing strategy that instead
              aims to confirm individual hypotheses. Experiment 1 shows that
              individual behavior is described best by a mixture of these two
              alternatives. In Experiment 2 we find that people are able to
              adaptively alter their behavior and adopt the discriminatory
              model more often after experiencing that the confirmatory
              strategy leads to a subjective performance decrement. In
              Experiment 3, time pressure leads to the opposite effect of
              inducing a change towards the simpler positive testing strategy.
              These findings suggest that there is no single strategy that
              describes how intervention decisions are made. Instead, people
              select strategies in an adaptive fashion that trades off their
              expected performance and cognitive effort.",
  journal  = "Cogn. Psychol.",
  volume   =  79,
  pages    = "102--133",
  month    =  jun,
  year     =  2015,
  keywords = "Causal learning; Hypothesis testing; Information gain;
              Interventions; Self-directed learning",
  language = "en"
}

@ARTICLE{Allen2020-tf,
  title    = "Rapid {Trial-and-Error} Learning with Simulation Supports
              Flexible Tool Use and Physical Reasoning",
  author   = "Allen, Kelsey R and Smith, Kevin A and Tenenbaum, Joshua B",
  abstract = "Many animals, and an increasing number of artificial agents,
              display sophisticated capabilities to perceive and manipulate
              objects. But human beings remain distinctive in their capacity
              for flexible, creative tool use---using objects in new ways to
              act on the world, achieve a goal, or solve a problem. To study
              this type of general physical problem solving, we introduce the
              Virtual Tools game. In this game, people solve a large range of
              challenging physical puzzles in just a handful of attempts. We
              propose that the flexibility of human physical problem solving
              rests on an ability to imagine the effects of hypothesized
              actions, while the efficiency of human search arises from rich
              action priors which are updated via observations of the world. We
              instantiate these components in the ``sample, simulate, update''
              (SSUP) model and show that it captures human performance across
              30 levels of the Virtual Tools game. More broadly, this model
              provides a mechanism for explaining how people condense general
              physical knowledge into actionable, task-specific plans to
              achieve flexible and efficient physical problem solving.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  117,
  number   =  47,
  pages    = "29302--29310",
  month    =  nov,
  year     =  2020,
  keywords = "skimmed;comp-cog-sci"
}

@MISC{Andreas2017-kq,
  title     = "Learning with {{Latent Language}}",
  author    = "Andreas, Jacob and Klein, Dan and Levine, Sergey",
  abstract  = "The named concepts and compositional operators present in
               natural language provide a rich source of information about the
               kinds of abstractions humans use to navigate the world. Can this
               linguistic background knowledge improve the generality and
               efficiency of learned classifiers and control policies? This
               paper aims to show that using the space of natural language
               strings as a parameter space is an effective way to capture
               natural task structure. In a pretraining phase, we learn a
               language interpretation model that transforms inputs (e.g.
               images) into outputs (e.g. labels) given natural language
               descriptions. To learn a new concept (e.g. a classifier), we
               search directly in the space of descriptions to minimize the
               interpreter's loss on training examples. Crucially, our models
               do not require language data to learn these concepts: language
               is used only in pretraining to impose structure on subsequent
               learning. Results on image classification, text editing, and
               reinforcement learning show that, in all settings, models with a
               linguistic parameterization outperform those without.",
  journal   = "arXiv [cs]",
  publisher = "arXiv",
  number    = "arXiv:1711.00482",
  month     =  nov,
  year      =  2017,
  keywords  = "Computer Science - Computation and Language,Computer Science -
               Neural and Evolutionary Computing;comp-cog-sci"
}

@UNPUBLISHED{Andrychowicz2016-uq,
  title    = "Learning to Learn by Gradient Descent by Gradient Descent",
  author   = "Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and
              Hoffman, Matthew W and Pfau, David and Schaul, Tom and
              Shillingford, Brendan and de Freitas, Nando",
  abstract = "The move from hand-designed features to learned features in
              machine learning has been wildly successful. In spite of this,
              optimization algorithms are still designed by hand. In this paper
              we show how the design of an optimization algorithm can be cast
              as a learning problem, allowing the algorithm to learn to exploit
              structure in the problems of interest in an automatic way. Our
              learned algorithms, implemented by LSTMs, outperform generic,
              hand-designed competitors on the tasks for which they are
              trained, and also generalize well to new tasks with similar
              structure. We demonstrate this on a number of tasks, including
              simple convex problems, training neural networks, and styling
              images with neural art.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Machine Learning,Computer Science - Neural and
              Evolutionary Computing;comp-cog-sci"
}

@INCOLLECTION{Ashby2011-au,
  title     = "{{COVIS}}",
  booktitle = "Formal {{Approaches}} in {{Categorization}}",
  author    = "Ashby, F Gregory and Paul, Erick J and Maddox, W Todd",
  editor    = "Pothos, Emmanuel M and Wills, Andy J",
  abstract  = "The COVIS model of category learning assumes separate rule-based
               and procedural-learning categorization systems that compete for
               access to response production. The rule-based system selects and
               tests simple verbalizable hypotheses about category membership.
               The procedurallearning system gradually associates
               categorization responses with regions of perceptual space via
               reinforcement learning.",
  publisher = "Cambridge University Press",
  pages     = "65--87",
  year      =  2011,
  address   = "Cambridge",
  keywords  = "comp-cog-sci"
}

@ARTICLE{Bengio2013-nf,
  title    = "Representation {{Learning}}: {{A Review}} and {{New
              Perspectives}}",
  author   = "Bengio, Y and Courville, A and Vincent, P",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical
              connections between representation learning, density estimation,
              and manifold learning.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  month    =  aug,
  year     =  2013,
  keywords = "Abstracts,autoencoder,Boltzmann machine,Deep learning,Feature
              extraction,feature learning,Learning systems,Machine
              learning,Manifolds,neural nets,Neural networks,representation
              learning,Speech recognition,unsupervised learning;comp-cog-sci"
}

@ARTICLE{Bonner2018-ii,
  title    = "Computational Mechanisms Underlying Cortical Responses to the
              Affordance Properties of Visual Scenes",
  author   = "Bonner, Michael F and Epstein, Russell A",
  editor   = "Einh{\"a}user, Wolfgang",
  abstract = "Biologically inspired deep convolutional neural networks (CNNs),
              trained for computer vision tasks, have been found to predict
              cortical responses with remarkable accuracy. However, the
              internal operations of these models remain poorly understood, and
              the factors that account for their success are unknown. Here we
              develop a set of techniques for using CNNs to gain insights into
              the computational mechanisms underlying cortical responses. We
              focused on responses in the occipital place area (OPA), a
              scene-selective region of dorsal occipitoparietal cortex. In a
              previous study, we showed that fMRI activation patterns in the
              OPA contain information about the navigational affordances of
              scenes; that is, information about where one can and cannot move
              within the immediate environment. We hypothesized that this
              affordance information could be extracted using a set of purely
              feedforward computations. To test this idea, we examined a deep
              CNN with a feedforward architecture that had been previously
              trained for scene classification. We found that responses in the
              CNN to scene images were highly predictive of fMRI responses in
              the OPA. Moreover the CNN accounted for the portion of OPA
              variance relating to the navigational affordances of scenes. The
              CNN could thus serve as an image-computable candidate model of
              affordance-related responses in the OPA. We then ran a series of
              in silico experiments on this model to gain insights into its
              internal operations. These analyses showed that the computation
              of affordance-related features relied heavily on visual
              information at high-spatial frequencies and cardinal
              orientations, both of which have previously been identified as
              lowlevel stimulus preferences of scene-selective visual cortex.
              These computations also exhibited a strong preference for
              information in the lower visual field, which is consistent with
              known retinotopic biases in the OPA. Visualizations of feature
              selectivity within the CNN suggested that affordance-based
              responses encoded features that define the layout of the spatial
              environment, such as boundary-defining junctions and large
              extended surfaces. Together, these results map the sensory
              functions of the OPA onto a fully quantitative model that
              provides insights into its visual computations. More broadly,
              they advance integrative techniques for understanding visual
              cortex across multiple level of analysis: from the identification
              of cortical sensory functions to the modeling of their underlying
              algorithms.",
  journal  = "PLoS Comput. Biol.",
  volume   =  14,
  number   =  4,
  pages    = "e1006111",
  month    =  apr,
  year     =  2018,
  keywords = "comp-cog-sci"
}

@ARTICLE{Bonner2021-zd,
  title    = "Object Representations in the Human Brain Reflect the
              {Co-Occurrence} Statistics of Vision and Language",
  author   = "Bonner, Michael F and Epstein, Russell A",
  abstract = "Abstract A central regularity of visual perception is the
              co-occurrence of objects in the natural environment. Here we use
              machine learning and fMRI to test the hypothesis that object
              co-occurrence statistics are encoded in the human visual system
              and elicited by the perception of individual objects. We
              identified low-dimensional representations that capture the
              latent statistical structure of object co-occurrence in
              real-world scenes, and we mapped these statistical
              representations onto voxel-wise fMRI responses during object
              viewing. We found that cortical responses to single objects were
              predicted by the statistical ensembles in which they typically
              occur, and that this link between objects and their visual
              contexts was made most strongly in parahippocampal cortex,
              overlapping with the anterior portion of scene-selective
              parahippocampal place area. In contrast, a language-based
              statistical model of the co-occurrence of object names in written
              text predicted responses in neighboring regions of
              object-selective visual cortex. Together, these findings show
              that the sensory coding of objects in the human brain reflects
              the latent statistics of object context in visual and linguistic
              experience.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "4081",
  month    =  dec,
  year     =  2021,
  keywords = "comp-cog-sci"
}

@ARTICLE{Botvinick2017-xr,
  title    = "Building Machines That Learn and Think for Themselves",
  author   = "Botvinick, Matthew and Barrett, David G T and Battaglia, Peter
              and de Freitas, Nando and Kumaran, Darshan and Leibo, Joel Z and
              Lillicrap, Timothy and Modayil, Joseph and Mohamed, Shakir and
              Rabinowitz, Neil C and Rezende, Danilo J and Santoro, Adam and
              Schaul, Tom and Summerfield, Christopher and Wayne, Greg and
              Weber, Theophane and Wierstra, Daan and Legg, Shane and Hassabis,
              Demis",
  abstract = "We agree with Lake and colleagues on their list of `key
              ingredients' for building humanlike intelligence, including the
              idea that model-based reasoning is essential. However, we favor
              an approach that centers on one additional ingredient: autonomy.
              In particular, we aim toward agents that can both build and
              exploit their own internal models, with minimal human
              hand-engineering. We believe an approach centered on autonomous
              learning has the greatest chance of success as we scale toward
              real-world complexity, tackling domains for which ready-made
              formal models are not available. Here we survey several important
              examples of the progress that has been made toward building
              autonomous agents with humanlike abilities, and highlight some
              outstanding challenges.",
  journal  = "Behav. Brain Sci.",
  volume   =  40,
  pages    = "e255",
  year     =  2017,
  keywords = "comp-cog-sci"
}

@ARTICLE{Bourlard1988-if,
  title    = "{Auto-Association} by Multilayer Perceptrons and Singular Value
              Decomposition",
  author   = "Bourlard, H and Kamp, Y",
  abstract = "The multilayer perceptron, when working in auto-association mode,
              is sometimes considered as an interesting candidate to perform
              data compression or dimensionality reduction of the feature space
              in information processing applications. The present paper shows
              that, for auto-association, the nonlinearities of the hidden
              units are useless and that the optimal parameter values can be
              derived directly by purely linear techniques relying on singular
              value decomposition and low rank matrix approximation, similar in
              spirit to the well-known Karhunen-Lo6ve transform. This approach
              appears thus as an efficient alternative to the general error
              back-propagation algorithm commonly used for training multilayer
              perceptrons. Moreover, it also gives a clear interpretation of
              the r61e of the different parameters.",
  journal  = "Biol. Cybern.",
  volume   =  59,
  number   = "4-5",
  pages    = "291--294",
  month    =  sep,
  year     =  1988,
  keywords = "comp-cog-sci"
}

@ARTICLE{Carey2004-ya,
  title    = "Bootstrapping \& the Origin of Concepts",
  author   = "Carey, Susan",
  journal  = "Daedalus",
  volume   =  133,
  number   =  1,
  pages    = "59--68",
  month    =  jan,
  year     =  2004,
  keywords = "comp-cog-sci"
}

@ARTICLE{Carey1978-gm,
  title    = "Acquiring a {{Single New Word}}",
  author   = "Carey, Susan and Bartlett, Elsa",
  volume   =  15,
  pages    = "14",
  year     =  1978,
  keywords = "comp-cog-sci"
}

@ARTICLE{Cassimatis_undated-lu,
  title    = "Artificial {{Intelligence}} and {{Cognitive Science Have}} the
              {{Same Problem}}",
  author   = "Cassimatis, Nicholas L",
  abstract = "Cognitive scientists attempting to explain human intelligence
              share a puzzle with artificial intelligence researchers aiming to
              create computers that exhibit humanlevel intelligence: how can a
              system composed of relatively unintelligent parts (such as
              neurons or transistors) behave intelligently? I argue that
              although cognitive science has made significant progress towards
              many of its goals, that solving the puzzle of intelligence
              requires special standards and methods in addition to those
              already employed in cognitive science. To promote such research,
              I suggest creating a subfield within cognitive science called
              intelligence science and propose some guidelines for research
              addressing the intelligence puzzle.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@MISC{Chollet2019-di,
  title     = "On the {{Measure}} of {{Intelligence}}",
  author    = "Chollet, Fran{\c c}ois",
  abstract  = "To make deliberate progress towards more intelligent and more
               human-like artificial systems, we need to be following an
               appropriate feedback signal: we need to be able to define and
               evaluate intelligence in a way that enables comparisons between
               two systems, as well as comparisons with humans. Over the past
               hundred years, there has been an abundance of attempts to define
               and measure intelligence, across both the fields of psychology
               and AI. We summarize and critically assess these definitions and
               evaluation approaches, while making apparent the two historical
               conceptions of intelligence that have implicitly guided them. We
               note that in practice, the contemporary AI community still
               gravitates towards benchmarking intelligence by comparing the
               skill exhibited by AIs and humans at specific tasks, such as
               board games and video games. We argue that solely measuring
               skill at any given task falls short of measuring intelligence,
               because skill is heavily modulated by prior knowledge and
               experience: unlimited priors or unlimited training data allow
               experimenters to ``buy'' arbitrary levels of skills for a
               system, in a way that masks the system's own generalization
               power. We then articulate a new formal definition of
               intelligence based on Algorithmic Information Theory, describing
               intelligence as skill-acquisition efficiency and highlighting
               the concepts of scope, generalization difficulty, priors, and
               experience, as critical pieces to be accounted for in
               characterizing intelligent systems. Using this definition, we
               propose a set of guidelines for what a general AI benchmark
               should look like. Finally, we present a new benchmark closely
               following these guidelines, the Abstraction and Reasoning Corpus
               (ARC), built upon an explicit set of priors designed to be as
               close as possible to innate human priors. We argue that ARC can
               be used to measure a human-like form of general fluid
               intelligence and that it enables fair general intelligence
               comparisons between AI systems and humans.",
  journal   = "arXiv [cs]",
  publisher = "arXiv",
  number    = "arXiv:1911.01547",
  month     =  nov,
  year      =  2019,
  keywords  = "Computer Science - Artificial Intelligence;comp-cog-sci"
}

@ARTICLE{Chomsky1956-aj,
  title    = "Three Models for the Description of Language",
  author   = "Chomsky, N",
  abstract = "We investigate several conceptions of linguistic structure to
              determine whether or not they can provide simple and
              ``revealing'' grammars that generate all of the sentences of
              English and only these. We find that no finite-state Markov
              process that produces symbols with transition from state to state
              can serve as an English grammar. Furthermore, the particular
              subclass of such processes that producen-order statistical
              approximations to English do not come closer, with increasingn,
              to matching the output of an English grammar. We formalize-the
              notions of ``phrase structure'' and show that this gives us a
              method for describing language which is essentially more
              powerful, though still representable as a rather elementary type
              of finite-state process. Nevertheless, it is successful only when
              limited to a small subset of simple sentences. We study the
              formal properties of a set of grammatical transformations that
              carry sentences with phrase structure into new sentences with
              derived phrase structure, showing that transformational grammars
              are processes of the same elementary type as phrase-structure
              grammars; that the grammar of English is materially simplified if
              phrase structure description is limited to a kernel of simple
              sentences from which all other sentences are constructed by
              repeated transformations; and that this view of linguistic
              structure gives a certain insight into the use and understanding
              of language.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "113--124",
  month    =  sep,
  year     =  1956,
  keywords = "Impedance matching,Kernel,Laboratories,Markov processes,Natural
              languages,Research and development,Testing;comp-cog-sci"
}

@ARTICLE{Choromanska_undated-tg,
  title    = "The {{Loss Surfaces}} of {{Multilayer Networks}}",
  author   = "Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and
              Arous, Gerard Ben and LeCun, Yann",
  pages    = "13",
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Darwiche2017-sr,
  title    = "{Human-{{Level} Intelligence}} or {{{Animal-Like} Abilities}}?",
  author   = "Darwiche, Adnan",
  abstract = "The vision systems of the eagle and the snake outperform
              everything that we can make in the laboratory, but snakes and
              eagles cannot build an eyeglass or a telescope or a microscope.
              (Judea Pearl)",
  month    =  jul,
  year     =  2017,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computers and Society,Computer Science - Machine
              Learning,Statistics - Machine Learning;comp-cog-sci"
}

@MISC{De_Raedt2020-hk,
  title     = "From {{Statistical Relational}} to {{{Neuro-Symbolic} Artificial
               Intelligence}}",
  author    = "De Raedt, Luc and Duman{\v c}i{\'c}, Sebastijan and Manhaeve,
               Robin and Marra, Giuseppe",
  abstract  = "Neuro-symbolic and statistical relational artificial
               intelligence both integrate frameworks for learning with logical
               reasoning. This survey identifies several parallels across seven
               different dimensions between these two fields. These cannot only
               be used to characterize and position neuro-symbolic artificial
               intelligence approaches but also to identify a number of
               directions for further research.",
  journal   = "arXiv [cs]",
  publisher = "arXiv",
  number    = "arXiv:2003.08316",
  month     =  mar,
  year      =  2020,
  keywords  = "Computer Science - Artificial Intelligence;comp-cog-sci"
}

@ARTICLE{Dekker2022-lm,
  title     = "Determinants of Human Compositional Generalization",
  author    = "Dekker, Ronald Boris and Otto, Fabian and Summerfield,
               Christopher",
  abstract  = "Generalisation (or transfer) is the ability to repurpose
               knowledge in novel settings. It is often asserted that
               generalisation is an important ingredient of human intelligence,
               but its extent, nature and determinants have proved
               controversial. Here, we re-examine this question with a new
               paradigm that formalises the transfer learning problem as one of
               recomposing existing functions to solve unseen problems. We find
               that people can generalise compositionally in ways that are
               elusive for standard neural networks, and that human
               generalisation benefits from training regimes in which items are
               axis-aligned and temporally correlated. We describe a neural
               network model based around a Hebbian gating process which can
               capture how human generalisation benefits from different
               training curricula. We additionally find that adult humans tend
               to learn composable functions asynchronously, exhibiting
               discontinuities in learning that resemble those seen in child
               development.",
  publisher = "PsyArXiv",
  month     =  mar,
  year      =  2022,
  keywords  = "Cognitive Psychology,Computational Neuroscience,Concepts and
               Categories,generalisation,Learning,neural
               network,Neuroscience,Social and Behavioral Sciences;comp-cog-sci"
}

@ARTICLE{Dwivedi2021-zk,
  title     = "Unveiling Functions of the Visual Cortex Using {Task-Specific}
               Deep Neural Networks",
  author    = "Dwivedi, Kshitij and Bonner, Michael F and Cichy, Radoslaw
               Martin and Roig, Gemma",
  abstract  = "The human visual cortex enables visual perception through a
               cascade of hierarchical computations in cortical regions with
               distinct functionalities. Here, we introduce an AI-driven
               approach to discover the functional mapping of the visual
               cortex. We related human brain responses to scene images
               measured with functional MRI (fMRI) systematically to a diverse
               set of deep neural networks (DNNs) optimized to perform
               different scene perception tasks. We found a structured mapping
               between DNN tasks and brain regions along the ventral and dorsal
               visual streams. Low-level visual tasks mapped onto early brain
               regions, 3-dimensional scene perception tasks mapped onto the
               dorsal stream, and semantic tasks mapped onto the ventral
               stream. This mapping was of high fidelity, with more than 60\%
               of the explainable variance in nine key regions being explained.
               Together, our results provide a novel functional mapping of the
               human visual cortex and demonstrate the power of the
               computational approach.",
  journal   = "PLoS Comput. Biol.",
  publisher = "Public Library of Science",
  volume    =  17,
  number    =  8,
  pages     = "e1009267",
  month     =  aug,
  year      =  2021,
  keywords  = "Functional magnetic resonance imaging,Linear regression
               analysis,Neural networks,Permutation,Semantics,Sensory
               perception,Vision,Visual cortex;comp-cog-sci"
}

@MISC{Ellis2020-xn,
  title     = "{{{DreamCoder}}}: {{Growing}} Generalizable, Interpretable
               Knowledge with {Wake-Sleep} {{Bayesian}} Program Learning",
  author    = "Ellis, Kevin and Wong, Catherine and Nye, Maxwell and
               Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and
               Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B",
  abstract  = "Expert problem-solving is driven by powerful languages for
               thinking about problems and their solutions. Acquiring expertise
               means learning these languages -- systems of concepts, alongside
               the skills to use them. We present DreamCoder, a system that
               learns to solve problems by writing programs. It builds
               expertise by creating programming languages for expressing
               domain concepts, together with neural networks to guide the
               search for programs within these languages. A ``wake-sleep''
               learning algorithm alternately extends the language with new
               symbolic abstractions and trains the neural network on imagined
               and replayed problems. DreamCoder solves both classic inductive
               programming tasks and creative tasks such as drawing pictures
               and building scenes. It rediscovers the basics of modern
               functional programming, vector algebra and classical physics,
               including Newton's and Coulomb's laws. Concepts are built
               compositionally from those learned earlier, yielding
               multi-layered symbolic representations that are interpretable
               and transferrable to new tasks, while still growing scalably and
               flexibly with experience.",
  journal   = "arXiv [cs]",
  publisher = "arXiv",
  number    = "arXiv:2006.08381",
  month     =  jun,
  year      =  2020,
  keywords  = "Computer Science - Artificial Intelligence,Computer Science -
               Machine Learning;comp-cog-sci"
}

@BOOK{Epstein2009-te,
  title     = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  publisher = "Springer Netherlands",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "comp-cog-sci"
}

@ARTICLE{Erickson2011-on,
  title    = "Exercise Training Increases Size of Hippocampus and Improves
              Memory",
  author   = "Erickson, K I and Voss, M W and Prakash, R S and Basak, C and
              Szabo, A and Chaddock, L and Kim, J S and Heo, S and Alves, H and
              White, S M and Wojcicki, T R and Mailey, E and Vieira, V J and
              Martin, S A and Pence, B D and Woods, J A and McAuley, E and
              Kramer, A F",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  108,
  number   =  7,
  pages    = "3017--3022",
  month    =  feb,
  year     =  2011,
  keywords = "exercise-brain"
}

@UNPUBLISHED{Feinman2021-mh,
  title    = "Learning {{{Task-General} Representations}} with {{Generative
              {Neuro-Symbolic} Modeling}}",
  author   = "Feinman, Reuben and Lake, Brenden M",
  abstract = "People can learn rich, general-purpose conceptual representations
              from only raw perceptual inputs. Current machine learning
              approaches fall well short of these human standards, although
              different modeling traditions often have complementary strengths.
              Symbolic models can capture the compositional and causal
              knowledge that enables flexible generalization, but they struggle
              to learn from raw inputs, relying on strong abstractions and
              simplifying assumptions. Neural network models can learn directly
              from raw data, but they struggle to capture compositional and
              causal structure and typically must retrain to tackle new tasks.
              We bring together these two traditions to learn generative models
              of concepts that capture rich compositional and causal structure,
              while learning from raw data. We develop a generative
              neuro-symbolic (GNS) model of handwritten character concepts that
              uses the control flow of a probabilistic program, coupled with
              symbolic stroke primitives and a symbolic image renderer, to
              represent the causal and compositional processes by which
              characters are formed. The distributions of parts (strokes), and
              correlations between parts, are modeled with neural network
              subroutines, allowing the model to learn directly from raw data
              and express nonparametric statistical relationships. We apply our
              model to the Omniglot challenge of human-level concept learning,
              using a background set of alphabets to learn an expressive prior
              distribution over character drawings. In a subsequent evaluation,
              our GNS model uses probabilistic inference to learn rich
              conceptual representations from a single training image that
              generalize to 4 unique tasks, succeeding where previous work has
              fallen short.",
  month    =  jan,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Fodor1988-gq,
  title    = "Connectionism and Cognitive Architecture: {{A}} Critical Analysis",
  author   = "Fodor, Jerry A and Pylyshyn, Zenon W",
  abstract = "This paper explores differences between Connectionist proposals
              for cognitive architecture and the sorts of models that have
              traditionally been assumed in cognitive science. We claim that
              the major distinction is that, while both Connectionist and
              Classical architectures postulate representational mental states,
              the latter but not the former are committed to a symbol-level of
              representation, or to a `language of thought': i.e., to
              representational states that have combinatorial syntactic and
              semantic structure. Several arguments for combinatorial structure
              in mental representations are then reviewed. These include
              arguments based on the `systematicity' of mental representation:
              i.e., on the fact that cognitive capacities always exhibit
              certain symmetries, so that the ability to entertain a given
              thought implies the ability to entertain thoughts with
              semantically related contents. We claim that such arguments make
              a powerful case that mind/brain architecture is not Connectionist
              at the cognitive level. We then consider the possibility that
              Connectionism may provide an account of the neural (or `abstract
              neurological') structures in which Classical cognitive
              architecture is implemented. We survey a number of the standard
              arguments that have been offered in favor of Connectionism, and
              conclude that they are coherent only on this interpretation.
              R{\'e}sum{\'e} Cet article{\'e}tudie les diff{\'e}rences entre
              mod{\`e}les connectionistes et mod{\`e}les classiques de la
              structure cognitive. Nous pensons que, bien que les deux types de
              mod{\`e}les stipulent l'existence d'{\'e}tats mentaux
              repr{\'e}sentationnels, la diff{\'e}rence essentielle est que
              seuls les mod{\`e}les classiques requi{\`e}rent l'existence d'un
              niveau de repr{\'e}sentation symbolique---un ``langage de la
              pens{\'e}e''---, c'est-{\`a}-dire d'{\'e}tats
              repr{\'e}sentationnels poss{\'e}dant une structure syntaxique et
              s{\'e}mantique. Nous examinons ensuite diff{\'e}rents arguments
              qui militent en faveur de l'existence de repr{\'e}sentations
              mentales ayant ces propri{\'e}t{\'e}s. Certains de ces arguments
              reposent sur la ``syst{\'e}maticit{\'e}'' des repr{\'e}sentations
              mentales, c'est-{\`a}-dire sur le fait que les capacit{\'e}s
              cognitives exhibent toujours certaines sym{\'e}tries, de sorte
              que la capacit{\'e}d'entretenir certaines pens{\'e}es implique la
              capacit{\'e}d'entretenir d'autres pens{\'e}es apparent{\'e}es par
              leur contenu s{\'e}mantique. Nous pensons que ces arguments
              montrent de mani{\`e}re convainquante que l'architecture de
              l'esprit/du cerveau n'est pas connectioniste au niveau cognitif.
              Nous nous demandons ensuite s'il est possible d'interpr{\'e}ter
              le connectionisme comme une analyse des structures neuronales (ou
              des structures neurologiques ``abstraites'') dans lesquelles est
              r{\'e}alis{\'e}e l'architecture cognitive classique. Nous
              examinons plusieurs des arguments avanc{\'e}s habituellement en
              d{\'e}fense du connectionisme, et en concluons que ceux-ci n'ont
              de sens que dans cette interpr{\'e}tation.",
  journal  = "Cognition",
  volume   =  28,
  number   =  1,
  pages    = "3--71",
  year     =  1988,
  keywords = "comp-cog-sci"
}

@ARTICLE{Fontana2010-ie,
  title    = "Extending {{Healthy Life {Span--From} Yeast}} to {{Humans}}",
  author   = "Fontana, L and Partridge, L and Longo, V D",
  journal  = "Science",
  volume   =  328,
  number   =  5976,
  pages    = "321--326",
  month    =  apr,
  year     =  2010,
  keywords = "longevity"
}

@INPROCEEDINGS{Gal2016-il,
  title     = "Dropout as a {{Bayesian Approximation}}: {{Representing Model
               Uncertainty}} in {{Deep Learning}}",
  booktitle = "Proceedings of The 33rd International Conference on Machine
               Learning",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
  abstract  = "Deep learning tools have gained tremendous attention in applied
               machine learning. However such tools for regression and
               classification do not capture model uncertainty. In comparison,
               Bayesian models offer a mathematically grounded framework to
               reason about model uncertainty, but usually come with a
               prohibitive computational cost. In this paper we develop a new
               theoretical framework casting dropout training in deep neural
               networks (NNs) as approximate Bayesian inference in deep
               Gaussian processes. A direct result of this theory gives us
               tools to model uncertainty with dropout NNs -- extracting
               information from existing models that has been thrown away so
               far. This mitigates the problem of representing uncertainty in
               deep learning without sacrificing either computational
               complexity or test accuracy. We perform an extensive study of
               the properties of dropout's uncertainty. Various network
               architectures and non-linearities are assessed on tasks of
               regression and classification, using MNIST as an example. We
               show a considerable improvement in predictive log-likelihood and
               RMSE compared to existing state-of-the-art methods, and finish
               by using dropout's uncertainty in deep reinforcement learning.",
  publisher = "PMLR",
  volume    =  48,
  pages     = "1050--1059",
  series    = "Proceedings of Machine Learning Research",
  year      =  2016,
  address   = "New York, New York, USA",
  keywords  = "machine-learning"
}

@ARTICLE{Gandhi2021-hw,
  title    = "Baby {{Intuitions Benchmark}} ({{{BIB}})}: {{Discerning}} the
              Goals, Preferences, and Actions of Others",
  author   = "Gandhi, Kanishk and Stojnic, Gala and Lake, Brenden M and Dillon,
              Moira R",
  journal  = "CoRR",
  volume   = "abs/2102.11938",
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning;comp-cog-sci"
}

@ARTICLE{Gershman2015-pa,
  title    = "Computational Rationality: {{A}} Converging Paradigm for
              Intelligence in Brains, Minds, and Machines",
  author   = "Gershman, Samuel J and Horvitz, Eric J and Tenenbaum, Joshua B",
  abstract = "After growing up together, and mostly growing apart in the second
              half of the 20th century, the fields of artificial intelligence
              (AI), cognitive science, and neuroscience are reconverging on a
              shared view of the computational foundations of intelligence that
              promotes valuable cross-disciplinary exchanges on questions,
              methods, and results. We chart advances over the past several
              decades that address challenges of perception and action under
              uncertainty through the lens of computation. Advances include the
              development of representations and inferential procedures for
              large-scale probabilistic inference and machinery for enabling
              reflection and decisions about tradeoffs in effort, precision,
              and timeliness of computations. These tools are deployed toward
              the goal of computational rationality: identifying decisions with
              highest expected utility, while taking into consideration the
              costs of computation in complex real-world problems in which most
              relevant calculations can only be approximated. We highlight key
              concepts with examples that show the potential for interchange
              between computer science, cognitive science, and neuroscience.",
  journal  = "Science",
  volume   =  349,
  number   =  6245,
  pages    = "273--278",
  month    =  jul,
  year     =  2015,
  keywords = "comp-cog-sci"
}

@ARTICLE{Ghahramani2015-ko,
  title     = "Probabilistic Machine Learning and Artificial Intelligence",
  author    = "Ghahramani, Zoubin",
  abstract  = "How can a machine learn from experience? Probabilistic modelling
               provides a framework for understanding what learning is, and has
               therefore emerged as one of the principal theoretical and
               practical approaches for designing machines that learn from data
               acquired through experience. The probabilistic framework, which
               describes how to represent and manipulate uncertainty about
               models and predictions, has a central role in scientific data
               analysis, machine learning, robotics, cognitive science and
               artificial intelligence. This Review provides an introduction to
               this framework, and discusses some of the state-of-the-art
               advances in the field, namely, probabilistic programming,
               Bayesian optimization, data compression and automatic model
               discovery.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "452--459",
  month     =  may,
  year      =  2015,
  keywords  = "comp-cog-sci"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Girshick_undated-ee,
  title    = "Rich {{Feature Hierarchies}} for {{Accurate Object Detection}}
              and {{Semantic Segmentation}}",
  author   = "Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik,
              Jitendra",
  abstract = "Object detection performance, as measured on the canonical PASCAL
              VOC dataset, has plateaued in the last few years. The
              best-performing methods are complex ensemble systems that
              typically combine multiple low-level image features with
              high-level context. In this paper, we propose a simple and
              scalable detection algorithm that improves mean average precision
              (mAP) by more than 30\% relative to the previous best result on
              VOC 2012---achieving a mAP of 53.3\%. Our approach combines two
              key insights: (1) one can apply high-capacity convolutional
              neural networks (CNNs) to bottom-up region proposals in order to
              localize and segment objects and (2) when labeled training data
              is scarce, supervised pre-training for an auxiliary task,
              followed by domain-specific fine-tuning, yields a significant
              performance boost. Since we combine region proposals with CNNs,
              we call our method R-CNN: Regions with CNN features. We also
              present experiments that provide insight into what the network
              learns, revealing a rich hierarchy of image features. Source code
              for the complete system is available at
              http://www.cs.berkeley.edu/ rbg/rcnn.",
  pages    = "8",
  keywords = "comp-cog-sci"
}

@ARTICLE{Goodman2008-ee,
  title    = "A {{Rational Analysis}} of {{{Rule-Based} Concept Learning}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Feldman, Jacob and
              Griffiths, Thomas L",
  abstract = "This article proposes a new model of human concept learning that
              provides a rational analysis of learning feature-based concepts.
              This model is built upon Bayesian inference for a grammatically
              structured hypothesis space---a concept language of logical
              rules. This article compares the model predictions to human
              generalization judgments in several well-known category learning
              experiments, and finds good agreement for both average and
              individual participant generalizations. This article further
              investigates judgments for a broad set of 7-feature concepts---a
              more natural setting in several ways---and again finds that the
              model explains human performance.",
  journal  = "Cogn. Sci.",
  volume   =  32,
  number   =  1,
  pages    = "108--154",
  month    =  jan,
  year     =  2008,
  keywords = "comp-cog-sci"
}

@ARTICLE{Goodman2014-kq,
  title    = "Concepts in a {{Probabilistic Language}} of {{Thought}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, Tobias",
  pages    = "25",
  year     =  2014,
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Goyal2020-od,
  title    = "Recurrent {{Independent Mechanisms}}",
  author   = "Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani,
              Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf,
              Bernhard",
  abstract = "Learning modular structures which reflect the dynamics of the
              environment can lead to better generalization and robustness to
              changes which only affect a few of the underlying causes. We
              propose Recurrent Independent Mechanisms (RIMs), a new recurrent
              architecture in which multiple groups of recurrent cells operate
              with nearly independent transition dynamics, communicate only
              sparingly through the bottleneck of attention, and are only
              updated at time steps where they are most relevant. We show that
              this leads to specialization amongst the RIMs, which in turn
              allows for dramatically improved generalization on tasks where
              some factors of variation differ systematically between training
              and evaluation.",
  month    =  nov,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@MISC{Goyal2021-ki,
  title     = "Inductive {{Biases}} for {{Deep Learning}} of {{{Higher-Level}
               Cognition}}",
  author    = "Goyal, Anirudh and Bengio, Yoshua",
  abstract  = "A fascinating hypothesis is that human and animal intelligence
               could be explained by a few principles (rather than an
               encyclopedic list of heuristics). If that hypothesis was
               correct, we could more easily both understand our own
               intelligence and build intelligent machines. Just like in
               physics, the principles themselves would not be sufficient to
               predict the behavior of complex systems like brains, and
               substantial computation might be needed to simulate human-like
               intelligence. This hypothesis would suggest that studying the
               kind of inductive biases that humans and animals exploit could
               help both clarify these principles and provide inspiration for
               AI research and neuroscience theories. Deep learning already
               exploits several key inductive biases, and this work considers a
               larger list, focusing on those which concern mostly higher-level
               and sequential conscious processing. The objective of clarifying
               these particular principles is that they could potentially help
               us build AI systems benefiting from humans' abilities in terms
               of flexible out-of-distribution and systematic generalization,
               which is currently an area where a large gap exists between
               state-of-the-art machine learning and human intelligence.",
  journal   = "arXiv [cs, stat]",
  publisher = "arXiv",
  number    = "arXiv:2011.15091",
  month     =  feb,
  year      =  2021,
  keywords  = "Computer Science - Artificial Intelligence,Computer Science -
               Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@UNPUBLISHED{Greff2020-gr,
  title    = "On the {{Binding Problem}} in {{Artificial Neural Networks}}",
  author   = "Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber,
              J{\"u}rgen",
  abstract = "Contemporary neural networks still fall short of human-level
              generalization, which extends far beyond our direct experiences.
              In this paper, we argue that the underlying cause for this
              shortcoming is their inability to dynamically and flexibly bind
              information that is distributed throughout the network. This
              binding problem affects their capacity to acquire a compositional
              understanding of the world in terms of symbol-like entities (like
              objects), which is crucial for generalizing in predictable and
              systematic ways. To address this issue, we propose a unifying
              framework that revolves around forming meaningful entities from
              unstructured sensory inputs (segregation), maintaining this
              separation of information at a representational level
              (representation), and using these entities to construct new
              inferences, predictions, and behaviors (composition). Our
              analysis draws inspiration from a wealth of research in
              neuroscience and cognitive psychology, and surveys relevant
              mechanisms from the machine learning literature, to help identify
              a combination of inductive biases that allow symbolic information
              processing to emerge naturally in neural networks. We believe
              that a compositional approach to AI, in terms of grounded
              symbol-like representations, is of fundamental importance for
              realizing human-level generalization, and we hope that this paper
              may contribute towards that goal as a reference and inspiration.",
  month    =  dec,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,I.2.6;comp-cog-sci"
}

@ARTICLE{Griffiths2010-wc,
  title    = "Probabilistic Models of Cognition: Exploring Representations and
              Inductive Biases",
  author   = "Griffiths, Thomas L and Chater, Nick and Kemp, Charles and
              Perfors, Amy and Tenenbaum, Joshua B",
  journal  = "Trends Cogn. Sci.",
  volume   =  14,
  number   =  8,
  pages    = "357--364",
  month    =  aug,
  year     =  2010,
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Grosse2012-pf,
  title    = "Exploiting Compositionality to Explore a Large Space of Model
              Structures",
  author   = "Grosse, Roger and Salakhutdinov, Ruslan R and Freeman, William T
              and Tenenbaum, Joshua B",
  abstract = "The recent proliferation of richly structured probabilistic
              models raises the question of how to automatically determine an
              appropriate model for a dataset. We investigate this question for
              a space of matrix decomposition models which can express a
              variety of widely used models from unsupervised learning. To
              enable model selection, we organize these models into a
              context-free grammar which generates a wide variety of structures
              through the compositional application of a few simple rules. We
              use our grammar to generically and efficiently infer latent
              components and estimate predictive likelihood for nearly 2500
              structures using a small toolbox of reusable algorithms. Using a
              greedy search over our grammar, we automatically choose the
              decomposition structure from raw data by evaluating only a small
              fraction of all models. The proposed method typically finds the
              correct structure for synthetic data and backs off gracefully to
              simpler models under heavy noise. It learns sensible structures
              for datasets as diverse as image patches, motion capture, 20
              Questions, and U.S. Senate votes, all using exactly the same
              code.",
  month    =  oct,
  year     =  2012,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@ARTICLE{Gureckis2012-xz,
  title    = "{Self-{{Directed} Learning}}: {{A Cognitive}} and {{Computational
              Perspective}}",
  author   = "Gureckis, Todd M and Markant, Douglas B",
  abstract = "A widely advocated idea in education is that people learn better
              when the flow of experience is under their control (i.e.,
              learning is self-directed). However, the reasons why volitional
              control might result in superior acquisition and the limits to
              such advantages remain poorly understood. In this article, we
              review the issue from both a cognitive and computational
              perspective. On the cognitive side, self-directed learning allows
              individuals to focus effort on useful information they do not yet
              possess, can expose information that is inaccessible via passive
              observation, and may enhance the encoding and retention of
              materials. On the computational side, the development of
              efficient ``active learning'' algorithms that can select their
              own training data is an emerging research topic in machine
              learning. This review argues that recent advances in these
              related fields may offer a fresh theoretical perspective on how
              people gather information to support their own learning.",
  journal  = "Perspect. Psychol. Sci.",
  volume   =  7,
  number   =  5,
  pages    = "464--481",
  month    =  sep,
  year     =  2012,
  keywords = "active learning,intervention-based causal learning,machine
              learning,self-directed learning,self-regulated study;comp-cog-sci"
}

@INCOLLECTION{Hauser2004-go,
  title     = "Evolutionary and Developmental Foundations of Human Knowledge",
  booktitle = "The {{Cognitive Neurosciences Iii}}",
  author    = "Hauser, Marc D and Spelke, Elizabeth",
  editor    = "Gazzaniga, Michael S",
  publisher = "MIT Press",
  year      =  2004,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Hawkins2019-jn,
  title    = "A {{Framework}} for {{Intelligence}} and {{Cortical Function
              Based}} on {{Grid Cells}} in the {{Neocortex}}",
  author   = "Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy,
              Scott and Ahmad, Subutai",
  journal  = "Front. Neural Circuits",
  volume   =  12,
  pages    = "121",
  month    =  jan,
  year     =  2019,
  keywords = "comp-cog-sci"
}

@INPROCEEDINGS{He2016-if,
  title      = "Deep {{Residual Learning}} for {{Image Recognition}}",
  booktitle  = "2016 {{{IEEE} Conference}} on {{Computer Vision}} and {{Pattern
                Recognition}} ({{{CVPR}}})",
  author     = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  abstract   = "Deeper neural networks are more difficult to train. We present
                a residual learning framework to ease the training of networks
                that are substantially deeper than those used previously. We
                explicitly reformulate the layers as learning residual
                functions with reference to the layer inputs, instead of
                learning unreferenced functions. We provide comprehensive
                empirical evidence showing that these residual networks are
                easier to optimize, and can gain accuracy from considerably
                increased depth. On the ImageNet dataset we evaluate residual
                nets with a depth of up to 152 layers---8$\times$ deeper than
                VGG nets [40] but still having lower complexity. An ensemble of
                these residual nets achieves 3.57\% error on the ImageNet test
                set. This result won the 1st place on the ILSVRC 2015
                classification task. We also present analysis on CIFAR-10 with
                100 and 1000 layers.",
  publisher  = "IEEE",
  pages      = "770--778",
  month      =  jun,
  year       =  2016,
  address    = "Las Vegas, NV, USA",
  keywords   = "machine-learning",
  conference = "2016 IEEE Conference on Computer Vision and Pattern Recognition
                (CVPR)"
}

@ARTICLE{Hill2019-uj,
  title    = "{{{BDNF}}}, Endurance Activity, and Mechanisms Underlying the
              Evolution of Hominin Brains",
  author   = "Hill, Tyler and Polk, John D",
  abstract = "Objectives As a complex, polygenic trait, brain size has likely
              been influenced by a range of direct and indirect selection
              pressures for both cognitive and non-cognitive functions and
              capabilities. It has been hypothesized that hominin brain
              expansion was, in part, a correlated response to selection acting
              on aerobic capacity (Raichlen \& Polk, 2013). According to this
              hypothesis, selection for aerobic capacity increased the activity
              of various signaling molecules, including those involved in brain
              growth. One key molecule is brain-derived neurotrophic factor
              (BDNF), a protein that regulates neuronal development, survival,
              and plasticity in mammals. This review updates, partially tests,
              and expands Raichlen and Polk's (2013) hypothesis by evaluating
              evidence for BDNF as a mediator of brain size. Discussion We
              contend that selection for endurance capabilities in a hot
              climate favored changes to muscle composition, mitochondrial
              dynamics and increased energy budget through pathways involving
              regulation of PGC-1$\alpha$ and MEF2 genes, both of which promote
              BDNF activity. In addition, the evolution of hairlessness and the
              skin's thermoregulatory response provide other molecular pathways
              that promote both BDNF activity and neurotransmitter synthesis.
              We discuss how these pathways contributed to the evolution of
              brain size and function in human evolution and propose avenues
              for future research. Our results support Raichlen and Polk's
              contention that selection for non-cognitive functions has direct
              mechanistic linkages to the evolution of brain size in hominins.",
  journal  = "Am. J. Phys. Anthropol.",
  volume   =  168,
  number   = "S67",
  pages    = "47--62",
  year     =  2019,
  keywords = "BDNF,brain
              growth,exercise,MEF2,neurotrophins,PGC-1$\alpha$,thermoregulation;exercise-brain"
}

@ARTICLE{Hung2018-fi,
  title     = "Effect of {{Acute Exercise Mode}} on {{Serum {Brain-Derived}
               Neurotrophic Factor}} ({{{BDNF}}}) and {{Task Switching
               Performance}}",
  author    = "Hung, Chiao-Ling and Tseng, Jun-Wei and Chao, Hsiao-Han and
               Hung, Tsung-Min and Wang, Ho-Seng",
  abstract  = "Previous studies have consistently reported a positive effect of
               acute exercise on cognition, particularly on executive function.
               However, most studies have focused on aerobic and resistant
               forms of exercise. The purpose of this study was to compare the
               effect of `open-skill' with `closed-skill' exercise (defined in
               terms of the predictability of the performing environment) on
               brain-derived neurotrophic factor (BDNF) production and task
               switching performance. Twenty young adult males participated in
               both closed (running) and open (badminton) skill exercise
               sessions in a counterbalanced order on separate days. The
               exercise sessions consisted of 5 min of warm up exercises
               followed by 30 min of running or badminton. The exercise
               intensity was set at 60\% ($\pm$5\%) of the heart rate reserve
               level (HRR) with HR being monitored by a wireless heart rate
               monitor. Blood samples were taken and participation in a
               task-switching paradigm occurred before and after each exercise
               session. Results showed no differences in serum BDNF or
               task-switching performance at the pre-test stage, however,
               badminton exercise resulted in significantly higher serum BDNF
               levels (a proxy for levels of BDNF in the brain) and near
               significant smaller global switching costs relative to running.
               This study has provided preliminary evidence in support the
               relative benefits of open-skills exercises on BDNF and executive
               function.",
  journal   = "J. Clin. Med. Res.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  10,
  pages     = "301",
  month     =  oct,
  year      =  2018,
  keywords  = "closed-skill,executive function,open-skill,switch
               cost;exercise-brain"
}

@ARTICLE{Japkowicz2000-wi,
  title    = "Nonlinear {{Autoassociation Is Not Equivalent}} to {{PCA}}",
  author   = "Japkowicz, Nathalie and Hanson, Stephen Jos{\'e} and Gluck, Mark
              A",
  abstract = "A common misperception within the neural network community is
              that even with nonlinearities in their hidden layer,
              autoassociators trained with backpropagation are equivalent to
              linear methods such as principal component analysis (PCA). Our
              purpose is to demonstrate that nonlinear autoassociators actually
              behave differently from linear methods and that they can
              outperform these methods when used for latent extraction,
              projection, and classification. While linear autoassociators
              emulate PCA, and thus exhibit a flat or unimodal reconstruction
              error surface, autoassociators with nonlinearities in their
              hidden layer learn domains by building error reconstruction
              surfaces that, depending on the task, contain multiple local
              valleys. This interpolation bias allows nonlinear autoassociators
              to represent appropriate classifications of nonlinear multimodal
              domains, in contrast to linear autoassociators, which are
              inappropriate for such tasks. In fact, autoassociators with
              hidden unit nonlinearities can be shown to perform nonlinear
              classification and nonlinear recognition.",
  journal  = "Neural Comput.",
  volume   =  12,
  number   =  3,
  pages    = "531--545",
  month    =  mar,
  year     =  2000,
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Johnson2021-mw,
  title    = "Fast and Flexible: {{Human}} Program Induction in Abstract
              Reasoning Tasks",
  author   = "Johnson, Aysja and Vong, Wai Keen and Lake, Brenden M and
              Gureckis, Todd M",
  abstract = "The Abstraction and Reasoning Corpus (ARC) is a challenging
              program induction dataset that was recently proposed by Chollet
              (2019). Here, we report the first set of results collected from a
              behavioral study of humans solving a subset of tasks from ARC (40
              out of 1000). Although this subset of tasks contains considerable
              variation, our results showed that humans were able to infer the
              underlying program and generate the correct test output for a
              novel test input example, with an average of 80\% of tasks solved
              per participant, and with 65\% of tasks being solved by more than
              80\% of participants. Additionally, we find interesting patterns
              of behavioral consistency and variability within the action
              sequences during the generation process, the natural language
              descriptions to describe the transformations for each task, and
              the errors people made. Our findings suggest that people can
              quickly and reliably determine the relevant features and
              properties of a task to compose a correct solution. Future
              modeling work could incorporate these findings, potentially by
              connecting the natural language descriptions we collected here to
              the underlying semantics of ARC.",
  month    =  mar,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Human-Computer Interaction,Computer Science - Machine
              Learning;comp-cog-sci"
}

@ARTICLE{Kemp2009-oz,
  title    = "Structured Statistical Models of Inductive Reasoning",
  author   = "Kemp, Charles and Tenenbaum, Joshua B",
  abstract = "Everyday inductive inferences are often guided by rich background
              knowledge. Formal models of induction should aim to incorporate
              this knowledge and should explain how different kinds of
              knowledge lead to the distinctive patterns of reasoning found in
              different inductive contexts. This article presents a Bayesian
              framework that attempts to meet both goals and describe 4
              applications of the framework: a taxonomic model, a spatial
              model, a threshold model, and a causal model. Each model makes
              probabilistic inferences about the extensions of novel
              properties, but the priors for the 4 models are defined over
              different kinds of structures that capture different
              relationships between the categories in a domain. The framework
              therefore shows how statistical inference can operate over
              structured background knowledge, and the authors argue that this
              interaction between structure and statistics is critical for
              explaining the power and flexibility of human reasoning.",
  journal  = "Psychol. Rev.",
  volume   =  116,
  number   =  1,
  pages    = "20--58",
  year     =  2009,
  keywords = "comp-cog-sci"
}

@ARTICLE{Kruschke1993-eg,
  title    = "Human {{Category Learning}}: {{Implications}} for
              {{Backpropagation Models}}",
  author   = "Kruschke, John K",
  abstract = "Backpropagation (Rumelhart et al., 1986a) was proposed as a
              general learning algorithm for multi-layer perceptrons. This a n
              d e demonstrates chat a standard version of backprop fails to
              attend selectively to input dimensions in the same way as humans,
              suffers catastrophic forgetting of previously learned
              associations when novel exemplars are [rained, and can be overly
              sensitive to linear categoy boundaries. Another connecrionist
              model, A L C O V E (Krwchke 1990, 1992), does nor suffer those
              failures. Previous researchers identified these problems; the
              present article repons quantitative fits of the models to new
              human learning data. A L C O V E can be functionally approximated
              by a network that uses linear-sigmoid hidden nodes, like standard
              backprop. Ir is argued that models of human category learning
              should incorporate quasi-local representations and dimensional
              artention learning, as well as error-driuen learning, to address
              simulraneously all three phenomena.",
  journal  = "Conn. Sci.",
  volume   =  5,
  number   =  1,
  pages    = "3--36",
  month    =  jan,
  year     =  1993,
  keywords = "comp-cog-sci"
}

@ARTICLE{Lake2015-ap,
  title    = "{Human-Level} Concept Learning through Probabilistic Program
              Induction",
  author   = "Lake, B M and Salakhutdinov, R and Tenenbaum, J B",
  journal  = "Science",
  volume   =  350,
  number   =  6266,
  pages    = "1332--1338",
  month    =  dec,
  year     =  2015,
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Lake2016-zm,
  title    = "Building {{Machines That Learn}} and {{Think Like People}}",
  author   = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and
              Gershman, Samuel J",
  abstract = "Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people.
              Many advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats
              humans in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we
              argue that these machines should (a) build causal models of the
              world that support explanation and understanding, rather than
              merely solving pattern recognition problems; (b) ground learning
              in intuitive theories of physics and psychology, to support and
              enrich the knowledge that is learned; and (c) harness
              compositionality and learning-to-learn to rapidly acquire and
              generalize knowledge to new tasks and situations. We suggest
              concrete challenges and promising routes towards these goals that
              can combine the strengths of recent neural network advances with
              more structured cognitive models.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Lake2018-gk,
  title    = "The {{Emergence}} of {{Organizing Structure}} in {{Conceptual
              Representation}}",
  author   = "Lake, Brenden M and Lawrence, Neil D and Tenenbaum, Joshua B",
  abstract = "Both scientists and children make important structural
              discoveries, yet their computational underpinnings are not well
              understood. Structure discovery has previously been formalized as
              probabilistic inference about the right structural form---where
              form could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum,
              2008). Although this approach can learn intuitive organizations,
              including a tree for animals and a ring for the color circle, it
              assumes a strong inductive bias that considers only these
              particular forms, and each form is explicitly provided as initial
              knowledge. Here we introduce a new computational model of how
              organizing structure can be discovered, utilizing a broad
              hypothesis space with a preference for sparse connectivity. Given
              that the inductive bias is more general, the model's initial
              knowledge shows little qualitative resemblance to some of the
              discoveries it supports. As a consequence, the model can also
              learn complex structures for domains that lack intuitive
              description, as well as predict human property induction
              judgments without explicit structural forms. By allowing form to
              emerge from sparsity, our approach clarifies how both the
              richness and flexibility of human conceptual organization can
              coexist.",
  journal  = "Cogn. Sci.",
  volume   =  42,
  number   = "S3",
  pages    = "809--832",
  year     =  2018,
  keywords = "Bayesian modeling,Sparsity,Structure discovery,Unsupervised
              learning;comp-cog-sci"
}

@UNPUBLISHED{Lake2019-dt,
  title    = "Compositional Generalization through Meta {Sequence-to-Sequence}
              Learning",
  author   = "Lake, Brenden M",
  abstract = "People can learn a new concept and use it compositionally,
              understanding how to ``blicket twice'' after learning how to
              ``blicket.'' In contrast, powerful sequence-tosequence (seq2seq)
              neural networks fail such tests of compositionality, especially
              when composing new concepts together with existing concepts. In
              this paper, I show how memory-augmented neural networks can be
              trained to generalize compositionally through meta seq2seq
              learning. In this approach, models train on a series of seq2seq
              problems to acquire the compositional skills needed to solve new
              seq2seq problems. Meta se2seq learning solves several of the SCAN
              tests for compositional learning and can learn to apply implicit
              rules to variables.",
  month    =  oct,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Machine
              Learning;comp-cog-sci"
}

@UNPUBLISHED{Lake2019-ng,
  title    = "Human {Few-Shot} Learning of Compositional Instructions",
  author   = "Lake, Brenden M and Linzen, Tal and Baroni, Marco",
  abstract = "People learn in fast and flexible ways that have not been
              emulated by machines. Once a person learns a new verb ``dax,'' he
              or she can effortlessly understand how to ``dax twice,'' ``walk
              and dax,'' or ``dax vigorously.'' There have been striking recent
              improvements in machine learning for natural language processing,
              yet the best algorithms require vast amounts of experience and
              struggle to generalize new concepts in compositional ways. To
              better understand these distinctively human abilities, we study
              the compositional skills of people through languagelike
              instruction learning tasks. Our results show that people can
              learn and use novel functional concepts from very few examples
              (few-shot learning), successfully applying familiar functions to
              novel inputs. People can also compose concepts in complex ways
              that go beyond the provided demonstrations. Two additional
              experiments examined the assumptions and inductive biases that
              people make when solving these tasks, revealing three biases:
              mutual exclusivity, one-to-one mappings, and iconic
              concatenation. We discuss the implications for cognitive modeling
              and the potential for building machines with more human-like
              language learning capabilities.",
  month    =  may,
  year     =  2019,
  keywords = "Computer Science - Computation and Language;comp-cog-sci"
}

@ARTICLE{Lake2019-zq,
  title    = "The {{Omniglot}} Challenge: A 3-Year Progress Report",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "97--104",
  month    =  oct,
  year     =  2019,
  keywords = "comp-cog-sci"
}

@ARTICLE{Lake_undated-bl,
  title    = "Concept Learning as Motor Program Induction: {{A}} {Large-Scale}
              Empirical Study",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  abstract = "Human concept learning is particularly impressive in two
              respects: the internal structure of concepts can be
              representationally rich, and yet the very same concepts can also
              be learned from just a few examples. Several decades of research
              have dramatically advanced our understanding of these two aspects
              of concepts. While the richness and speed of concept learning are
              most often studied in isolation, the power of human concepts may
              be best explained through their synthesis. This paper presents a
              large-scale empirical study of one-shot concept learning,
              suggesting that rich generative knowledge in the form of a motor
              program can be induced from just a single example of a novel
              concept. Participants were asked to draw novel handwritten
              characters given a reference form, and we recorded the motor data
              used for production. Multiple drawers of the same character not
              only produced visually similar drawings, but they also showed a
              striking correspondence in their strokes, as measured by their
              number, shape, order, and direction. This suggests that
              participants can infer a rich motorbased concept from a single
              example. We also show that the motor programs induced by
              individual subjects provide a powerful basis for one-shot
              classification, yielding far higher accuracy than
              state-of-the-art pattern recognition methods based on just the
              visual form.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Lake2021-tz,
  title     = "Word Meaning in Minds and Machines",
  author    = "Lake, Brenden M and Murphy, Gregory L",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  year      =  2021,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Lake2020-wr,
  title    = "People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few
              Examples}}",
  author   = "Lake, Brenden M and Piantadosi, Steven T",
  abstract = "Machine learning has made major advances in categorizing objects
              in images, yet the best algorithms miss important aspects of how
              people learn and think about categories. People can learn richer
              concepts from fewer examples, including causal models that
              explain how members of a category are formed. Here, we explore
              the limits of this human ability to infer causal
              ``programs''---latent generating processes with nontrivial
              algorithmic properties---from one, two, or three visual examples.
              People were asked to extrapolate the programs in several ways,
              for both classifying and generating new examples. As a theory of
              these inductive abilities, we present a Bayesian program learning
              model that searches the space of programs for the best
              explanation of the observations. Although variable, people's
              judgments are broadly consistent with the model and inconsistent
              with several alternatives, including a pretrained deep neural
              network for object recognition, indicating that people can learn
              and reason with rich algorithmic abstractions from sparse input
              data.",
  journal  = "Computational Brain \& Behavior",
  volume   =  3,
  number   =  1,
  pages    = "54--65",
  month    =  mar,
  year     =  2020,
  keywords = "comp-cog-sci"
}

@ARTICLE{Landau1988-oy,
  title    = "The Importance of Shape in Early Lexical Learning",
  author   = "Landau, Barbara and Smith, Linda B and Jones, Susan S",
  journal  = "Cogn. Dev.",
  volume   =  3,
  number   =  3,
  pages    = "299--321",
  month    =  jul,
  year     =  1988,
  keywords = "development"
}

@ARTICLE{LeCun2015-hm,
  title     = "Deep Learning",
  author    = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  abstract  = "Deep learning allows computational models that are composed of
               multiple processing layers to learn representations of data with
               multiple levels of abstraction. These methods have dramatically
               improved the state-of-the-art in speech recognition, visual
               object recognition, object detection and many other domains such
               as drug discovery and genomics. Deep learning discovers
               intricate structure in large data sets by using the
               backpropagation algorithm to indicate how a machine should
               change its internal parameters that are used to compute the
               representation in each layer from the representation in the
               previous layer. Deep convolutional nets have brought about
               breakthroughs in processing images, video, speech and audio,
               whereas recurrent nets have shone light on sequential data such
               as text and speech.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "436--444",
  month     =  may,
  year      =  2015,
  keywords  = "machine-learning"
}

@INPROCEEDINGS{Lerer2016-kb,
  title      = "Learning {{Physical Intuition}} of {{Block Towers}} by
                {{Example}}",
  booktitle  = "International {{Conference}} on {{Machine Learning}}",
  author     = "Lerer, Adam and Gross, Sam and Fergus, Rob",
  publisher  = "PMLR",
  pages      = "430--438",
  month      =  jun,
  year       =  2016,
  keywords   = "comp-cog-sci",
  conference = "International Conference on Machine Learning"
}

@ARTICLE{Levine2014-bg,
  title    = "Low {{Protein Intake Is Associated}} with a {{Major Reduction}}
              in {{{IGF-1}}}, {{Cancer}}, and {{Overall Mortality}} in the 65
              and {{Younger}} but {{Not Older Population}}",
  author   = "Levine, Morgan E and Suarez, Jorge A and Brandhorst, Sebastian
              and Balasubramanian, Priya and Cheng, Chia-Wei and Madia,
              Federica and Fontana, Luigi and Mirisola, Mario G and
              Guevara-Aguirre, Jaime and Wan, Junxiang and Passarino, Giuseppe
              and Kennedy, Brian K and Wei, Min and Cohen, Pinchas and
              Crimmins, Eileen M and Longo, Valter D",
  abstract = "Mice and humans with growth hormone receptor/ IGF-1 deficiencies
              display major reductions in agerelated diseases. Because protein
              restriction reduces GHR-IGF-1 activity, we examined links between
              protein intake and mortality. Respondents aged 50--65 reporting
              high protein intake had a 75\% increase in overall mortality and
              a 4-fold increase in cancer death risk during the following 18
              years. These associations were either abolished or attenuated if
              the proteins were plant derived. Conversely, high protein intake
              was associated with reduced cancer and overall mortality in
              respondents over 65, but a 5-fold increase in diabetes mortality
              across all ages. Mouse studies confirmed the effect of high
              protein intake and GHR-IGF-1 signaling on the incidence and
              progression of breast and melanoma tumors, but also the
              detrimental effects of a low protein diet in the very old. These
              results suggest that low protein intake during middle age
              followed by moderate to high protein consumption in old adults
              may optimize healthspan and longevity.",
  journal  = "Cell Metab.",
  volume   =  19,
  number   =  3,
  pages    = "407--417",
  month    =  mar,
  year     =  2014,
  keywords = "longevity"
}

@ARTICLE{Lloyd2014-ua,
  title    = "Automatic {{Construction}} and {{{Natural-Language} Description}}
              of {{Nonparametric Regression Models}}",
  author   = "Lloyd, James and Duvenaud, David and Grosse, Roger and Tenenbaum,
              Joshua and Ghahramani, Zoubin",
  abstract = "This paper presents the beginnings of an automatic statistician,
              focusing on regression problems. Our system explores an
              open-ended space of statistical models to discover a good
              explanation of a data set, and then produces a detailed report
              with figures and natural-language text. Our approach treats
              unknown regression functions nonparametrically using Gaussian
              processes, which has two important consequences. First, Gaussian
              processes can model functions in terms of high-level properties
              (e.g. smoothness, trends, periodicity, changepoints). Taken
              together with the compositional structure of our language of
              models this allows us to automatically describe functions in
              simple terms. Second, the use of flexible nonparametric models
              and a rich language for composing them in an open-ended manner
              also results in state-of-the-art extrapolation performance
              evaluated over 13 real time series data sets from various
              domains.",
  journal  = "Proc. Conf. AAAI Artif. Intell.",
  volume   =  28,
  number   =  1,
  month    =  jun,
  year     =  2014,
  keywords = "Regression;comp-cog-sci"
}

@INCOLLECTION{Lupyan2021-sj,
  title     = "Does {{Vocabulary Help Structure}} the {{Mind}}?",
  booktitle = "Minnesota {{Symposia}} on {{Child Psychology}}",
  author    = "Lupyan, Gary and Zettersten, Martin",
  abstract  = "The idea that language shapes thinking seemed plausible when
               scientists were in the dark about how thinking works. This
               chapter describes several mechanisms by which the words of a
               language can help structure knowledge and navigate cognitive
               problems. It is because thought and language seem so closely
               linked that language is so often used as a window to thought.
               The cognitive priority view faces two serious problems. The
               first is accounting for the cross-linguistic diversity of
               vocabularies. The second problem is the problem of origin. The
               chapter discusses new data aimed at both collecting verbal
               complexity measures independently from the original Bongard
               problems themselves, and collecting more objective measures of
               solution accuracy. It provides further evidence for the idea
               that easier-to-name visual features are more likely to be used
               by people when judging visual similarity.",
  publisher = "John Wiley \& Sons, Ltd",
  pages     = "160--199",
  year      =  2021,
  keywords  = "Bongard problems,cognitive priority,cross-linguistic
               diversity,name visual features,verbal complexity,visual
               similarity,vocabularies;comp-cog-sci"
}

@ARTICLE{Mahowald1991-jj,
  title    = "The {{Silicon Retina}}",
  author   = "Mahowald, Misha A and Mead, Carver",
  journal  = "Sci. Am.",
  pages    = "9",
  year     =  1991,
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Mao2019-ht,
  title    = "The {{{Neuro-Symbolic} Concept Learner}}: {{Interpreting
              Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}",
  author   = "Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum,
              Joshua B and Wu, Jiajun",
  abstract = "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model
              that learns visual concepts, words, and semantic parsing of
              sentences without explicit supervision on any of them; instead,
              our model learns by simply looking at images and reading paired
              questions and answers. Our model builds an object-based scene
              representation and translates sentences into executable, symbolic
              programs. To bridge the learning of two modules, we use a
              neuro-symbolic reasoning module that executes these programs on
              the latent scene representation. Analogical to human concept
              learning, the perception module learns visual concepts based on
              the language description of the object being referred to.
              Meanwhile, the learned visual concepts facilitate learning new
              words and parsing new sentences. We use curriculum learning to
              guide the searching over the large compositional space of images
              and language. Extensive experiments demonstrate the accuracy and
              efficiency of our model on learning visual concepts, word
              representations, and semantic parsing of sentences. Further, our
              method allows easy generalization to new object attributes,
              compositions, language concepts, scenes and questions, and even
              new program domains. It also empowers applications including
              visual question answering and bidirectional image-text retrieval.",
  month    =  apr,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Computer Vision and
              Pattern Recognition,Computer Science - Machine
              Learning;comp-cog-sci"
}

@INCOLLECTION{Minsky2019-mb,
  title     = "A {{Framework For Representing Knowledge}}",
  booktitle = "A {{Framework For Representing Knowledge}}",
  author    = "Minsky, M",
  abstract  = "A Framework For Representing Knowledge was published in Frame
               Conceptions and Text Understanding on page 1.",
  publisher = "De Gruyter",
  pages     = "1--25",
  month     =  jul,
  year      =  2019,
  keywords  = "comp-cog-sci"
}

@BOOK{Minsky1988-bi,
  title     = "Perceptrons: Expanded Edition",
  author    = "Minsky, Marvin L and Papert, Seymour A",
  publisher = "MIT press",
  year      =  1988,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Mnih2015-vd,
  title     = "{Human-Level} Control through Deep Reinforcement Learning",
  author    = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
               Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and
               Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and
               Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
               Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
               Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis",
  abstract  = "An artificial agent is developed that learns to play a diverse
               range of classic Atari 2600 computer games directly from sensory
               experience, achieving a performance comparable to that of an
               expert human player; this work paves the way to building
               general-purpose learning algorithms that bridge the divide
               between perception and action.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  518,
  number    =  7540,
  pages     = "529--533",
  month     =  feb,
  year      =  2015,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Newell1956-lm,
  title    = "The Logic Theory {Machine--{{A}}} Complex Information Processing
              System",
  author   = "Newell, A and Simon, H",
  abstract = "In this paper we describe a complex information processing
              system, which we call the logic theory machine, that is capable
              of discovering proofs for theorems in symbolic logic. This
              system, in contrast to the systematic algorithms that are
              ordinarily employed in computation, relies heavily on heuristic
              methods similar to those that have been observed in . human
              problem solving activity. The specification is written in a
              formal language, of the nature of a pseudo-code, that is suitable
              for coding for digital computers. However, the present paper is
              concerned exclusively with specification of the system, and not
              with its realization in a computer. The logic theory machine is
              part of a program of research to understand complex information
              processing systems by specifying and synthesizing a substantial
              variety of such systems for empirical study.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "61--79",
  month    =  sep,
  year     =  1956,
  keywords = "Automatic programming,Formal languages,Heuristic
              algorithms,Humans,Information analysis,Information
              processing,Logic,Pattern recognition,Problem-solving;comp-cog-sci"
}

@ARTICLE{Nobandegani_undated-cc,
  title    = "Example {{Generation Under Constraints Using Cascade Correlation
              Neural Nets}}",
  author   = "Nobandegani, Ardavan S and Shultz, Thomas R",
  abstract = "Humans not only can effortlessly imagine a wide range of novel
              instances and scenarios when prompted (e.g., a new shirt), but
              more remarkably, they can adequately generate examples which
              satisfy a given set of constraints (e.g., a new, dotted, pink
              shirt). Recently, Nobandegani and Shultz (2017) proposed a
              framework which permits converting deterministic, discriminative
              neural nets into probabilistic generative models. In this work,
              we formally show that an extension of this framework allows for
              generating examples under a wide range of constraints.
              Furthermore, we show that this framework is consistent with
              developmental findings on children's generative abilities, and
              can account for a developmental shift in infants' probabilistic
              learning and reasoning. We discuss the importance of integrating
              Bayesian and connectionist approaches to computational
              developmental psychology, and how our work contributes to that
              research.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Nye_undated-np,
  title    = "Learning {{Compositional Rules}} via {{Neural Program Synthesis}}",
  author   = "Nye, Maxwell I and Solar-Lezama, Armando and Tenenbaum, Joshua B
              and Lake, Brenden M",
  abstract = "Many aspects of human reasoning, including language, require
              learning rules from very little data. Humans can do this, often
              learning systematic rules from very few examples, and combining
              these rules to form compositional rule-based systems. Current
              neural architectures, on the other hand, often fail to generalize
              in a compositional manner, especially when evaluated in ways that
              vary systematically from training. In this work, we present a
              neuro-symbolic model which learns entire rule systems from a
              small set of examples. Instead of directly predicting outputs
              from inputs, we train our model to induce the explicit system of
              rules governing a set of previously seen examples, drawing upon
              techniques from the neural program synthesis literature. Our
              rule-synthesis approach outperforms neural meta-learning
              techniques in three domains: an artificial instruction-learning
              domain used to evaluate human learning, the SCAN challenge
              datasets, and learning rule-based translations of number words
              into integers for a wide range of human languages.",
  pages    = "11",
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Peterson2016-lj,
  title    = "Adapting {{Deep Network Features}} to {{Capture Psychological
              Representations}}",
  author   = "Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L",
  abstract = "Deep neural networks have become increasingly successful at
              solving classic perception problems such as object recognition,
              semantic segmentation, and scene understanding, often reaching or
              surpassing human-level accuracy. This success is due in part to
              the ability of DNNs to learn useful representations of
              high-dimensional inputs, a problem that humans must also solve.
              We examine the relationship between the representations learned
              by these networks and human psychological representations
              recovered from similarity judgments. We find that deep features
              learned in service of object classification account for a
              significant amount of the variance in human similarity judgments
              for a set of animal images. However, these features do not
              capture some qualitative distinctions that are a key part of
              human representations. To remedy this, we develop a method for
              adapting deep features to align with human similarity judgments,
              resulting in image representations that can potentially be used
              to extend the scope of psychological experiments.",
  month    =  aug,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science - Neural
              and Evolutionary Computing;comp-cog-sci"
}

@ARTICLE{Piantadosi2016-na,
  title     = "The Logical Primitives of Thought: {{Empirical}} Foundations for
               Compositional Cognitive Models",
  author    = "Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  volume    =  123,
  number    =  4,
  pages     = "392",
  year      =  2016,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Piantadosi2021-ie,
  title    = "The {{Computational Origin}} of {{Representation}}",
  author   = "Piantadosi, Steven T",
  abstract = "Each of our theories of mental representation provides some
              insight into how the mind works. However, these insights often
              seem incompatible, as the debates between symbolic, dynamical,
              emergentist, sub-symbolic, and grounded approaches to cognition
              attest. Mental representations---whatever they are---must share
              many features with each of our theories of representation, and
              yet there are few hypotheses about how a synthesis could be
              possible. Here, I develop a theory of the underpinnings of
              symbolic cognition that shows how sub-symbolic dynamics may give
              rise to higher-level cognitive representations of structures,
              systems of knowledge, and algorithmic processes. This theory
              implements a version of conceptual role semantics by positing an
              internal universal representation language in which learners may
              create mental models to capture dynamics they observe in the
              world. The theory formalizes one account of how truly novel
              conceptual content may arise, allowing us to explain how even
              elementary logical and computational operations may be learned
              from a more primitive basis. I provide an implementation that
              learns to represent a variety of structures, including logic,
              number, kinship trees, regular languages, context-free languages,
              domains of theories like magnetism, dominance hierarchies, list
              structures, quantification, and computational primitives like
              repetition, reversal, and recursion. This account is based on
              simple discrete dynamical processes that could be implemented in
              a variety of different physical or biological systems. In
              particular, I describe how the required dynamics can be directly
              implemented in a connectionist framework. The resulting theory
              provides an ``assembly language'' for cognition, where high-level
              theories of symbolic computation can be implemented in simple
              dynamics that themselves could be encoded in biologically
              plausible systems.",
  journal  = "Minds Mach.",
  volume   =  31,
  number   =  1,
  pages    = "1--58",
  month    =  mar,
  year     =  2021,
  keywords = "comp-cog-sci"
}

@ARTICLE{Piantadosi2016-fe,
  title    = "Four {{Problems Solved}} by the {{Probabilistic Language}} of
              {{Thought}}",
  author   = "Piantadosi, Steven T and Jacobs, Robert A",
  abstract = "We argue for the advantages of the probabilistic language of
              thought (pLOT), a recently emerging approach to modeling human
              cognition. Work using this framework demonstrates how the pLOT
              (a) refines the debate between symbols and statistics in
              cognitive modeling, (b) permits theories that draw on insights
              from both nativist and empiricist approaches, (c) explains the
              origins of novel and complex computational concepts, and (d)
              provides a framework for abstraction that can link sensation and
              conception. In each of these areas, the pLOT provides a
              productive middle ground between historical divides in cognitive
              psychology, pointing to a promising way forward for the field.",
  journal  = "Curr. Dir. Psychol. Sci.",
  volume   =  25,
  number   =  1,
  pages    = "54--59",
  month    =  feb,
  year     =  2016,
  keywords = "comp-cog-sci"
}

@ARTICLE{Piloto2022-yi,
  title    = "Intuitive Physics Learning in a {Deep-Learning} Model Inspired by
              Developmental Psychology",
  author   = "Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and
              Botvinick, Matthew",
  abstract = "Abstract `Intuitive physics' enables our pragmatic engagement
              with the physical world and forms a key component of `common
              sense' aspects of thought. Current artificial intelligence
              systems pale in their understanding of intuitive physics, in
              comparison to even very young children. Here we address this gap
              between humans and machines by drawing on the field of
              developmental psychology. First, we introduce and open-source a
              machine-learning dataset designed to evaluate conceptual
              understanding of intuitive physics, adopting the
              violation-of-expectation (VoE) paradigm from developmental
              psychology. Second, we build a deep-learning system that learns
              intuitive physics directly from visual data, inspired by studies
              of visual cognition in children. We demonstrate that our model
              can learn a diverse set of physical concepts, which depends
              critically on object-level representations, consistent with
              findings from developmental psychology. We consider the
              implications of these results both for AI and for research on
              human cognition.",
  journal  = "Nature Human Behaviour",
  month    =  jul,
  year     =  2022,
  keywords = "development"
}

@ARTICLE{Pitt_undated-sp,
  title    = "Exact {{Number Concepts Are Limited}} to the {{Verbal Count
              Range}}",
  author   = "Pitt, Benjamin and Gibson, Edward and Piantadosi, Steven T",
  abstract = "Previous findings suggest that mentally representing exact
              numbers larger than four depends on a verbal count routine (e.g.,
              ``one, two, three . . .''). However, these findings are
              controversial because they rely on comparisons across radically
              different languages and cultures. We tested the role of language
              in number concepts within a single population---the Tsimane' of
              Bolivia---in which knowledge of number words varies across
              individual adults. We used a novel data-analysis model to
              quantify the point at which participants (N = 30) switched from
              exact to approximate number representations during a simple
              numerical matching task. The results show that these behavioral
              switch points were bounded by participants' verbal count ranges;
              their representations of exact cardinalities were limited to the
              number words they knew. Beyond that range, they resorted to
              numerical approximation. These results resolve competing accounts
              of previous findings and provide unambiguous evidence that large
              exact number concepts are enabled by language.",
  pages    = "11",
  keywords = "comp-cog-sci"
}

@ARTICLE{Premack1997-aj,
  title    = "Infants {{Attribute {Value}}$\pm$} to the {{{Goal-Directed}
              Actions}} of {{Self-propelled Objects}}",
  author   = "Premack, David and Premack, Ann James",
  abstract = "Motion is a fundamental source of information for basic human
              interpretations; it is basic to the fundamental concept of
              causality and, the present model argues, equally basic to the
              fundamental concept of intentionality.The model is based on two
              main assumptions: When an infant perceives an object (1) moving
              spontaneously and (2) displaying goaldirected action, it will
              interpret the object as intentional and assign to it the unique
              properties of the psychological domain. The key property tested
              was: Do infants attribute value to interactions between
              intentional objects using criteria specified by the model?We
              showed infants (average age 52 weeks) computer-generated
              animations of spontaneously moving ``balls,'' using looking time
              in a standard habituation/dishabituation paradigm. In two
              positive interactions, one ball either ``caressed'' another, or
              ``helped'' it achieve its goal; whereas in two negative
              interactions, one ball either ``hit`` another, or ``prevented''
              it from achieving its goal. In keeping with predictions of the
              model, when transferred to a negative condition, infants who had
              been habituated on a positive condition showed greater
              dishabituation than those habituated on a negative condition. The
              results could not be easily explained by the similarity relations
              among the animations depicting the interactions.The results
              suggest that well before the age when the child can ascribe
              mental states or has a ``theory of mind,'' it recognizes the
              goals of self-propelled objects and attributes value to the
              interactions between them.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  9,
  number   =  6,
  pages    = "848--856",
  month    =  nov,
  year     =  1997,
  keywords = "development"
}

@UNPUBLISHED{Radford2021-jz,
  title    = "Learning {{Transferable Visual Models From Natural Language
              Supervision}}",
  author   = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
              Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
              and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
              Krueger, Gretchen and Sutskever, Ilya",
  abstract = "State-of-the-art computer vision systems are trained to predict a
              fixed set of predetermined object categories. This restricted
              form of supervision limits their generality and usability since
              additional labeled data is needed to specify any other visual
              concept. Learning directly from raw text about images is a
              promising alternative which leverages a much broader source of
              supervision. We demonstrate that the simple pre-training task of
              predicting which caption goes with which image is an efficient
              and scalable way to learn SOTA image representations from scratch
              on a dataset of 400 million (image, text) pairs collected from
              the internet. After pre-training, natural language is used to
              reference learned visual concepts (or describe new ones) enabling
              zero-shot transfer of the model to downstream tasks. We study the
              performance of this approach by benchmarking on over 30 different
              existing computer vision datasets, spanning tasks such as OCR,
              action recognition in videos, geo-localization, and many types of
              fine-grained object classification. The model transfers
              non-trivially to most tasks and is often competitive with a fully
              supervised baseline without the need for any dataset specific
              training. For instance, we match the accuracy of the original
              ResNet-50 on ImageNet zero-shot without needing to use any of the
              1.28 million training examples it was trained on. We release our
              code and pre-trained model weights at
              https://github.com/OpenAI/CLIP.",
  month    =  feb,
  year     =  2021,
  keywords = "Computer Science - Computer Vision and Pattern
              Recognition,Computer Science - Machine Learning;machine-learning"
}

@UNPUBLISHED{Rezende2016-ge,
  title    = "{One-{{Shot} Generalization}} in {{Deep Generative Models}}",
  author   = "Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo
              and Gregor, Karol and Wierstra, Daan",
  abstract = "Humans have an impressive ability to reason about new concepts
              and experiences from just a single example. In particular, humans
              have an ability for one-shot generalization: an ability to
              encounter a new concept, understand its structure, and then be
              able to generate compelling alternative variations of the
              concept. We develop machine learning systems with this important
              capacity by developing new deep generative models, models that
              combine the representational power of deep learning with the
              inferential power of Bayesian reasoning. We develop a class of
              sequential generative models that are built on the principles of
              feedback and attention. These two characteristics lead to
              generative models that are among the state-of-the art in density
              estimation and image generation. We demonstrate the one-shot
              generalization ability of our models using three tasks:
              unconditional sampling, generating new exemplars of a given
              concept, and generating new exemplars of a family of concepts. In
              all cases our models are able to generate compelling and diverse
              samples---having seen new examples just once---providing an
              important class of general-purpose models for one-shot machine
              learning.",
  month    =  may,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;machine-learning"
}

@ARTICLE{Rich2018-zj,
  title     = "The Limits of Learning: {{Exploration}}, Generalization, and the
               Development of Learning Traps",
  author    = "Rich, Alexander S and Gureckis, Todd M",
  abstract  = "Learning usually improves the accuracy of beliefs through the
               accumulation of experience. But are there limits to learning
               that prevent us from accurately understanding our world? In this
               article we investigate the concept of a ``learning trap''---the
               formation of a stable false belief even with extensive
               experience. Our review highlights how these traps develop
               through the interaction of learning and decision making in
               unknown environments. We further document a particularly
               pernicious learning trap driven by selective attention, a
               mechanism often assumed to facilitate learning in complex
               environments. Using computer simulation, we demonstrate the key
               attributes of the agent and environment that lead to this new
               type of learning trap. Then, in a series of experiments we
               present evidence that people robustly fall into this trap, even
               in the presence of various interventions predicted to meliorate
               it. These results highlight a fundamental limit to learning and
               adaptive behavior that impacts individuals, organizations,
               animals, and machines. (PsycInfo Database Record (c) 2020 APA,
               all rights reserved)",
  journal   = "J. Exp. Psychol. Gen.",
  publisher = "American Psychological Association",
  volume    =  147,
  number    =  11,
  pages     = "1553--1570",
  year      =  2018,
  address   = "US",
  keywords  = "Decision Making,Development,Environment,False
               Beliefs,Generalization (Learning),Learning,Learning
               Environment,Selective Attention;comp-cog-sci"
}

@BOOK{Rojas2013-pv,
  title     = "Neural Networks: A Systematic Introduction",
  author    = "Rojas, Ra{\'u}l",
  publisher = "Springer Science \& Business Media",
  year      =  2013,
  keywords  = "machine-learning"
}

@ARTICLE{Rosenblatt1958-kl,
  title    = "The Perceptron: {{A}} Probabilistic Model for Information Storage
              and Organization in the Brain",
  author   = "Rosenblatt, F",
  journal  = "Psychol. Rev.",
  volume   =  65,
  number   =  6,
  pages    = "386--408",
  year     =  1958,
  keywords = "comp-cog-sci"
}

@ARTICLE{Ruegsegger2017-yh,
  title     = "Running from {{Disease}}: {{Molecular Mechanisms Associating
               Dopamine}} and {{Leptin Signaling}} in the {{Brain}} with
               {{Physical Inactivity}}, {{Obesity}}, and {{Type}} 2
               {{Diabetes}}",
  author    = "Ruegsegger, Gregory N and Booth, Frank W",
  abstract  = "Physical inactivity is a primary contributor to diseases such as
               obesity, cardiovascular disease, and Type 2 diabetes.
               Accelerometry data suggest that a majority of U.S. adults fail
               to perform substantial levels of physical activity needed to
               improve health. Thus, understanding the molecular factors that
               stimulate physical activity, and physical inactivity, is
               imperative for the development of strategies to reduce sedentary
               behavior and in turn prevent chronic disease. Despite many of
               the well-known health benefits of physical activity being
               described, little is known about genetic and biological factors
               that may influence this complex behavior. The mesolimbic
               dopamine system regulates motivating and rewarding behavior as
               well as motor movement. Here, we present data supporting the
               hypothesis that obesity may mechanistically lower voluntary
               physical activity levels via dopamine dysregulation. In doing
               so, we review data that suggests mesolimbic dopamine activity is
               a strong contributor to voluntary physical activity behavior. We
               also summarize findings suggesting that obesity leads to central
               dopaminergic dysfunction, which in turn contributes to
               reductions in physical activity that often accompany obesity.
               Additionally, we highlight examples in which central leptin
               activity influences physical activity levels in a
               dopamine-dependent manner. Future elucidation of these
               mechanisms will help support strategies to increase physical
               activity levels in obese patients and prevent diseases caused by
               physical inactivity.",
  journal   = "Front. Endocrinol.",
  publisher = "Frontiers",
  volume    =  0,
  year      =  2017,
  keywords  = "Dopamine,Leptin,Motivation,Nucleus Accumbens,physical
               activity,physical inactivity;exercise-brain"
}

@TECHREPORT{Rule2018-zm,
  title       = "Learning List Concepts through Program Induction",
  author      = "Rule, Joshua and Schulz, Eric and Piantadosi, Steven T and
                 Tenenbaum, Joshua B",
  abstract    = "Humans master complex systems of interrelated concepts like
                 mathematics and natural language. Previous work suggests
                 learning these systems relies on iteratively and directly
                 revising a language-like conceptual representation. We
                 introduce and assess a novel concept learning paradigm called
                 Martha's Magical Machines that captures complex relationships
                 between concepts. We model human concept learning in this
                 paradigm as a search in the space of term rewriting systems,
                 previously developed as an abstract model of computation. Our
                 model accurately predicts that participants learn some
                 transformations more easily than others and that they learn
                 harder concepts more easily using a bootstrapping curriculum
                 focused on their compositional parts. Our results suggest that
                 term rewriting systems may be a useful model of human
                 conceptual representations.",
  institution = "Animal Behavior and Cognition",
  month       =  may,
  year        =  2018,
  keywords    = "comp-cog-sci"
}

@ARTICLE{Rule2020-db,
  title    = "The {{Child}} as {{Hacker}}",
  author   = "Rule, Joshua S and Tenenbaum, Joshua B and Piantadosi, Steven T",
  journal  = "Trends Cogn. Sci.",
  volume   =  24,
  number   =  11,
  pages    = "900--915",
  month    =  nov,
  year     =  2020,
  keywords = "development"
}

@UNPUBLISHED{Scherrer2021-wg,
  title    = "Learning {{Neural Causal Models}} with {{Active Interventions}}",
  author   = "Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and
              Goyal, Anirudh and Schwab, Patrick and Sch{\"o}lkopf, Bernhard
              and Mozer, Michael C and Bengio, Yoshua and Bauer, Stefan and Ke,
              Nan Rosemary",
  abstract = "Discovering causal structures from data is a challenging
              inference problem of fundamental importance in all areas of
              science. The appealing scaling properties of neural networks have
              recently led to a surge of interest in differentiable neural
              network-based methods for learning causal structures from data.
              So far differentiable causal discovery has focused on static
              datasets of observational or interventional origin. In this work,
              we introduce an active intervention-targeting mechanism which
              enables a quick identification of the underlying causal structure
              of the data-generating process. Our method significantly reduces
              the required number of interactions compared with random
              intervention targeting and is applicable for both discrete and
              continuous optimization formulations of learning the underlying
              directed acyclic graph (DAG) from data. We examine the proposed
              method across a wide range of settings and demonstrate superior
              performance on multiple benchmarks from simulated to real-world
              data.",
  month    =  sep,
  year     =  2021,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@UNPUBLISHED{Shultz2021-hq,
  title    = "A {{Computational Model}} of {{Infant Learning}} and
              {{Reasoning}} with {{Probabilities}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments reveal that 6- to 12-month-old infants can
              learn probabilities and reason with them. In this work, we
              present a novel computational system called Neural Probability
              Learner and Sampler (NPLS) that learns and reasons with
              probabilities, providing a computationally sufficient mechanism
              to explain infant probabilistic learning and inference. In 24
              computer simulations, NPLS simulations show how probability
              distributions can emerge naturally from neural-network learning
              of event sequences, providing a novel explanation of infant
              probabilistic learning and reasoning. Three mathematical proofs
              show how and why NPLS simulates the infant results so accurately.
              The results are situated in relation to seven other active
              research lines. This work provides an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  month    =  jun,
  year     =  2021,
  keywords = "Quantitative Biology - Neurons and Cognition;comp-cog-sci"
}

@ARTICLE{Shultz_undated-cv,
  title    = "Probability {{Without Counting}} and {{Dividing}}: {{A Fresh
              Computational Perspective}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments show that preverbal infants can reason
              probabilistically. This raises a deep puzzle because infants lack
              the counting and dividing abilities presumably required to
              compute probabilities. In the standard way of computing
              probabilities, they would have to count or accurately estimate
              large frequencies and divide those values by their total. Here,
              we present a novel neural-network model that learns and uses
              probability distributions without explicit counting or dividing.
              Probability distributions emerge naturally from neural-network
              learning of event sequences, providing a computationally
              sufficient explanation of how infants could succeed at
              probabilistic reasoning. Several alternative explanations are
              discussed and ruled out. Our work bears on several other active
              literatures, and it suggests an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  pages    = "7",
  keywords = "comp-cog-sci"
}

@TECHREPORT{Smolensky2022-ts,
  title       = "Neurocompositional Computing in Human and Machine
                 Intelligence: {{A}} Tutorial",
  author      = "Smolensky, Paul and McCoy, R Thomas and Fernandez, Roland and
                 Goldrick, Matthew and Gao, Jianfeng",
  abstract    = "The past decade has produced a revolution in Artificial
                 Intelligence (AI), after a half-century of AI repeatedly
                 failing to meet expectations. What explains the dramatic
                 change from 20th-century to 21st-century AI, and how can
                 remaining limitations of current AI be overcome? Until now,
                 the widely accepted narrative has attributed the recent
                 progress in AI to technical engineering advances that have
                 yielded massive increases in the quantity of computational
                 resources and training data available to support statistical
                 learning in deep artificial neural networks. Although these
                 quantitative engineering innovations are important, here we
                 show that the latest advances in AI are not solely due to
                 quantitative increases in computing power but also qualitative
                 changes in how that computing power is deployed. These
                 qualitative changes have brought about a new type of computing
                 that we call neurocompositional computing. In
                 neurocompositional computing, neural networks exploit two
                 scientific principles that contemporary theory in cognitive
                 science maintains are simultaneously necessary to enable
                 human-level cognition. The Compositionality Principle asserts
                 that encodings of complex information are structures that are
                 systematically composed from simpler structured encodings. The
                 Continuity Principle states that the encoding and processing
                 of information is formalized with real numbers that vary
                 continuously. These principles have seemed irreconcilable
                 until the recent mathematical discovery that compositionality
                 can be realized not only through the traditional discrete
                 methods of symbolic computing, well developed in 20th-century
                 AI, but also through novel forms of continuous neural
                 computing---neurocompositional computing. The unprecedented
                 progress of 21st-century AI has resulted from the use of
                 limited---first-generation---forms of neurocompositional
                 computing. We show that the new techniques now being deployed
                 in second-generation neurocompositional computing create AI
                 systems that are not only more robust and accurate than
                 current systems, but also more comprehensible---making it
                 possible to diagnose errors in, and exert human control over,
                 artificial neural networks through interpretation of their
                 internal states and direct intervention upon those states.
                 Note: This tutorial is intended for those new to this topic,
                 and does not assume familiarity with cognitive science, AI, or
                 deep learning. Appendices provide more advanced material. Each
                 figure, and the associated box explaining it, provides an
                 exposition, illustration, or further details of a main point
                 of the paper; in order to make these figures relatively
                 self-contained, it has sometimes been necessary to repeat some
                 material from the text. For a brief introduction and
                 additional development of some of this material see
                 ``Neurocompositional computing: From the central paradox of
                 cognition to a new generation of ai systems''
                 (arXiv:2205.01128; to appear, AI Magazine)",
  institution = "Microsoft",
  month       =  may,
  year        =  2022,
  keywords    = "comp-cog-sci"
}

@ARTICLE{Spelke2007-uu,
  title    = "Core Knowledge",
  author   = "Spelke, Elizabeth S and Kinzler, Katherine D",
  abstract = "Human cognition is founded, in part, on four systems for
              representing objects, actions, number, and space. It may be
              based, as well, on a fifth system for representing social
              partners. Each system has deep roots in human phylogeny and
              ontogeny, and it guides and shapes the mental lives of adults.
              Converging research on human infants, non-human primates,
              children and adults in diverse cultures can aid both
              understanding of these systems and attempts to overcome their
              limits.",
  journal  = "Dev. Sci.",
  volume   =  10,
  number   =  1,
  pages    = "89--96",
  year     =  2007,
  keywords = "development"
}

@INPROCEEDINGS{Stuhlmuller2010-ka,
  title     = "Learning Structured Generative Concepts",
  author    = "Stuhlmuller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D",
  publisher = "Cognitive Science Society",
  year      =  2010,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Tenenbaum2011-ud,
  title    = "How to {{Grow}} a {{Mind}}: {{Statistics}}, {{Structure}}, and
              {{Abstraction}}",
  author   = "Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and
              Goodman, Noah D",
  abstract = "In coming to understand the world---in learning concepts,
              acquiring language, and grasping causal relations---our minds
              make inferences that appear to go far beyond the data available.
              How do we do it? This review describes recent approaches to
              reverse-engineering human learning and cognitive development and,
              in parallel, engineering more humanlike machine learning systems.
              Computational models that perform probabilistic inference over
              hierarchies of flexibly structured representations can address
              some of the deepest questions about the nature and origins of
              human thought: How does abstract knowledge guide learning and
              reasoning from sparse data? What forms does our knowledge take,
              across different domains and tasks? And how is that abstract
              knowledge itself acquired?",
  journal  = "Science",
  volume   =  331,
  number   =  6022,
  pages    = "1279--1285",
  month    =  mar,
  year     =  2011,
  keywords = "comp-cog-sci"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Turing2009-sc,
  title     = "Computing {{Machinery}} and {{Intelligence}}",
  booktitle = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  author    = "Turing, Alan M",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  abstract  = "I propose to consider the question, ``Can machines think?''
               This should begin with definitions of the meaning of the terms
               ``machine'' and ``think''. The definitions might be framed so as
               to reflect so far as possible the normal use of the words, but
               this attitude is dangerous. If the meaning of the words
               ``machine'' and ``think'' are to be found by examining how they
               are commonly used it is difficult to escape the conclusion that
               the meaning and the answer to the question, ``Can machines
               think?'' is to be sought in a statistical survey such as a
               Gallup poll.",
  publisher = "Springer Netherlands",
  pages     = "23--65",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "Computing Machinery,Digital Computer,Performance Capacity,Real
               Robot,Turing Machine;comp-cog-sci"
}

@ARTICLE{Vaswani_undated-fb,
  title    = "Attention Is {{All}} You {{Need}}",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz
              and Polosukhin, Illia",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks that include an
              encoder and a decoder. The best performing models also connect
              the encoder and decoder through an attention mechanism. We
              propose a new simple network architecture, the Transformer, based
              solely on attention mechanisms, dispensing with recurrence and
              convolutions entirely. Experiments on two machine translation
              tasks show these models to be superior in quality while being
              more parallelizable and requiring significantly less time to
              train. Our model achieves 28.4 BLEU on the WMT 2014
              Englishto-German translation task, improving over the existing
              best results, including ensembles, by over 2 BLEU. On the WMT
              2014 English-to-French translation task, our model establishes a
              new single-model state-of-the-art BLEU score of 41.0 after
              training for 3.5 days on eight GPUs, a small fraction of the
              training costs of the best models from the literature.",
  pages    = "11",
  keywords = "machine-learning"
}

@ARTICLE{Watkins1992-pk,
  title     = "{Q-Learning}",
  author    = "Watkins, Christopher Jch and Dayan, Peter",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  8,
  number    = "3-4",
  pages     = "279--292",
  year      =  1992,
  keywords  = "machine-learning"
}

@ARTICLE{Wellman_undated-aj,
  title    = "Cognitive {{Development}}: {{Foundational Theories}} of {{Core
              Domains}}",
  author   = "Wellman, Henry M and Gelman, Susan A",
  pages    = "39",
  keywords = "development"
}

@INBOOK{2021-fc,
  title     = "Perceptron",
  booktitle = "Wikipedia",
  author    = "{Wikipedia}",
  abstract  = "In machine learning, the perceptron is an algorithm for
               supervised learning of binary classifiers. A binary classifier
               is a function which can decide whether or not an input,
               represented by a vector of numbers, belongs to some specific
               class. It is a type of linear classifier, i.e. a classification
               algorithm that makes its predictions based on a linear predictor
               function combining a set of weights with the feature vector.",
  month     =  aug,
  year      =  2021,
  keywords  = "comp-cog-sci"
}

@MISC{noauthor_undated-by,
  title        = "Readings in Cognitive Science : A Perspective from Psychology
                  and Artificial Intelligence | {{{McGill} University Library}}",
  howpublished = "\url{https://mcgill.on.worldcat.org/v2/oclc/555237322}",
  note         = "Accessed: 2021-9-17",
  keywords     = "comp-cog-sci"
}

@MISC{noauthor_undated-xy,
  title        = "Efficient Inverse Graphics in Biological Face Processing |
                  {{Science Advances}}",
  howpublished = "\url{https://advances.sciencemag.org/content/6/10/eaax5979/tab-pdf?__cf_chl_jschl_tk__=7edc4ca4dbf68389fcff61bcb4f9b2933065972f-1626234016-0-AUXGIGlyGqNMVHJhjRyQlw9DOb7_Nsxa5__OpC2K69M9D8q3ft1l8LC648BjbyZssjf1qXDRh9heKB23bAUqhMchMuBuTHxr1gBzgLl0YZz-fvX_BbIfEdkuGqgS2wIQwcCLXUhX7Vuu9lepXXTiDRdqgoBR_3HgolzgHQ6iOR88SOt_HkomiuuG19xpy9eHwbtkXyxVeuqVct2S5FKj2c_gF7JoxpOsDzMQ2ClPLRcttkLxGd5Y2NSg0hSEDqyY9X-MJK6yri2xtNvCQoUyFH2cOstnm8cMcp0PKaq91OAXSh_A3uCIJLJF8Dvy5m58BYllfEUROmN2Gs2l0Mvt-pdW5ClIo4ZFzQ_JlY8SX5yrdk3Nv7rREduZqg_ZKj_rSXsnQccxZkIUOnWnfk3HWWQJqVOfzh9HFpMYsr2jkFs-ljn7ZmKqdkoo5QRPi1XbSg}",
  note         = "Accessed: 2021-7-14",
  keywords     = "comp-cog-sci"
}

@ARTICLE{Baars2007-hu,
  title    = "An Architectural Model of Conscious and Unconscious Brain
              Functions: {{Global Workspace Theory}} and {{IDA}}",
  author   = "Baars, Bernard J and Franklin, Stan",
  abstract = "While neural net models have been developed to a high degree of
              sophistication, they have some drawbacks at a more integrative,
              ``architectural'' level of analysis. We describe a ``hybrid''
              cognitive architecture that is implementable in neuronal nets,
              and which has uniform brainlike features, including
              activation-passing and highly distributed ``codelets,''
              implementable as small-scale neural nets. Empirically, this
              cognitive architecture accounts qualitatively for the data
              described by Baars' Global Workspace Theory (GWT), and Franklin's
              LIDA architecture, including state-of-the-art models of conscious
              contents in action-planning, Baddeley-style Working Memory, and
              working models of episodic and semantic longterm memory. These
              terms are defined both conceptually and empirically for the
              current theoretical domain. The resulting architecture meets four
              desirable goals for a unified theory of cognition: practical
              workability, autonomous agency, a plausible role for conscious
              cognition, and translatability into plausible neural terms. It
              also generates testable predictions, both empirical and
              computational.",
  journal  = "Neural Netw.",
  volume   =  20,
  number   =  9,
  pages    = "955--961",
  series   = "Brain and Consciousness",
  month    =  nov,
  year     =  2007,
  keywords = "Cognitive architecture,Conscious cognition,Global workspace
              theory,LIDA architecture;comp-cog-sci"
}
