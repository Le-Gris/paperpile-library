% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chevalier2022-ot,
  title     = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part {II}: Editorial note",
  author    = "Chevalier, Nicolas and Lipina, Sebasti{\'a}n and Scerif, Gaia
               and Segretin, M Soledad",
  abstract  = "Special issue on development of self-regulation, cognitive
               control, and executive function, Part II: Editorial note ---
               University of Edinburgh Research Explorer Skip to main
               navigation Skip to â€¦",
  journal   = "Dev. Sci.",
  publisher = "research.ed.ac.uk",
  volume    =  25,
  number    =  6,
  pages     = "e13326",
  month     =  nov,
  year      =  2022,
  keywords  = "development",
  language  = "en",
  issn      = "1363-755X, 1467-7687",
  pmid      = "36112772",
  doi       = "10.1111/desc.13326"
}

@INBOOK{Lombrozo2019-my,
  title     = "``Learning by Thinking'' in Science and in Everyday Life",
  author    = "Lombrozo, Tania",
  abstract  = "AbstractThis chapter introduces ``learning by thinking'' (LbT)
               as a form of learning distinct from familiar forms of learning
               through observation. When learning b",
  publisher = "Oxford University Press",
  month     =  dec,
  year      =  2019,
  keywords  = "project 1",
  language  = "en",
  doi       = "10.1093/oso/9780190212308.003.0010"
}

@ARTICLE{Gordon2020-qy,
  title         = "Compressing {BERT}: Studying the Effects of Weight Pruning
                   on Transfer Learning",
  author        = "Gordon, Mitchell A and Duh, Kevin and Andrews, Nicholas",
  abstract      = "Pre-trained universal feature extractors, such as BERT for
                   natural language processing and VGG for computer vision,
                   have become effective methods for improving deep learning
                   models without requiring more labeled data. While effective,
                   feature extractors like BERT may be prohibitively large for
                   some deployment scenarios. We explore weight pruning for
                   BERT and ask: how does compression during pre-training
                   affect transfer learning? We find that pruning affects
                   transfer learning in three broad regimes. Low levels of
                   pruning (30-40\%) do not affect pre-training loss or
                   transfer to downstream tasks at all. Medium levels of
                   pruning increase the pre-training loss and prevent useful
                   pre-training information from being transferred to
                   downstream tasks. High levels of pruning additionally
                   prevent models from fitting downstream datasets, leading to
                   further degradation. Finally, we observe that fine-tuning
                   BERT on a specific task does not improve its prunability. We
                   conclude that BERT can be pruned once during pre-training
                   rather than separately for each task without affecting
                   performance.",
  month         =  feb,
  year          =  2020,
  keywords      = "read;nlp",
  archivePrefix = "arXiv",
  eprint        = "2002.08307",
  primaryClass  = "cs.CL",
  arxivid       = "2002.08307"
}

@ARTICLE{Yang2022-ho,
  title         = "{TextPruner}: A Model Pruning Toolkit for {Pre-Trained}
                   Language Models",
  author        = "Yang, Ziqing and Cui, Yiming and Chen, Zhigang",
  abstract      = "Pre-trained language models have been prevailed in natural
                   language processing and become the backbones of many NLP
                   tasks, but the demands for computational resources have
                   limited their applications. In this paper, we introduce
                   TextPruner, an open-source model pruning toolkit designed
                   for pre-trained language models, targeting fast and easy
                   model compression. TextPruner offers structured
                   post-training pruning methods, including vocabulary pruning
                   and transformer pruning, and can be applied to various
                   models and tasks. We also propose a self-supervised pruning
                   method that can be applied without the labeled data. Our
                   experiments with several NLP tasks demonstrate the ability
                   of TextPruner to reduce the model size without re-training
                   the model.",
  month         =  mar,
  year          =  2022,
  keywords      = "skimmed;nlp",
  archivePrefix = "arXiv",
  eprint        = "2203.15996",
  primaryClass  = "cs.CL",
  arxivid       = "2203.15996"
}

@ARTICLE{Zafrir2021-pm,
  title         = "Prune Once for All: Sparse {Pre-Trained} Language Models",
  author        = "Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen,
                   Haihao and Wasserblat, Moshe",
  abstract      = "Transformer-based language models are applied to a wide
                   range of applications in natural language processing.
                   However, they are inefficient and difficult to deploy. In
                   recent years, many compression algorithms have been proposed
                   to increase the implementation efficiency of large
                   Transformer-based models on target hardware. In this work we
                   present a new method for training sparse pre-trained
                   Transformer language models by integrating weight pruning
                   and model distillation. These sparse pre-trained models can
                   be used to transfer learning for a wide range of tasks while
                   maintaining their sparsity pattern. We demonstrate our
                   method with three known architectures to create sparse
                   pre-trained BERT-Base, BERT-Large and DistilBERT. We show
                   how the compressed sparse pre-trained models we trained
                   transfer their knowledge to five different downstream
                   natural language tasks with minimal accuracy loss. Moreover,
                   we show how to further compress the sparse models' weights
                   to 8bit precision using quantization-aware training. For
                   example, with our sparse pre-trained BERT-Large fine-tuned
                   on SQuADv1.1 and quantized to 8bit we achieve a compression
                   ratio of $40$X for the encoder with less than $1\%$ accuracy
                   loss. To the best of our knowledge, our results show the
                   best compression-to-accuracy ratio for BERT-Base,
                   BERT-Large, and DistilBERT.",
  month         =  nov,
  year          =  2021,
  keywords      = "read;nlp",
  archivePrefix = "arXiv",
  eprint        = "2111.05754",
  primaryClass  = "cs.CL",
  arxivid       = "2111.05754"
}

@ARTICLE{Peterson2021-nw,
  title    = "Using large-scale experiments and machine learning to discover
              theories of human decision-making",
  author   = "Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and
              Reichman, Daniel and Griffiths, Thomas L",
  abstract = "Predicting and understanding how people make decisions has been a
              long-standing goal in many fields, with quantitative models of
              human decision-making informing research in both the social
              sciences and engineering. We show how progress toward this goal
              can be accelerated by using large datasets to power
              machine-learning algorithms that are constrained to produce
              interpretable psychological theories. Conducting the largest
              experiment on risky choice to date and analyzing the results
              using gradient-based optimization of differentiable decision
              theories implemented through artificial neural networks, we were
              able to recapitulate historical discoveries, establish that there
              is room to improve on existing theories, and discover a new, more
              accurate model of human decision-making in a form that preserves
              the insights from centuries of research.",
  journal  = "Science",
  volume   =  372,
  number   =  6547,
  pages    = "1209--1214",
  month    =  jun,
  year     =  2021,
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "34112693",
  doi      = "10.1126/science.abe2629"
}

@ARTICLE{Vysogorets2021-wi,
  title         = "Connectivity Matters: Neural Network Pruning Through the
                   Lens of Effective Sparsity",
  author        = "Vysogorets, Artem and Kempe, Julia",
  abstract      = "Neural network pruning is a fruitful area of research with
                   surging interest in high sparsity regimes. Benchmarking in
                   this domain heavily relies on faithful representation of the
                   sparsity of subnetworks, which has been traditionally
                   computed as the fraction of removed connections (direct
                   sparsity). This definition, however, fails to recognize
                   unpruned parameters that detached from input or output
                   layers of underlying subnetworks, potentially
                   underestimating actual effective sparsity: the fraction of
                   inactivated connections. While this effect might be
                   negligible for moderately pruned networks (up to 10-100
                   compression rates), we find that it plays an increasing role
                   for thinner subnetworks, greatly distorting comparison
                   between different pruning algorithms. For example, we show
                   that effective compression of a randomly pruned
                   LeNet-300-100 can be orders of magnitude larger than its
                   direct counterpart, while no discrepancy is ever observed
                   when using SynFlow for pruning [Tanaka et al., 2020]. In
                   this work, we adopt the lens of effective sparsity to
                   reevaluate several recent pruning algorithms on common
                   benchmark architectures (e.g., LeNet-300-100, VGG-19,
                   ResNet-18) and discover that their absolute and relative
                   performance changes dramatically in this new and more
                   appropriate framework. To aim for effective, rather than
                   direct, sparsity, we develop a low-cost extension to most
                   pruning algorithms. Further, equipped with effective
                   sparsity as a reference frame, we partially reconfirm that
                   random pruning with appropriate sparsity allocation across
                   layers performs as well or better than more sophisticated
                   algorithms for pruning at initialization [Su et al., 2020].
                   In response to this observation, using a simple analogy of
                   pressure distribution in coupled cylinders from physics, we
                   design novel layerwise sparsity quotas that outperform all
                   existing baselines in the context of random pruning.",
  month         =  jul,
  year          =  2021,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "2107.02306",
  primaryClass  = "cs.LG",
  arxivid       = "2107.02306"
}

@ARTICLE{Brown2020-gd,
  title         = "Language Models are {Few-Shot} Learners",
  author        = "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah,
                   Melanie and Kaplan, Jared and Dhariwal, Prafulla and
                   Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and
                   Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel
                   and Krueger, Gretchen and Henighan, Tom and Child, Rewon and
                   Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and
                   Winter, Clemens and Hesse, Christopher and Chen, Mark and
                   Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess,
                   Benjamin and Clark, Jack and Berner, Christopher and
                   McCandlish, Sam and Radford, Alec and Sutskever, Ilya and
                   Amodei, Dario",
  abstract      = "Recent work has demonstrated substantial gains on many NLP
                   tasks and benchmarks by pre-training on a large corpus of
                   text followed by fine-tuning on a specific task. While
                   typically task-agnostic in architecture, this method still
                   requires task-specific fine-tuning datasets of thousands or
                   tens of thousands of examples. By contrast, humans can
                   generally perform a new language task from only a few
                   examples or from simple instructions - something which
                   current NLP systems still largely struggle to do. Here we
                   show that scaling up language models greatly improves
                   task-agnostic, few-shot performance, sometimes even reaching
                   competitiveness with prior state-of-the-art fine-tuning
                   approaches. Specifically, we train GPT-3, an autoregressive
                   language model with 175 billion parameters, 10x more than
                   any previous non-sparse language model, and test its
                   performance in the few-shot setting. For all tasks, GPT-3 is
                   applied without any gradient updates or fine-tuning, with
                   tasks and few-shot demonstrations specified purely via text
                   interaction with the model. GPT-3 achieves strong
                   performance on many NLP datasets, including translation,
                   question-answering, and cloze tasks, as well as several
                   tasks that require on-the-fly reasoning or domain
                   adaptation, such as unscrambling words, using a novel word
                   in a sentence, or performing 3-digit arithmetic. At the same
                   time, we also identify some datasets where GPT-3's few-shot
                   learning still struggles, as well as some datasets where
                   GPT-3 faces methodological issues related to training on
                   large web corpora. Finally, we find that GPT-3 can generate
                   samples of news articles which human evaluators have
                   difficulty distinguishing from articles written by humans.
                   We discuss broader societal impacts of this finding and of
                   GPT-3 in general.",
  month         =  may,
  year          =  2020,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "2005.14165",
  primaryClass  = "cs.CL",
  arxivid       = "2005.14165"
}

@MISC{Wang_undated-gn,
  title        = "Glue: A multi-task benchmark and analysis platform for
                  natural language {understand-ING}",
  author       = "Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill,
                  Felix and Levy, Omer and Bowman, Samuel R",
  abstract     = "For natural language understanding (NLU) technology to be
                  maximally useful, it must be able to process language in a
                  way that is not exclusive to a single task, genre, or
                  dataset. In pursuit of this objective, we introduce the
                  General Language Understanding Evaluation (GLUE) benchmark, a
                  collection of tools for evaluating the performance of models
                  across a diverse set of existing NLU tasks. By including
                  tasks with limited training data, GLUE is designed to favor
                  and encourage models that share general linguistic knowledge
                  across tasks. GLUE also includes a hand-crafted diagnostic
                  test suite that enables detailed linguistic analysis of
                  models. We evaluate baselines based on current methods for
                  transfer and representation learning and find that multi-task
                  training on all tasks performs better than training a
                  separate model per task. However, the low absolute
                  performance of our best model indicates the need for improved
                  general NLU systems.",
  howpublished = "\url{https://openreview.net/pdf?id=rJ4km2R5t7}",
  note         = "Accessed: 2022-10-21",
  keywords     = "nlp"
}

@BOOK{noauthor_2015-uh,
  title    = "The Conceptual Mind : New Directions in the Study of Concepts",
  abstract = "``The study of concepts has advanced dramatically in recent
              years, with exciting new findings and theoretical developments.
              Core concepts have been investigated in greater depth and new
              lines of inquiry have blossomed, with researchers from an ever
              broader range of disciplines making important contributions. In
              this volume, leading philosophers and cognitive scientists offer
              original essays that present the state-of-the-art in the study of
              concepts. These essays, all commissioned for this book, do not
              merely present the usual surveys and overviews; rather, they
              offer the latest work on concepts by a diverse group of theorists
              as well as discussions of the ideas that should guide research
              over the next decade. The book is an essential companion volume
              to the earlier Concepts: Core Readings, the definitive source for
              classic texts on the nature of concepts. The essays cover
              concepts as they relate to animal cognition, the brain,
              evolution, perception, and language, concepts across cultures,
              concept acquisition and conceptual change, concepts and
              normativity, concepts in context, and conceptual
              individuation''--MIT CogNet.",
  year     =  2015,
  keywords = "Electronic books;comp-cog-sci",
  isbn     = "9780262326872",
  lccn     = "2014034214"
}

@INPROCEEDINGS{Krizhevsky2012-bm,
  title     = "{{ImageNet}} Classification with Deep Convolutional Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E",
  editor    = "Pereira, F and Burges, C J and Bottou, L and Weinberger, K Q",
  publisher = "Curran Associates, Inc.",
  volume    =  25,
  year      =  2012,
  keywords  = "machine-learning"
}

@ARTICLE{Thomas_McCoy2020-zy,
  title         = "Universal linguistic inductive biases via meta-learning",
  author        = "Thomas McCoy, R and Grant, Erin and Smolensky, Paul and
                   Griffiths, Thomas L and Linzen, Tal",
  abstract      = "How do learners acquire languages from the limited data
                   available to them? This process must involve some inductive
                   biases - factors that affect how a learner generalizes - but
                   it is unclear which inductive biases can explain observed
                   patterns in language acquisition. To facilitate
                   computational modeling aimed at addressing this question, we
                   introduce a framework for giving particular linguistic
                   inductive biases to a neural network model; such a model can
                   then be used to empirically explore the effects of those
                   inductive biases. This framework disentangles universal
                   inductive biases, which are encoded in the initial values of
                   a neural network's parameters, from non-universal factors,
                   which the neural network must learn from data in a given
                   language. The initial state that encodes the inductive
                   biases is found with meta-learning, a technique through
                   which a model discovers how to acquire new languages more
                   easily via exposure to many possible languages. By
                   controlling the properties of the languages that are used
                   during meta-learning, we can control the inductive biases
                   that meta-learning imparts. We demonstrate this framework
                   with a case study based on syllable structure. First, we
                   specify the inductive biases that we intend to give our
                   model, and then we translate those inductive biases into a
                   space of languages from which a model can meta-learn.
                   Finally, using existing analysis techniques, we verify that
                   our approach has imparted the linguistic inductive biases
                   that it was intended to impart.",
  month         =  jun,
  year          =  2020,
  keywords      = "read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2006.16324",
  primaryClass  = "cs.CL",
  arxivid       = "2006.16324"
}

@INBOOK{Bassok2012-sg,
  title     = "Problem Solving",
  author    = "Bassok, Miriam and Novick, Laura R",
  abstract  = "Abstract. This chapter follows the historical development of
               research on problem solving. It begins with a description of two
               research traditions that addressed",
  publisher = "Oxford University Press",
  month     =  mar,
  year      =  2012,
  language  = "en",
  doi       = "10.1093/oxfordhb/9780199734689.013.0021"
}

@ARTICLE{Ellis2022-je,
  title    = "Synthesizing theories of human language with Bayesian program
              induction",
  author   = "Ellis, Kevin and Albright, Adam and Solar-Lezama, Armando and
              Tenenbaum, Joshua B and O'Donnell, Timothy J",
  abstract = "Automated, data-driven construction and evaluation of scientific
              models and theories is a long-standing challenge in artificial
              intelligence. We present a framework for algorithmically
              synthesizing models of a basic part of human language:
              morpho-phonology, the system that builds word forms from sounds.
              We integrate Bayesian inference with program synthesis and
              representations inspired by linguistic theory and cognitive
              models of learning and discovery. Across 70 datasets from 58
              diverse languages, our system synthesizes human-interpretable
              models for core aspects of each language's morpho-phonology,
              sometimes approaching models posited by human linguists. Joint
              inference across all 70 data sets automatically synthesizes a
              meta-model encoding interpretable cross-language typological
              tendencies. Finally, the same algorithm captures few-shot
              learning dynamics, acquiring new morphophonological rules from
              just one or a few examples. These results suggest routes to more
              powerful machine-enabled discovery of interpretable models in
              linguistics and other scientific domains.",
  journal  = "Nat. Commun.",
  volume   =  13,
  number   =  1,
  pages    = "5024",
  month    =  aug,
  year     =  2022,
  keywords = "skimmed;comp-cog-sci",
  language = "en",
  issn     = "2041-1723",
  pmid     = "36042196",
  doi      = "10.1038/s41467-022-32012-w",
  pmc      = "PMC9427767"
}

@ARTICLE{Valkov2018-sq,
  title         = "{HOUDINI}: Lifelong Learning as Program Synthesis",
  author        = "Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and
                   Sutton, Charles and Chaudhuri, Swarat",
  abstract      = "We present a neurosymbolic framework for the lifelong
                   learning of algorithmic tasks that mix perception and
                   procedural reasoning. Reusing high-level concepts across
                   domains and learning complex procedures are key challenges
                   in lifelong learning. We show that a program synthesis
                   approach that combines gradient descent with combinatorial
                   search over programs can be a more effective response to
                   these challenges than purely neural methods. Our framework,
                   called HOUDINI, represents neural networks as strongly
                   typed, differentiable functional programs that use symbolic
                   higher-order combinators to compose a library of neural
                   functions. Our learning algorithm consists of: (1) a
                   symbolic program synthesizer that performs a type-directed
                   search over parameterized programs, and decides on the
                   library functions to reuse, and the architectures to combine
                   them, while learning a sequence of tasks; and (2) a neural
                   module that trains these programs using stochastic gradient
                   descent. We evaluate HOUDINI on three benchmarks that
                   combine perception with the algorithmic tasks of counting,
                   summing, and shortest-path computation. Our experiments show
                   that HOUDINI transfers high-level concepts more effectively
                   than traditional transfer learning and progressive neural
                   networks, and that the typed representation of networks
                   significantly accelerates the search.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  eprint        = "1804.00218",
  primaryClass  = "cs.LG",
  arxivid       = "1804.00218"
}

@ARTICLE{Raffel2019-jw,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   {Text-to-Text} Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of
                   transfer learning techniques for NLP by introducing a
                   unified framework that converts all text-based language
                   problems into a text-to-text format. Our systematic study
                   compares pre-training objectives, architectures, unlabeled
                   data sets, transfer approaches, and other factors on dozens
                   of language understanding tasks. By combining the insights
                   from our exploration with scale and our new ``Colossal Clean
                   Crawled Corpus'', we achieve state-of-the-art results on
                   many benchmarks covering summarization, question answering,
                   text classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our data set,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "1910.10683",
  primaryClass  = "cs.LG",
  arxivid       = "1910.10683"
}

@MISC{Bramley_undated-rs,
  title        = "Grounding compositional hypothesis generation in specific
                  instances",
  author       = "Bramley, Neil R and Tenenbaum, Joshua B",
  abstract     = "A number of recent computational models treat concept
                  learning as a form of probabilistic rule induction in a space
                  of language-like, compositional concepts. Inference in such
                  models frequently requires repeatedly sampling from a
                  (infinite) distribution over possible concept rules and
                  comparing their relative likelihood in light of current data
                  or evidence. However, we argue that most existing algorithms
                  for top-down sampling are inefficient and cognitively
                  implausible accounts of human hypothesis generation. As a
                  result, we propose an alternative, Instance Driven Generator
                  (IDG), that constructs bottom-up hypotheses directly out of
                  encountered positive instances of a concept. Using a novel
                  rule induction task based on the children's game Zendo, we
                  compare these ``bottomup'' and ``top-down'' approaches to
                  inference. We find that the bottom-up IDG model accounts
                  better for human inferences and results in a computationally
                  more tractable inference mechanism for concept learning
                  models based on a probabilistic language of thought.",
  howpublished = "\url{https://www.bramleylab.ppls.ed.ac.uk/pdfs/bramley2018zendo.pdf}",
  note         = "Accessed: 2022-10-16",
  keywords     = "project 1;comp-cog-sci"
}

@ARTICLE{Nisbett1977-oz,
  title    = "Telling more than we can know: Verbal reports on mental processes",
  author   = "Nisbett, Richard E and Wilson, Timothy D",
  abstract = "Reviews evidence which suggests that there may be little or no
              direct introspective access to higher order cognitive processes.
              Ss are sometimes (a) unaware of the existence of a stimulus that
              importantly influenced a response, (b) unaware of the existence
              of the response, and (c) unaware that the stimulus has affected
              the response. It is proposed that when people attempt to report
              on their cognitive processes, that is, on the processes mediating
              the effects of a stimulus on a response, they do not do so on the
              basis of any true introspection. Instead, their reports are based
              on a priori, implicit causal theories, or judgments about the
              extent to which a particular stimulus is a plausible cause of a
              given response. This suggests that though people may not be able
              to observe directly their cognitive processes, they will
              sometimes be able to report accurately about them. Accurate
              reports will occur when influential stimuli are salient and are
              plausible causes of the responses they produce, and will not
              occur when stimuli are not salient or are not plausible causes.
              (86 ref) (PsycINFO Database Record (c) 2018 APA, all rights
              reserved)",
  journal  = "Psychol. Rev.",
  volume   =  84,
  number   =  3,
  pages    = "231--259",
  month    =  may,
  year     =  1977,
  keywords = "project 1",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/0033-295X.84.3.231"
}

@ARTICLE{Griffiths2019-hj,
  title    = "Doing more with less: Meta-reasoning and meta-learning in humans
              and machines",
  author   = "Griffiths, Thomas L and Callaway, Frederick and Chang, Michael B
              and Grant, Erin and Krueger, Paul M and Lieder, Falk",
  abstract = "Artificial intelligence systems use an increasing amount of
              computation and data to solve very specific problems. By
              contrast, human minds solve a wide range of problems using a
              fixed amount of computation and limited experience. We identify
              two abilities that we see as crucial to this kind of general
              intelligence: meta-reasoning (deciding how to allocate
              computational resources) and meta-learning (modeling the learning
              environment to make better use of limited data). We summarize the
              relevant AI literature and relate the resulting ideas to recent
              work in psychology. (PsycINFO Database Record (c) 2019 APA, all
              rights reserved)",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "24--30",
  month    =  oct,
  year     =  2019,
  issn     = "2352-1546, 2352-1554",
  doi      = "10.1016/j.cobeha.2019.01.005"
}

@ARTICLE{De_Jong1998-ci,
  title     = "Scientific Discovery Learning with Computer Simulations of
               Conceptual Domains",
  author    = "De Jong, Ton and Van Joolingen, Wouter R",
  abstract  = "Scientific discovery learning is a highly self-directed and
               constructivistic form of learning. A computer simulation is a
               type of computer-based environment that is well suited for
               discovery learning, the main task of the learner being to infer,
               through experimentation, characteristics of the model underlying
               the simulation. In this article we give a review of the observed
               effectiveness and efficiency of discovery learning in simulation
               environments together with problems that learners may encounter
               in discovery learning, and we discuss how simulations may be
               combined with instructional support in order to overcome these
               problems.",
  journal   = "Rev. Educ. Res.",
  publisher = "American Educational Research Association",
  volume    =  68,
  number    =  2,
  pages     = "179--201",
  month     =  jun,
  year      =  1998,
  keywords  = "project 1",
  issn      = "0034-6543",
  doi       = "10.3102/00346543068002179"
}

@ARTICLE{Gopnik2012-iu,
  title    = "Scientific thinking in young children: theoretical advances,
              empirical research, and policy implications",
  author   = "Gopnik, Alison",
  abstract = "New theoretical ideas and empirical research show that very young
              children's learning and thinking are strikingly similar to much
              learning and thinking in science. Preschoolers test hypotheses
              against data and make causal inferences; they learn from
              statistics and informal experimentation, and from watching and
              listening to others. The mathematical framework of probabilistic
              models and Bayesian inference can describe this learning in
              precise ways. These discoveries have implications for early
              childhood education and policy. In particular, they suggest both
              that early childhood experience is extremely important and that
              the trend toward more structured and academic early childhood
              programs is misguided.",
  journal  = "Science",
  volume   =  337,
  number   =  6102,
  pages    = "1623--1627",
  month    =  sep,
  year     =  2012,
  keywords = "project 1",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "23019643",
  doi      = "10.1126/science.1223416"
}

@ARTICLE{Xu2022-eq,
  title         = "{EST}: Evaluating Scientific Thinking in Artificial Agents",
  author        = "Xu, Manjie and Jiang, Guangyuan and Zhang, Chi and Zhu,
                   Song-Chun and Zhu, Yixin",
  abstract      = "Theoretical ideas and empirical research have shown us a
                   seemingly surprising result: children, even very young
                   toddlers, demonstrate learning and thinking in a strikingly
                   similar manner to scientific reasoning in formal research.
                   Encountering a novel phenomenon, children make hypotheses
                   against data, conduct causal inference from observation,
                   test their theory via experimentation, and correct the
                   proposition if inconsistency arises. Rounds of such
                   processes continue until the underlying mechanism is found.
                   Towards building machines that can learn and think like
                   people, one natural question for us to ask is: whether the
                   intelligence we achieve today manages to perform such a
                   scientific thinking process, and if any, at what level. In
                   this work, we devise the EST environment for evaluating the
                   scientific thinking ability in artificial agents. Motivated
                   by the stream of research on causal discovery, we build our
                   interactive EST environment based on Blicket detection.
                   Specifically, in each episode of EST, an agent is presented
                   with novel observations and asked to figure out all objects'
                   Blicketness. At each time step, the agent proposes new
                   experiments to validate its hypothesis and updates its
                   current belief. By evaluating Reinforcement Learning (RL)
                   agents on both a symbolic and visual version of this task,
                   we notice clear failure of today's learning methods in
                   reaching a level of intelligence comparable to humans. Such
                   inefficacy of learning in scientific thinking calls for
                   future research in building humanlike intelligence.",
  month         =  jun,
  year          =  2022,
  keywords      = "skimmed;project 1;machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2206.09203",
  primaryClass  = "cs.AI",
  arxivid       = "2206.09203"
}

@ARTICLE{Lohse2022-nm,
  title    = "Hypotheses in adult-child interactions stimulate children's
              reasoning and verbalizations",
  author   = "Lohse, Karoline and Hildebrandt, Andrea and Hildebrandt, Frauke",
  abstract = "Adult-child interactions can support children's development and
              are established as predictors of program quality in early
              childhood settings. However, the linguistic components that
              constitute positive interactions have not yet been studied in
              detail. This study investigates the effects of hypotheses
              proposed by adults on children's responses in a dyadic
              picture-book viewing situation. In 2 experiments, adults' use of
              hypotheses (e.g., ``Maybe this is a dwarf's door'') was tested
              against the use of instructive statements (``This is a dwarf's
              door'') and in combination with open questions (``What do you
              think, why is the door so small?''). In Experiment 1, hypotheses
              differed from instructions only by the modal marker ``maybe''.
              Children's responses to hypotheses were longer and contained more
              self-generated explanations as compared to responses to
              instructions. The use of hypotheses also seemed to encourage
              children to attach more importance to their own explanations. In
              Experiment 2, combining hypotheses with open-ended why questions
              elicited longer responses but no more self-generated explanations
              in children than open-ended questions alone. Results indicate
              that subtle differences in adults' utterances can directly
              influence children's reasoning and children's contributions to
              dialogues.",
  journal  = "Early Child. Res. Q.",
  volume   =  58,
  pages    = "254--263",
  month    =  jan,
  year     =  2022,
  keywords = "Adult-child interactions; sustained shared thinking; hypotheses;
              open questions;development",
  issn     = "0885-2006",
  doi      = "10.1016/j.ecresq.2021.09.014"
}

@ARTICLE{Bramley2018-zb,
  title    = "Intuitive experimentation in the physical world",
  author   = "Bramley, Neil R and Gerstenberg, Tobias and Tenenbaum, Joshua B
              and Gureckis, Todd M",
  abstract = "Many aspects of our physical environment are hidden. For example,
              it is hard to estimate how heavy an object is from visual
              observation alone. In this paper we examine how people actively
              ``experiment'' within the physical world to discover such latent
              properties. In the first part of the paper, we develop a novel
              framework for the quantitative analysis of the information
              produced by physical interactions. We then describe two
              experiments that present participants with moving objects in
              ``microworlds'' that operate according to continuous
              spatiotemporal dynamics similar to everyday physics (i.e., forces
              of gravity, friction, etc.). Participants were asked to interact
              with objects in the microworlds in order to identify their
              masses, or the forces of attraction/repulsion that governed their
              movement. Using our modeling framework, we find that learners who
              freely interacted with the physical system selectively produced
              evidence that revealed the physical property consistent with
              their inquiry goal. As a result, their inferences were more
              accurate than for passive observers and, in some contexts, for
              yoked participants who watched video replays of an active
              learner's interactions. We characterize active learners' actions
              into a range of micro-experiment strategies and discuss how these
              might be learned or generalized from past experience. The
              technical contribution of this work is the development of a novel
              analytic framework and methodology for the study of interactively
              learning about the physical world. Its empirical contribution is
              the demonstration of sophisticated goal directed human active
              learning in a naturalistic context.",
  journal  = "Cogn. Psychol.",
  volume   =  105,
  pages    = "9--38",
  month    =  sep,
  year     =  2018,
  keywords = "Active learning; Experimental design; Mental simulation; Physical
              understanding;read;project 1;comp-cog-sci",
  language = "en",
  issn     = "0010-0285, 1095-5623",
  pmid     = "29885534",
  doi      = "10.1016/j.cogpsych.2018.05.001"
}

@ARTICLE{Coenen2015-wp,
  title    = "Strategies to intervene on causal systems are adaptively selected",
  author   = "Coenen, Anna and Rehder, Bob and Gureckis, Todd M",
  abstract = "How do people choose interventions to learn about causal systems?
              Here, we considered two possibilities. First, we test an
              information sampling model, information gain, which values
              interventions that can discriminate between a learner's
              hypotheses (i.e. possible causal structures). We compare this
              discriminatory model to a positive testing strategy that instead
              aims to confirm individual hypotheses. Experiment 1 shows that
              individual behavior is described best by a mixture of these two
              alternatives. In Experiment 2 we find that people are able to
              adaptively alter their behavior and adopt the discriminatory
              model more often after experiencing that the confirmatory
              strategy leads to a subjective performance decrement. In
              Experiment 3, time pressure leads to the opposite effect of
              inducing a change towards the simpler positive testing strategy.
              These findings suggest that there is no single strategy that
              describes how intervention decisions are made. Instead, people
              select strategies in an adaptive fashion that trades off their
              expected performance and cognitive effort.",
  journal  = "Cogn. Psychol.",
  volume   =  79,
  pages    = "102--133",
  month    =  jun,
  year     =  2015,
  keywords = "Causal learning; Hypothesis testing; Information gain;
              Interventions; Self-directed learning;project 1",
  language = "en",
  issn     = "0010-0285, 1095-5623",
  pmid     = "25935867",
  doi      = "10.1016/j.cogpsych.2015.02.004"
}

@ARTICLE{Bonawitz2011-pi,
  title    = "The double-edged sword of pedagogy: Instruction limits
              spontaneous exploration and discovery",
  author   = "Bonawitz, Elizabeth and Shafto, Patrick and Gweon, Hyowon and
              Goodman, Noah D and Spelke, Elizabeth and Schulz, Laura",
  abstract = "Motivated by computational analyses, we look at how teaching
              affects exploration and discovery. In Experiment 1, we
              investigated children's exploratory play after an adult
              pedagogically demonstrated a function of a toy, after an
              interrupted pedagogical demonstration, after a na{\"\i}ve adult
              demonstrated the function, and at baseline. Preschoolers in the
              pedagogical condition focused almost exclusively on the target
              function; by contrast, children in the other conditions explored
              broadly. In Experiment 2, we show that children restrict their
              exploration both after direct instruction to themselves and after
              overhearing direct instruction given to another child; they do
              not show this constraint after observing direct instruction given
              to an adult or after observing a non-pedagogical intentional
              action. We discuss these findings as the result of rational
              inductive biases. In pedagogical contexts, a teacher's failure to
              provide evidence for additional functions provides evidence for
              their absence; such contexts generalize from child to child
              (because children are likely to have comparable states of
              knowledge) but not from adult to child. Thus, pedagogy promotes
              efficient learning but at a cost: children are less likely to
              perform potentially irrelevant actions but also less likely to
              discover novel information.",
  journal  = "Cognition",
  volume   =  120,
  number   =  3,
  pages    = "322--330",
  month    =  sep,
  year     =  2011,
  keywords = "development",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "21216395",
  doi      = "10.1016/j.cognition.2010.10.001",
  pmc      = "PMC3369499"
}

@ARTICLE{Dumont2022-am,
  title    = "Transactional longitudinal relations between accuracy and
              reaction time on a measure of cognitive flexibility at 5, 6, and
              7 years of age",
  author   = "Dumont, {\'E}milie and Castellanos-Ryan, Natalie and Parent,
              Sophie and Jacques, Sophie and S{\'e}guin, Jean R and Zelazo,
              Philip David",
  abstract = "Whereas accuracy is used as an indicator of cognitive flexibility
              in preschool-age children, reaction time (RT), or a combination
              of accuracy and RT, provide better indices of performance as
              children transition to school. Theoretical models and
              cross-sectional studies suggest that a speed-accuracy tradeoff
              may be operating across this transition, but the lack of
              longitudinal studies makes this transition difficult to
              understand. The current study explored the longitudinal and
              bidirectional associations between accuracy and RT on the DCCS
              (mixed block) at 5, 6, and 7 years of age using cross-lagged
              panel analyses. The study also examined the roles of working
              memory and language, as potential longitudinal mediators between
              RT at Time X and accuracy at Time X + 1, and explored the role of
              inhibitory control. The sample consisted of 425 children from the
              Quebec Longitudinal Study of Child Development. Results show
              lagged associations from slower RT to greater improvements in
              accuracy between 5 and 6 years and between 6 and 7 years.
              Further, higher accuracy at 6 years predicted faster RT at 7
              years. Only working memory acted as a partial mediator between RT
              at 5 years and accuracy at 6 years. These results provide needed
              longitudinal evidence to support theoretical claims that slower
              RT precedes improved accuracy in the development of cognitive
              flexibility, that working memory may be involved in the early
              stage of this process, and that accuracy and reaction time become
              more efficient in later stages of this process.",
  journal  = "Dev. Sci.",
  volume   =  25,
  number   =  5,
  pages    = "e13254",
  month    =  sep,
  year     =  2022,
  keywords = "DCCS; cognitive flexibility; cross-lagged panel; longitudinal;
              school transition; working memory;development",
  language = "en",
  issn     = "1363-755X, 1467-7687",
  pmid     = "35195319",
  doi      = "10.1111/desc.13254"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chowdhery2022-un,
  title    = "{PaLM}: Scaling Language Modeling with Pathways",
  author   = "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and
              Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, P
              and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian
              and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and
              Maynez, Joshua and Rao, Abhishek B and Barnes, Parker and Tay, Yi
              and Shazeer, Noam M and Prabhakaran, Vinodkumar and Reif, Emily
              and Du, Nan and Hutchinson, B and Pope, Reiner and Bradbury,
              James and Austin, Jacob and Isard, M and Gur-Ari, Guy and Yin,
              Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, S and
              Dev, Sunipa and Michalewski, H and Garc{\'\i}a, Xavier and Misra,
              Vedant and Robinson, Kevin and Fedus, L and Zhou, Denny and
              Ippolito, Daphne and Luan, D and Lim, Hyeontaek and Zoph, Barret
              and Spiridonov, A and Sepassi, Ryan and Dohan, David and Agrawal,
              Shivani and Omernick, Mark and Dai, Andrew M and Pillai, T S and
              Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child,
              Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei
              and Wang, Xuezhi and Saeta, Brennan and D{\'\i}az, Mark and
              Firat, Orhan and Catasta, Michele and Wei, Jason and
              Meier-Hellstern, K and Eck, D and Dean, J and Petrov, Slav and
              Fiedel, Noah",
  abstract = "A 540-billion parameter, densely activated, Transformer language
              model, which is called PaLM achieves breakthrough performance,
              outperforming the state-of-the-art on a suite of multi-step
              reasoning tasks, and outperforming average human performance on
              the recently released BIG-bench benchmark. Large language models
              have been shown to achieve remarkable performance across a
              variety of natural language tasks using few-shot learning , which
              drastically reduces the number of task-speciï¬c training examples
              needed to adapt the model to a particular application. To further
              our understanding of the impact of scale on few-shot learning, we
              trained a 540-billion parameter, densely activated, Transformer
              language model, which we call Pathways Language Model (PaLM). We
              trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system
              which enables highly eï¬ƒcient training across multiple TPU Pods.
              We demonstrate continued beneï¬ts of scaling by achieving
              state-of-the-art few-shot learning results on hundreds of
              language understanding and generation benchmarks. On a number of
              these tasks, PaLM 540B achieves breakthrough performance,
              outperforming the ï¬netuned state-of-the-art on a suite of
              multi-step reasoning tasks, and outperforming average human
              performance on the recently released BIG-bench benchmark. A
              signiï¬cant number of BIG-bench tasks showed discontinuous
              improvements from model scale, meaning that performance steeply
              increased as we scaled to our largest model. PaLM also has strong
              capabilities in multilingual tasks and source code generation,
              which we demonstrate on a wide array of benchmarks. We
              additionally provide a comprehensive analysis on bias and
              toxicity, and study the extent of training data memorization with
              respect to model scale. Finally, we discuss the ethical
              considerations related to large language models and discuss
              potential mitigation strategies.",
  journal  = "ArXiv",
  year     =  2022,
  keywords = "nlp",
  language = "en",
  arxivid  = "2204.02311"
}

@ARTICLE{Xu2013-bw,
  title     = "Infants Are Rational Constructivist Learners",
  author    = "Xu, Fei and Kushnir, Tamar",
  abstract  = "What is the nature of human learning, and what insights can be
               gained from understanding early learning in infants and young
               children? This is an important question for understanding the
               human mind, the origins of knowledge, scientific reasoning, and
               how to best structure our educational environment. In this
               article, we argue for a new approach to cognitive development:
               rational constructivism. This view characterizes the child as a
               rational constructive learner, and it sees early learning as
               rational, statistical, and inferential. Empirical evidence for
               this approach has been accumulating rapidly, and a set of
               domain-general statistical and inferential mechanisms have been
               uncovered to explain why infants and young children learn so
               fast and so well.",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  22,
  number    =  1,
  pages     = "28--32",
  month     =  feb,
  year      =  2013,
  keywords  = "development",
  issn      = "0963-7214",
  doi       = "10.1177/0963721412469396"
}

@INPROCEEDINGS{McCoy2019-zo,
  title     = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in
               Natural Language Inference",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  author    = "McCoy, Tom and Pavlick, Ellie and Linzen, Tal",
  abstract  = "A machine learning system can score well on a given test set by
               relying on heuristics that are effective for frequent example
               types but break down in more challenging cases. We study this
               issue within natural language inference (NLI), the task of
               determining whether one sentence entails another. We hypothesize
               that statistical NLI models may adopt three fallible syntactic
               heuristics: the lexical overlap heuristic, the subsequence
               heuristic, and the constituent heuristic. To determine whether
               models have adopted these heuristics, we introduce a controlled
               evaluation set called HANS (Heuristic Analysis for NLI Systems),
               which contains many examples where the heuristics fail. We find
               that models trained on MNLI, including BERT, a state-of-the-art
               model, perform very poorly on HANS, suggesting that they have
               indeed adopted these heuristics. We conclude that there is
               substantial room for improvement in NLI systems, and that the
               HANS dataset can motivate and measure progress in this area.",
  publisher = "Association for Computational Linguistics",
  pages     = "3428--3448",
  month     =  jul,
  year      =  2019,
  address   = "Florence, Italy",
  keywords  = "nlp",
  doi       = "10.18653/v1/P19-1334"
}

@ARTICLE{Ribeiro2020-xl,
  title         = "Beyond Accuracy: Behavioral Testing of {NLP} models with
                   {CheckList}",
  author        = "Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos
                   and Singh, Sameer",
  abstract      = "Although measuring held-out accuracy has been the primary
                   approach to evaluate generalization, it often overestimates
                   the performance of NLP models, while alternative approaches
                   for evaluating models either focus on individual tasks or on
                   specific behaviors. Inspired by principles of behavioral
                   testing in software engineering, we introduce CheckList, a
                   task-agnostic methodology for testing NLP models. CheckList
                   includes a matrix of general linguistic capabilities and
                   test types that facilitate comprehensive test ideation, as
                   well as a software tool to generate a large and diverse
                   number of test cases quickly. We illustrate the utility of
                   CheckList with tests for three tasks, identifying critical
                   failures in both commercial and state-of-art models. In a
                   user study, a team responsible for a commercial sentiment
                   analysis model found new and actionable bugs in an
                   extensively tested model. In another user study, NLP
                   practitioners with CheckList created twice as many tests,
                   and found almost three times as many bugs as users without
                   it.",
  month         =  may,
  year          =  2020,
  keywords      = "nlp",
  archivePrefix = "arXiv",
  eprint        = "2005.04118",
  primaryClass  = "cs.CL",
  arxivid       = "2005.04118"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Tenney2022-km,
  title    = "What do you learn from context? Probing for sentence structure in
              contextualized word representations",
  author   = "Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and
              Poliak, Adam and Thomas McCoy, R and Kim, Najoung and Van Durme,
              Benjamin and Bowman, Samuel R and Das, Dipanjan and Pavlick,
              Ellie",
  abstract = "Contextualized representation models such as ELMo (Peters et al.,
              2018a) and BERT (Devlin et al., 2018) have recently achieved
              state-of-the-art results on a diverse array of downstream NLP
              tasks. Building on recent token-level probing work, we introduce
              a novel edge probing task design and construct a broad suite of
              sub-sentence tasks derived from the traditional structured NLP
              pipeline. We probe word-level contextual representations from
              four recent models and investigate how they encode sentence
              structure across a range of syntactic, semantic, local, and
              long-range phenomena. We find that existing models trained on
              language modeling and translation produce strong representations
              for syntactic phenomena, but only offer comparably small
              improvements on semantic tasks over a non-contextual baseline.",
  journal  = "https://openreview.net â€º forum â€º
              id=SJzSgnRcKXhttps://openreview.net â€º forum â€º id=SJzSgnRcKX",
  month    =  feb,
  year     =  2022,
  keywords = "nlp"
}

@UNPUBLISHED{Dubey2021-zq,
  title    = "Aha! moments correspond to metacognitive prediction errors",
  author   = "Dubey, Rachit and Ho, Mark K and Mehta, Hermish and Griffiths,
              Tom",
  abstract = "Psychologists have long been fascinated with understanding the
              nature of Aha! moments, moments when we transition from not
              knowing to suddenly realizing the solution to a problem. In this
              work, we present a theoretical framework that explains why we
              experience Aha! moments. Our theory posits that during
              problem-solving, in addition to solving the problem, people also
              maintain a metacognitive model of their ability to solve the
              problem as well as a prediction about the time it would take them
              to solve that problem. Aha! moments arise when we experience a
              positive error in this metacognitive prediction, i.e. when we
              solve a problem much faster than we expected to solve it. We
              posit that this metacognitive error is analogous to a positive
              reward prediction error thereby explaining why we feel so good
              after an Aha! moment. We provide support to our theory across
              three large-scale pre-registered experiments on problem solving,
              demonstrating a link between metacognitive prediction errors and
              Aha! moments. These results highlight the importance of
              metacognitive prediction errors and deepen our understanding of
              human metareasoning.",
  month    =  jun,
  year     =  2021,
  keywords = "Aha! moment; Insight; metacognition; monitoring and control;
              prediction errors; problem solving; reinforcement
              learning;project 1",
  doi      = "10.31234/osf.io/c5v42"
}

@INPROCEEDINGS{Gerstenberg2015-lf,
  title     = "How, whether, why: Causal judgments as counterfactual contrasts",
  booktitle = "{CogSci}",
  author    = "Gerstenberg, Tobias and Goodman, Noah D and Lagnado, David A and
               Tenenbaum, Joshua B",
  year      =  2015,
  keywords  = "comp-cog-sci;project 2"
}

@ARTICLE{Bakhtin2019-hv,
  title         = "{PHYRE}: A new benchmark for physical reasoning",
  author        = "Bakhtin, Anton and van der Maaten, Laurens and Johnson,
                   Justin and Gustafson, Laura and Girshick, Ross",
  abstract      = "Understanding and reasoning about physics is an important
                   ability of intelligent agents. We develop the PHYRE
                   benchmark for physical reasoning that contains a set of
                   simple classical mechanics puzzles in a 2D physical
                   environment. The benchmark is designed to encourage the
                   development of learning algorithms that are sample-efficient
                   and generalize well across puzzles. We test several modern
                   learning algorithms on PHYRE and find that these algorithms
                   fall short in solving the puzzles efficiently. We expect
                   that PHYRE will encourage the development of novel
                   sample-efficient agents that learn efficient but useful
                   models of physics. For code and to play PHYRE for yourself,
                   please visit https://player.phyre.ai.",
  month         =  aug,
  year          =  2019,
  keywords      = "read;project 2",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1908.05656",
  primaryClass  = "cs.LG",
  arxivid       = "1908.05656"
}

@UNPUBLISHED{Ludwin-Peery2021-pr,
  title    = "Limits on Simulation Approaches in Intuitive Physics",
  author   = "Ludwin-Peery, Ethan and Bramley, Neil R and Davis, Ernest and
              Gureckis, Todd M",
  abstract = "A popular explanation of the human ability for physical reasoning
              is that it depends on a sophisticated ability to perform mental
              simulations. According to this perspective, physical reasoning
              problems are approached by repeatedly simulating relevant aspects
              of a scenario, with noise, and making judgments based on
              aggregation over these simulations. In this paper, we describe
              three core tenets of simulation approaches, theoretical
              commitments that must be present in order for a simulation
              approach to be viable. The identification of these tenets
              threatens the plausibility of simulation as a theory of physical
              reasoning, because they appear to be incompatible with what we
              know about cognition more generally. To investigate this apparent
              contradiction, we describe three experiments involving simple
              physical judgments and predictions, and argue their results
              challenge these core predictions of theories of mental
              simulation.",
  month    =  jan,
  year     =  2021,
  keywords = "project 2",
  doi      = "10.31234/osf.io/xhzuc"
}

@ARTICLE{Fabricius2021-zg,
  title    = "Perceptual Access Reasoning ({PAR}) in Developing a
              Representational Theory of Mind",
  author   = "Fabricius, William V and Gonzales, Christopher R and Pesch,
              Annelise and Weimer, Amy A and Pugliese, John and Carroll,
              Kathleen and Bolnick, Rebecca R and Kupfer, Anne S and Eisenberg,
              Nancy and Spinrad, Tracy L",
  abstract = "An important part of children's social and cognitive development
              is their understanding that people are psychological beings with
              internal, mental states including desire, intention, perception,
              and belief. A full understanding of people as psychological
              beings requires a representational theory of mind (ToM), which is
              an understanding that mental states can faithfully represent
              reality, or misrepresent reality. For the last 35 years,
              researchers have relied on false-belief tasks as the gold
              standard to test children's understanding that beliefs can
              misrepresent reality. In false-belief tasks, children are asked
              to reason about the behavior of agents who have false beliefs
              about situations. Although a large body of evidence indicates
              that most children pass false-belief tasks by the end of the
              preschool years, the evidence we present in this monograph
              suggests that most children do not understand false beliefs or,
              surprisingly, even true beliefs until middle childhood. We argue
              that young children pass false-belief tasks without understanding
              false beliefs by using perceptual access reasoning (PAR). With
              PAR, children understand that seeing leads to knowing in the
              moment, but not that knowing also arises from thinking or
              persists as memory and belief after the situation changes. By the
              same token, PAR leads children to fail true-belief tasks. PAR
              theory can account for performance on other traditional tests of
              representational ToM and related tasks, and can account for the
              factors that have been found to correlate with or affect both
              true- and false-belief performance. The theory provides a new
              laboratory measure which we label the belief understanding scale
              (BUS). This scale can distinguish between a child who is
              operating with PAR versus a child who is understanding beliefs.
              This scale provides a method needed to allow the study of the
              development of representational ToM. In this monograph, we report
              the outcome of the tests that we have conducted of predictions
              generated by PAR theory. The findings demonstrated signature PAR
              limitations in reasoning about the mind during the ages when
              children are hypothesized to be using PAR. In Chapter II,
              secondary analyses of the published true-belief literature
              revealed that children failed several types of true-belief tasks.
              Chapters III through IX describe new empirical data collected
              across multiple studies between 2003 and 2014 from 580 children
              aged 4-7 years, as well as from a small sample of 14 adults.
              Participants were recruited from the Phoenix, Arizona
              metropolitan area. All participants were native English-speakers.
              Children were recruited from university-sponsored and community
              preschools and daycare centers, and from hospital maternity
              wards. Adults were university students who participated to
              partially fulfill course requirements for research participation.
              Sociometric data were collected only in Chapter IX, and are fully
              reported there. In Chapter III, minor alterations in task
              procedures produced wide variations in children's performance in
              3-option false-belief tasks. In Chapter IV, we report findings
              which show that the developmental lag between children's
              understanding ignorance and understanding false belief is longer
              than the lag reported in previous studies. In Chapter V, children
              did not distinguish between agents who have false beliefs versus
              agents who have no beliefs. In Chapter VI, findings showed that
              children found it no easier to reason about true beliefs than to
              reason about false beliefs. In Chapter VII, when children were
              asked to justify their correct answers in false-belief tasks,
              they did not reference agents' false beliefs. Similarly, in
              Chapter VIII, when children were asked to explain agents' actions
              in false-belief tasks, they did not reference agents' false
              beliefs. In Chapter IX, children who were identified as using PAR
              differed from children who understood beliefs along three
              dimensions-in levels of social development, inhibitory control,
              and kindergarten adjustment. Although the findings need
              replication and additional studies of alternative
              interpretations, the collection of results reported in this
              monograph challenges the prevailing view that representational
              ToM is in place by the end of the preschool years. Furthermore,
              the pattern of findings is consistent with the proposal that PAR
              is the developmental precursor of representational ToM. The
              current findings also raise questions about claims that infants
              and toddlers demonstrate ToM-related abilities, and that
              representational ToM is innate.",
  journal  = "Monogr. Soc. Res. Child Dev.",
  volume   =  86,
  number   =  3,
  pages    = "7--154",
  month    =  sep,
  year     =  2021,
  language = "en",
  issn     = "0037-976X, 1540-5834",
  pmid     = "34580875",
  doi      = "10.1111/mono.12432",
  pmc      = "PMC9292623"
}

@ARTICLE{Allen2022-as,
  title    = "Physical Design using Differentiable Learned Simulators",
  author   = "Allen, Kelsey R and Lopez-Guevara, Tatiana and Stachenfeld, K and
              Sanchez-Gonzalez, Alvaro and Battaglia, P and Hamrick, Jessica B
              and Pfaff, T",
  abstract = "This work explores a simple, fast, and robust approach to inverse
              design which combines learned forward simulators based on graph
              neural networks with gradient-based design optimization, and
              suggests that despite some remaining challenges, machine
              learning-based simulators are maturing to the point where they
              can support general-purpose design optimization across a variety
              of domains. Designing physical artifacts that serve a purpose---
              such as tools and other functional structures---is central to
              engineering as well as everyday human behavior. Though automating
              design has tremendous promise, general-purpose methods do not yet
              exist. Here we explore a simple, fast, and robust approach to
              inverse design which combines learned forward simulators based on
              graph neural networks with gradient-based design optimization.
              Our approach solves high-dimensional problems with complex
              physical dynamics, including designing surfaces and tools to
              manipulate fluid flows and optimizing the shape of an airfoil to
              minimize drag. This framework produces highquality designs by
              propagating gradients through trajectories of hundreds of steps,
              even when using models that were pre-trained for single-step
              predictions on data substantially different from the design
              tasks. In our fluid manipulation tasks, the resulting designs
              outperformed those found by sampling-based optimization
              techniques. In airfoil design, they matched the quality of those
              obtained with a specialized solver. Our results suggest that
              despite some remaining challenges, machine learning-based
              simulators are maturing to the point where they can support
              general-purpose design optimization across a variety of domains.",
  journal  = "ArXiv",
  year     =  2022,
  keywords = "comp-cog-sci;project 2",
  language = "en",
  arxivid  = "2202.00728"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{ScottL_undated-sr,
  title    = "An overview of the mental model theory",
  author   = "{ScottL}",
  abstract = "There is dispute about what exactly a ``mental model'' is and the
              concepts related to it are often aren't clarified well. One
              feature of them that is generally accepted is that ``the
              structure of mentalâ€¦",
  keywords = "read;project 2"
}

@ARTICLE{Balestriero2021-hw,
  title         = "Learning in High Dimension Always Amounts to Extrapolation",
  author        = "Balestriero, Randall and Pesenti, Jerome and LeCun, Yann",
  abstract      = "The notion of interpolation and extrapolation is fundamental
                   in various fields from deep learning to function
                   approximation. Interpolation occurs for a sample $x$
                   whenever this sample falls inside or on the boundary of the
                   given dataset's convex hull. Extrapolation occurs when $x$
                   falls outside of that convex hull. One fundamental
                   (mis)conception is that state-of-the-art algorithms work so
                   well because of their ability to correctly interpolate
                   training data. A second (mis)conception is that
                   interpolation happens throughout tasks and datasets, in
                   fact, many intuitions and theories rely on that assumption.
                   We empirically and theoretically argue against those two
                   points and demonstrate that on any high-dimensional ($>$100)
                   dataset, interpolation almost surely never happens. Those
                   results challenge the validity of our current
                   interpolation/extrapolation definition as an indicator of
                   generalization performances.",
  month         =  oct,
  year          =  2021,
  keywords      = "machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2110.09485",
  primaryClass  = "cs.LG",
  arxivid       = "2110.09485"
}

@ARTICLE{Ullman2017-fo,
  title    = "Mind Games: Game Engines as an Architecture for Intuitive Physics",
  author   = "Ullman, Tomer D and Spelke, Elizabeth and Battaglia, Peter and
              Tenenbaum, Joshua B",
  abstract = "We explore the hypothesis that many intuitive physical inferences
              are based on a mental physics engine that is analogous in many
              ways to the machine physics engines used in building interactive
              video games. We describe the key features of game physics engines
              and their parallels in human mental representation, focusing
              especially on the intuitive physics of young infants where the
              hypothesis helps to unify many classic and otherwise puzzling
              phenomena, and may provide the basis for a computational account
              of how the physical knowledge of infants develops. This
              hypothesis also explains several `physics illusions', and helps
              to inform the development of artificial intelligence (AI) systems
              with more human-like common sense.",
  journal  = "Trends Cogn. Sci.",
  volume   =  21,
  number   =  9,
  pages    = "649--665",
  month    =  sep,
  year     =  2017,
  keywords = "project 2",
  issn     = "1364-6613",
  doi      = "10.1016/j.tics.2017.05.012"
}

@ARTICLE{Johnson-Laird2010-yq,
  title    = "Mental models and human reasoning",
  author   = "Johnson-Laird, Philip N",
  abstract = "To be rational is to be able to reason. Thirty years ago
              psychologists believed that human reasoning depended on formal
              rules of inference akin to those of a logical calculus. This
              hypothesis ran into difficulties, which led to an alternative
              view: reasoning depends on envisaging the possibilities
              consistent with the starting point---a perception of the world, a
              set of assertions, a memory, or some mixture of them. We
              construct mental models of each distinct possibility and derive a
              conclusion from them. The theory predicts systematic errors in
              our reasoning, and the evidence corroborates this prediction.
              Yet, our ability to use counterexamples to refute invalid
              inferences provides a foundation for rationality. On this
              account, reasoning is a simulation of the world fleshed out with
              our knowledge, not a formal rearrangement of the logical
              skeletons of sentences.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  107,
  number   =  43,
  pages    = "18243--18250",
  year     =  2010,
  keywords = "project 1",
  eprint   = "https://www.pnas.org/doi/pdf/10.1073/pnas.1012933107",
  doi      = "10.1073/pnas.1012933107"
}

@INCOLLECTION{Forbus1988-zh,
  title     = "Chapter 7 - Qualitative Physics: Past, Present, and Future",
  booktitle = "Exploring Artificial Intelligence",
  author    = "Forbus, Kenneth D",
  editor    = "Shrobe, Howard E and {the American Association for Artificial
               Intelligence}",
  abstract  = "Publisher Summary Qualitative physics is concerned with
               representing and reasoning about the physical world. The goal of
               qualitative physics is to capture both the commonsense knowledge
               of the person on the street and the tacit knowledge underlying
               the quantitative knowledge used by engineers and scientists. The
               key to qualitative physics is to find ways to represent
               continuous properties of the world by discrete systems of
               symbols. One can always quantize something continuous, but not
               all quantizations are equally useful. One way to state the idea
               is the relevance principle: The distinctions made by a
               quantization must be relevant to the kind of reasoning
               performed. This chapter describes what qualitative physics is,
               why one should be doing it, and where it came from. It discusses
               some open problems in qualitative physics.",
  publisher = "Morgan Kaufmann",
  pages     = "239--296",
  month     =  jan,
  year      =  1988,
  keywords  = "skimmed;project 2",
  isbn      = "9780934613675",
  doi       = "10.1016/B978-0-934613-67-5.50011-3"
}

@ARTICLE{Lupyan2016-mq,
  title     = "The centrality of language in human cognition",
  author    = "Lupyan, Gary",
  abstract  = "The emergence of language---a productive and combinatorial
               system of communication---has been hailed as one of the major
               transitions in evolution. By enabling symbolic culture, language
               allows humans to draw on and expand on the knowledge of their
               ancestors and peers. A common assumption among linguists and
               psychologists is that although language is critical to our
               ability to share our thoughts, it plays a minor, if any, role in
               generating, controlling, and structuring them. I examine some
               assumptions that led to this view of language and discuss an
               alternative according to which normal human cognition is
               language-augmented cognition. I focus on one of the fundamental
               design features of language---the use of words as symbolic
               cues---and argue that language acts as a high-level control
               system for the mind, allowing individuals to sculpt mental
               representations of others as well as their own.",
  journal   = "Lang. Learn.",
  publisher = "Wiley",
  volume    =  66,
  number    =  3,
  pages     = "516--553",
  month     =  sep,
  year      =  2016,
  keywords  = "comp-cog-sci;language",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en",
  issn      = "0023-8333, 1467-9922",
  doi       = "10.1111/lang.12155"
}

@ARTICLE{Rosedahl2022-py,
  title    = "Linear separability, irrelevant variability, and categorization
              difficulty",
  author   = "Rosedahl, Luke A and Ashby, F Gregory",
  abstract = "In rule-based (RB) category-learning tasks, the optimal strategy
              is a simple explicit rule, whereas in information-integration
              (II) tasks, the optimal strategy is impossible to describe
              verbally. This study investigates the effects of two different
              category properties on learning difficulty in category learning
              tasks-namely, linear separability and variability on stimulus
              dimensions that are irrelevant to the categorization decision.
              Previous research had reported that linearly separable II
              categories are easier to learn than nonlinearly separable
              categories, but Experiment 1, which compared performance on
              linearly and nonlinearly separable categories that were equated
              as closely as possible on all other factors that might affect
              difficulty, found that linear separability had no effect on
              learning. Experiments 1 and 2 together also established a novel
              dissociation between RB and II category learning: increasing
              variability on irrelevant stimulus dimensions impaired II
              learning but not RB learning. These results are all predicted by
              the best available measures of difficulty in RB and II tasks.
              (PsycInfo Database Record (c) 2022 APA, all rights reserved).",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  48,
  number   =  2,
  pages    = "159--172",
  month    =  feb,
  year     =  2022,
  language = "en",
  issn     = "0278-7393, 1939-1285",
  pmid     = "33871263",
  doi      = "10.1037/xlm0001000",
  pmc      = "PMC8523591"
}

@ARTICLE{Chronicle2004-bv,
  title    = "What makes an insight problem? The roles of heuristics, goal
              conception, and solution recoding in knowledge-lean problems",
  author   = "Chronicle, Edward P and MacGregor, James N and Ormerod, Thomas C",
  abstract = "Four experiments investigated transformation problems with
              insight characteristics. In Experiment 1, performance on a
              version of the 6-coin problem that had a concrete and
              visualizable solution followed a hill-climbing heuristic.
              Experiment 2 demonstrated that the difficulty of a version of the
              problem that potentially required insight for solution stems from
              the same hill-climbing heuristic, which creates an implicit
              conceptual block. Experiment 3 confirmed that the difficulty of
              the potential insight solution is conceptual, not procedural.
              Experiment 4 demonstrated the same principles of move selection
              on the 6-coin problem and the 10-coin (triangle) problem. It is
              argued that hill-climbing heuristics provide a common framework
              for understanding transformation and insight problem solving.
              Postsolution receding may account for part of the phenomenology
              of insight.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  30,
  number   =  1,
  pages    = "14--27",
  month    =  jan,
  year     =  2004,
  keywords = "cog-sci;project 1",
  language = "en",
  issn     = "0278-7393",
  pmid     = "14736293",
  doi      = "10.1037/0278-7393.30.1.14"
}

@MISC{Gershman_undated-tc,
  title        = "Perceptual multistability as Markov chain Monte Carlo
                  inference",
  author       = "Gershman, Samuel J and Vul, Edward and Tenenbaum, Joshua B",
  howpublished = "\url{https://gershmanlab.com/pubs/GershmanVulTenenbaum09.pdf}",
  note         = "Accessed: 2023-1-19",
  keywords     = "comp-cog-sci;project 1"
}

@ARTICLE{Wilson2013-cg,
  title    = "Embodied Cognition is Not What you Think it is",
  author   = "Wilson, Andrew D and Golonka, Sabrina",
  abstract = "The most exciting hypothesis in cognitive science right now is
              the theory that cognition is embodied. Like all good ideas in
              cognitive science, however, embodiment immediately came to mean
              six different things. The most common definitions involve the
              straight-forward claim that ``states of the body modify states of
              the mind.'' However, the implications of embodiment are actually
              much more radical than this. If cognition can span the brain,
              body, and the environment, then the ``states of mind'' of
              disembodied cognitive science won't exist to be modified.
              Cognition will instead be an extended system assembled from a
              broad array of resources. Taking embodiment seriously therefore
              requires both new methods and theory. Here we outline four key
              steps that research programs should follow in order to fully
              engage with the implications of embodiment. The first step is to
              conduct a task analysis, which characterizes from a first person
              perspective the specific task that a perceiving-acting cognitive
              agent is faced with. The second step is to identify the
              task-relevant resources the agent has access to in order to solve
              the task. These resources can span brain, body, and environment.
              The third step is to identify how the agent can assemble these
              resources into a system capable of solving the problem at hand.
              The last step is to test the agent's performance to confirm that
              agent is actually using the solution identified in step 3. We
              explore these steps in more detail with reference to two useful
              examples (the outfielder problem and the A-not-B error), and
              introduce how to apply this analysis to the thorny question of
              language use. Embodied cognition is more than we think it is, and
              we have the tools we need to realize its full potential.",
  journal  = "Front. Psychol.",
  volume   =  4,
  pages    = "58",
  month    =  feb,
  year     =  2013,
  keywords = "A-not-B error; dynamical systems; embodied cognition; language;
              outfielder problem; replacement hypothesis; robotics;project
              1;cog-sci",
  language = "en",
  issn     = "1664-1078",
  pmid     = "23408669",
  doi      = "10.3389/fpsyg.2013.00058",
  pmc      = "PMC3569617"
}

@UNPUBLISHED{Schulz2017-ij,
  title    = "Strategic exploration in human adaptive control",
  author   = "Schulz, Eric and Klenske, Edgar D and Bramley, Neil R and
              Speekenbrink, Maarten",
  abstract = "Abstract How do people explore in order to gain rewards in
              uncertain dynamical systems? Within a reinforcement learning
              paradigm, control normally involves trading off between
              exploration (i.e. trying out actions in order to gain more
              knowledge about the system) and exploitation (i.e. using current
              knowledge of the system to maximize reward). We study a novel
              control task in which participants must steer a boat on a grid,
              aiming to follow a path of high reward whilst learning how their
              actions affect the boat's position. We find that participants
              explore strategically yet conservatively, exploring more when
              mistakes are less costly and practicing actions that will be
              required later on.",
  journal  = "bioRxiv",
  pages    = "110486",
  month    =  may,
  year     =  2017,
  keywords = "project 1",
  language = "en",
  doi      = "10.1101/110486"
}

@ARTICLE{Thevenot2008-ob,
  title    = "A generalization of the representational change theory from
              insight to non-insight problems: the case of arithmetic word
              problems",
  author   = "Thevenot, Catherine and Oakhill, Jane",
  abstract = "This paper provides evidence for a possible generalization of
              Knoblich and colleagues' representational change theory
              [Knoblich, G., Ohlsson, S., Haider, H., \& Rhenius, D. (1999).
              Constraint relaxation and chunk decomposition in insight problem
              solving. Journal of Experimental Psychology: Learning, Memory,
              and Cognition, 25, 1534-1555; Knoblich, G., Ohlsson, S., \&
              Raney, G. E. (2001). An eye movement study of insight problem.
              Memory and Cognition, 29, 1000-1009] outside its original scope
              of application. While this theory has been proposed to explain
              insight problem solving, we demonstrate here that its main
              concepts, namely, constraint relaxation and chunk decomposition,
              are applicable to incremental problem solving. In a first
              experiment, we confirm, as already shown by problem solving and
              reasoning researchers, that individuals avoid the construction of
              alternative representations of the problems when possible. In the
              second and third experiments, we show that alternative
              representations of arithmetic problems are easier to construct
              and maintain when they violate constraints of narrow rather than
              wide scope. The specificity of insight problem solving is
              discussed in the light of these new findings.",
  journal  = "Acta Psychol.",
  volume   =  129,
  number   =  3,
  pages    = "315--324",
  month    =  nov,
  year     =  2008,
  keywords = "project 1",
  language = "en",
  issn     = "0001-6918, 1873-6297",
  pmid     = "18834964",
  doi      = "10.1016/j.actpsy.2008.08.008"
}

@ARTICLE{Ollinger2008-fw,
  title    = "Investigating the effect of mental set on insight problem solving",
  author   = "Ollinger, Michael and Jones, Gary and Knoblich, G{\"u}nther",
  abstract = "Mental set is the tendency to solve certain problems in a fixed
              way based on previous solutions to similar problems. The moment
              of insight occurs when a problem cannot be solved using solution
              methods suggested by prior experience and the problem solver
              suddenly realizes that the solution requires different solution
              methods. Mental set and insight have often been linked together
              and yet no attempt thus far has systematically examined the
              interplay between the two. Three experiments are presented that
              examine the extent to which sets of noninsight and insight
              problems affect the subsequent solutions of insight test
              problems. The results indicate a subtle interplay between mental
              set and insight: when the set involves noninsight problems, no
              mental set effects are shown for the insight test problems, yet
              when the set involves insight problems, both facilitation and
              inhibition can be seen depending on the type of insight problem
              presented in the set. A two process model is detailed to explain
              these findings that combines the representational change
              mechanism with that of proceduralization.",
  journal  = "Exp. Psychol.",
  volume   =  55,
  number   =  4,
  pages    = "269--282",
  year     =  2008,
  keywords = "project 1",
  language = "en",
  issn     = "1618-3169",
  pmid     = "18683624",
  doi      = "10.1027/1618-3169.55.4.269"
}

@INCOLLECTION{McClelland1988-qm,
  title     = "The appeal of parallel distributed processing",
  booktitle = "Readings in cognitive science: A perspective from psychology and
               artificial intelligence , (pp",
  author    = "McClelland, James L and Rumelhart, David E and Hinton, G E",
  editor    = "Collins, Allan M",
  abstract  = "multiple simultaneous constraints parallel distributed
               processing [PDP] / examples of PDP models representation and
               learning in PDP models origins of parallel distributed
               processing (PsycInfo Database Record (c) 2022 APA, all rights
               reserved)",
  volume    =  661,
  pages     = "52--72",
  year      =  1988,
  keywords  = "read;ccm2023;comp-cog-sci"
}

@ARTICLE{Elman1990-pd,
  title     = "Finding structure in time",
  author    = "Elman, Jeffrey L",
  abstract  = "Time underlies many interesting human behaviors. Thus, the
               question of how to represent time in connectionist models is
               very important. One approach is to represent time implicitly by
               its effects on processing rather than explicitly (as in a
               spatial representation). The current report develops a proposal
               along these lines first described by Jordan (1986) which
               involves the use of recurrent links in order to provide networks
               with a dynamic memory. In this approach, hidden unit patterns
               are fed back to themselves; the internal representations which
               develop thus reflect task demands in the context of prior
               internal states. A set of simulations is reported which range
               from relatively simple problems (temporal version of XOR) to
               discovering syntactic/semantic features for words. The networks
               are able to learn interesting internal representations which
               incorporate task demands with memory demands; indeed, in this
               approach the notion of memory is inextricably bound up with task
               processing. These representations reveal a rich structure, which
               allows them to be highly context-dependent while also expressing
               generalizations across classes of items. These representations
               suggest a method for representing lexical categories and the
               type/token distinction.",
  journal   = "Cogn. Sci.",
  publisher = "Wiley",
  volume    =  14,
  number    =  2,
  pages     = "179--211",
  month     =  mar,
  year      =  1990,
  keywords  = "ccm2023;comp-cog-sci",
  language  = "en",
  issn      = "0364-0213, 1551-6709",
  doi       = "10.1207/s15516709cog1402\_1"
}

@ARTICLE{Ollinger2014-qq,
  title    = "The dynamics of search, impasse, and representational change
              provide a coherent explanation of difficulty in the nine-dot
              problem",
  author   = "{\"O}llinger, Michael and Jones, Gary and Knoblich, G{\"u}nther",
  abstract = "The nine-dot problem is often used to demonstrate and explain
              mental impasse, creativity, and out of the box thinking. The
              present study investigated the interplay of a restricted initial
              search space, the likelihood of invoking a representational
              change, and the subsequent constraining of an unrestricted search
              space. In three experimental conditions, participants worked on
              different versions of the nine-dot problem that hinted at
              removing particular sources of difficulty from the standard
              problem. The hints were incremental such that the first suggested
              a possible route for a solution attempt; the second additionally
              indicated the dot at which lines meet on the solution path; and
              the final condition also provided non-dot locations that appear
              in the solution path. The results showed that in the experimental
              conditions, representational change is encountered more quickly
              and problems are solved more often than for the control group. We
              propose a cognitive model that focuses on general problem-solving
              heuristics and representational change to explain problem
              difficulty.",
  journal  = "Psychol. Res.",
  volume   =  78,
  number   =  2,
  pages    = "266--275",
  month    =  mar,
  year     =  2014,
  keywords = "read;project 1",
  language = "en",
  issn     = "0340-0727, 1430-2772",
  pmid     = "23708954",
  doi      = "10.1007/s00426-013-0494-8"
}

@ARTICLE{Goyal2021-dr,
  title         = "Neural Production Systems: Learning {Rule-Governed} Visual
                   Dynamics",
  author        = "Goyal, Anirudh and Didolkar, Aniket and Ke, Nan Rosemary and
                   Blundell, Charles and Beaudoin, Philippe and Heess, Nicolas
                   and Mozer, Michael and Bengio, Yoshua",
  abstract      = "Visual environments are structured, consisting of distinct
                   objects or entities. These entities have properties -- both
                   visible and latent -- that determine the manner in which
                   they interact with one another. To partition images into
                   entities, deep-learning researchers have proposed structural
                   inductive biases such as slot-based architectures. To model
                   interactions among entities, equivariant graph neural nets
                   (GNNs) are used, but these are not particularly well suited
                   to the task for two reasons. First, GNNs do not predispose
                   interactions to be sparse, as relationships among
                   independent entities are likely to be. Second, GNNs do not
                   factorize knowledge about interactions in an
                   entity-conditional manner. As an alternative, we take
                   inspiration from cognitive science and resurrect a classic
                   approach, production systems, which consist of a set of rule
                   templates that are applied by binding placeholder variables
                   in the rules to specific entities. Rules are scored on their
                   match to entities, and the best fitting rules are applied to
                   update entity properties. In a series of experiments, we
                   demonstrate that this architecture achieves a flexible,
                   dynamic flow of control and serves to factorize
                   entity-specific and rule-based information. This
                   disentangling of knowledge achieves robust future-state
                   prediction in rich visual environments, outperforming
                   state-of-the-art methods using GNNs, and allows for the
                   extrapolation from simple (few object) environments to more
                   complex environments.",
  month         =  mar,
  year          =  2021,
  keywords      = "compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2103.01937",
  primaryClass  = "cs.AI",
  arxivid       = "2103.01937"
}

@ARTICLE{Sumers2023-kl,
  title    = "Show or tell? Exploring when (and why) teaching with language
              outperforms demonstration",
  author   = "Sumers, Theodore R and Ho, Mark K and Hawkins, Robert D and
              Griffiths, Thomas L",
  abstract = "People use a wide range of communicative acts across different
              modalities, from concrete demonstrations to abstract language.
              While these modalities are typically studied independently, we
              take a comparative approach and ask when and why one modality
              might outperform another. We present a series of real-time,
              multi-player experiments asking participants to teach concepts
              using either demonstrations or language. Our first experiment
              (N=416) asks when language might outperform demonstration. We
              manipulate the complexity of the concept being taught and find
              that language communicates complex concepts more effectively than
              demonstration. We then ask why language succeeds in this setting.
              We hypothesized that language allowed teachers to reference
              abstract object features (e.g., shapes and colors), while
              demonstration teachers could only provide concrete examples
              (specific positive or negative objects). To test this hypothesis,
              our second experiment (N=568) ablated object features from the
              teacher's interface. This manipulation severely impaired
              linguistic (but not demonstrative) teaching. Our findings suggest
              that language communicates complex concepts by directly
              transmitting abstract rules. In contrast, demonstrations transmit
              examples, requiring the learner to infer the rules.",
  journal  = "Cognition",
  volume   =  232,
  pages    = "105326",
  month    =  mar,
  year     =  2023,
  keywords = "Abstraction; Communication; Demonstration; Language;
              Pedagogy;comp-cog-sci",
  language = "en",
  issn     = "0010-0277, 1873-7838",
  pmid     = "36473238",
  doi      = "10.1016/j.cognition.2022.105326"
}

@ARTICLE{Ho2022-iy,
  title    = "People construct simplified mental representations to plan",
  author   = "Ho, Mark K and Abel, David and Correa, Carlos G and Littman,
              Michael L and Cohen, Jonathan D and Griffiths, Thomas L",
  abstract = "One of the most striking features of human cognition is the
              ability to plan. Two aspects of human planning stand out-its
              efficiency and flexibility. Efficiency is especially impressive
              because plans must often be made in complex environments, and yet
              people successfully plan solutions to many everyday problems
              despite having limited cognitive resources1-3. Standard accounts
              in psychology, economics and artificial intelligence have
              suggested that human planning succeeds because people have a
              complete representation of a task and then use heuristics to plan
              future actions in that representation4-11. However, this approach
              generally assumes that task representations are fixed. Here we
              propose that task representations can be controlled and that such
              control provides opportunities to quickly simplify problems and
              more easily reason about them. We propose a computational account
              of this simplification process and, in a series of preregistered
              behavioural experiments, show that it is subject to online
              cognitive control12-14 and that people optimally balance the
              complexity of a task representation and its utility for planning
              and acting. These results demonstrate how strategically
              perceiving and conceiving problems facilitates the effective use
              of limited cognitive resources.",
  journal  = "Nature",
  volume   =  606,
  number   =  7912,
  pages    = "129--136",
  month    =  jun,
  year     =  2022,
  keywords = "project 1;comp-cog-sci",
  language = "en",
  issn     = "0028-0836, 1476-4687",
  pmid     = "35589843",
  doi      = "10.1038/s41586-022-04743-9",
  pmc      = "5111694"
}

@MISC{Kosoy_undated-ah,
  title    = "Learning Causal Overhypotheses through Exploration in Children
              and Computational Models",
  author   = "Kosoy, Eliza and Liu, Adrian and Collins, Jasmine and Chan, David
              M and Hamrick, Jessica B and Rosemary, Nan and Huang, Sandy Han
              and Kaufmann, Bryanna and Gopnik, Alison and Sch{\"o}lkopf,
              Bernhard and Uhler, Caroline and Zhang, Kun and Ke, N R and
              Canny, J",
  keywords = "comp-cog-sci;development",
  arxivid  = "2202.10430v1"
}

@ARTICLE{Lynch2019-eb,
  title         = "Learning latent plans from play",
  author        = "Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar,
                   Vikash and Tompson, Jonathan and Levine, Sergey and
                   Sermanet, Pierre",
  abstract      = "Acquiring a diverse repertoire of general-purpose skills
                   remains an open challenge for robotics. In this work, we
                   propose self-supervising control on top of human
                   teleoperated play data as a way to scale up skill learning.
                   Play has two properties that make it attractive compared to
                   conventional task demonstrations. Play is cheap, as it can
                   be collected in large quantities quickly without task
                   segmenting, labeling, or resetting to an initial state. Play
                   is naturally rich, covering ~4x more interaction space than
                   task demonstrations for the same amount of collection time.
                   To learn control from play, we introduce Play-LMP, a
                   self-supervised method that learns to organize play
                   behaviors in a latent space, then reuse them at test time to
                   achieve specific goals. Combining self-supervised control
                   with a diverse play dataset shifts the focus of skill
                   learning from a narrow and discrete set of tasks to the full
                   continuum of behaviors available in an environment. We find
                   that this combination generalizes well empirically---after
                   self-supervising on unlabeled play, our method substantially
                   outperforms individual expert-trained policies on 18
                   difficult user-specified visual manipulation tasks in a
                   simulated robotic tabletop environment. We additionally find
                   that play-supervised models, unlike their expert-trained
                   counterparts, are more robust to perturbations and exhibit
                   retrying-till-success behaviors. Finally, we find that our
                   agent organizes its latent plan space around functional
                   tasks, despite never being trained with task labels. Videos,
                   code and data are available at learning-from-play.github.io",
  month         =  mar,
  year          =  2019,
  keywords      = "comp-cog-sci;ARC Project",
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  eprint        = "1903.01973",
  primaryClass  = "cs.RO",
  arxivid       = "1903.01973"
}

@ARTICLE{Knoblich1999-jd,
  title    = "Constraint relaxation and chunk decomposition in insight problem
              solving",
  author   = "Knoblich, G{\"u}nther and Ohlsson, Stellan and Haider, Hilde and
              Rhenius, Detlef",
  abstract = "Insight problem solving is characterized by impasses, states of
              mind in which the thinker does not know what to do next. The
              authors hypothesized that impasses are broken by changing the
              problem representation, and 2 hypothetical mechanisms for
              representational change are described: the relaxation of
              constraints on the solution and the decomposition of perceptual
              chunks. These 2 mechanisms generate specific predictions about
              the relative difficulty of individual problems and about
              differential transfer effects. The predictions were tested in 4
              experiments using matchstick arithmetic problems. The results
              were consistent with the predictions. Representational change is
              a more powerful explanation for insight than alternative
              hypotheses, if the hypothesized change processes are specified in
              detail. Overcoming impasses in insight is a special case of the
              general need to override the imperatives of past experience in
              the face of novel conditions. (PsycINFO Database Record (c) 2016
              APA, all rights reserved)",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  25,
  number   =  6,
  pages    = "1534--1555",
  month    =  nov,
  year     =  1999,
  issn     = "0278-7393, 1939-1285",
  doi      = "10.1037/0278-7393.25.6.1534"
}

@ARTICLE{Meltzoff2009-eq,
  title    = "Foundations for a new science of learning",
  author   = "Meltzoff, Andrew N and Kuhl, Patricia K and Movellan, Javier and
              Sejnowski, Terrence J",
  abstract = "Human learning is distinguished by the range and complexity of
              skills that can be learned and the degree of abstraction that can
              be achieved compared with those of other species. Homo sapiens is
              also the only species that has developed formal ways to enhance
              learning: teachers, schools, and curricula. Human infants have an
              intense interest in people and their behavior and possess
              powerful implicit learning mechanisms that are affected by social
              interaction. Neuroscientists are beginning to understand the
              brain mechanisms underlying learning and how shared brain systems
              for perception and action support social learning. Machine
              learning algorithms are being developed that allow robots and
              computers to learn autonomously. New insights from many different
              fields are converging to create a new science of learning that
              may transform educational practices.",
  journal  = "Science",
  volume   =  325,
  number   =  5938,
  pages    = "284--288",
  month    =  jul,
  year     =  2009,
  keywords = "read;learning\&memory2023;cog-sci",
  language = "en",
  issn     = "0036-8075, 1095-9203",
  pmid     = "19608908",
  doi      = "10.1126/science.1175626",
  pmc      = "PMC2776823"
}

@ARTICLE{Zador2019-hq,
  title    = "A critique of pure learning and what artificial neural networks
              can learn from animal brains",
  author   = "Zador, Anthony M",
  abstract = "Artificial neural networks (ANNs) have undergone a revolution,
              catalyzed by better supervised learning algorithms. However, in
              stark contrast to young animals (including humans), training such
              networks requires enormous numbers of labeled examples, leading
              to the belief that animals must rely instead mainly on
              unsupervised learning. Here we argue that most animal behavior is
              not the result of clever learning algorithms-supervised or
              unsupervised-but is encoded in the genome. Specifically, animals
              are born with highly structured brain connectivity, which enables
              them to learn very rapidly. Because the wiring diagram is far too
              complex to be specified explicitly in the genome, it must be
              compressed through a ``genomic bottleneck''. The genomic
              bottleneck suggests a path toward ANNs capable of rapid learning.",
  journal  = "Nat. Commun.",
  volume   =  10,
  number   =  1,
  pages    = "3770",
  month    =  aug,
  year     =  2019,
  keywords = "read;cog-sci;learning\&memory2023",
  language = "en",
  issn     = "2041-1723",
  pmid     = "31434893",
  doi      = "10.1038/s41467-019-11786-6",
  pmc      = "PMC6704116"
}

@ARTICLE{McClelland2003-bz,
  title    = "The parallel distributed processing approach to semantic
              cognition",
  author   = "McClelland, James L and Rogers, Timothy T",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  4,
  number   =  4,
  pages    = "310--322",
  month    =  apr,
  year     =  2003,
  keywords = "comp-cog-sci;ccm2023",
  language = "en",
  issn     = "1471-003X",
  pmid     = "12671647",
  doi      = "10.1038/nrn1076"
}

@ARTICLE{Kaplan1990-zd,
  title    = "In search of insight",
  author   = "Kaplan, Craig A and Simon, Herbert A",
  abstract = "This paper describes the process of attaining the insight
              required to solve a particular problem---the Mutilated
              Checkerboard (MC) problem. It shows that attaining insight
              requires discovering an effective problem representation, and
              that performance on insight problems can be predicted from the
              availability of generators and constraints in the search for such
              a representation. To test these claims we varied the salience of
              features leading to the critical concept of parity in the MC
              problem. Using chronometric measures, verbal protocols, and
              computer simulations, we explored first why it is difficult to
              find a representation for the Checkerboard problem, and then
              tested four potential sources of search constraint for reducing
              the difficulty: cue salience manipulations, prior knowledge,
              hints, and heuristics. While subjects used each of these four
              sources of constraint, a particular heuristic---noticing
              properties of the situation that remained invariant during
              solution attempts (the Notice Invariants heuristic)---proved to
              be a particularly powerful means for focusing search. In
              conjunction with hints and independently, it played a major part
              in producing the insight that yielded an effective problem
              representation and solution.",
  journal  = "Cogn. Psychol.",
  volume   =  22,
  number   =  3,
  pages    = "374--419",
  month    =  jul,
  year     =  1990,
  keywords = "comp-cog-sci;project 1",
  issn     = "0010-0285",
  doi      = "10.1016/0010-0285(90)90008-R"
}

@ARTICLE{Nosofsky2022-fh,
  title    = "Generalization in Distant Regions of a {Rule-Described} Category
              Space: a Mixed Exemplar and {Logical-Rule-Based} Account",
  author   = "Nosofsky, Robert M and Hu, Mingjia",
  abstract = "An important question in the cognitive-psychology of category
              learning concerns the manner in which observers generalize their
              trained category knowledge at time of transfer. In recent work,
              Conaway and Kurtz (Conaway and Kurtz, Psychonomic Bulletin \&
              Review 24:1312--1323, 2017) reported results from a novel
              paradigm in which participants learned rule-described categories
              defined over two dimensions and then classified test items in
              distant transfer regions of the category space. The paradigm
              yielded results that challenged the predictions from both
              exemplar-based and logical-rule-based models of categorization
              but that the authors suggested were as predicted by a divergent
              auto-encoder (DIVA) model (Kurtz, Psychonomic Bulletin \& Review
              14:560--576, 2007, Kurtz, Psychology of learning and motivation,
              Academic Press, New York, 2015). In this article, we pursue these
              challenges by conducting replications and extensions of the
              original experiment and fitting a variety of computational models
              to the resulting data. We find that even an extended version of
              the exemplar model that makes allowance for learning-during-test
              (LDT) processes fails to account for the results. In addition,
              models that presume a mixture of salient logical rules also fail
              to account for the findings. However, as a proof of concept, we
              illustrate that a model that assumes a mixture of strategies
              across subjects---some relying on exemplar-based memories with
              LDT, and others on salient logical rules---provides an
              outstanding account of the data. By comparison, DIVA performs
              considerably worse than does this LDT-exemplar-rule mixture
              account. These results converge with past ones reported in the
              literature that point to multiple forms of category
              representation as well as to the role of LDT processes in
              influencing how observers generalize their category knowledge.",
  journal  = "Computational Brain \& Behavior",
  volume   =  5,
  number   =  4,
  pages    = "435--466",
  month    =  dec,
  year     =  2022,
  keywords = "comp-cog-sci",
  issn     = "2522-087X",
  doi      = "10.1007/s42113-022-00151-4"
}

@ARTICLE{Ash2006-bc,
  title    = "The nature of restructuring in insight: an individual-differences
              approach",
  author   = "Ash, Ivan K and Wiley, Jennifer",
  abstract = "The insightful problem-solving process has been proposed to
              involve three main phases: an initial representation phase, in
              which the solver inappropriately represents the problem; an
              initial search through the faulty problem space that may lead to
              impasse; and a postimpasse restructuring phase. Some theories
              propose that the restructuring phase involves controlled search
              processes, whereas other theories propose that restructuring is
              achieved through the automatic redistribution of activation in
              long-term memory. In this study, we used correlations between
              working memory (WM) span measures and problem-solving success to
              test the predictions of these different theories. One group of
              participants received a set of insight problems that allowed for
              a large initial faulty search space, whereas another group
              received a matched set that constrained the initial faulty search
              space in order to isolate the restructuring phase of the
              insightful process. The results suggest that increased ability to
              control attention (as measured by WM span tasks) predicts an
              individual's ability to successfully solve problems that involve
              both the initial search phase and the restructuring phase.
              However, individual differences in ability to control attention
              do not predict success on problems that isolate the restructuring
              phase. These results are interpreted as supporting an
              automatic-redistribution-of-activation account of restructuring.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  13,
  number   =  1,
  pages    = "66--73",
  month    =  feb,
  year     =  2006,
  keywords = "project 1;cog-sci",
  language = "en",
  issn     = "1069-9384",
  pmid     = "16724770",
  doi      = "10.3758/bf03193814"
}

@ARTICLE{Jones2003-sh,
  title    = "Testing two cognitive theories of insight",
  author   = "Jones, Gary",
  abstract = "Insight in problem solving occurs when the problem solver fails
              to see how to solve a problem and then--``aha!''--there is a
              sudden realization how to solve it. Two contemporary theories
              have been proposed to explain insight. The representational
              change theory (e.g., G. Knoblich, S. Ohlsson, \& G. E. Rainey,
              2001) proposes that insight occurs through relaxing self-imposed
              constraints on a problem and by decomposing chunked items in the
              problem. The progress monitoring theory (e.g., J. N. MacGregor,
              T. C. Ormerod, \& E. P. Chronicle, 2001) proposes that insight is
              only sought once it becomes apparent that the distance to the
              goal is unachievable in the moves remaining. These 2 theories are
              tested in an unlimited move problem, to which neither theory has
              previously been applied. The results lend support to both, but
              experimental manipulations to the problem suggest that the
              representational change theory is the better indicator of
              performance. The findings suggest that testable opposing
              predictions can be made to examine theories of insight and that
              the use of eye movement data is a fruitful method of both
              examining insight and testing theories of insight.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  29,
  number   =  5,
  pages    = "1017--1027",
  month    =  sep,
  year     =  2003,
  keywords = "project 1;cog-sci",
  language = "en",
  issn     = "0278-7393",
  pmid     = "14516232",
  doi      = "10.1037/0278-7393.29.5.1017"
}

@ARTICLE{Ollinger2013-zu,
  title    = "Cognitive mechanisms of insight: the role of heuristics and
              representational change in solving the eight-coin problem",
  author   = "{\"O}llinger, Michael and Jones, Gary and Faber, Amory H and
              Knoblich, G{\"u}nther",
  abstract = "The 8-coin insight problem requires the problem solver to move 2
              coins so that each coin touches exactly 3 others. Ormerod,
              MacGregor, and Chronicle (2002) explained differences in task
              performance across different versions of the 8-coin problem using
              the availability of particular moves in a 2-dimensional search
              space. We explored 2 further explanations by developing 6 new
              versions of the 8-coin problem in order to investigate the
              influence of grouping and self-imposed constraints on solutions.
              The results identified 2 sources of problem difficulty: first,
              the necessity to overcome the constraint that a solution can be
              found in 2-dimensional space and, second, the necessity to
              decompose perceptual groupings. A detailed move analysis
              suggested that the selection of moves was driven by the
              established representation rather than the application of the
              appropriate heuristics. Both results support the assumptions of
              representational change theory (Ohlsson, 1992).",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  39,
  number   =  3,
  pages    = "931--939",
  month    =  may,
  year     =  2013,
  keywords = "project 1;cog-sci",
  language = "en",
  issn     = "0278-7393, 1939-1285",
  pmid     = "22799283",
  doi      = "10.1037/a0029194"
}

@ARTICLE{Kershaw2013-hb,
  title     = "Multiple paths to transfer and constraint relaxation in insight
               problem solving",
  author    = "Kershaw, Trina C and Flynn, Christopher K and Gordon, Leamarie T",
  abstract  = "In two experiments participants received various training
               methods designed to relax constraints present in the Four-Tree
               problem (deBono, 1967), a difficult insight problem. Geometry
               misconceptions were corrected via direct instruction.
               Participants? difficulty with developing three-dimensional
               representations was addressed via spontaneous analogical
               transfer (Experiment 1) or via cued analogical transfer
               (Experiment 2). We found that, while both training methods were
               effective, alleviating multiple constraints was more effective
               than the alleviation of single constraints via training
               programmes (c.f. Kershaw Nokes \& Ohlsson, 2005) and multiple
               constraints are discussed.",
  journal   = "Think. Reason.",
  publisher = "Routledge",
  volume    =  19,
  number    =  1,
  pages     = "96--136",
  month     =  feb,
  year      =  2013,
  keywords  = "cog-sci;project 1",
  issn      = "1354-6783",
  doi       = "10.1080/13546783.2012.742852"
}

@ARTICLE{Chu2007-xx,
  title    = "Theory Driven Hints in the Cheap Necklace Problem: A Preliminary
              Investigation",
  author   = "Chu, Yun and Dewald, Andrew D and Chronicle, Edward P",
  abstract = "Three experiments investigated the effects of two hints derived
              from the Criterion for Satisfactory Progress theory (CSP) and
              Representational Change Theory (RCT) on the cheap necklace
              problem (insight problem). In Experiment 1, fewer participants
              given the CSP hint used an incorrect (maximizing) first move than
              participants given the RCT hint or control participants given no
              hint on a single attempt at the problem. Experiment 2 found the
              number of trials to solution was fewer in the CSP condition than
              in the control over ten trials, and there were fewer incorrect
              first moves in the CSP. The results appear to support the CSP
              theory. However, in Experiment 3, the CSP and RCT hints were
              combined yielding a 75\% solution rate over 34.88\% in the
              control. Perhaps aspects from both theories are employed during
              the problem solving process.",
  journal  = "The Journal of Problem Solving",
  volume   =  1,
  number   =  2,
  pages    = "4",
  year     =  2007,
  keywords = "project 1",
  issn     = "1932-6246",
  doi      = "10.7771/1932-6246.1010"
}

@ARTICLE{Chater2013-hv,
  title    = "Programs as causal models: speculations on mental programs and
              mental representation",
  author   = "Chater, Nick and Oaksford, Mike",
  abstract = "Judea Pearl has argued that counterfactuals and causality are
              central to intelligence, whether natural or artificial, and has
              helped create a rich mathematical and computational framework for
              formally analyzing causality. Here, we draw out connections
              between these notions and various current issues in cognitive
              science, including the nature of mental ``programs'' and mental
              representation. We argue that programs (consisting of algorithms
              and data structures) have a causal (counterfactual-supporting)
              structure; these counterfactuals can reveal the nature of mental
              representations. Programs can also provide a causal model of the
              external world. Such models are, we suggest, ubiquitous in
              perception, cognition, and language processing.",
  journal  = "Cogn. Sci.",
  volume   =  37,
  number   =  6,
  pages    = "1171--1191",
  month    =  aug,
  year     =  2013,
  keywords = "comp-cog-sci;project 1;project 2",
  language = "en",
  issn     = "0364-0213, 1551-6709",
  pmid     = "23855554",
  doi      = "10.1111/cogs.12062"
}

@ARTICLE{Chang2016-qi,
  title         = "A Compositional {Object-Based} Approach to Learning Physical
                   Dynamics",
  author        = "Chang, Michael B and Ullman, Tomer and Torralba, Antonio and
                   Tenenbaum, Joshua B",
  abstract      = "We present the Neural Physics Engine (NPE), a framework for
                   learning simulators of intuitive physics that naturally
                   generalize across variable object count and different scene
                   configurations. We propose a factorization of a physical
                   scene into composable object-based representations and a
                   neural network architecture whose compositional structure
                   factorizes object dynamics into pairwise interactions. Like
                   a symbolic physics engine, the NPE is endowed with generic
                   notions of objects and their interactions; realized as a
                   neural network, it can be trained via stochastic gradient
                   descent to adapt to specific object properties and dynamics
                   of different worlds. We evaluate the efficacy of our
                   approach on simple rigid body dynamics in two-dimensional
                   worlds. By comparing to less structured architectures, we
                   show that the NPE's compositional representation of the
                   structure in physical interactions improves its ability to
                   predict movement, generalize across variable object count
                   and different scene configurations, and infer latent
                   properties of objects such as mass.",
  month         =  dec,
  year          =  2016,
  keywords      = "project 2",
  archivePrefix = "arXiv",
  eprint        = "1612.00341",
  primaryClass  = "cs.AI",
  arxivid       = "1612.00341"
}

@ARTICLE{Van_Schijndel2021-qq,
  title    = "{Single-Stage} Prediction Models Do Not Explain the Magnitude of
              Syntactic Disambiguation Difficulty",
  author   = "van Schijndel, Marten and Linzen, Tal",
  abstract = "The disambiguation of a syntactically ambiguous sentence in favor
              of a less preferred parse can lead to slower reading at the
              disambiguation point. This phenomenon, referred to as a
              garden-path effect, has motivated models in which readers
              initially maintain only a subset of the possible parses of the
              sentence, and subsequently require time-consuming reanalysis to
              reconstruct a discarded parse. A more recent proposal argues that
              the garden-path effect can be reduced to surprisal arising in a
              fully parallel parser: words consistent with the initially
              dispreferred but ultimately correct parse are simply less
              predictable than those consistent with the incorrect parse. Since
              predictability has pervasive effects in reading far beyond
              garden-path sentences, this account, which dispenses with
              reanalysis mechanisms, is more parsimonious. Crucially, it
              predicts a linear effect of surprisal: the garden-path effect is
              expected to be proportional to the difference in word surprisal
              between the ultimately correct and ultimately incorrect
              interpretations. To test this prediction, we used recurrent
              neural network language models to estimate word-by-word surprisal
              for three temporarily ambiguous constructions. We then estimated
              the slowdown attributed to each bit of surprisal from human
              self-paced reading times, and used that quantity to predict
              syntactic disambiguation difficulty. Surprisal successfully
              predicted the existence of garden-path effects, but drastically
              underpredicted their magnitude, and failed to predict their
              relative severity across constructions. We conclude that a full
              explanation of syntactic disambiguation difficulty may require
              recovery mechanisms beyond predictability.",
  journal  = "Cogn. Sci.",
  volume   =  45,
  number   =  6,
  pages    = "e12988",
  month    =  jun,
  year     =  2021,
  keywords = "Garden paths; Information theory; Neural networks; Self-paced
              reading; Surprisal;compling-cogsci2023",
  language = "en",
  issn     = "0364-0213, 1551-6709",
  pmid     = "34170031",
  doi      = "10.1111/cogs.12988"
}

@ARTICLE{Arehalli2022-xn,
  title         = "Syntactic Surprisal From Neural Models Predicts, But
                   Underestimates, Human Processing Difficulty From Syntactic
                   Ambiguities",
  author        = "Arehalli, Suhas and Dillon, Brian and Linzen, Tal",
  abstract      = "Humans exhibit garden path effects: When reading sentences
                   that are temporarily structurally ambiguous, they slow down
                   when the structure is disambiguated in favor of the less
                   preferred alternative. Surprisal theory (Hale, 2001; Levy,
                   2008), a prominent explanation of this finding, proposes
                   that these slowdowns are due to the unpredictability of each
                   of the words that occur in these sentences. Challenging this
                   hypothesis, van Schijndel \& Linzen (2021) find that
                   estimates of the cost of word predictability derived from
                   language models severely underestimate the magnitude of
                   human garden path effects. In this work, we consider whether
                   this underestimation is due to the fact that humans weight
                   syntactic factors in their predictions more highly than
                   language models do. We propose a method for estimating
                   syntactic predictability from a language model, allowing us
                   to weigh the cost of lexical and syntactic predictability
                   independently. We find that treating syntactic
                   predictability independently from lexical predictability
                   indeed results in larger estimates of garden path. At the
                   same time, even when syntactic predictability is
                   independently weighted, surprisal still greatly
                   underestimate the magnitude of human garden path effects.
                   Our results support the hypothesis that predictability is
                   not the only factor responsible for the processing cost
                   associated with garden path sentences.",
  month         =  oct,
  year          =  2022,
  keywords      = "compling-cogsci2023",
  archivePrefix = "arXiv",
  eprint        = "2210.12187",
  primaryClass  = "cs.CL",
  arxivid       = "2210.12187"
}

@ARTICLE{Almaatouq2022-xz,
  title    = "Beyond Playing 20 Questions with Nature: Integrative Experiment
              Design in the Social and Behavioral Sciences",
  author   = "Almaatouq, Abdullah and Griffiths, Thomas L and Suchow, Jordan W
              and Whiting, Mark E and Evans, James and Watts, Duncan J",
  abstract = "The dominant paradigm of experiments in the social and behavioral
              sciences views an experiment as a test of a theory, where the
              theory is assumed to generalize beyond the experiment's specific
              conditions. According to this view, which Alan Newell once
              characterized as ``playing twenty questions with nature,'' theory
              is advanced one experiment at a time, and the integration of
              disparate findings is assumed to happen via the scientific
              publishing process. In this article, we argue that the process of
              integration is at best inefficient, and at worst it does not, in
              fact, occur. We further show that the challenge of integration
              cannot be adequately addressed by recently proposed reforms that
              focus on the reliability and replicability of individual
              findings, nor simply by conducting more or larger experiments.
              Rather, the problem arises from the imprecise nature of social
              and behavioral theories and, consequently, a lack of
              commensurability across experiments conducted under different
              conditions. Therefore, researchers must fundamentally rethink how
              they design experiments and how the experiments relate to theory.
              We specifically describe an alternative framework, integrative
              experiment design, which intrinsically promotes commensurability
              and continuous integration of knowledge. In this paradigm,
              researchers explicitly map the design space of possible
              experiments associated with a given research question, embracing
              many potentially relevant theories rather than focusing on just
              one. The researchers then iteratively generate theories and test
              them with experiments explicitly sampled from the design space,
              allowing results to be integrated across experiments. Given
              recent methodological and technological developments, we conclude
              that this approach is feasible and would generate more-reliable,
              more-cumulative empirical and theoretical knowledge than the
              current paradigm-and with far greater efficiency.",
  journal  = "Behav. Brain Sci.",
  pages    = "1--55",
  month    =  dec,
  year     =  2022,
  keywords = "(in)commensurability; cumulative knowledge; experiments;
              generalizability",
  language = "en",
  issn     = "0140-525X, 1469-1825",
  pmid     = "36539303",
  doi      = "10.1017/S0140525X22002874"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Eichenbaum2008-pi,
  title        = "Learning \& memory",
  author       = "Eichenbaum, H",
  abstract     = "â€¦ Next, Eichenbaum incorporates animal and human â€¦ Eichenbaum
                  organizes the text around multiple memory systems, moving
                  from simple to more complex forms of learning and memory â€¦",
  publisher    = "aabmc.org",
  year         =  2008,
  howpublished = "\url{https://aabmc.org/sites/default/files/webform/stories-photos/pdf-learning--memory-howard-eichenbaum-pdf-download-free-book-3e8075a.pdf}",
  note         = "Accessed: 2023-1-30",
  keywords     = "learning\&memory2023"
}

@ARTICLE{Ormerod2002-tb,
  title    = "Dynamics and constraints in insight problem solving",
  author   = "Ormerod, Thomas C and MacGregor, James N and Chronicle, Edward P",
  abstract = "This article reports 2 experiments that investigated performance
              on a novel insight problem, the 8-coin problem. The authors
              hypothesized that participants would make certain initial moves
              (strategic moves) that seemed to make progress according to the
              problem instructions but that nonetheless would guarantee failure
              to solve the problem. Experiment 1 manipulated the starting state
              of the problem and showed that overall solution rates were lower
              when such strategic moves were available. Experiment 2 showed
              that failure to capitalize on visual hints about the correct
              first move was also associated with the availability of strategic
              moves. The results are interpreted in terms of an
              information-processing framework previously applied to the 9-dot
              problem. The authors argue that in addition to the operation of
              inappropriate constraints, a full account of insight problem
              solving must incorporate a dynamic that steers solution-seeking
              activity toward the constraints.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  28,
  number   =  4,
  pages    = "791--799",
  month    =  jul,
  year     =  2002,
  keywords = "cog-sci;project 1",
  language = "en",
  issn     = "0278-7393",
  pmid     = "12109769",
  doi      = "10.1037//0278-7393.28.4.791"
}

@ARTICLE{Allen2020-tf,
  title    = "Rapid {Trial-and-Error} Learning with Simulation Supports
              Flexible Tool Use and Physical Reasoning",
  author   = "Allen, Kelsey R and Smith, Kevin A and Tenenbaum, Joshua B",
  abstract = "Many animals, and an increasing number of artificial agents,
              display sophisticated capabilities to perceive and manipulate
              objects. But human beings remain distinctive in their capacity
              for flexible, creative tool use---using objects in new ways to
              act on the world, achieve a goal, or solve a problem. To study
              this type of general physical problem solving, we introduce the
              Virtual Tools game. In this game, people solve a large range of
              challenging physical puzzles in just a handful of attempts. We
              propose that the flexibility of human physical problem solving
              rests on an ability to imagine the effects of hypothesized
              actions, while the efficiency of human search arises from rich
              action priors which are updated via observations of the world. We
              instantiate these components in the ``sample, simulate, update''
              (SSUP) model and show that it captures human performance across
              30 levels of the Virtual Tools game. More broadly, this model
              provides a mechanism for explaining how people condense general
              physical knowledge into actionable, task-specific plans to
              achieve flexible and efficient physical problem solving.",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  117,
  number   =  47,
  pages    = "29302--29310",
  month    =  nov,
  year     =  2020,
  keywords = "read;comp-cog-sci;project 2",
  issn     = "0027-8424, 1091-6490",
  doi      = "10.1073/pnas.1912341117"
}

@MISC{Andreas2017-kq,
  title         = "Learning with {{Latent Language}}",
  author        = "Andreas, Jacob and Klein, Dan and Levine, Sergey",
  abstract      = "The named concepts and compositional operators present in
                   natural language provide a rich source of information about
                   the kinds of abstractions humans use to navigate the world.
                   Can this linguistic background knowledge improve the
                   generality and efficiency of learned classifiers and control
                   policies? This paper aims to show that using the space of
                   natural language strings as a parameter space is an
                   effective way to capture natural task structure. In a
                   pretraining phase, we learn a language interpretation model
                   that transforms inputs (e.g. images) into outputs (e.g.
                   labels) given natural language descriptions. To learn a new
                   concept (e.g. a classifier), we search directly in the space
                   of descriptions to minimize the interpreter's loss on
                   training examples. Crucially, our models do not require
                   language data to learn these concepts: language is used only
                   in pretraining to impose structure on subsequent learning.
                   Results on image classification, text editing, and
                   reinforcement learning show that, in all settings, models
                   with a linguistic parameterization outperform those without.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:1711.00482",
  month         =  nov,
  year          =  2017,
  keywords      = "Computer Science - Computation and Language,Computer Science
                   - Neural and Evolutionary Computing;read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "1711.00482",
  primaryClass  = "cs",
  arxivid       = "1711.00482"
}

@UNPUBLISHED{Andrychowicz2016-uq,
  title    = "Learning to Learn by Gradient Descent by Gradient Descent",
  author   = "Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and
              Hoffman, Matthew W and Pfau, David and Schaul, Tom and
              Shillingford, Brendan and de Freitas, Nando",
  abstract = "The move from hand-designed features to learned features in
              machine learning has been wildly successful. In spite of this,
              optimization algorithms are still designed by hand. In this paper
              we show how the design of an optimization algorithm can be cast
              as a learning problem, allowing the algorithm to learn to exploit
              structure in the problems of interest in an automatic way. Our
              learned algorithms, implemented by LSTMs, outperform generic,
              hand-designed competitors on the tasks for which they are
              trained, and also generalize well to new tasks with similar
              structure. We demonstrate this on a number of tasks, including
              simple convex problems, training neural networks, and styling
              images with neural art.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Machine Learning,Computer Science - Neural and
              Evolutionary Computing;comp-cog-sci;machine-learning"
}

@INCOLLECTION{Ashby2011-au,
  title     = "{{COVIS}}",
  booktitle = "Formal {{Approaches}} in {{Categorization}}",
  author    = "Ashby, F Gregory and Paul, Erick J and Maddox, W Todd",
  editor    = "Pothos, Emmanuel M and Wills, Andy J",
  abstract  = "The COVIS model of category learning assumes separate rule-based
               and procedural-learning categorization systems that compete for
               access to response production. The rule-based system selects and
               tests simple verbalizable hypotheses about category membership.
               The procedurallearning system gradually associates
               categorization responses with regions of perceptual space via
               reinforcement learning.",
  publisher = "Cambridge University Press",
  pages     = "65--87",
  year      =  2011,
  address   = "Cambridge",
  keywords  = "comp-cog-sci",
  isbn      = "9780511921322",
  doi       = "10.1017/CBO9780511921322.004"
}

@ARTICLE{Bengio2013-nf,
  title    = "Representation {{Learning}}: {{A Review}} and {{New
              Perspectives}}",
  author   = "Bengio, Y and Courville, A and Vincent, P",
  abstract = "The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used,
              and the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, autoencoders, manifold learning, and deep networks. This
              motivates longer term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical
              connections between representation learning, density estimation,
              and manifold learning.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  35,
  number   =  8,
  pages    = "1798--1828",
  month    =  aug,
  year     =  2013,
  keywords = "Abstracts,autoencoder,Boltzmann machine,Deep learning,Feature
              extraction,feature learning,Learning systems,Machine
              learning,Manifolds,neural nets,Neural networks,representation
              learning,Speech recognition,unsupervised learning;comp-cog-sci",
  issn     = "0162-8828, 2160-9292",
  doi      = "10.1109/TPAMI.2013.50"
}

@ARTICLE{Bonner2018-ii,
  title    = "Computational Mechanisms Underlying Cortical Responses to the
              Affordance Properties of Visual Scenes",
  author   = "Bonner, Michael F and Epstein, Russell A",
  editor   = "Einh{\"a}user, Wolfgang",
  abstract = "Biologically inspired deep convolutional neural networks (CNNs),
              trained for computer vision tasks, have been found to predict
              cortical responses with remarkable accuracy. However, the
              internal operations of these models remain poorly understood, and
              the factors that account for their success are unknown. Here we
              develop a set of techniques for using CNNs to gain insights into
              the computational mechanisms underlying cortical responses. We
              focused on responses in the occipital place area (OPA), a
              scene-selective region of dorsal occipitoparietal cortex. In a
              previous study, we showed that fMRI activation patterns in the
              OPA contain information about the navigational affordances of
              scenes; that is, information about where one can and cannot move
              within the immediate environment. We hypothesized that this
              affordance information could be extracted using a set of purely
              feedforward computations. To test this idea, we examined a deep
              CNN with a feedforward architecture that had been previously
              trained for scene classification. We found that responses in the
              CNN to scene images were highly predictive of fMRI responses in
              the OPA. Moreover the CNN accounted for the portion of OPA
              variance relating to the navigational affordances of scenes. The
              CNN could thus serve as an image-computable candidate model of
              affordance-related responses in the OPA. We then ran a series of
              in silico experiments on this model to gain insights into its
              internal operations. These analyses showed that the computation
              of affordance-related features relied heavily on visual
              information at high-spatial frequencies and cardinal
              orientations, both of which have previously been identified as
              lowlevel stimulus preferences of scene-selective visual cortex.
              These computations also exhibited a strong preference for
              information in the lower visual field, which is consistent with
              known retinotopic biases in the OPA. Visualizations of feature
              selectivity within the CNN suggested that affordance-based
              responses encoded features that define the layout of the spatial
              environment, such as boundary-defining junctions and large
              extended surfaces. Together, these results map the sensory
              functions of the OPA onto a fully quantitative model that
              provides insights into its visual computations. More broadly,
              they advance integrative techniques for understanding visual
              cortex across multiple level of analysis: from the identification
              of cortical sensory functions to the modeling of their underlying
              algorithms.",
  journal  = "PLoS Comput. Biol.",
  volume   =  14,
  number   =  4,
  pages    = "e1006111",
  month    =  apr,
  year     =  2018,
  keywords = "comp-cog-sci",
  issn     = "1553-734X, 1553-7358",
  doi      = "10.1371/journal.pcbi.1006111"
}

@ARTICLE{Bonner2021-zd,
  title    = "Object Representations in the Human Brain Reflect the
              {Co-Occurrence} Statistics of Vision and Language",
  author   = "Bonner, Michael F and Epstein, Russell A",
  abstract = "Abstract A central regularity of visual perception is the
              co-occurrence of objects in the natural environment. Here we use
              machine learning and fMRI to test the hypothesis that object
              co-occurrence statistics are encoded in the human visual system
              and elicited by the perception of individual objects. We
              identified low-dimensional representations that capture the
              latent statistical structure of object co-occurrence in
              real-world scenes, and we mapped these statistical
              representations onto voxel-wise fMRI responses during object
              viewing. We found that cortical responses to single objects were
              predicted by the statistical ensembles in which they typically
              occur, and that this link between objects and their visual
              contexts was made most strongly in parahippocampal cortex,
              overlapping with the anterior portion of scene-selective
              parahippocampal place area. In contrast, a language-based
              statistical model of the co-occurrence of object names in written
              text predicted responses in neighboring regions of
              object-selective visual cortex. Together, these findings show
              that the sensory coding of objects in the human brain reflects
              the latent statistics of object context in visual and linguistic
              experience.",
  journal  = "Nat. Commun.",
  volume   =  12,
  number   =  1,
  pages    = "4081",
  month    =  dec,
  year     =  2021,
  keywords = "comp-cog-sci",
  issn     = "2041-1723",
  doi      = "10.1038/s41467-021-24368-2"
}

@ARTICLE{Botvinick2017-xr,
  title    = "Building Machines That Learn and Think for Themselves",
  author   = "Botvinick, Matthew and Barrett, David G T and Battaglia, Peter
              and de Freitas, Nando and Kumaran, Darshan and Leibo, Joel Z and
              Lillicrap, Timothy and Modayil, Joseph and Mohamed, Shakir and
              Rabinowitz, Neil C and Rezende, Danilo J and Santoro, Adam and
              Schaul, Tom and Summerfield, Christopher and Wayne, Greg and
              Weber, Theophane and Wierstra, Daan and Legg, Shane and Hassabis,
              Demis",
  abstract = "We agree with Lake and colleagues on their list of `key
              ingredients' for building humanlike intelligence, including the
              idea that model-based reasoning is essential. However, we favor
              an approach that centers on one additional ingredient: autonomy.
              In particular, we aim toward agents that can both build and
              exploit their own internal models, with minimal human
              hand-engineering. We believe an approach centered on autonomous
              learning has the greatest chance of success as we scale toward
              real-world complexity, tackling domains for which ready-made
              formal models are not available. Here we survey several important
              examples of the progress that has been made toward building
              autonomous agents with humanlike abilities, and highlight some
              outstanding challenges.",
  journal  = "Behav. Brain Sci.",
  volume   =  40,
  pages    = "e255",
  year     =  2017,
  keywords = "comp-cog-sci",
  issn     = "0140-525X, 1469-1825",
  doi      = "10.1017/S0140525X17000048"
}

@ARTICLE{Bourlard1988-if,
  title    = "{Auto-Association} by Multilayer Perceptrons and Singular Value
              Decomposition",
  author   = "Bourlard, H and Kamp, Y",
  abstract = "The multilayer perceptron, when working in auto-association mode,
              is sometimes considered as an interesting candidate to perform
              data compression or dimensionality reduction of the feature space
              in information processing applications. The present paper shows
              that, for auto-association, the nonlinearities of the hidden
              units are useless and that the optimal parameter values can be
              derived directly by purely linear techniques relying on singular
              value decomposition and low rank matrix approximation, similar in
              spirit to the well-known Karhunen-Lo6ve transform. This approach
              appears thus as an efficient alternative to the general error
              back-propagation algorithm commonly used for training multilayer
              perceptrons. Moreover, it also gives a clear interpretation of
              the r61e of the different parameters.",
  journal  = "Biol. Cybern.",
  volume   =  59,
  number   = "4-5",
  pages    = "291--294",
  month    =  sep,
  year     =  1988,
  keywords = "comp-cog-sci",
  issn     = "0340-1200, 1432-0770",
  doi      = "10.1007/BF00332918"
}

@ARTICLE{Carey2004-ya,
  title    = "Bootstrapping \& the Origin of Concepts",
  author   = "Carey, Susan",
  journal  = "Daedalus",
  volume   =  133,
  number   =  1,
  pages    = "59--68",
  month    =  jan,
  year     =  2004,
  keywords = "comp-cog-sci",
  issn     = "0011-5266, 1548-6192",
  doi      = "10.1162/001152604772746701"
}

@ARTICLE{Carey1978-gm,
  title    = "Acquiring a {{Single New Word}}",
  author   = "Carey, Susan and Bartlett, Elsa",
  volume   =  15,
  pages    = "14",
  year     =  1978,
  keywords = "comp-cog-sci"
}

@ARTICLE{Cassimatis_undated-lu,
  title    = "Artificial {{Intelligence}} and {{Cognitive Science Have}} the
              {{Same Problem}}",
  author   = "Cassimatis, Nicholas L",
  abstract = "Cognitive scientists attempting to explain human intelligence
              share a puzzle with artificial intelligence researchers aiming to
              create computers that exhibit humanlevel intelligence: how can a
              system composed of relatively unintelligent parts (such as
              neurons or transistors) behave intelligently? I argue that
              although cognitive science has made significant progress towards
              many of its goals, that solving the puzzle of intelligence
              requires special standards and methods in addition to those
              already employed in cognitive science. To promote such research,
              I suggest creating a subfield within cognitive science called
              intelligence science and propose some guidelines for research
              addressing the intelligence puzzle.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@MISC{Chollet2019-di,
  title         = "On the {{Measure}} of {{Intelligence}}",
  author        = "Chollet, Fran{\c c}ois",
  abstract      = "To make deliberate progress towards more intelligent and
                   more human-like artificial systems, we need to be following
                   an appropriate feedback signal: we need to be able to define
                   and evaluate intelligence in a way that enables comparisons
                   between two systems, as well as comparisons with humans.
                   Over the past hundred years, there has been an abundance of
                   attempts to define and measure intelligence, across both the
                   fields of psychology and AI. We summarize and critically
                   assess these definitions and evaluation approaches, while
                   making apparent the two historical conceptions of
                   intelligence that have implicitly guided them. We note that
                   in practice, the contemporary AI community still gravitates
                   towards benchmarking intelligence by comparing the skill
                   exhibited by AIs and humans at specific tasks, such as board
                   games and video games. We argue that solely measuring skill
                   at any given task falls short of measuring intelligence,
                   because skill is heavily modulated by prior knowledge and
                   experience: unlimited priors or unlimited training data
                   allow experimenters to ``buy'' arbitrary levels of skills
                   for a system, in a way that masks the system's own
                   generalization power. We then articulate a new formal
                   definition of intelligence based on Algorithmic Information
                   Theory, describing intelligence as skill-acquisition
                   efficiency and highlighting the concepts of scope,
                   generalization difficulty, priors, and experience, as
                   critical pieces to be accounted for in characterizing
                   intelligent systems. Using this definition, we propose a set
                   of guidelines for what a general AI benchmark should look
                   like. Finally, we present a new benchmark closely following
                   these guidelines, the Abstraction and Reasoning Corpus
                   (ARC), built upon an explicit set of priors designed to be
                   as close as possible to innate human priors. We argue that
                   ARC can be used to measure a human-like form of general
                   fluid intelligence and that it enables fair general
                   intelligence comparisons between AI systems and humans.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:1911.01547",
  month         =  nov,
  year          =  2019,
  keywords      = "Computer Science - Artificial Intelligence;read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "1911.01547",
  primaryClass  = "cs",
  arxivid       = "1911.01547"
}

@ARTICLE{Chomsky1956-aj,
  title    = "Three Models for the Description of Language",
  author   = "Chomsky, N",
  abstract = "We investigate several conceptions of linguistic structure to
              determine whether or not they can provide simple and
              ``revealing'' grammars that generate all of the sentences of
              English and only these. We find that no finite-state Markov
              process that produces symbols with transition from state to state
              can serve as an English grammar. Furthermore, the particular
              subclass of such processes that producen-order statistical
              approximations to English do not come closer, with increasingn,
              to matching the output of an English grammar. We formalize-the
              notions of ``phrase structure'' and show that this gives us a
              method for describing language which is essentially more
              powerful, though still representable as a rather elementary type
              of finite-state process. Nevertheless, it is successful only when
              limited to a small subset of simple sentences. We study the
              formal properties of a set of grammatical transformations that
              carry sentences with phrase structure into new sentences with
              derived phrase structure, showing that transformational grammars
              are processes of the same elementary type as phrase-structure
              grammars; that the grammar of English is materially simplified if
              phrase structure description is limited to a kernel of simple
              sentences from which all other sentences are constructed by
              repeated transformations; and that this view of linguistic
              structure gives a certain insight into the use and understanding
              of language.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "113--124",
  month    =  sep,
  year     =  1956,
  keywords = "Impedance matching,Kernel,Laboratories,Markov processes,Natural
              languages,Research and development,Testing;comp-cog-sci",
  issn     = "2168-2712",
  doi      = "10.1109/TIT.1956.1056813"
}

@ARTICLE{Choromanska_undated-tg,
  title    = "The {{Loss Surfaces}} of {{Multilayer Networks}}",
  author   = "Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and
              Arous, Gerard Ben and LeCun, Yann",
  pages    = "13",
  keywords = "comp-cog-sci"
}

@UNPUBLISHED{Darwiche2017-sr,
  title    = "{Human-{{Level} Intelligence}} or {{{Animal-Like} Abilities}}?",
  author   = "Darwiche, Adnan",
  abstract = "The vision systems of the eagle and the snake outperform
              everything that we can make in the laboratory, but snakes and
              eagles cannot build an eyeglass or a telescope or a microscope.
              (Judea Pearl)",
  month    =  jul,
  year     =  2017,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computers and Society,Computer Science - Machine
              Learning,Statistics - Machine Learning;comp-cog-sci"
}

@MISC{De_Raedt2020-hk,
  title         = "From {{Statistical Relational}} to {{{Neuro-Symbolic}
                   Artificial Intelligence}}",
  author        = "De Raedt, Luc and Duman{\v c}i{\'c}, Sebastijan and
                   Manhaeve, Robin and Marra, Giuseppe",
  abstract      = "Neuro-symbolic and statistical relational artificial
                   intelligence both integrate frameworks for learning with
                   logical reasoning. This survey identifies several parallels
                   across seven different dimensions between these two fields.
                   These cannot only be used to characterize and position
                   neuro-symbolic artificial intelligence approaches but also
                   to identify a number of directions for further research.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2003.08316",
  month         =  mar,
  year          =  2020,
  keywords      = "Computer Science - Artificial Intelligence;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2003.08316",
  primaryClass  = "cs",
  arxivid       = "2003.08316",
  doi           = "10.48550/arXiv.2003.08316"
}

@ARTICLE{Dekker2022-lm,
  title     = "Determinants of Human Compositional Generalization",
  author    = "Dekker, Ronald Boris and Otto, Fabian and Summerfield,
               Christopher",
  abstract  = "Generalisation (or transfer) is the ability to repurpose
               knowledge in novel settings. It is often asserted that
               generalisation is an important ingredient of human intelligence,
               but its extent, nature and determinants have proved
               controversial. Here, we re-examine this question with a new
               paradigm that formalises the transfer learning problem as one of
               recomposing existing functions to solve unseen problems. We find
               that people can generalise compositionally in ways that are
               elusive for standard neural networks, and that human
               generalisation benefits from training regimes in which items are
               axis-aligned and temporally correlated. We describe a neural
               network model based around a Hebbian gating process which can
               capture how human generalisation benefits from different
               training curricula. We additionally find that adult humans tend
               to learn composable functions asynchronously, exhibiting
               discontinuities in learning that resemble those seen in child
               development.",
  publisher = "PsyArXiv",
  month     =  mar,
  year      =  2022,
  keywords  = "Cognitive Psychology,Computational Neuroscience,Concepts and
               Categories,generalisation,Learning,neural
               network,Neuroscience,Social and Behavioral Sciences;comp-cog-sci",
  doi       = "10.31234/osf.io/qnpw6"
}

@ARTICLE{Dwivedi2021-zk,
  title     = "Unveiling Functions of the Visual Cortex Using {Task-Specific}
               Deep Neural Networks",
  author    = "Dwivedi, Kshitij and Bonner, Michael F and Cichy, Radoslaw
               Martin and Roig, Gemma",
  abstract  = "The human visual cortex enables visual perception through a
               cascade of hierarchical computations in cortical regions with
               distinct functionalities. Here, we introduce an AI-driven
               approach to discover the functional mapping of the visual
               cortex. We related human brain responses to scene images
               measured with functional MRI (fMRI) systematically to a diverse
               set of deep neural networks (DNNs) optimized to perform
               different scene perception tasks. We found a structured mapping
               between DNN tasks and brain regions along the ventral and dorsal
               visual streams. Low-level visual tasks mapped onto early brain
               regions, 3-dimensional scene perception tasks mapped onto the
               dorsal stream, and semantic tasks mapped onto the ventral
               stream. This mapping was of high fidelity, with more than 60\%
               of the explainable variance in nine key regions being explained.
               Together, our results provide a novel functional mapping of the
               human visual cortex and demonstrate the power of the
               computational approach.",
  journal   = "PLoS Comput. Biol.",
  publisher = "Public Library of Science",
  volume    =  17,
  number    =  8,
  pages     = "e1009267",
  month     =  aug,
  year      =  2021,
  keywords  = "Functional magnetic resonance imaging,Linear regression
               analysis,Neural networks,Permutation,Semantics,Sensory
               perception,Vision,Visual cortex;comp-cog-sci",
  issn      = "1553-734X, 1553-7358",
  doi       = "10.1371/journal.pcbi.1009267"
}

@MISC{Ellis2020-xn,
  title         = "{{{DreamCoder}}}: {{Growing}} Generalizable, Interpretable
                   Knowledge with {Wake-Sleep} {{Bayesian}} Program Learning",
  author        = "Ellis, Kevin and Wong, Catherine and Nye, Maxwell and
                   Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and
                   Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua
                   B",
  abstract      = "Expert problem-solving is driven by powerful languages for
                   thinking about problems and their solutions. Acquiring
                   expertise means learning these languages -- systems of
                   concepts, alongside the skills to use them. We present
                   DreamCoder, a system that learns to solve problems by
                   writing programs. It builds expertise by creating
                   programming languages for expressing domain concepts,
                   together with neural networks to guide the search for
                   programs within these languages. A ``wake-sleep'' learning
                   algorithm alternately extends the language with new symbolic
                   abstractions and trains the neural network on imagined and
                   replayed problems. DreamCoder solves both classic inductive
                   programming tasks and creative tasks such as drawing
                   pictures and building scenes. It rediscovers the basics of
                   modern functional programming, vector algebra and classical
                   physics, including Newton's and Coulomb's laws. Concepts are
                   built compositionally from those learned earlier, yielding
                   multi-layered symbolic representations that are
                   interpretable and transferrable to new tasks, while still
                   growing scalably and flexibly with experience.",
  journal       = "arXiv [cs]",
  publisher     = "arXiv",
  number        = "arXiv:2006.08381",
  month         =  jun,
  year          =  2020,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning;read;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2006.08381",
  primaryClass  = "cs",
  arxivid       = "2006.08381"
}

@BOOK{Epstein2009-te,
  title     = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  publisher = "Springer Netherlands",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "comp-cog-sci",
  isbn      = "9781402096242, 9781402067105",
  doi       = "10.1007/978-1-4020-6710-5"
}

@ARTICLE{Erickson2011-on,
  title    = "Exercise Training Increases Size of Hippocampus and Improves
              Memory",
  author   = "Erickson, K I and Voss, M W and Prakash, R S and Basak, C and
              Szabo, A and Chaddock, L and Kim, J S and Heo, S and Alves, H and
              White, S M and Wojcicki, T R and Mailey, E and Vieira, V J and
              Martin, S A and Pence, B D and Woods, J A and McAuley, E and
              Kramer, A F",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  108,
  number   =  7,
  pages    = "3017--3022",
  month    =  feb,
  year     =  2011,
  keywords = "exercise-brain",
  issn     = "0027-8424, 1091-6490",
  doi      = "10.1073/pnas.1015950108"
}

@UNPUBLISHED{Feinman2021-mh,
  title    = "Learning {{{Task-General} Representations}} with {{Generative
              {Neuro-Symbolic} Modeling}}",
  author   = "Feinman, Reuben and Lake, Brenden M",
  abstract = "People can learn rich, general-purpose conceptual representations
              from only raw perceptual inputs. Current machine learning
              approaches fall well short of these human standards, although
              different modeling traditions often have complementary strengths.
              Symbolic models can capture the compositional and causal
              knowledge that enables flexible generalization, but they struggle
              to learn from raw inputs, relying on strong abstractions and
              simplifying assumptions. Neural network models can learn directly
              from raw data, but they struggle to capture compositional and
              causal structure and typically must retrain to tackle new tasks.
              We bring together these two traditions to learn generative models
              of concepts that capture rich compositional and causal structure,
              while learning from raw data. We develop a generative
              neuro-symbolic (GNS) model of handwritten character concepts that
              uses the control flow of a probabilistic program, coupled with
              symbolic stroke primitives and a symbolic image renderer, to
              represent the causal and compositional processes by which
              characters are formed. The distributions of parts (strokes), and
              correlations between parts, are modeled with neural network
              subroutines, allowing the model to learn directly from raw data
              and express nonparametric statistical relationships. We apply our
              model to the Omniglot challenge of human-level concept learning,
              using a background set of alphabets to learn an expressive prior
              distribution over character drawings. In a subsequent evaluation,
              our GNS model uses probabilistic inference to learn rich
              conceptual representations from a single training image that
              generalize to 4 unique tasks, succeeding where previous work has
              fallen short.",
  month    =  jan,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Fodor1988-gq,
  title    = "Connectionism and Cognitive Architecture: {{A}} Critical Analysis",
  author   = "Fodor, Jerry A and Pylyshyn, Zenon W",
  abstract = "This paper explores differences between Connectionist proposals
              for cognitive architecture and the sorts of models that have
              traditionally been assumed in cognitive science. We claim that
              the major distinction is that, while both Connectionist and
              Classical architectures postulate representational mental states,
              the latter but not the former are committed to a symbol-level of
              representation, or to a `language of thought': i.e., to
              representational states that have combinatorial syntactic and
              semantic structure. Several arguments for combinatorial structure
              in mental representations are then reviewed. These include
              arguments based on the `systematicity' of mental representation:
              i.e., on the fact that cognitive capacities always exhibit
              certain symmetries, so that the ability to entertain a given
              thought implies the ability to entertain thoughts with
              semantically related contents. We claim that such arguments make
              a powerful case that mind/brain architecture is not Connectionist
              at the cognitive level. We then consider the possibility that
              Connectionism may provide an account of the neural (or `abstract
              neurological') structures in which Classical cognitive
              architecture is implemented. We survey a number of the standard
              arguments that have been offered in favor of Connectionism, and
              conclude that they are coherent only on this interpretation.
              R{\'e}sum{\'e} Cet article{\'e}tudie les diff{\'e}rences entre
              mod{\`e}les connectionistes et mod{\`e}les classiques de la
              structure cognitive. Nous pensons que, bien que les deux types de
              mod{\`e}les stipulent l'existence d'{\'e}tats mentaux
              repr{\'e}sentationnels, la diff{\'e}rence essentielle est que
              seuls les mod{\`e}les classiques requi{\`e}rent l'existence d'un
              niveau de repr{\'e}sentation symbolique---un ``langage de la
              pens{\'e}e''---, c'est-{\`a}-dire d'{\'e}tats
              repr{\'e}sentationnels poss{\'e}dant une structure syntaxique et
              s{\'e}mantique. Nous examinons ensuite diff{\'e}rents arguments
              qui militent en faveur de l'existence de repr{\'e}sentations
              mentales ayant ces propri{\'e}t{\'e}s. Certains de ces arguments
              reposent sur la ``syst{\'e}maticit{\'e}'' des repr{\'e}sentations
              mentales, c'est-{\`a}-dire sur le fait que les capacit{\'e}s
              cognitives exhibent toujours certaines sym{\'e}tries, de sorte
              que la capacit{\'e}d'entretenir certaines pens{\'e}es implique la
              capacit{\'e}d'entretenir d'autres pens{\'e}es apparent{\'e}es par
              leur contenu s{\'e}mantique. Nous pensons que ces arguments
              montrent de mani{\`e}re convainquante que l'architecture de
              l'esprit/du cerveau n'est pas connectioniste au niveau cognitif.
              Nous nous demandons ensuite s'il est possible d'interpr{\'e}ter
              le connectionisme comme une analyse des structures neuronales (ou
              des structures neurologiques ``abstraites'') dans lesquelles est
              r{\'e}alis{\'e}e l'architecture cognitive classique. Nous
              examinons plusieurs des arguments avanc{\'e}s habituellement en
              d{\'e}fense du connectionisme, et en concluons que ceux-ci n'ont
              de sens que dans cette interpr{\'e}tation.",
  journal  = "Cognition",
  volume   =  28,
  number   =  1,
  pages    = "3--71",
  year     =  1988,
  keywords = "comp-cog-sci",
  issn     = "0010-0277",
  doi      = "10.1016/0010-0277(88)90031-5"
}

@ARTICLE{Fontana2010-ie,
  title    = "Extending {{Healthy Life {Span--From} Yeast}} to {{Humans}}",
  author   = "Fontana, L and Partridge, L and Longo, V D",
  journal  = "Science",
  volume   =  328,
  number   =  5976,
  pages    = "321--326",
  month    =  apr,
  year     =  2010,
  keywords = "longevity",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.1172539"
}

@INPROCEEDINGS{Gal2016-il,
  title     = "Dropout as a {{Bayesian Approximation}}: {{Representing Model
               Uncertainty}} in {{Deep Learning}}",
  booktitle = "Proceedings of The 33rd International Conference on Machine
               Learning",
  author    = "Gal, Yarin and Ghahramani, Zoubin",
  editor    = "Balcan, Maria Florina and Weinberger, Kilian Q",
  abstract  = "Deep learning tools have gained tremendous attention in applied
               machine learning. However such tools for regression and
               classification do not capture model uncertainty. In comparison,
               Bayesian models offer a mathematically grounded framework to
               reason about model uncertainty, but usually come with a
               prohibitive computational cost. In this paper we develop a new
               theoretical framework casting dropout training in deep neural
               networks (NNs) as approximate Bayesian inference in deep
               Gaussian processes. A direct result of this theory gives us
               tools to model uncertainty with dropout NNs -- extracting
               information from existing models that has been thrown away so
               far. This mitigates the problem of representing uncertainty in
               deep learning without sacrificing either computational
               complexity or test accuracy. We perform an extensive study of
               the properties of dropout's uncertainty. Various network
               architectures and non-linearities are assessed on tasks of
               regression and classification, using MNIST as an example. We
               show a considerable improvement in predictive log-likelihood and
               RMSE compared to existing state-of-the-art methods, and finish
               by using dropout's uncertainty in deep reinforcement learning.",
  publisher = "PMLR",
  volume    =  48,
  pages     = "1050--1059",
  series    = "Proceedings of Machine Learning Research",
  year      =  2016,
  address   = "New York, New York, USA",
  keywords  = "machine-learning"
}

@ARTICLE{Gandhi2021-hw,
  title         = "Baby {{Intuitions Benchmark}} ({{{BIB}})}: {{Discerning}}
                   the Goals, Preferences, and Actions of Others",
  author        = "Gandhi, Kanishk and Stojnic, Gala and Lake, Brenden M and
                   Dillon, Moira R",
  journal       = "CoRR",
  volume        = "abs/2102.11938",
  year          =  2021,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning;comp-cog-sci",
  archivePrefix = "arXiv",
  eprint        = "2102.11938",
  arxivid       = "2102.11938"
}

@ARTICLE{Gershman2015-pa,
  title    = "Computational Rationality: {{A}} Converging Paradigm for
              Intelligence in Brains, Minds, and Machines",
  author   = "Gershman, Samuel J and Horvitz, Eric J and Tenenbaum, Joshua B",
  abstract = "After growing up together, and mostly growing apart in the second
              half of the 20th century, the fields of artificial intelligence
              (AI), cognitive science, and neuroscience are reconverging on a
              shared view of the computational foundations of intelligence that
              promotes valuable cross-disciplinary exchanges on questions,
              methods, and results. We chart advances over the past several
              decades that address challenges of perception and action under
              uncertainty through the lens of computation. Advances include the
              development of representations and inferential procedures for
              large-scale probabilistic inference and machinery for enabling
              reflection and decisions about tradeoffs in effort, precision,
              and timeliness of computations. These tools are deployed toward
              the goal of computational rationality: identifying decisions with
              highest expected utility, while taking into consideration the
              costs of computation in complex real-world problems in which most
              relevant calculations can only be approximated. We highlight key
              concepts with examples that show the potential for interchange
              between computer science, cognitive science, and neuroscience.",
  journal  = "Science",
  volume   =  349,
  number   =  6245,
  pages    = "273--278",
  month    =  jul,
  year     =  2015,
  keywords = "comp-cog-sci",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.aac6076"
}

@ARTICLE{Ghahramani2015-ko,
  title     = "Probabilistic Machine Learning and Artificial Intelligence",
  author    = "Ghahramani, Zoubin",
  abstract  = "How can a machine learn from experience? Probabilistic modelling
               provides a framework for understanding what learning is, and has
               therefore emerged as one of the principal theoretical and
               practical approaches for designing machines that learn from data
               acquired through experience. The probabilistic framework, which
               describes how to represent and manipulate uncertainty about
               models and predictions, has a central role in scientific data
               analysis, machine learning, robotics, cognitive science and
               artificial intelligence. This Review provides an introduction to
               this framework, and discusses some of the state-of-the-art
               advances in the field, namely, probabilistic programming,
               Bayesian optimization, data compression and automatic model
               discovery.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "452--459",
  month     =  may,
  year      =  2015,
  keywords  = "comp-cog-sci",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14541"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Girshick_undated-ee,
  title    = "Rich {{Feature Hierarchies}} for {{Accurate Object Detection}}
              and {{Semantic Segmentation}}",
  author   = "Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik,
              Jitendra",
  abstract = "Object detection performance, as measured on the canonical PASCAL
              VOC dataset, has plateaued in the last few years. The
              best-performing methods are complex ensemble systems that
              typically combine multiple low-level image features with
              high-level context. In this paper, we propose a simple and
              scalable detection algorithm that improves mean average precision
              (mAP) by more than 30\% relative to the previous best result on
              VOC 2012---achieving a mAP of 53.3\%. Our approach combines two
              key insights: (1) one can apply high-capacity convolutional
              neural networks (CNNs) to bottom-up region proposals in order to
              localize and segment objects and (2) when labeled training data
              is scarce, supervised pre-training for an auxiliary task,
              followed by domain-specific fine-tuning, yields a significant
              performance boost. Since we combine region proposals with CNNs,
              we call our method R-CNN: Regions with CNN features. We also
              present experiments that provide insight into what the network
              learns, revealing a rich hierarchy of image features. Source code
              for the complete system is available at
              http://www.cs.berkeley.edu/ Ëœrbg/rcnn.",
  pages    = "8",
  keywords = "comp-cog-sci"
}

@ARTICLE{Goodman2008-ee,
  title    = "A {{Rational Analysis}} of {{{Rule-Based} Concept Learning}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Feldman, Jacob and
              Griffiths, Thomas L",
  abstract = "This article proposes a new model of human concept learning that
              provides a rational analysis of learning feature-based concepts.
              This model is built upon Bayesian inference for a grammatically
              structured hypothesis space---a concept language of logical
              rules. This article compares the model predictions to human
              generalization judgments in several well-known category learning
              experiments, and finds good agreement for both average and
              individual participant generalizations. This article further
              investigates judgments for a broad set of 7-feature concepts---a
              more natural setting in several ways---and again finds that the
              model explains human performance.",
  journal  = "Cogn. Sci.",
  volume   =  32,
  number   =  1,
  pages    = "108--154",
  month    =  jan,
  year     =  2008,
  keywords = "comp-cog-sci",
  issn     = "0364-0213",
  doi      = "10.1080/03640210701802071"
}

@ARTICLE{Goodman2014-kq,
  title    = "Concepts in a {{Probabilistic Language}} of {{Thought}}",
  author   = "Goodman, Noah D and Tenenbaum, Joshua B and Gerstenberg, Tobias",
  pages    = "25",
  year     =  2014,
  keywords = "read;comp-cog-sci"
}

@UNPUBLISHED{Goyal2020-od,
  title    = "Recurrent {{Independent Mechanisms}}",
  author   = "Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani,
              Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf,
              Bernhard",
  abstract = "Learning modular structures which reflect the dynamics of the
              environment can lead to better generalization and robustness to
              changes which only affect a few of the underlying causes. We
              propose Recurrent Independent Mechanisms (RIMs), a new recurrent
              architecture in which multiple groups of recurrent cells operate
              with nearly independent transition dynamics, communicate only
              sparingly through the bottleneck of attention, and are only
              updated at time steps where they are most relevant. We show that
              this leads to specialization amongst the RIMs, which in turn
              allows for dramatically improved generalization on tasks where
              some factors of variation differ systematically between training
              and evaluation.",
  month    =  nov,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;comp-cog-sci"
}

@MISC{Goyal2021-ki,
  title         = "Inductive {{Biases}} for {{Deep Learning}} of
                   {{{Higher-Level} Cognition}}",
  author        = "Goyal, Anirudh and Bengio, Yoshua",
  abstract      = "A fascinating hypothesis is that human and animal
                   intelligence could be explained by a few principles (rather
                   than an encyclopedic list of heuristics). If that hypothesis
                   was correct, we could more easily both understand our own
                   intelligence and build intelligent machines. Just like in
                   physics, the principles themselves would not be sufficient
                   to predict the behavior of complex systems like brains, and
                   substantial computation might be needed to simulate
                   human-like intelligence. This hypothesis would suggest that
                   studying the kind of inductive biases that humans and
                   animals exploit could help both clarify these principles and
                   provide inspiration for AI research and neuroscience
                   theories. Deep learning already exploits several key
                   inductive biases, and this work considers a larger list,
                   focusing on those which concern mostly higher-level and
                   sequential conscious processing. The objective of clarifying
                   these particular principles is that they could potentially
                   help us build AI systems benefiting from humans' abilities
                   in terms of flexible out-of-distribution and systematic
                   generalization, which is currently an area where a large gap
                   exists between state-of-the-art machine learning and human
                   intelligence.",
  journal       = "arXiv [cs, stat]",
  publisher     = "arXiv",
  number        = "arXiv:2011.15091",
  month         =  feb,
  year          =  2021,
  keywords      = "Computer Science - Artificial Intelligence,Computer Science
                   - Machine Learning,Statistics - Machine
                   Learning;comp-cog-sci;machine-learning",
  archivePrefix = "arXiv",
  eprint        = "2011.15091",
  primaryClass  = "cs, stat",
  arxivid       = "2011.15091"
}

@UNPUBLISHED{Greff2020-gr,
  title    = "On the {{Binding Problem}} in {{Artificial Neural Networks}}",
  author   = "Greff, Klaus and van Steenkiste, Sjoerd and Schmidhuber,
              J{\"u}rgen",
  abstract = "Contemporary neural networks still fall short of human-level
              generalization, which extends far beyond our direct experiences.
              In this paper, we argue that the underlying cause for this
              shortcoming is their inability to dynamically and flexibly bind
              information that is distributed throughout the network. This
              binding problem affects their capacity to acquire a compositional
              understanding of the world in terms of symbol-like entities (like
              objects), which is crucial for generalizing in predictable and
              systematic ways. To address this issue, we propose a unifying
              framework that revolves around forming meaningful entities from
              unstructured sensory inputs (segregation), maintaining this
              separation of information at a representational level
              (representation), and using these entities to construct new
              inferences, predictions, and behaviors (composition). Our
              analysis draws inspiration from a wealth of research in
              neuroscience and cognitive psychology, and surveys relevant
              mechanisms from the machine learning literature, to help identify
              a combination of inductive biases that allow symbolic information
              processing to emerge naturally in neural networks. We believe
              that a compositional approach to AI, in terms of grounded
              symbol-like representations, is of fundamental importance for
              realizing human-level generalization, and we hope that this paper
              may contribute towards that goal as a reference and inspiration.",
  month    =  dec,
  year     =  2020,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,I.2.6;comp-cog-sci"
}

@ARTICLE{Griffiths2010-wc,
  title    = "Probabilistic Models of Cognition: Exploring Representations and
              Inductive Biases",
  author   = "Griffiths, Thomas L and Chater, Nick and Kemp, Charles and
              Perfors, Amy and Tenenbaum, Joshua B",
  journal  = "Trends Cogn. Sci.",
  volume   =  14,
  number   =  8,
  pages    = "357--364",
  month    =  aug,
  year     =  2010,
  keywords = "comp-cog-sci",
  issn     = "1364-6613",
  doi      = "10.1016/j.tics.2010.05.004"
}

@UNPUBLISHED{Grosse2012-pf,
  title    = "Exploiting Compositionality to Explore a Large Space of Model
              Structures",
  author   = "Grosse, Roger and Salakhutdinov, Ruslan R and Freeman, William T
              and Tenenbaum, Joshua B",
  abstract = "The recent proliferation of richly structured probabilistic
              models raises the question of how to automatically determine an
              appropriate model for a dataset. We investigate this question for
              a space of matrix decomposition models which can express a
              variety of widely used models from unsupervised learning. To
              enable model selection, we organize these models into a
              context-free grammar which generates a wide variety of structures
              through the compositional application of a few simple rules. We
              use our grammar to generically and efficiently infer latent
              components and estimate predictive likelihood for nearly 2500
              structures using a small toolbox of reusable algorithms. Using a
              greedy search over our grammar, we automatically choose the
              decomposition structure from raw data by evaluating only a small
              fraction of all models. The proposed method typically finds the
              correct structure for synthetic data and backs off gracefully to
              simpler models under heavy noise. It learns sensible structures
              for datasets as diverse as image patches, motion capture, 20
              Questions, and U.S. Senate votes, all using exactly the same
              code.",
  month    =  oct,
  year     =  2012,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@ARTICLE{Gureckis2012-xz,
  title    = "{Self-{{Directed} Learning}}: {{A Cognitive}} and {{Computational
              Perspective}}",
  author   = "Gureckis, Todd M and Markant, Douglas B",
  abstract = "A widely advocated idea in education is that people learn better
              when the flow of experience is under their control (i.e.,
              learning is self-directed). However, the reasons why volitional
              control might result in superior acquisition and the limits to
              such advantages remain poorly understood. In this article, we
              review the issue from both a cognitive and computational
              perspective. On the cognitive side, self-directed learning allows
              individuals to focus effort on useful information they do not yet
              possess, can expose information that is inaccessible via passive
              observation, and may enhance the encoding and retention of
              materials. On the computational side, the development of
              efficient ``active learning'' algorithms that can select their
              own training data is an emerging research topic in machine
              learning. This review argues that recent advances in these
              related fields may offer a fresh theoretical perspective on how
              people gather information to support their own learning.",
  journal  = "Perspect. Psychol. Sci.",
  volume   =  7,
  number   =  5,
  pages    = "464--481",
  month    =  sep,
  year     =  2012,
  keywords = "active learning,intervention-based causal learning,machine
              learning,self-directed learning,self-regulated
              study;comp-cog-sci;project 1",
  issn     = "1745-6916, 1745-6924",
  doi      = "10.1177/1745691612454304"
}

@INCOLLECTION{Hauser2004-go,
  title     = "Evolutionary and Developmental Foundations of Human Knowledge",
  booktitle = "The {{Cognitive Neurosciences Iii}}",
  author    = "Hauser, Marc D and Spelke, Elizabeth",
  editor    = "Gazzaniga, Michael S",
  publisher = "MIT Press",
  year      =  2004,
  keywords  = "comp-cog-sci"
}

@ARTICLE{Hawkins2019-jn,
  title    = "A {{Framework}} for {{Intelligence}} and {{Cortical Function
              Based}} on {{Grid Cells}} in the {{Neocortex}}",
  author   = "Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy,
              Scott and Ahmad, Subutai",
  journal  = "Front. Neural Circuits",
  volume   =  12,
  pages    = "121",
  month    =  jan,
  year     =  2019,
  keywords = "comp-cog-sci",
  issn     = "1662-5110",
  doi      = "10.3389/fncir.2018.00121"
}

@INPROCEEDINGS{He2016-if,
  title      = "Deep {{Residual Learning}} for {{Image Recognition}}",
  booktitle  = "2016 {{{IEEE} Conference}} on {{Computer Vision}} and {{Pattern
                Recognition}} ({{{CVPR}}})",
  author     = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
  abstract   = "Deeper neural networks are more difficult to train. We present
                a residual learning framework to ease the training of networks
                that are substantially deeper than those used previously. We
                explicitly reformulate the layers as learning residual
                functions with reference to the layer inputs, instead of
                learning unreferenced functions. We provide comprehensive
                empirical evidence showing that these residual networks are
                easier to optimize, and can gain accuracy from considerably
                increased depth. On the ImageNet dataset we evaluate residual
                nets with a depth of up to 152 layers---8$\times$ deeper than
                VGG nets [40] but still having lower complexity. An ensemble of
                these residual nets achieves 3.57\% error on the ImageNet test
                set. This result won the 1st place on the ILSVRC 2015
                classification task. We also present analysis on CIFAR-10 with
                100 and 1000 layers.",
  publisher  = "IEEE",
  pages      = "770--778",
  month      =  jun,
  year       =  2016,
  address    = "Las Vegas, NV, USA",
  keywords   = "machine-learning",
  conference = "2016 IEEE Conference on Computer Vision and Pattern Recognition
                (CVPR)",
  isbn       = "9781467388511",
  doi        = "10.1109/CVPR.2016.90"
}

@ARTICLE{Hill2019-uj,
  title    = "{{{BDNF}}}, Endurance Activity, and Mechanisms Underlying the
              Evolution of Hominin Brains",
  author   = "Hill, Tyler and Polk, John D",
  abstract = "Objectives As a complex, polygenic trait, brain size has likely
              been influenced by a range of direct and indirect selection
              pressures for both cognitive and non-cognitive functions and
              capabilities. It has been hypothesized that hominin brain
              expansion was, in part, a correlated response to selection acting
              on aerobic capacity (Raichlen \& Polk, 2013). According to this
              hypothesis, selection for aerobic capacity increased the activity
              of various signaling molecules, including those involved in brain
              growth. One key molecule is brain-derived neurotrophic factor
              (BDNF), a protein that regulates neuronal development, survival,
              and plasticity in mammals. This review updates, partially tests,
              and expands Raichlen and Polk's (2013) hypothesis by evaluating
              evidence for BDNF as a mediator of brain size. Discussion We
              contend that selection for endurance capabilities in a hot
              climate favored changes to muscle composition, mitochondrial
              dynamics and increased energy budget through pathways involving
              regulation of PGC-1$\alpha$ and MEF2 genes, both of which promote
              BDNF activity. In addition, the evolution of hairlessness and the
              skin's thermoregulatory response provide other molecular pathways
              that promote both BDNF activity and neurotransmitter synthesis.
              We discuss how these pathways contributed to the evolution of
              brain size and function in human evolution and propose avenues
              for future research. Our results support Raichlen and Polk's
              contention that selection for non-cognitive functions has direct
              mechanistic linkages to the evolution of brain size in hominins.",
  journal  = "Am. J. Phys. Anthropol.",
  volume   =  168,
  number   = "S67",
  pages    = "47--62",
  year     =  2019,
  keywords = "BDNF,brain
              growth,exercise,MEF2,neurotrophins,PGC-1$\alpha$,thermoregulation;exercise-brain",
  issn     = "0002-9483, 1096-8644",
  doi      = "10.1002/ajpa.23762"
}

@ARTICLE{Hung2018-fi,
  title     = "Effect of {{Acute Exercise Mode}} on {{Serum {Brain-Derived}
               Neurotrophic Factor}} ({{{BDNF}}}) and {{Task Switching
               Performance}}",
  author    = "Hung, Chiao-Ling and Tseng, Jun-Wei and Chao, Hsiao-Han and
               Hung, Tsung-Min and Wang, Ho-Seng",
  abstract  = "Previous studies have consistently reported a positive effect of
               acute exercise on cognition, particularly on executive function.
               However, most studies have focused on aerobic and resistant
               forms of exercise. The purpose of this study was to compare the
               effect of `open-skill' with `closed-skill' exercise (defined in
               terms of the predictability of the performing environment) on
               brain-derived neurotrophic factor (BDNF) production and task
               switching performance. Twenty young adult males participated in
               both closed (running) and open (badminton) skill exercise
               sessions in a counterbalanced order on separate days. The
               exercise sessions consisted of 5 min of warm up exercises
               followed by 30 min of running or badminton. The exercise
               intensity was set at 60\% ($\pm$5\%) of the heart rate reserve
               level (HRR) with HR being monitored by a wireless heart rate
               monitor. Blood samples were taken and participation in a
               task-switching paradigm occurred before and after each exercise
               session. Results showed no differences in serum BDNF or
               task-switching performance at the pre-test stage, however,
               badminton exercise resulted in significantly higher serum BDNF
               levels (a proxy for levels of BDNF in the brain) and near
               significant smaller global switching costs relative to running.
               This study has provided preliminary evidence in support the
               relative benefits of open-skills exercises on BDNF and executive
               function.",
  journal   = "J. Clin. Med. Res.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  10,
  pages     = "301",
  month     =  oct,
  year      =  2018,
  keywords  = "closed-skill,executive function,open-skill,switch
               cost;exercise-brain",
  issn      = "1918-3003",
  doi       = "10.3390/jcm7100301"
}

@ARTICLE{Japkowicz2000-wi,
  title    = "Nonlinear {{Autoassociation Is Not Equivalent}} to {{PCA}}",
  author   = "Japkowicz, Nathalie and Hanson, Stephen Jos{\'e} and Gluck, Mark
              A",
  abstract = "A common misperception within the neural network community is
              that even with nonlinearities in their hidden layer,
              autoassociators trained with backpropagation are equivalent to
              linear methods such as principal component analysis (PCA). Our
              purpose is to demonstrate that nonlinear autoassociators actually
              behave differently from linear methods and that they can
              outperform these methods when used for latent extraction,
              projection, and classification. While linear autoassociators
              emulate PCA, and thus exhibit a flat or unimodal reconstruction
              error surface, autoassociators with nonlinearities in their
              hidden layer learn domains by building error reconstruction
              surfaces that, depending on the task, contain multiple local
              valleys. This interpolation bias allows nonlinear autoassociators
              to represent appropriate classifications of nonlinear multimodal
              domains, in contrast to linear autoassociators, which are
              inappropriate for such tasks. In fact, autoassociators with
              hidden unit nonlinearities can be shown to perform nonlinear
              classification and nonlinear recognition.",
  journal  = "Neural Comput.",
  volume   =  12,
  number   =  3,
  pages    = "531--545",
  month    =  mar,
  year     =  2000,
  keywords = "comp-cog-sci",
  issn     = "0899-7667",
  doi      = "10.1162/089976600300015691"
}

@UNPUBLISHED{Johnson2021-mw,
  title    = "Fast and Flexible: {{Human}} Program Induction in Abstract
              Reasoning Tasks",
  author   = "Johnson, Aysja and Vong, Wai Keen and Lake, Brenden M and
              Gureckis, Todd M",
  abstract = "The Abstraction and Reasoning Corpus (ARC) is a challenging
              program induction dataset that was recently proposed by Chollet
              (2019). Here, we report the first set of results collected from a
              behavioral study of humans solving a subset of tasks from ARC (40
              out of 1000). Although this subset of tasks contains considerable
              variation, our results showed that humans were able to infer the
              underlying program and generate the correct test output for a
              novel test input example, with an average of 80\% of tasks solved
              per participant, and with 65\% of tasks being solved by more than
              80\% of participants. Additionally, we find interesting patterns
              of behavioral consistency and variability within the action
              sequences during the generation process, the natural language
              descriptions to describe the transformations for each task, and
              the errors people made. Our findings suggest that people can
              quickly and reliably determine the relevant features and
              properties of a task to compose a correct solution. Future
              modeling work could incorporate these findings, potentially by
              connecting the natural language descriptions we collected here to
              the underlying semantics of ARC.",
  month    =  mar,
  year     =  2021,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Human-Computer Interaction,Computer Science - Machine
              Learning;read;comp-cog-sci"
}

@ARTICLE{Kemp2009-oz,
  title    = "Structured Statistical Models of Inductive Reasoning",
  author   = "Kemp, Charles and Tenenbaum, Joshua B",
  abstract = "Everyday inductive inferences are often guided by rich background
              knowledge. Formal models of induction should aim to incorporate
              this knowledge and should explain how different kinds of
              knowledge lead to the distinctive patterns of reasoning found in
              different inductive contexts. This article presents a Bayesian
              framework that attempts to meet both goals and describe 4
              applications of the framework: a taxonomic model, a spatial
              model, a threshold model, and a causal model. Each model makes
              probabilistic inferences about the extensions of novel
              properties, but the priors for the 4 models are defined over
              different kinds of structures that capture different
              relationships between the categories in a domain. The framework
              therefore shows how statistical inference can operate over
              structured background knowledge, and the authors argue that this
              interaction between structure and statistics is critical for
              explaining the power and flexibility of human reasoning.",
  journal  = "Psychol. Rev.",
  volume   =  116,
  number   =  1,
  pages    = "20--58",
  year     =  2009,
  keywords = "comp-cog-sci",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/a0014282"
}

@ARTICLE{Kruschke1993-eg,
  title    = "Human {{Category Learning}}: {{Implications}} for
              {{Backpropagation Models}}",
  author   = "Kruschke, John K",
  abstract = "Backpropagation (Rumelhart et al., 1986a) was proposed as a
              general learning algorithm for multi-layer perceptrons. This a n
              d e demonstrates chat a standard version of backprop fails to
              attend selectively to input dimensions in the same way as humans,
              suffers catastrophic forgetting of previously learned
              associations when novel exemplars are [rained, and can be overly
              sensitive to linear categoy boundaries. Another connecrionist
              model, A L C O V E (Krwchke 1990, 1992), does nor suffer those
              failures. Previous researchers identified these problems; the
              present article repons quantitative fits of the models to new
              human learning data. A L C O V E can be functionally approximated
              by a network that uses linear-sigmoid hidden nodes, like standard
              backprop. Ir is argued that models of human category learning
              should incorporate quasi-local representations and dimensional
              artention learning, as well as error-driuen learning, to address
              simulraneously all three phenomena.",
  journal  = "Conn. Sci.",
  volume   =  5,
  number   =  1,
  pages    = "3--36",
  month    =  jan,
  year     =  1993,
  keywords = "comp-cog-sci",
  issn     = "0954-0091, 1360-0494",
  doi      = "10.1080/09540099308915683"
}

@ARTICLE{Lake2015-ap,
  title    = "{Human-Level} Concept Learning through Probabilistic Program
              Induction",
  author   = "Lake, B M and Salakhutdinov, R and Tenenbaum, J B",
  journal  = "Science",
  volume   =  350,
  number   =  6266,
  pages    = "1332--1338",
  month    =  dec,
  year     =  2015,
  keywords = "comp-cog-sci",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.aab3050"
}

@UNPUBLISHED{Lake2016-zm,
  title    = "Building {{Machines That Learn}} and {{Think Like People}}",
  author   = "Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and
              Gershman, Samuel J",
  abstract = "Recent progress in artificial intelligence (AI) has renewed
              interest in building systems that learn and think like people.
              Many advances have come from using deep neural networks trained
              end-to-end in tasks such as object recognition, video games, and
              board games, achieving performance that equals or even beats
              humans in some respects. Despite their biological inspiration and
              performance achievements, these systems differ from human
              intelligence in crucial ways. We review progress in cognitive
              science suggesting that truly human-like learning and thinking
              machines will have to reach beyond current engineering trends in
              both what they learn, and how they learn it. Specifically, we
              argue that these machines should (a) build causal models of the
              world that support explanation and understanding, rather than
              merely solving pattern recognition problems; (b) ground learning
              in intuitive theories of physics and psychology, to support and
              enrich the knowledge that is learned; and (c) harness
              compositionality and learning-to-learn to rapidly acquire and
              generalize knowledge to new tasks and situations. We suggest
              concrete challenges and promising routes towards these goals that
              can combine the strengths of recent neural network advances with
              more structured cognitive models.",
  month    =  nov,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science -
              Machine Learning,Computer Science - Neural and Evolutionary
              Computing,Statistics - Machine Learning;comp-cog-sci"
}

@ARTICLE{Lake2018-gk,
  title    = "The {{Emergence}} of {{Organizing Structure}} in {{Conceptual
              Representation}}",
  author   = "Lake, Brenden M and Lawrence, Neil D and Tenenbaum, Joshua B",
  abstract = "Both scientists and children make important structural
              discoveries, yet their computational underpinnings are not well
              understood. Structure discovery has previously been formalized as
              probabilistic inference about the right structural form---where
              form could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum,
              2008). Although this approach can learn intuitive organizations,
              including a tree for animals and a ring for the color circle, it
              assumes a strong inductive bias that considers only these
              particular forms, and each form is explicitly provided as initial
              knowledge. Here we introduce a new computational model of how
              organizing structure can be discovered, utilizing a broad
              hypothesis space with a preference for sparse connectivity. Given
              that the inductive bias is more general, the model's initial
              knowledge shows little qualitative resemblance to some of the
              discoveries it supports. As a consequence, the model can also
              learn complex structures for domains that lack intuitive
              description, as well as predict human property induction
              judgments without explicit structural forms. By allowing form to
              emerge from sparsity, our approach clarifies how both the
              richness and flexibility of human conceptual organization can
              coexist.",
  journal  = "Cogn. Sci.",
  volume   =  42,
  number   = "S3",
  pages    = "809--832",
  year     =  2018,
  keywords = "Bayesian modeling,Sparsity,Structure discovery,Unsupervised
              learning;comp-cog-sci",
  issn     = "0364-0213, 1551-6709",
  doi      = "10.1111/cogs.12580"
}

@UNPUBLISHED{Lake2019-dt,
  title    = "Compositional Generalization through Meta {Sequence-to-Sequence}
              Learning",
  author   = "Lake, Brenden M",
  abstract = "People can learn a new concept and use it compositionally,
              understanding how to ``blicket twice'' after learning how to
              ``blicket.'' In contrast, powerful sequence-tosequence (seq2seq)
              neural networks fail such tests of compositionality, especially
              when composing new concepts together with existing concepts. In
              this paper, I show how memory-augmented neural networks can be
              trained to generalize compositionally through meta seq2seq
              learning. In this approach, models train on a series of seq2seq
              problems to acquire the compositional skills needed to solve new
              seq2seq problems. Meta se2seq learning solves several of the SCAN
              tests for compositional learning and can learn to apply implicit
              rules to variables.",
  month    =  oct,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Machine
              Learning;comp-cog-sci"
}

@UNPUBLISHED{Lake2019-ng,
  title    = "Human {Few-Shot} Learning of Compositional Instructions",
  author   = "Lake, Brenden M and Linzen, Tal and Baroni, Marco",
  abstract = "People learn in fast and flexible ways that have not been
              emulated by machines. Once a person learns a new verb ``dax,'' he
              or she can effortlessly understand how to ``dax twice,'' ``walk
              and dax,'' or ``dax vigorously.'' There have been striking recent
              improvements in machine learning for natural language processing,
              yet the best algorithms require vast amounts of experience and
              struggle to generalize new concepts in compositional ways. To
              better understand these distinctively human abilities, we study
              the compositional skills of people through languagelike
              instruction learning tasks. Our results show that people can
              learn and use novel functional concepts from very few examples
              (few-shot learning), successfully applying familiar functions to
              novel inputs. People can also compose concepts in complex ways
              that go beyond the provided demonstrations. Two additional
              experiments examined the assumptions and inductive biases that
              people make when solving these tasks, revealing three biases:
              mutual exclusivity, one-to-one mappings, and iconic
              concatenation. We discuss the implications for cognitive modeling
              and the potential for building machines with more human-like
              language learning capabilities.",
  month    =  may,
  year     =  2019,
  keywords = "Computer Science - Computation and Language;comp-cog-sci"
}

@ARTICLE{Lake2019-zq,
  title    = "The {{Omniglot}} Challenge: A 3-Year Progress Report",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "97--104",
  month    =  oct,
  year     =  2019,
  keywords = "comp-cog-sci",
  issn     = "2352-1546",
  doi      = "10.1016/j.cobeha.2019.04.007"
}

@ARTICLE{Lake_undated-bl,
  title    = "Concept Learning as Motor Program Induction: {{A}} {Large-Scale}
              Empirical Study",
  author   = "Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B",
  abstract = "Human concept learning is particularly impressive in two
              respects: the internal structure of concepts can be
              representationally rich, and yet the very same concepts can also
              be learned from just a few examples. Several decades of research
              have dramatically advanced our understanding of these two aspects
              of concepts. While the richness and speed of concept learning are
              most often studied in isolation, the power of human concepts may
              be best explained through their synthesis. This paper presents a
              large-scale empirical study of one-shot concept learning,
              suggesting that rich generative knowledge in the form of a motor
              program can be induced from just a single example of a novel
              concept. Participants were asked to draw novel handwritten
              characters given a reference form, and we recorded the motor data
              used for production. Multiple drawers of the same character not
              only produced visually similar drawings, but they also showed a
              striking correspondence in their strokes, as measured by their
              number, shape, order, and direction. This suggests that
              participants can infer a rich motorbased concept from a single
              example. We also show that the motor programs induced by
              individual subjects provide a powerful basis for one-shot
              classification, yielding far higher accuracy than
              state-of-the-art pattern recognition methods based on just the
              visual form.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Lake2021-tz,
  title     = "Word Meaning in Minds and Machines",
  author    = "Lake, Brenden M and Murphy, Gregory L",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  year      =  2021,
  keywords  = "comp-cog-sci",
  issn      = "0033-295X, 1939-1471",
  doi       = "10.1037/rev0000297"
}

@ARTICLE{Lake2020-wr,
  title    = "People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few
              Examples}}",
  author   = "Lake, Brenden M and Piantadosi, Steven T",
  abstract = "Machine learning has made major advances in categorizing objects
              in images, yet the best algorithms miss important aspects of how
              people learn and think about categories. People can learn richer
              concepts from fewer examples, including causal models that
              explain how members of a category are formed. Here, we explore
              the limits of this human ability to infer causal
              ``programs''---latent generating processes with nontrivial
              algorithmic properties---from one, two, or three visual examples.
              People were asked to extrapolate the programs in several ways,
              for both classifying and generating new examples. As a theory of
              these inductive abilities, we present a Bayesian program learning
              model that searches the space of programs for the best
              explanation of the observations. Although variable, people's
              judgments are broadly consistent with the model and inconsistent
              with several alternatives, including a pretrained deep neural
              network for object recognition, indicating that people can learn
              and reason with rich algorithmic abstractions from sparse input
              data.",
  journal  = "Computational Brain \& Behavior",
  volume   =  3,
  number   =  1,
  pages    = "54--65",
  month    =  mar,
  year     =  2020,
  keywords = "comp-cog-sci",
  issn     = "2522-0861, 2522-087X",
  doi      = "10.1007/s42113-019-00053-y"
}

@ARTICLE{Landau1988-oy,
  title    = "The Importance of Shape in Early Lexical Learning",
  author   = "Landau, Barbara and Smith, Linda B and Jones, Susan S",
  journal  = "Cogn. Dev.",
  volume   =  3,
  number   =  3,
  pages    = "299--321",
  month    =  jul,
  year     =  1988,
  keywords = "development",
  issn     = "0885-2014",
  doi      = "10.1016/0885-2014(88)90014-7"
}

@ARTICLE{LeCun2015-hm,
  title     = "Deep Learning",
  author    = "LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey",
  abstract  = "Deep learning allows computational models that are composed of
               multiple processing layers to learn representations of data with
               multiple levels of abstraction. These methods have dramatically
               improved the state-of-the-art in speech recognition, visual
               object recognition, object detection and many other domains such
               as drug discovery and genomics. Deep learning discovers
               intricate structure in large data sets by using the
               backpropagation algorithm to indicate how a machine should
               change its internal parameters that are used to compute the
               representation in each layer from the representation in the
               previous layer. Deep convolutional nets have brought about
               breakthroughs in processing images, video, speech and audio,
               whereas recurrent nets have shone light on sequential data such
               as text and speech.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  521,
  number    =  7553,
  pages     = "436--444",
  month     =  may,
  year      =  2015,
  keywords  = "machine-learning;ccm2023",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14539"
}

@INPROCEEDINGS{Lerer2016-kb,
  title      = "Learning {{Physical Intuition}} of {{Block Towers}} by
                {{Example}}",
  booktitle  = "International {{Conference}} on {{Machine Learning}}",
  author     = "Lerer, Adam and Gross, Sam and Fergus, Rob",
  publisher  = "PMLR",
  pages      = "430--438",
  month      =  jun,
  year       =  2016,
  keywords   = "comp-cog-sci",
  conference = "International Conference on Machine Learning"
}

@ARTICLE{Levine2014-bg,
  title    = "Low {{Protein Intake Is Associated}} with a {{Major Reduction}}
              in {{{IGF-1}}}, {{Cancer}}, and {{Overall Mortality}} in the 65
              and {{Younger}} but {{Not Older Population}}",
  author   = "Levine, Morgan E and Suarez, Jorge A and Brandhorst, Sebastian
              and Balasubramanian, Priya and Cheng, Chia-Wei and Madia,
              Federica and Fontana, Luigi and Mirisola, Mario G and
              Guevara-Aguirre, Jaime and Wan, Junxiang and Passarino, Giuseppe
              and Kennedy, Brian K and Wei, Min and Cohen, Pinchas and
              Crimmins, Eileen M and Longo, Valter D",
  abstract = "Mice and humans with growth hormone receptor/ IGF-1 deficiencies
              display major reductions in agerelated diseases. Because protein
              restriction reduces GHR-IGF-1 activity, we examined links between
              protein intake and mortality. Respondents aged 50--65 reporting
              high protein intake had a 75\% increase in overall mortality and
              a 4-fold increase in cancer death risk during the following 18
              years. These associations were either abolished or attenuated if
              the proteins were plant derived. Conversely, high protein intake
              was associated with reduced cancer and overall mortality in
              respondents over 65, but a 5-fold increase in diabetes mortality
              across all ages. Mouse studies confirmed the effect of high
              protein intake and GHR-IGF-1 signaling on the incidence and
              progression of breast and melanoma tumors, but also the
              detrimental effects of a low protein diet in the very old. These
              results suggest that low protein intake during middle age
              followed by moderate to high protein consumption in old adults
              may optimize healthspan and longevity.",
  journal  = "Cell Metab.",
  volume   =  19,
  number   =  3,
  pages    = "407--417",
  month    =  mar,
  year     =  2014,
  keywords = "longevity",
  issn     = "1550-4131",
  doi      = "10.1016/j.cmet.2014.02.006"
}

@ARTICLE{Lloyd2014-ua,
  title    = "Automatic {{Construction}} and {{{Natural-Language} Description}}
              of {{Nonparametric Regression Models}}",
  author   = "Lloyd, James and Duvenaud, David and Grosse, Roger and Tenenbaum,
              Joshua and Ghahramani, Zoubin",
  abstract = "This paper presents the beginnings of an automatic statistician,
              focusing on regression problems. Our system explores an
              open-ended space of statistical models to discover a good
              explanation of a data set, and then produces a detailed report
              with figures and natural-language text. Our approach treats
              unknown regression functions nonparametrically using Gaussian
              processes, which has two important consequences. First, Gaussian
              processes can model functions in terms of high-level properties
              (e.g. smoothness, trends, periodicity, changepoints). Taken
              together with the compositional structure of our language of
              models this allows us to automatically describe functions in
              simple terms. Second, the use of flexible nonparametric models
              and a rich language for composing them in an open-ended manner
              also results in state-of-the-art extrapolation performance
              evaluated over 13 real time series data sets from various
              domains.",
  journal  = "Proc. Conf. AAAI Artif. Intell.",
  volume   =  28,
  number   =  1,
  month    =  jun,
  year     =  2014,
  keywords = "Regression;comp-cog-sci",
  issn     = "2159-5399, 2374-3468"
}

@INCOLLECTION{Lupyan2021-sj,
  title     = "Does {{Vocabulary Help Structure}} the {{Mind}}?",
  booktitle = "Minnesota {{Symposia}} on {{Child Psychology}}",
  author    = "Lupyan, Gary and Zettersten, Martin",
  abstract  = "The idea that language shapes thinking seemed plausible when
               scientists were in the dark about how thinking works. This
               chapter describes several mechanisms by which the words of a
               language can help structure knowledge and navigate cognitive
               problems. It is because thought and language seem so closely
               linked that language is so often used as a window to thought.
               The cognitive priority view faces two serious problems. The
               first is accounting for the cross-linguistic diversity of
               vocabularies. The second problem is the problem of origin. The
               chapter discusses new data aimed at both collecting verbal
               complexity measures independently from the original Bongard
               problems themselves, and collecting more objective measures of
               solution accuracy. It provides further evidence for the idea
               that easier-to-name visual features are more likely to be used
               by people when judging visual similarity.",
  publisher = "John Wiley \& Sons, Ltd",
  pages     = "160--199",
  year      =  2021,
  keywords  = "Bongard problems,cognitive priority,cross-linguistic
               diversity,name visual features,verbal complexity,visual
               similarity,vocabularies;comp-cog-sci",
  isbn      = "9781119684527",
  doi       = "10.1002/9781119684527.ch6"
}

@ARTICLE{Mahowald1991-jj,
  title    = "The {{Silicon Retina}}",
  author   = "Mahowald, Misha A and Mead, Carver",
  journal  = "Sci. Am.",
  pages    = "9",
  year     =  1991,
  keywords = "comp-cog-sci",
  issn     = "0036-8733"
}

@UNPUBLISHED{Mao2019-ht,
  title    = "The {{{Neuro-Symbolic} Concept Learner}}: {{Interpreting
              Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}",
  author   = "Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum,
              Joshua B and Wu, Jiajun",
  abstract = "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model
              that learns visual concepts, words, and semantic parsing of
              sentences without explicit supervision on any of them; instead,
              our model learns by simply looking at images and reading paired
              questions and answers. Our model builds an object-based scene
              representation and translates sentences into executable, symbolic
              programs. To bridge the learning of two modules, we use a
              neuro-symbolic reasoning module that executes these programs on
              the latent scene representation. Analogical to human concept
              learning, the perception module learns visual concepts based on
              the language description of the object being referred to.
              Meanwhile, the learned visual concepts facilitate learning new
              words and parsing new sentences. We use curriculum learning to
              guide the searching over the large compositional space of images
              and language. Extensive experiments demonstrate the accuracy and
              efficiency of our model on learning visual concepts, word
              representations, and semantic parsing of sentences. Further, our
              method allows easy generalization to new object attributes,
              compositions, language concepts, scenes and questions, and even
              new program domains. It also empowers applications including
              visual question answering and bidirectional image-text retrieval.",
  month    =  apr,
  year     =  2019,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computation and Language,Computer Science - Computer Vision and
              Pattern Recognition,Computer Science - Machine
              Learning;skimmed;comp-cog-sci"
}

@INCOLLECTION{Minsky2019-mb,
  title     = "A {{Framework For Representing Knowledge}}",
  booktitle = "A {{Framework For Representing Knowledge}}",
  author    = "Minsky, M",
  abstract  = "A Framework For Representing Knowledge was published in Frame
               Conceptions and Text Understanding on page 1.",
  publisher = "De Gruyter",
  pages     = "1--25",
  month     =  jul,
  year      =  2019,
  keywords  = "comp-cog-sci",
  isbn      = "9783110858778",
  doi       = "10.1515/9783110858778-003"
}

@BOOK{Minsky1988-bi,
  title     = "Perceptrons: Expanded Edition",
  author    = "Minsky, Marvin L and Papert, Seymour A",
  publisher = "MIT press",
  year      =  1988,
  keywords  = "comp-cog-sci",
  isbn      = "9780262631112"
}

@ARTICLE{Mnih2015-vd,
  title     = "{Human-Level} Control through Deep Reinforcement Learning",
  author    = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
               Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and
               Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and
               Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
               Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
               Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis",
  abstract  = "An artificial agent is developed that learns to play a diverse
               range of classic Atari 2600 computer games directly from sensory
               experience, achieving a performance comparable to that of an
               expert human player; this work paves the way to building
               general-purpose learning algorithms that bridge the divide
               between perception and action.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  518,
  number    =  7540,
  pages     = "529--533",
  month     =  feb,
  year      =  2015,
  keywords  = "comp-cog-sci",
  issn      = "0028-0836, 1476-4687",
  doi       = "10.1038/nature14236"
}

@ARTICLE{Newell1956-lm,
  title    = "The Logic Theory {Machine--{{A}}} Complex Information Processing
              System",
  author   = "Newell, A and Simon, H",
  abstract = "In this paper we describe a complex information processing
              system, which we call the logic theory machine, that is capable
              of discovering proofs for theorems in symbolic logic. This
              system, in contrast to the systematic algorithms that are
              ordinarily employed in computation, relies heavily on heuristic
              methods similar to those that have been observed in . human
              problem solving activity. The specification is written in a
              formal language, of the nature of a pseudo-code, that is suitable
              for coding for digital computers. However, the present paper is
              concerned exclusively with specification of the system, and not
              with its realization in a computer. The logic theory machine is
              part of a program of research to understand complex information
              processing systems by specifying and synthesizing a substantial
              variety of such systems for empirical study.",
  journal  = "IRE Transactions on Information Theory",
  volume   =  2,
  number   =  3,
  pages    = "61--79",
  month    =  sep,
  year     =  1956,
  keywords = "Automatic programming,Formal languages,Heuristic
              algorithms,Humans,Information analysis,Information
              processing,Logic,Pattern recognition,Problem-solving;comp-cog-sci",
  issn     = "2168-2712",
  doi      = "10.1109/TIT.1956.1056797"
}

@ARTICLE{Nobandegani_undated-cc,
  title    = "Example {{Generation Under Constraints Using Cascade Correlation
              Neural Nets}}",
  author   = "Nobandegani, Ardavan S and Shultz, Thomas R",
  abstract = "Humans not only can effortlessly imagine a wide range of novel
              instances and scenarios when prompted (e.g., a new shirt), but
              more remarkably, they can adequately generate examples which
              satisfy a given set of constraints (e.g., a new, dotted, pink
              shirt). Recently, Nobandegani and Shultz (2017) proposed a
              framework which permits converting deterministic, discriminative
              neural nets into probabilistic generative models. In this work,
              we formally show that an extension of this framework allows for
              generating examples under a wide range of constraints.
              Furthermore, we show that this framework is consistent with
              developmental findings on children's generative abilities, and
              can account for a developmental shift in infants' probabilistic
              learning and reasoning. We discuss the importance of integrating
              Bayesian and connectionist approaches to computational
              developmental psychology, and how our work contributes to that
              research.",
  pages    = "6",
  keywords = "comp-cog-sci"
}

@ARTICLE{Nye_undated-np,
  title    = "Learning {{Compositional Rules}} via {{Neural Program Synthesis}}",
  author   = "Nye, Maxwell I and Solar-Lezama, Armando and Tenenbaum, Joshua B
              and Lake, Brenden M",
  abstract = "Many aspects of human reasoning, including language, require
              learning rules from very little data. Humans can do this, often
              learning systematic rules from very few examples, and combining
              these rules to form compositional rule-based systems. Current
              neural architectures, on the other hand, often fail to generalize
              in a compositional manner, especially when evaluated in ways that
              vary systematically from training. In this work, we present a
              neuro-symbolic model which learns entire rule systems from a
              small set of examples. Instead of directly predicting outputs
              from inputs, we train our model to induce the explicit system of
              rules governing a set of previously seen examples, drawing upon
              techniques from the neural program synthesis literature. Our
              rule-synthesis approach outperforms neural meta-learning
              techniques in three domains: an artificial instruction-learning
              domain used to evaluate human learning, the SCAN challenge
              datasets, and learning rule-based translations of number words
              into integers for a wide range of human languages.",
  pages    = "11",
  keywords = "read;comp-cog-sci"
}

@UNPUBLISHED{Peterson2016-lj,
  title    = "Adapting {{Deep Network Features}} to {{Capture Psychological
              Representations}}",
  author   = "Peterson, Joshua C and Abbott, Joshua T and Griffiths, Thomas L",
  abstract = "Deep neural networks have become increasingly successful at
              solving classic perception problems such as object recognition,
              semantic segmentation, and scene understanding, often reaching or
              surpassing human-level accuracy. This success is due in part to
              the ability of DNNs to learn useful representations of
              high-dimensional inputs, a problem that humans must also solve.
              We examine the relationship between the representations learned
              by these networks and human psychological representations
              recovered from similarity judgments. We find that deep features
              learned in service of object classification account for a
              significant amount of the variance in human similarity judgments
              for a set of animal images. However, these features do not
              capture some qualitative distinctions that are a key part of
              human representations. To remedy this, we develop a method for
              adapting deep features to align with human similarity judgments,
              resulting in image representations that can potentially be used
              to extend the scope of psychological experiments.",
  month    =  aug,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Computer Vision and Pattern Recognition,Computer Science - Neural
              and Evolutionary Computing;comp-cog-sci"
}

@ARTICLE{Piantadosi2016-na,
  title     = "The Logical Primitives of Thought: {{Empirical}} Foundations for
               Compositional Cognitive Models",
  author    = "Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D",
  journal   = "Psychol. Rev.",
  publisher = "US: American Psychological Association",
  volume    =  123,
  number    =  4,
  pages     = "392",
  year      =  2016,
  keywords  = "comp-cog-sci",
  issn      = "0033-295X, 1939-1471",
  doi       = "10.1037/a0039980"
}

@ARTICLE{Piantadosi2021-ie,
  title    = "The {{Computational Origin}} of {{Representation}}",
  author   = "Piantadosi, Steven T",
  abstract = "Each of our theories of mental representation provides some
              insight into how the mind works. However, these insights often
              seem incompatible, as the debates between symbolic, dynamical,
              emergentist, sub-symbolic, and grounded approaches to cognition
              attest. Mental representations---whatever they are---must share
              many features with each of our theories of representation, and
              yet there are few hypotheses about how a synthesis could be
              possible. Here, I develop a theory of the underpinnings of
              symbolic cognition that shows how sub-symbolic dynamics may give
              rise to higher-level cognitive representations of structures,
              systems of knowledge, and algorithmic processes. This theory
              implements a version of conceptual role semantics by positing an
              internal universal representation language in which learners may
              create mental models to capture dynamics they observe in the
              world. The theory formalizes one account of how truly novel
              conceptual content may arise, allowing us to explain how even
              elementary logical and computational operations may be learned
              from a more primitive basis. I provide an implementation that
              learns to represent a variety of structures, including logic,
              number, kinship trees, regular languages, context-free languages,
              domains of theories like magnetism, dominance hierarchies, list
              structures, quantification, and computational primitives like
              repetition, reversal, and recursion. This account is based on
              simple discrete dynamical processes that could be implemented in
              a variety of different physical or biological systems. In
              particular, I describe how the required dynamics can be directly
              implemented in a connectionist framework. The resulting theory
              provides an ``assembly language'' for cognition, where high-level
              theories of symbolic computation can be implemented in simple
              dynamics that themselves could be encoded in biologically
              plausible systems.",
  journal  = "Minds Mach.",
  volume   =  31,
  number   =  1,
  pages    = "1--58",
  month    =  mar,
  year     =  2021,
  keywords = "comp-cog-sci",
  issn     = "0924-6495, 1572-8641",
  doi      = "10.1007/s11023-020-09540-9"
}

@ARTICLE{Piantadosi2016-fe,
  title    = "Four {{Problems Solved}} by the {{Probabilistic Language}} of
              {{Thought}}",
  author   = "Piantadosi, Steven T and Jacobs, Robert A",
  abstract = "We argue for the advantages of the probabilistic language of
              thought (pLOT), a recently emerging approach to modeling human
              cognition. Work using this framework demonstrates how the pLOT
              (a) refines the debate between symbols and statistics in
              cognitive modeling, (b) permits theories that draw on insights
              from both nativist and empiricist approaches, (c) explains the
              origins of novel and complex computational concepts, and (d)
              provides a framework for abstraction that can link sensation and
              conception. In each of these areas, the pLOT provides a
              productive middle ground between historical divides in cognitive
              psychology, pointing to a promising way forward for the field.",
  journal  = "Curr. Dir. Psychol. Sci.",
  volume   =  25,
  number   =  1,
  pages    = "54--59",
  month    =  feb,
  year     =  2016,
  keywords = "comp-cog-sci",
  issn     = "0963-7214, 1467-8721",
  doi      = "10.1177/0963721415609581"
}

@ARTICLE{Piloto2022-yi,
  title    = "Intuitive Physics Learning in a {Deep-Learning} Model Inspired by
              Developmental Psychology",
  author   = "Piloto, Luis S and Weinstein, Ari and Battaglia, Peter and
              Botvinick, Matthew",
  abstract = "Abstract `Intuitive physics' enables our pragmatic engagement
              with the physical world and forms a key component of `common
              sense' aspects of thought. Current artificial intelligence
              systems pale in their understanding of intuitive physics, in
              comparison to even very young children. Here we address this gap
              between humans and machines by drawing on the field of
              developmental psychology. First, we introduce and open-source a
              machine-learning dataset designed to evaluate conceptual
              understanding of intuitive physics, adopting the
              violation-of-expectation (VoE) paradigm from developmental
              psychology. Second, we build a deep-learning system that learns
              intuitive physics directly from visual data, inspired by studies
              of visual cognition in children. We demonstrate that our model
              can learn a diverse set of physical concepts, which depends
              critically on object-level representations, consistent with
              findings from developmental psychology. We consider the
              implications of these results both for AI and for research on
              human cognition.",
  journal  = "Nature Human Behaviour",
  month    =  jul,
  year     =  2022,
  keywords = "development",
  issn     = "2397-3374",
  doi      = "10.1038/s41562-022-01394-8"
}

@ARTICLE{Pitt_undated-sp,
  title    = "Exact {{Number Concepts Are Limited}} to the {{Verbal Count
              Range}}",
  author   = "Pitt, Benjamin and Gibson, Edward and Piantadosi, Steven T",
  abstract = "Previous findings suggest that mentally representing exact
              numbers larger than four depends on a verbal count routine (e.g.,
              ``one, two, three . . .''). However, these findings are
              controversial because they rely on comparisons across radically
              different languages and cultures. We tested the role of language
              in number concepts within a single population---the Tsimane' of
              Bolivia---in which knowledge of number words varies across
              individual adults. We used a novel data-analysis model to
              quantify the point at which participants (N = 30) switched from
              exact to approximate number representations during a simple
              numerical matching task. The results show that these behavioral
              switch points were bounded by participants' verbal count ranges;
              their representations of exact cardinalities were limited to the
              number words they knew. Beyond that range, they resorted to
              numerical approximation. These results resolve competing accounts
              of previous findings and provide unambiguous evidence that large
              exact number concepts are enabled by language.",
  pages    = "11",
  keywords = "comp-cog-sci"
}

@ARTICLE{Premack1997-aj,
  title    = "Infants {{Attribute {Value}}$\pm$} to the {{{Goal-Directed}
              Actions}} of {{Self-propelled Objects}}",
  author   = "Premack, David and Premack, Ann James",
  abstract = "Motion is a fundamental source of information for basic human
              interpretations; it is basic to the fundamental concept of
              causality and, the present model argues, equally basic to the
              fundamental concept of intentionality.The model is based on two
              main assumptions: When an infant perceives an object (1) moving
              spontaneously and (2) displaying goaldirected action, it will
              interpret the object as intentional and assign to it the unique
              properties of the psychological domain. The key property tested
              was: Do infants attribute value to interactions between
              intentional objects using criteria specified by the model?We
              showed infants (average age 52 weeks) computer-generated
              animations of spontaneously moving ``balls,'' using looking time
              in a standard habituation/dishabituation paradigm. In two
              positive interactions, one ball either ``caressed'' another, or
              ``helped'' it achieve its goal; whereas in two negative
              interactions, one ball either ``hit`` another, or ``prevented''
              it from achieving its goal. In keeping with predictions of the
              model, when transferred to a negative condition, infants who had
              been habituated on a positive condition showed greater
              dishabituation than those habituated on a negative condition. The
              results could not be easily explained by the similarity relations
              among the animations depicting the interactions.The results
              suggest that well before the age when the child can ascribe
              mental states or has a ``theory of mind,'' it recognizes the
              goals of self-propelled objects and attributes value to the
              interactions between them.",
  journal  = "J. Cogn. Neurosci.",
  volume   =  9,
  number   =  6,
  pages    = "848--856",
  month    =  nov,
  year     =  1997,
  keywords = "development",
  issn     = "0898-929X",
  doi      = "10.1162/jocn.1997.9.6.848"
}

@UNPUBLISHED{Radford2021-jz,
  title    = "Learning {{Transferable Visual Models From Natural Language
              Supervision}}",
  author   = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh,
              Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish
              and Askell, Amanda and Mishkin, Pamela and Clark, Jack and
              Krueger, Gretchen and Sutskever, Ilya",
  abstract = "State-of-the-art computer vision systems are trained to predict a
              fixed set of predetermined object categories. This restricted
              form of supervision limits their generality and usability since
              additional labeled data is needed to specify any other visual
              concept. Learning directly from raw text about images is a
              promising alternative which leverages a much broader source of
              supervision. We demonstrate that the simple pre-training task of
              predicting which caption goes with which image is an efficient
              and scalable way to learn SOTA image representations from scratch
              on a dataset of 400 million (image, text) pairs collected from
              the internet. After pre-training, natural language is used to
              reference learned visual concepts (or describe new ones) enabling
              zero-shot transfer of the model to downstream tasks. We study the
              performance of this approach by benchmarking on over 30 different
              existing computer vision datasets, spanning tasks such as OCR,
              action recognition in videos, geo-localization, and many types of
              fine-grained object classification. The model transfers
              non-trivially to most tasks and is often competitive with a fully
              supervised baseline without the need for any dataset specific
              training. For instance, we match the accuracy of the original
              ResNet-50 on ImageNet zero-shot without needing to use any of the
              1.28 million training examples it was trained on. We release our
              code and pre-trained model weights at
              https://github.com/OpenAI/CLIP.",
  month    =  feb,
  year     =  2021,
  keywords = "Computer Science - Computer Vision and Pattern
              Recognition,Computer Science - Machine Learning;machine-learning"
}

@UNPUBLISHED{Rezende2016-ge,
  title    = "{One-{{Shot} Generalization}} in {{Deep Generative Models}}",
  author   = "Rezende, Danilo Jimenez and Mohamed, Shakir and Danihelka, Ivo
              and Gregor, Karol and Wierstra, Daan",
  abstract = "Humans have an impressive ability to reason about new concepts
              and experiences from just a single example. In particular, humans
              have an ability for one-shot generalization: an ability to
              encounter a new concept, understand its structure, and then be
              able to generate compelling alternative variations of the
              concept. We develop machine learning systems with this important
              capacity by developing new deep generative models, models that
              combine the representational power of deep learning with the
              inferential power of Bayesian reasoning. We develop a class of
              sequential generative models that are built on the principles of
              feedback and attention. These two characteristics lead to
              generative models that are among the state-of-the art in density
              estimation and image generation. We demonstrate the one-shot
              generalization ability of our models using three tasks:
              unconditional sampling, generating new exemplars of a given
              concept, and generating new exemplars of a family of concepts. In
              all cases our models are able to generate compelling and diverse
              samples---having seen new examples just once---providing an
              important class of general-purpose models for one-shot machine
              learning.",
  month    =  may,
  year     =  2016,
  keywords = "Computer Science - Artificial Intelligence,Computer Science -
              Machine Learning,Statistics - Machine Learning;machine-learning"
}

@ARTICLE{Rich2018-zj,
  title     = "The Limits of Learning: {{Exploration}}, Generalization, and the
               Development of Learning Traps",
  author    = "Rich, Alexander S and Gureckis, Todd M",
  abstract  = "Learning usually improves the accuracy of beliefs through the
               accumulation of experience. But are there limits to learning
               that prevent us from accurately understanding our world? In this
               article we investigate the concept of a ``learning trap''---the
               formation of a stable false belief even with extensive
               experience. Our review highlights how these traps develop
               through the interaction of learning and decision making in
               unknown environments. We further document a particularly
               pernicious learning trap driven by selective attention, a
               mechanism often assumed to facilitate learning in complex
               environments. Using computer simulation, we demonstrate the key
               attributes of the agent and environment that lead to this new
               type of learning trap. Then, in a series of experiments we
               present evidence that people robustly fall into this trap, even
               in the presence of various interventions predicted to meliorate
               it. These results highlight a fundamental limit to learning and
               adaptive behavior that impacts individuals, organizations,
               animals, and machines. (PsycInfo Database Record (c) 2020 APA,
               all rights reserved)",
  journal   = "J. Exp. Psychol. Gen.",
  publisher = "American Psychological Association",
  volume    =  147,
  number    =  11,
  pages     = "1553--1570",
  year      =  2018,
  address   = "US",
  keywords  = "Decision Making,Development,Environment,False
               Beliefs,Generalization (Learning),Learning,Learning
               Environment,Selective Attention;comp-cog-sci",
  issn      = "0096-3445, 1939-2222",
  doi       = "10.1037/xge0000466"
}

@BOOK{Rojas2013-pv,
  title     = "Neural Networks: A Systematic Introduction",
  author    = "Rojas, Ra{\'u}l",
  publisher = "Springer Science \& Business Media",
  year      =  2013,
  keywords  = "machine-learning",
  isbn      = "9783642610684"
}

@ARTICLE{Rosenblatt1958-kl,
  title    = "The Perceptron: {{A}} Probabilistic Model for Information Storage
              and Organization in the Brain",
  author   = "Rosenblatt, F",
  journal  = "Psychol. Rev.",
  volume   =  65,
  number   =  6,
  pages    = "386--408",
  year     =  1958,
  keywords = "read;comp-cog-sci",
  issn     = "0033-295X, 1939-1471",
  doi      = "10.1037/h0042519"
}

@ARTICLE{Ruegsegger2017-yh,
  title     = "Running from {{Disease}}: {{Molecular Mechanisms Associating
               Dopamine}} and {{Leptin Signaling}} in the {{Brain}} with
               {{Physical Inactivity}}, {{Obesity}}, and {{Type}} 2
               {{Diabetes}}",
  author    = "Ruegsegger, Gregory N and Booth, Frank W",
  abstract  = "Physical inactivity is a primary contributor to diseases such as
               obesity, cardiovascular disease, and Type 2 diabetes.
               Accelerometry data suggest that a majority of U.S. adults fail
               to perform substantial levels of physical activity needed to
               improve health. Thus, understanding the molecular factors that
               stimulate physical activity, and physical inactivity, is
               imperative for the development of strategies to reduce sedentary
               behavior and in turn prevent chronic disease. Despite many of
               the well-known health benefits of physical activity being
               described, little is known about genetic and biological factors
               that may influence this complex behavior. The mesolimbic
               dopamine system regulates motivating and rewarding behavior as
               well as motor movement. Here, we present data supporting the
               hypothesis that obesity may mechanistically lower voluntary
               physical activity levels via dopamine dysregulation. In doing
               so, we review data that suggests mesolimbic dopamine activity is
               a strong contributor to voluntary physical activity behavior. We
               also summarize findings suggesting that obesity leads to central
               dopaminergic dysfunction, which in turn contributes to
               reductions in physical activity that often accompany obesity.
               Additionally, we highlight examples in which central leptin
               activity influences physical activity levels in a
               dopamine-dependent manner. Future elucidation of these
               mechanisms will help support strategies to increase physical
               activity levels in obese patients and prevent diseases caused by
               physical inactivity.",
  journal   = "Front. Endocrinol.",
  publisher = "Frontiers",
  volume    =  0,
  year      =  2017,
  keywords  = "Dopamine,Leptin,Motivation,Nucleus Accumbens,physical
               activity,physical inactivity;exercise-brain",
  issn      = "1664-2392",
  doi       = "10.3389/fendo.2017.00109"
}

@TECHREPORT{Rule2018-zm,
  title       = "Learning List Concepts through Program Induction",
  author      = "Rule, Joshua and Schulz, Eric and Piantadosi, Steven T and
                 Tenenbaum, Joshua B",
  abstract    = "Humans master complex systems of interrelated concepts like
                 mathematics and natural language. Previous work suggests
                 learning these systems relies on iteratively and directly
                 revising a language-like conceptual representation. We
                 introduce and assess a novel concept learning paradigm called
                 Martha's Magical Machines that captures complex relationships
                 between concepts. We model human concept learning in this
                 paradigm as a search in the space of term rewriting systems,
                 previously developed as an abstract model of computation. Our
                 model accurately predicts that participants learn some
                 transformations more easily than others and that they learn
                 harder concepts more easily using a bootstrapping curriculum
                 focused on their compositional parts. Our results suggest that
                 term rewriting systems may be a useful model of human
                 conceptual representations.",
  institution = "Animal Behavior and Cognition",
  month       =  may,
  year        =  2018,
  keywords    = "comp-cog-sci",
  doi         = "10.1101/321505"
}

@ARTICLE{Rule2020-db,
  title    = "The {{Child}} as {{Hacker}}",
  author   = "Rule, Joshua S and Tenenbaum, Joshua B and Piantadosi, Steven T",
  journal  = "Trends Cogn. Sci.",
  volume   =  24,
  number   =  11,
  pages    = "900--915",
  month    =  nov,
  year     =  2020,
  keywords = "skimmed;development",
  issn     = "1364-6613",
  doi      = "10.1016/j.tics.2020.07.005"
}

@UNPUBLISHED{Scherrer2021-wg,
  title    = "Learning {{Neural Causal Models}} with {{Active Interventions}}",
  author   = "Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and
              Goyal, Anirudh and Schwab, Patrick and Sch{\"o}lkopf, Bernhard
              and Mozer, Michael C and Bengio, Yoshua and Bauer, Stefan and Ke,
              Nan Rosemary",
  abstract = "Discovering causal structures from data is a challenging
              inference problem of fundamental importance in all areas of
              science. The appealing scaling properties of neural networks have
              recently led to a surge of interest in differentiable neural
              network-based methods for learning causal structures from data.
              So far differentiable causal discovery has focused on static
              datasets of observational or interventional origin. In this work,
              we introduce an active intervention-targeting mechanism which
              enables a quick identification of the underlying causal structure
              of the data-generating process. Our method significantly reduces
              the required number of interactions compared with random
              intervention targeting and is applicable for both discrete and
              continuous optimization formulations of learning the underlying
              directed acyclic graph (DAG) from data. We examine the proposed
              method across a wide range of settings and demonstrate superior
              performance on multiple benchmarks from simulated to real-world
              data.",
  month    =  sep,
  year     =  2021,
  keywords = "Computer Science - Machine Learning,Statistics - Machine
              Learning;comp-cog-sci"
}

@UNPUBLISHED{Shultz2021-hq,
  title    = "A {{Computational Model}} of {{Infant Learning}} and
              {{Reasoning}} with {{Probabilities}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments reveal that 6- to 12-month-old infants can
              learn probabilities and reason with them. In this work, we
              present a novel computational system called Neural Probability
              Learner and Sampler (NPLS) that learns and reasons with
              probabilities, providing a computationally sufficient mechanism
              to explain infant probabilistic learning and inference. In 24
              computer simulations, NPLS simulations show how probability
              distributions can emerge naturally from neural-network learning
              of event sequences, providing a novel explanation of infant
              probabilistic learning and reasoning. Three mathematical proofs
              show how and why NPLS simulates the infant results so accurately.
              The results are situated in relation to seven other active
              research lines. This work provides an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  month    =  jun,
  year     =  2021,
  keywords = "Quantitative Biology - Neurons and
              Cognition;comp-cog-sci;development"
}

@ARTICLE{Shultz_undated-cv,
  title    = "Probability {{Without Counting}} and {{Dividing}}: {{A Fresh
              Computational Perspective}}",
  author   = "Shultz, Thomas R and Nobandegani, Ardavan S",
  abstract = "Recent experiments show that preverbal infants can reason
              probabilistically. This raises a deep puzzle because infants lack
              the counting and dividing abilities presumably required to
              compute probabilities. In the standard way of computing
              probabilities, they would have to count or accurately estimate
              large frequencies and divide those values by their total. Here,
              we present a novel neural-network model that learns and uses
              probability distributions without explicit counting or dividing.
              Probability distributions emerge naturally from neural-network
              learning of event sequences, providing a computationally
              sufficient explanation of how infants could succeed at
              probabilistic reasoning. Several alternative explanations are
              discussed and ruled out. Our work bears on several other active
              literatures, and it suggests an effective way to integrate
              Bayesian and neural-network approaches to cognition.",
  pages    = "7",
  keywords = "comp-cog-sci;development"
}

@TECHREPORT{Smolensky2022-ts,
  title       = "Neurocompositional Computing in Human and Machine
                 Intelligence: {{A}} Tutorial",
  author      = "Smolensky, Paul and McCoy, R Thomas and Fernandez, Roland and
                 Goldrick, Matthew and Gao, Jianfeng",
  abstract    = "The past decade has produced a revolution in Artificial
                 Intelligence (AI), after a half-century of AI repeatedly
                 failing to meet expectations. What explains the dramatic
                 change from 20th-century to 21st-century AI, and how can
                 remaining limitations of current AI be overcome? Until now,
                 the widely accepted narrative has attributed the recent
                 progress in AI to technical engineering advances that have
                 yielded massive increases in the quantity of computational
                 resources and training data available to support statistical
                 learning in deep artificial neural networks. Although these
                 quantitative engineering innovations are important, here we
                 show that the latest advances in AI are not solely due to
                 quantitative increases in computing power but also qualitative
                 changes in how that computing power is deployed. These
                 qualitative changes have brought about a new type of computing
                 that we call neurocompositional computing. In
                 neurocompositional computing, neural networks exploit two
                 scientific principles that contemporary theory in cognitive
                 science maintains are simultaneously necessary to enable
                 human-level cognition. The Compositionality Principle asserts
                 that encodings of complex information are structures that are
                 systematically composed from simpler structured encodings. The
                 Continuity Principle states that the encoding and processing
                 of information is formalized with real numbers that vary
                 continuously. These principles have seemed irreconcilable
                 until the recent mathematical discovery that compositionality
                 can be realized not only through the traditional discrete
                 methods of symbolic computing, well developed in 20th-century
                 AI, but also through novel forms of continuous neural
                 computing---neurocompositional computing. The unprecedented
                 progress of 21st-century AI has resulted from the use of
                 limited---first-generation---forms of neurocompositional
                 computing. We show that the new techniques now being deployed
                 in second-generation neurocompositional computing create AI
                 systems that are not only more robust and accurate than
                 current systems, but also more comprehensible---making it
                 possible to diagnose errors in, and exert human control over,
                 artificial neural networks through interpretation of their
                 internal states and direct intervention upon those states.
                 Note: This tutorial is intended for those new to this topic,
                 and does not assume familiarity with cognitive science, AI, or
                 deep learning. Appendices provide more advanced material. Each
                 figure, and the associated box explaining it, provides an
                 exposition, illustration, or further details of a main point
                 of the paper; in order to make these figures relatively
                 self-contained, it has sometimes been necessary to repeat some
                 material from the text. For a brief introduction and
                 additional development of some of this material see
                 ``Neurocompositional computing: From the central paradox of
                 cognition to a new generation of ai systems''
                 (arXiv:2205.01128; to appear, AI Magazine)",
  institution = "Microsoft",
  month       =  may,
  year        =  2022,
  keywords    = "read;comp-cog-sci"
}

@ARTICLE{Spelke2007-uu,
  title    = "Core Knowledge",
  author   = "Spelke, Elizabeth S and Kinzler, Katherine D",
  abstract = "Human cognition is founded, in part, on four systems for
              representing objects, actions, number, and space. It may be
              based, as well, on a fifth system for representing social
              partners. Each system has deep roots in human phylogeny and
              ontogeny, and it guides and shapes the mental lives of adults.
              Converging research on human infants, non-human primates,
              children and adults in diverse cultures can aid both
              understanding of these systems and attempts to overcome their
              limits.",
  journal  = "Dev. Sci.",
  volume   =  10,
  number   =  1,
  pages    = "89--96",
  year     =  2007,
  keywords = "development",
  issn     = "1363-755X, 1467-7687",
  doi      = "10.1111/j.1467-7687.2007.00569.x"
}

@INPROCEEDINGS{Stuhlmuller2010-ka,
  title     = "Learning Structured Generative Concepts",
  author    = "Stuhlmuller, Andreas and Tenenbaum, Joshua B and Goodman, Noah D",
  publisher = "Cognitive Science Society",
  year      =  2010,
  keywords  = "comp-cog-sci",
  isbn      = "9781617388903"
}

@ARTICLE{Tenenbaum2011-ud,
  title    = "How to {{Grow}} a {{Mind}}: {{Statistics}}, {{Structure}}, and
              {{Abstraction}}",
  author   = "Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L and
              Goodman, Noah D",
  abstract = "In coming to understand the world---in learning concepts,
              acquiring language, and grasping causal relations---our minds
              make inferences that appear to go far beyond the data available.
              How do we do it? This review describes recent approaches to
              reverse-engineering human learning and cognitive development and,
              in parallel, engineering more humanlike machine learning systems.
              Computational models that perform probabilistic inference over
              hierarchies of flexibly structured representations can address
              some of the deepest questions about the nature and origins of
              human thought: How does abstract knowledge guide learning and
              reasoning from sparse data? What forms does our knowledge take,
              across different domains and tasks? And how is that abstract
              knowledge itself acquired?",
  journal  = "Science",
  volume   =  331,
  number   =  6022,
  pages    = "1279--1285",
  month    =  mar,
  year     =  2011,
  keywords = "read;comp-cog-sci;project 2",
  issn     = "0036-8075, 1095-9203",
  doi      = "10.1126/science.1192788"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Turing2009-sc,
  title     = "Computing {{Machinery}} and {{Intelligence}}",
  booktitle = "Parsing the {{Turing Test}}: {{Philosophical}} and
               {{Methodological Issues}} in the {{Quest}} for the {{Thinking
               Computer}}",
  author    = "Turing, Alan M",
  editor    = "Epstein, Robert and Roberts, Gary and Beber, Grace",
  abstract  = "I propose to consider the question, ``Can machines think?''â™£
               This should begin with definitions of the meaning of the terms
               ``machine'' and ``think''. The definitions might be framed so as
               to reflect so far as possible the normal use of the words, but
               this attitude is dangerous. If the meaning of the words
               ``machine'' and ``think'' are to be found by examining how they
               are commonly used it is difficult to escape the conclusion that
               the meaning and the answer to the question, ``Can machines
               think?'' is to be sought in a statistical survey such as a
               Gallup poll.",
  publisher = "Springer Netherlands",
  pages     = "23--65",
  year      =  2009,
  address   = "Dordrecht",
  keywords  = "Computing Machinery,Digital Computer,Performance Capacity,Real
               Robot,Turing Machine;comp-cog-sci",
  isbn      = "9781402067105",
  doi       = "10.1007/978-1-4020-6710-5\_3"
}

@ARTICLE{Vaswani_undated-fb,
  title    = "Attention Is {{All}} You {{Need}}",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz
              and Polosukhin, Illia",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks that include an
              encoder and a decoder. The best performing models also connect
              the encoder and decoder through an attention mechanism. We
              propose a new simple network architecture, the Transformer, based
              solely on attention mechanisms, dispensing with recurrence and
              convolutions entirely. Experiments on two machine translation
              tasks show these models to be superior in quality while being
              more parallelizable and requiring significantly less time to
              train. Our model achieves 28.4 BLEU on the WMT 2014
              Englishto-German translation task, improving over the existing
              best results, including ensembles, by over 2 BLEU. On the WMT
              2014 English-to-French translation task, our model establishes a
              new single-model state-of-the-art BLEU score of 41.0 after
              training for 3.5 days on eight GPUs, a small fraction of the
              training costs of the best models from the literature.",
  pages    = "11",
  keywords = "machine-learning"
}

@ARTICLE{Watkins1992-pk,
  title     = "{Q-Learning}",
  author    = "Watkins, Christopher Jch and Dayan, Peter",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  8,
  number    = "3-4",
  pages     = "279--292",
  year      =  1992,
  keywords  = "machine-learning",
  issn      = "0885-6125"
}

@ARTICLE{Wellman_undated-aj,
  title    = "Cognitive {{Development}}: {{Foundational Theories}} of {{Core
              Domains}}",
  author   = "Wellman, Henry M and Gelman, Susan A",
  pages    = "39",
  keywords = "development"
}

@INBOOK{2021-fc,
  title     = "Perceptron",
  booktitle = "Wikipedia",
  author    = "{Wikipedia}",
  abstract  = "In machine learning, the perceptron is an algorithm for
               supervised learning of binary classifiers. A binary classifier
               is a function which can decide whether or not an input,
               represented by a vector of numbers, belongs to some specific
               class. It is a type of linear classifier, i.e. a classification
               algorithm that makes its predictions based on a linear predictor
               function combining a set of weights with the feature vector.",
  month     =  aug,
  year      =  2021,
  keywords  = "comp-cog-sci"
}

@MISC{noauthor_undated-by,
  title        = "Readings in Cognitive Science : A Perspective from Psychology
                  and Artificial Intelligence | {{{McGill} University Library}}",
  howpublished = "\url{https://mcgill.on.worldcat.org/v2/oclc/555237322}",
  note         = "Accessed: 2021-9-17",
  keywords     = "comp-cog-sci"
}

@MISC{noauthor_undated-xy,
  title        = "Efficient Inverse Graphics in Biological Face Processing |
                  {{Science Advances}}",
  howpublished = "\url{https://advances.sciencemag.org/content/6/10/eaax5979/tab-pdf?__cf_chl_jschl_tk__=7edc4ca4dbf68389fcff61bcb4f9b2933065972f-1626234016-0-AUXGIGlyGqNMVHJhjRyQlw9DOb7_Nsxa5__OpC2K69M9D8q3ft1l8LC648BjbyZssjf1qXDRh9heKB23bAUqhMchMuBuTHxr1gBzgLl0YZz-fvX_BbIfEdkuGqgS2wIQwcCLXUhX7Vuu9lepXXTiDRdqgoBR_3HgolzgHQ6iOR88SOt_HkomiuuG19xpy9eHwbtkXyxVeuqVct2S5FKj2c_gF7JoxpOsDzMQ2ClPLRcttkLxGd5Y2NSg0hSEDqyY9X-MJK6yri2xtNvCQoUyFH2cOstnm8cMcp0PKaq91OAXSh_A3uCIJLJF8Dvy5m58BYllfEUROmN2Gs2l0Mvt-pdW5ClIo4ZFzQ_JlY8SX5yrdk3Nv7rREduZqg_ZKj_rSXsnQccxZkIUOnWnfk3HWWQJqVOfzh9HFpMYsr2jkFs-ljn7ZmKqdkoo5QRPi1XbSg}",
  note         = "Accessed: 2021-7-14",
  keywords     = "comp-cog-sci"
}

@ARTICLE{Baars2007-hu,
  title    = "An Architectural Model of Conscious and Unconscious Brain
              Functions: {{Global Workspace Theory}} and {{IDA}}",
  author   = "Baars, Bernard J and Franklin, Stan",
  abstract = "While neural net models have been developed to a high degree of
              sophistication, they have some drawbacks at a more integrative,
              ``architectural'' level of analysis. We describe a ``hybrid''
              cognitive architecture that is implementable in neuronal nets,
              and which has uniform brainlike features, including
              activation-passing and highly distributed ``codelets,''
              implementable as small-scale neural nets. Empirically, this
              cognitive architecture accounts qualitatively for the data
              described by Baars' Global Workspace Theory (GWT), and Franklin's
              LIDA architecture, including state-of-the-art models of conscious
              contents in action-planning, Baddeley-style Working Memory, and
              working models of episodic and semantic longterm memory. These
              terms are defined both conceptually and empirically for the
              current theoretical domain. The resulting architecture meets four
              desirable goals for a unified theory of cognition: practical
              workability, autonomous agency, a plausible role for conscious
              cognition, and translatability into plausible neural terms. It
              also generates testable predictions, both empirical and
              computational.",
  journal  = "Neural Netw.",
  volume   =  20,
  number   =  9,
  pages    = "955--961",
  series   = "Brain and Consciousness",
  month    =  nov,
  year     =  2007,
  keywords = "Cognitive architecture,Conscious cognition,Global workspace
              theory,LIDA architecture;comp-cog-sci",
  issn     = "0893-6080",
  doi      = "10.1016/j.neunet.2007.09.013"
}
